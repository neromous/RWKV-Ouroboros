{
    "proj_dir": "/home/neromous/Documents/RWKV-Ourboros/resources/output",
    "port": 3000,
    "debug": false,
    "use_model": "rwkv-v6-state",
    "db": {
      "host": "http://127.0.0.1:9001",
      "api_for_transact" : "",
      "api_for_query" : "",
      "api_for_remove" : ""
    },
    "optimizer":{
        "gradient_accumulation_steps" : 1
    },
    "lora": {
        "lora_on" : false,
         "r" : 64,
        "alpha" : 128,
        "parts" : ["att", "ffn", "ln"],
        "train_state": true,
        "path": "/home/neromous/Documents/RWKV-Ourboros/resources/output/lora/default.pth"
      },
    "jit_on": "0",
    "add_init_prompt_after_train": true,
    "model": {
      "load_model": "/home/neromous/Documents/RWKV-Ourboros/resources/output/default.pth",
      "rwkv_version": "v6",
      "n_embd": -1,
      "n_layer": -1,
      "vocab_size": -1,
      "ctx_len": 1024,
      "dtype": "bf16",
      "head_size": 64,
      "head_size_a": 64,
      "head_size_divisor": 8,
      "chunk_len" : 24
    },
    "trainer": {
      "train_type": "lora",
      "infctx_on": true,
      "warmup_steps": 8,
      "ctx_len": 1024,
      "grad_cp": 0,
      "dropout": 0,
      "window": 0,
      "min_loss": 0,
      "max_loss": 10,
      "min_loss_fix": 0.05,
      "max_loss_fix": 1,
      "ctx_parts": "0",
      "tokenizer": "/home/neromous/Documents/RWKV-Ourboros/resources/vocab/rwkv_vocab_v20230424.txt",
      "head_size": 64,
      "weight_decay": 0.01,
      "dim_att": 0,
      "dim_ffn": 0,
      "my_qa_mask": 0,
      "my_pos_emb": 0,
      "tiny_att_dim": 0,
      "pre_ffn": 0,
      "head_qk": 0,
      "qa_mask":true,
      "layerwise_lr": 1,
      "lr_init": 0.1,
      "beta1": 0.9,
      "beta2": 0.999,
      "adam_eps": 1.0e-8,
      "optimzer_style":"adam",
      "adamw_mode": true,
      "my_pile_stage": 1
    },
    "inference": {
      "tokenizer": "/home/neromous/Documents/RWKV-Ourboros/resources/vocab/rwkv_vocab_v20230424.txt",
      "max_tokens": 300,
      "use_pos_cfg": false,
      "use_neg_cfg": false,
      "cfg_pos_alpha": 0.4,
      "cfg_neg_alpha": 0.2,
      "prompt_config": {
        "temperature": 1,
        "top_p": 0.85,
        "top_k": 0,
        "alpha_frequency": 0.2,
        "alpha_presence": 0.2,
        "alpha_decay": 1,
        "token_ban": [0],
        "token_stop": [65535],
        "chunk_len": 256,
        "token_count": 0,
        "over": true
      }
    }
  }
  