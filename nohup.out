[2023-09-30 14:43:29,895] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 14:43:29,915] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 14:43:32,381] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 14:43:32,381] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 14:43:32,381] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 14:43:32,381] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 14:43:32,381] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.616151332855225 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 14:44:20,319] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 14:44:20,320] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 14:44:23,626] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 14:44:23,628] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 14:44:23,628] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 14:44:23,667] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 14:44:23,667] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 14:44:23,667] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 14:44:23,667] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 14:44:23,667] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 14:44:23,668] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 14:44:23,668] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.23317265510559082 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-09-30 14:44:50,011] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 14:44:50,012] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 14:44:50,012] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.04 GB, percent = 17.5%
[2023-09-30 14:45:09,469] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 14:45:09,470] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 14:45:09,470] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.25 GB, percent = 48.7%
[2023-09-30 14:45:09,471] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 14:45:11,081] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 14:45:11,082] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 14:45:11,082] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.27 GB, percent = 48.7%
[2023-09-30 14:45:11,106] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 14:45:11,106] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 14:45:11,106] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fb77d5f10d0>
[2023-09-30 14:45:11,106] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 14:45:11,107] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 14:45:11,108] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 14:45:11,108] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 14:45:11,108] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 14:45:11,108] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 14:45:11,108] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 14:45:11,108] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 14:45:11,108] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 14:45:11,108] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb77d5c80d0>
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 14:45:11,111] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00045418739318847656 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

[2023-09-30 16:38:02,153] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 16:38:02,173] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 16:38:04,699] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 16:38:04,699] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 16:38:04,699] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 16:38:04,699] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 16:38:04,699] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
[2023-09-30 16:38:22,008] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1207084
[2023-09-30 16:38:22,009] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.4222633838653564 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 16:38:27,349] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 16:38:27,350] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 16:38:27,999] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 16:38:28,001] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 16:38:28,001] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 16:38:28,025] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 16:38:28,025] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 16:38:28,025] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 16:38:28,025] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 16:38:28,025] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 16:38:28,025] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 16:38:28,025] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.22358965873718262 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(461598720, False), (24576, False), (24576, False)] 
[2023-09-30 16:38:32,351] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 16:38:32,351] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:38:32,352] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 8.85 GB, percent = 7.0%
[2023-09-30 16:38:35,089] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 16:38:35,090] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:38:35,090] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 14.33 GB, percent = 11.4%
[2023-09-30 16:38:35,090] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 16:38:35,750] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 16:38:35,751] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:38:35,751] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 14.33 GB, percent = 11.4%
[2023-09-30 16:38:35,766] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 16:38:35,766] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 16:38:35,766] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f9c5e773cd0>
[2023-09-30 16:38:35,766] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 16:38:35,767] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9c7ce4d040>
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 16:38:35,771] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0004134178161621094 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

Traceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/app.py", line 155, in train
    for token in data_iter:
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/models/scene.py", line 76, in yield_tokens
    output = tokens[ctx:]
NameError: name 'ctx' is not defined
127.0.0.1 - - [30/Sep/2023 16:39:41] "POST /train/tx-data HTTP/1.1" 500 752
[2023-09-30 16:40:45,891] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1239336
[2023-09-30 16:40:45,892] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 16:40:54,128] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 16:40:57,657] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 16:41:00,151] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 16:41:00,151] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 16:41:00,151] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 16:41:00,151] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 16:41:00,152] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.167691230773926 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 16:41:21,608] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 16:41:21,609] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 16:41:22,237] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 16:41:22,238] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 16:41:22,238] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 16:41:22,262] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 16:41:22,262] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 16:41:22,262] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 16:41:22,262] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 16:41:22,262] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 16:41:22,262] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 16:41:22,262] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.24089789390563965 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(461598720, False), (24576, False), (24576, False)] 
[2023-09-30 16:41:26,679] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 16:41:26,680] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:41:26,680] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 8.85 GB, percent = 7.0%
[2023-09-30 16:41:29,388] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 16:41:29,389] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:41:29,389] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 14.34 GB, percent = 11.4%
[2023-09-30 16:41:29,389] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 16:41:30,060] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 16:41:30,061] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:41:30,061] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 14.34 GB, percent = 11.4%
[2023-09-30 16:41:30,083] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 16:41:30,084] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 16:41:30,084] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f0bf8a70220>
[2023-09-30 16:41:30,084] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 16:41:30,085] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 16:41:30,085] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 16:41:30,085] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 16:41:30,085] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 16:41:30,085] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 16:41:30,085] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f0bf0d06b80>
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 16:41:30,088] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00039887428283691406 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

[2023-09-30 16:41:51,328] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2023-09-30 16:41:51,328] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-30 16:41:51,328] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-30 16:41:51,328] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2023-09-30 16:41:51,328] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
Traceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/app.py", line 159, in train
    m = model_engine.training_step(batch, model_engine=model_engine)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 790, in training_step
    logits = self(idx)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 765, in forward
    x = deepspeed.checkpointing.checkpoint(block, x)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/runtime/activation_checkpointing/checkpointing.py", line 713, in checkpoint
    CheckpointFunction.apply(function, all_outputs, *args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/runtime/activation_checkpointing/checkpointing.py", line 555, in forward
    outputs = run_function(*inputs_cuda)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 518, in forward
    x = x + self.att(self.ln1(x))
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 422, in forward
    sr, k, v = self.jit_func(x)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 410, in jit_func
    @MyFunction
    def jit_func(self, x):
        xx = self.time_shift(x) # Mix x with the previous timestep to produce xk, xv, xr
             ~~~~~~~~~~~~~~~ <--- HERE
        xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)
        xv = x * self.time_mix_v + xx * (1 - self.time_mix_v)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/padding.py", line 25, in forward
    def forward(self, input: Tensor) -> Tensor:
        return F.pad(input, self.padding, 'constant', self.value)
               ~~~~~ <--- HERE
RuntimeError: narrow(): length must be non-negative.

127.0.0.1 - - [30/Sep/2023 16:41:51] "POST /train/tx-data HTTP/1.1" 500 752
[2023-09-30 16:42:56,292] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1240743
[2023-09-30 16:42:56,293] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 16:43:00,215] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 16:43:00,235] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 16:43:02,761] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 16:43:02,761] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 16:43:02,761] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 16:43:02,761] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 16:43:02,761] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.2136683464050293 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 16:43:24,447] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 16:43:24,447] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 16:43:25,064] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 16:43:25,065] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 16:43:25,065] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 16:43:25,089] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 16:43:25,089] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 16:43:25,089] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 16:43:25,089] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 16:43:25,089] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 16:43:25,089] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 16:43:25,089] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.2330338954925537 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(461598720, False), (24576, False), (24576, False)] 
[2023-09-30 16:43:29,446] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 16:43:29,447] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:43:29,447] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 8.85 GB, percent = 7.0%
[2023-09-30 16:43:36,109] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 16:43:36,110] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:43:36,110] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 14.32 GB, percent = 11.4%
[2023-09-30 16:43:36,110] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 16:43:36,763] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 16:43:36,763] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:43:36,764] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 14.33 GB, percent = 11.4%
[2023-09-30 16:43:36,779] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 16:43:36,779] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 16:43:36,779] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f8381a33c70>
[2023-09-30 16:43:36,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 16:43:36,780] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f83940771f0>
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 16:43:36,783] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0003781318664550781 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

{'input_ids': [], 'attention_mask': None}
[2023-09-30 16:43:53,902] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2023-09-30 16:43:53,902] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-30 16:43:53,902] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-30 16:43:53,902] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2023-09-30 16:43:53,902] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
Traceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/app.py", line 160, in train
    m = model_engine.training_step(batch, model_engine=model_engine)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 790, in training_step
    logits = self(idx)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 765, in forward
    x = deepspeed.checkpointing.checkpoint(block, x)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/runtime/activation_checkpointing/checkpointing.py", line 713, in checkpoint
    CheckpointFunction.apply(function, all_outputs, *args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/runtime/activation_checkpointing/checkpointing.py", line 555, in forward
    outputs = run_function(*inputs_cuda)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 518, in forward
    x = x + self.att(self.ln1(x))
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 422, in forward
    sr, k, v = self.jit_func(x)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 410, in jit_func
    @MyFunction
    def jit_func(self, x):
        xx = self.time_shift(x) # Mix x with the previous timestep to produce xk, xv, xr
             ~~~~~~~~~~~~~~~ <--- HERE
        xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)
        xv = x * self.time_mix_v + xx * (1 - self.time_mix_v)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/padding.py", line 25, in forward
    def forward(self, input: Tensor) -> Tensor:
        return F.pad(input, self.padding, 'constant', self.value)
               ~~~~~ <--- HERE
RuntimeError: narrow(): length must be non-negative.

127.0.0.1 - - [30/Sep/2023 16:43:54] "POST /train/tx-data HTTP/1.1" 500 752
[2023-09-30 16:44:33,875] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1241445
[2023-09-30 16:44:33,876] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 16:44:35,625] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 16:44:35,645] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 16:44:38,155] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 16:44:38,155] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 16:44:38,155] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 16:44:38,155] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 16:44:38,155] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.2163853645324707 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 16:44:59,755] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 16:44:59,755] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 16:45:00,370] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 16:45:00,372] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 16:45:00,372] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 16:45:00,395] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 16:45:00,395] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 16:45:00,395] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 16:45:00,395] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 16:45:00,395] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 16:45:00,395] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 16:45:00,395] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.2518012523651123 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(461598720, False), (24576, False), (24576, False)] 
[2023-09-30 16:45:04,773] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 16:45:04,774] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:45:04,774] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 8.85 GB, percent = 7.0%
[2023-09-30 16:45:07,478] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 16:45:07,479] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:45:07,479] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 14.33 GB, percent = 11.4%
[2023-09-30 16:45:07,479] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 16:45:08,136] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 16:45:08,136] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:45:08,137] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 14.33 GB, percent = 11.4%
[2023-09-30 16:45:08,152] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 16:45:08,152] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 16:45:08,152] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f9b22e35b50>
[2023-09-30 16:45:08,152] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 16:45:08,153] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 16:45:08,153] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 16:45:08,153] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 16:45:08,153] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 16:45:08,153] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9b2d706d30>
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 16:45:08,156] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0003788471221923828 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

{'input_ids': [65535, 10444, 65530, 65532, 11164, 11164, 6134, 2058, 59, 33, 11640, 17772, 13091, 15446, 15898, 14734, 19137, 11957, 10260, 11957, 19156, 12605, 12605, 12605, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 13091, 14734, 19137, 11640, 17772, 13091, 15446, 15898, 14734, 19137, 12605, 16503, 12848, 17363, 10464, 11640, 17772, 13191, 13064, 10560, 17147, 11086, 14446, 10250, 10349, 18431, 15898, 10080, 65535, 261, 11111, 11111, 65535], 'attention_mask': None}
[2023-09-30 16:45:46,789] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2023-09-30 16:45:46,789] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-30 16:45:46,789] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-30 16:45:46,790] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2023-09-30 16:45:46,790] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
127.0.0.1 - - [30/Sep/2023 16:45:54] "POST /train/tx-data HTTP/1.1" 200 27
[2023-09-30 17:14:02,124] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1242055
[2023-09-30 17:14:02,126] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 18:36:34,507] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 18:36:34,527] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 18:36:37,056] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 18:36:37,056] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 18:36:37,056] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 18:36:37,056] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 18:36:37,056] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.662330150604248 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 18:37:32,089] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 18:37:32,089] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 18:37:35,898] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 18:37:35,901] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 18:37:35,901] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 18:37:35,940] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 18:37:35,940] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 18:37:35,940] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 18:37:35,940] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 18:37:35,940] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 18:37:35,940] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 18:37:35,940] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.2947425842285156 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-09-30 18:38:03,297] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 18:38:03,298] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 18:38:03,299] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.43 GB, percent = 17.8%
[2023-09-30 18:38:24,130] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 18:38:24,131] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 18:38:24,131] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.6 GB, percent = 49.0%
[2023-09-30 18:38:24,131] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 18:38:26,232] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 18:38:26,233] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 18:38:26,234] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.61 GB, percent = 49.0%
[2023-09-30 18:38:26,268] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 18:38:26,268] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 18:38:26,268] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f55adb2c640>
[2023-09-30 18:38:26,268] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 18:38:26,269] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 18:38:26,270] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 18:38:26,270] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 18:38:26,270] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 18:38:26,270] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 18:38:26,270] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 18:38:26,270] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 18:38:26,270] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 18:38:26,270] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 18:38:26,270] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f55adb1e970>
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 18:38:26,273] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0005602836608886719 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

[2023-09-30 19:20:04,963] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1276601
[2023-09-30 19:20:04,964] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 19:20:43,073] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 19:20:43,093] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 19:20:45,640] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 19:20:45,640] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 19:20:45,640] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 19:20:45,640] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 19:20:45,640] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.627546072006226 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 19:21:36,486] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 19:21:36,486] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 19:21:40,429] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 19:21:40,431] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 19:21:40,431] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 19:21:40,470] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 19:21:40,471] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 19:21:40,471] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 19:21:40,471] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 19:21:40,471] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 19:21:40,471] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 19:21:40,471] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.29407310485839844 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-09-30 19:22:15,587] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 19:22:15,588] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:22:15,588] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.37 GB, percent = 17.8%
[2023-09-30 19:22:36,301] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 19:22:36,302] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:22:36,303] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.59 GB, percent = 49.0%
[2023-09-30 19:22:36,303] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 19:22:38,390] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 19:22:38,391] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:22:38,391] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.59 GB, percent = 49.0%
[2023-09-30 19:22:38,420] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 19:22:38,421] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 19:22:38,421] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fbd700ea460>
[2023-09-30 19:22:38,421] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 19:22:38,422] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 19:22:38,422] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 19:22:38,422] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fbd6ff99460>
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 19:22:38,425] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0005447864532470703 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

192.168.0.103 - - [30/Sep/2023 19:23:50] "POST /inference/by-inf HTTP/1.1" 404 754
====== None



<|me|><|request|>
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>
<|over|>


Traceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/app.py", line 162, in inference_by_org
    msg = inferencer.generate(rwkv_rnn,msg)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/models/inference_helper.py", line 125, in generate
    logits , self.state = model(token, self.state)
TypeError: 'NoneType' object is not callable
192.168.0.103 - - [30/Sep/2023 19:24:33] "POST /inference/by-org HTTP/1.1" 500 759
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 12134.26it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 325782.05it/s]
127.0.0.1 - - [30/Sep/2023 19:25:55] "POST /inference/load-model HTTP/1.1" 200 26
====== None



<|me|><|request|>
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>
<|over|>


```python
def is_palindrome(string):
    """Check if a string is a palindrome."""
    return string == string[::-1]
```

## 


```console
True
```

## 

*   [Python Cookbook](https://docs.python.org/3/library/index.html)
*   [Learn Python the Hard Way](https://www. ZhaoxuQiu.com/Books/LearnPythonTheHardWay/)
*   [Automate the Boring Stuff with Python](https://automatetheboringstudio.withsm.com/)
*   [Automate the Boring Stuff with Python, Second Edition](https://automatetheboringstudio.withsm.com/)
*   [Automate the Boring Stuff with Python, Third Edition](https://automatetheboringstudio.withsm.com/)
*   [Automate the Boring Stuff with Python, Fourth Edition](https://automatetheboringstudio.withsm.com/)
*   [Automate the Boring Stuff with Python, FifthTraceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1765, in wrapper
    json_response = dumps(rv)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/json/__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type Message is not JSON serializable
192.168.0.103 - - [30/Sep/2023 19:26:23] "POST /inference/by-org HTTP/1.1" 500 759
[2023-09-30 19:29:14,213] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1289811
[2023-09-30 19:29:14,214] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 19:30:06,300] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 19:30:06,320] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 19:30:08,816] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 19:30:08,816] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 19:30:08,816] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 19:30:08,816] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 19:30:08,816] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.694763660430908 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 19:30:59,515] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 19:30:59,515] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 19:31:06,311] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 19:31:06,313] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 19:31:06,313] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 19:31:06,352] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 19:31:06,352] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 19:31:06,352] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 19:31:06,352] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 19:31:06,352] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 19:31:06,352] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 19:31:06,352] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.29123592376708984 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-09-30 19:31:30,257] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 19:31:30,258] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:31:30,258] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.4 GB, percent = 17.8%
[2023-09-30 19:31:50,752] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 19:31:50,753] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:31:50,753] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.59 GB, percent = 49.0%
[2023-09-30 19:31:50,753] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 19:31:52,863] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 19:31:52,864] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:31:52,864] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.59 GB, percent = 49.0%
[2023-09-30 19:31:52,890] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 19:31:52,891] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 19:31:52,891] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f648cba3700>
[2023-09-30 19:31:52,891] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 19:31:52,892] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 19:31:52,892] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 19:31:52,892] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f648ca081c0>
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 19:31:52,896] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 19:31:52,896] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00044608116149902344 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

====== None
Traceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/app.py", line 161, in inference_by_org
    while token[-1] in [0,65535,261,11]:
NameError: name 'token' is not defined
192.168.0.103 - - [30/Sep/2023 19:32:08] "POST /inference/by-org HTTP/1.1" 500 759
[2023-09-30 19:32:54,013] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1292675
[2023-09-30 19:32:54,014] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 19:33:36,996] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 19:33:37,016] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 19:33:39,543] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 19:33:39,543] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 19:33:39,543] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 19:33:39,543] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 19:33:39,543] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.655505657196045 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 19:34:32,972] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 19:34:32,973] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 19:34:36,763] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 19:34:36,765] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 19:34:36,766] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 19:34:36,805] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 19:34:36,805] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 19:34:36,805] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 19:34:36,805] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 19:34:36,805] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 19:34:36,805] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 19:34:36,806] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.3257639408111572 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-09-30 19:35:03,929] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 19:35:03,930] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:35:03,930] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.38 GB, percent = 17.8%
[2023-09-30 19:35:24,625] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 19:35:24,626] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:35:24,626] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.61 GB, percent = 49.0%
[2023-09-30 19:35:24,627] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 19:35:25,506] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 19:35:25,507] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:35:25,507] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.61 GB, percent = 49.0%
[2023-09-30 19:35:25,541] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 19:35:25,541] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 19:35:25,541] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f03a8cfe6a0>
[2023-09-30 19:35:25,541] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 19:35:25,542] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 19:35:25,543] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 19:35:25,543] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 19:35:25,543] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 19:35:25,543] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 19:35:25,543] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 19:35:25,543] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 19:35:25,543] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 19:35:25,543] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 19:35:25,543] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 19:35:25,543] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f03a8b83460>
[2023-09-30 19:35:25,543] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 19:35:25,546] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 19:35:25,546] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 19:35:25,546] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 19:35:25,546] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 19:35:25,546] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 19:35:25,546] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 19:35:25,546] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 19:35:25,546] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 19:35:25,546] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 19:35:25,546] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 19:35:25,546] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0004334449768066406 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

====== None
Traceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/app.py", line 165, in inference_by_org
    msg = inferencer.generate(rwkv_rnn,msg)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/models/inference_helper.py", line 125, in generate
    logits , self.state = model(token, self.state)
TypeError: 'NoneType' object is not callable
192.168.0.103 - - [30/Sep/2023 19:36:30] "POST /inference/by-org HTTP/1.1" 500 759
  0%|          | 0/582 [00:00<?, ?it/s]  1%|         | 8/582 [00:02<02:24,  3.97it/s]100%|| 582/582 [00:02<00:00, 288.18it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 567166.57it/s]
127.0.0.1 - - [30/Sep/2023 19:37:00] "POST /inference/load-model HTTP/1.1" 200 26
====== None
```python
def is_palindrome(string):
    """Check if a string is a palindrome."""
    return string == string[::-1]
```

## 


```console
True
```

## 

*   [PyCharm](https://www.jetbrains.com/pycharm/): Integrated Development Environment for Python, with syntax highlighting and debugging tools.
*   [PyCharm IDE](https://www.jetbrains.com/pycharm/): Integrated development environment for Python, with syntax highlighting and debugging tools, built on top of [JetBrains](https://www.jetbrains.com/)' IntelliJ IDEA productivity suite.
*   [PyCharm IDE tutorial](https://www.jetbrains.com/pycharm/tutorial/): Learn how to use PyCharm to write plugins for IntelliJ IDEA, including plugin architecture and plugin API reference documentation.
*   [PyCharm tutorial](https://www.jetbrains.com/pycharm/tutorial/): Learn how to use PyCharm to write plugins for IntelliJ IDEA,192.168.0.103 - - [30/Sep/2023 19:38:07] "POST /inference/by-org HTTP/1.1" 200 4068
====== None
## @ @

[@TimZhangLLL](https://github.com/TimZhangLLL)

[@TimZhangLLL](https://github.com/TimZhangLLL)

[@TimZhangLLL](https://github.com/TimZhangLLL)

[@TimZhangLLL](https://github.com/TimZhangLLL)

[@TimZhangLLL](https://github.com/TimZhangLLL)

[@TimZhangLLL](https://github.com/TimZhangLLL)192.168.0.103 - - [30/Sep/2023 19:38:38] "POST /inference/by-org HTTP/1.1" 200 4511
192.168.0.103 - - [30/Sep/2023 19:44:24] "POST /inference/by-org-no-state HTTP/1.1" 404 772
[2023-09-30 19:45:07,340] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1293729
[2023-09-30 19:45:07,341] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 19:46:07,284] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 19:46:10,131] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 19:46:12,598] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 19:46:12,598] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 19:46:12,598] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 19:46:12,598] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 19:46:12,598] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.585330009460449 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 19:47:03,163] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 19:47:03,164] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 19:47:09,893] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 19:47:09,895] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 19:47:09,895] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 19:47:09,934] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 19:47:09,934] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 19:47:09,934] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 19:47:09,935] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 19:47:09,935] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 19:47:09,935] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 19:47:09,935] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.2807796001434326 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-09-30 19:47:33,836] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 19:47:33,837] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:47:33,837] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.39 GB, percent = 17.8%
[2023-09-30 19:47:54,737] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 19:47:54,738] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:47:54,738] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.61 GB, percent = 49.0%
[2023-09-30 19:47:54,738] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 19:47:56,842] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 19:47:56,843] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:47:56,843] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.61 GB, percent = 49.0%
[2023-09-30 19:47:56,871] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 19:47:56,871] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 19:47:56,871] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fca0f8de8e0>
[2023-09-30 19:47:56,871] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 19:47:56,872] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fca0f6da9d0>
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 19:47:56,876] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 19:47:56,876] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 19:47:56,876] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 19:47:56,876] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0005488395690917969 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

====== None
org-mode:
1.  org-mode , C-c C-x 
2.  Shift+C-x  org mode
3.  Ctrl+C (Windows)  Command+C (Mac OS X)  cmd window,  org mode :
``` elisp
   (printf "\\n\\nOrg mode: \\n%s\\n"\\org_mode)
 Ctrl+V (Windows)  Command+V (Mac OS X) 
192.168.0.103 - - [30/Sep/2023 19:49:21] "POST /inference/by-org-no-state HTTP/1.1" 200 4261
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 32374.70it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 553157.70it/s]
====== None
Traceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/app.py", line 171, in inference_by_org
    msg = inferencer.generate(rwkv_rnn,msg)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/models/inference_helper.py", line 128, in generate
    logits[n] = -float('inf')
IndexError: list assignment index out of range
192.168.0.103 - - [30/Sep/2023 19:49:38] "POST /inference/by-org HTTP/1.1" 500 759
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 28787.74it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 564282.23it/s]
127.0.0.1 - - [30/Sep/2023 19:51:31] "POST /inference/load-model HTTP/1.1" 200 26
====== None
Traceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/app.py", line 171, in inference_by_org
    msg = inferencer.generate(rwkv_rnn,msg)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/models/inference_helper.py", line 128, in generate
    logits[n] = -float('inf')
IndexError: list assignment index out of range
192.168.0.103 - - [30/Sep/2023 19:51:42] "POST /inference/by-org HTTP/1.1" 500 759
192.168.0.103 - - [30/Sep/2023 19:52:29] "POST /train/by-org HTTP/1.1" 404 746
[2023-09-30 19:55:49,257] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1297923
[2023-09-30 19:55:49,258] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 19:56:03,785] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 19:56:03,805] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 19:56:06,318] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 19:56:06,319] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 19:56:06,319] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 19:56:06,319] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 19:56:06,319] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.607452154159546 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 19:56:59,396] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 19:56:59,396] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 19:57:02,062] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 19:57:02,064] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 19:57:02,064] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 19:57:02,103] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 19:57:02,103] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 19:57:02,103] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 19:57:02,103] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 19:57:02,103] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 19:57:02,103] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 19:57:02,103] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.3096616268157959 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-09-30 19:57:24,706] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 19:57:24,707] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:57:24,708] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.38 GB, percent = 17.8%
[2023-09-30 19:57:44,875] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 19:57:44,876] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:57:44,876] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.6 GB, percent = 49.0%
[2023-09-30 19:57:44,876] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 19:57:45,743] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 19:57:45,744] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:57:45,744] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.6 GB, percent = 49.0%
[2023-09-30 19:57:45,765] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 19:57:45,765] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 19:57:45,765] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fa1fb839f40>
[2023-09-30 19:57:45,765] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 19:57:45,766] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 19:57:45,766] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 19:57:45,766] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa1fb839640>
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 19:57:45,769] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.000385284423828125 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

  0%|          | 0/582 [00:00<?, ?it/s]  1%|         | 8/582 [00:00<00:46, 12.41it/s]100%|| 582/582 [00:00<00:00, 899.80it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 526663.41it/s]
====== None
=== [16503, 13685, 17097, 17688, 11124, 15478, 15850, 14734, 17109, 10760, 11, 65530, 65532, 12605, 17879, 16503, 10464, 10988, 15641, 14931, 12314, 10267, 14610, 19137, 12176, 12605, 10708, 10250, 17303, 11975, 16735, 11, 65535, 261, 65530, 65534, 11685, 14734, 19137, 12605, 10423, 11986, 16707, 10708, 10250, 17303, 11975, 16735, 10080, 10260, 17141, 19137, 12605, 17879, 16503, 13179, 11632, 14734, 10529, 12445, 13234, 12176, 10847, 12605, 13250, 12410, 28329, 65535, 261, 65530, 65532, 10464, 17879, 16503, 10373, 10303, 10529, 12445, 19156, 11, 65535, 261, 65530, 65534, 18117, 10658, 19137, 12605, 17879, 16503, 14862, 17222, 17148, 17303, 11975, 16735, 14734, 15261, 11496, 10080, 13091, 15034, 12206, 11975, 16735, 10079, 12472, 14638, 11975, 16735, 17147, 13091, 10687, 10390, 15261, 11496, 14734, 11975, 16735, 19156, 11, 65535, 261, 65530, 65532, 12605, 16667, 10292, 13091, 15034, 12206, 11975, 16735, 11, 65535, 261, 65530, 65534, 11685, 14734, 19137, 12605, 10423, 10292, 10464, 12848, 10494, 13179, 11632, 15478, 15907, 10080, 11454, 17148, 17303, 11975, 16735, 10285, 19137, 10293, 10370, 10678, 13091, 10250, 10447, 11043, 11017, 11889, 12363, 18295, 2453, 10788, 10348, 18005, 14734, 12201, 17093, 10460, 11920, 10080, 10390, 11454, 10708, 10460, 17141, 15052, 10285, 11002, 10792, 10333, 13279, 15033, 14987, 15036, 10838, 17387, 14734, 12325, 11168, 19137, 12266, 11726, 10588, 10250, 10349, 11656, 12418, 14734, 13370, 10080, 17148, 10349, 13370, 11574, 16672, 10390, 15752, 11634, 14791, 16519, 13203, 13234, 11124, 17141, 10985, 19137, 12202, 10265, 16056, 12348, 10333, 13279, 15033, 14987, 11656, 14734, 10838, 17387, 28329, 65535, 261, 65530, 65532, 15752, 12848, 10494, 16721, 10293, 10370, 10678, 14734, 11630, 16815, 14332, 12333, 11124, 15727, 13126, 10529, 12445, 11053, 19156, 11, 65535, 261, 65530, 65534, 11685, 14734, 19137, 12605, 10423, 10292, 10464, 12848, 10494, 16721, 10293, 10370, 10678, 14734, 14782, 10684, 10529, 12445, 10080, 18117, 10658, 19137, 17148, 10447, 10293, 10370, 10678, 11043, 11017, 11889, 12363, 14179, 2453, 12163, 14332, 11979, 19137, 10390, 13091, 10250, 10447, 12201, 17093, 13191, 10292, 14734, 10460, 11920, 10080, 10687, 13557, 19137, 10390, 13191, 10250, 11647, 13833, 11930, 14734, 18438, 10997, 11124, 13913, 17808, 14734, 14806, 14815, 19137, 15494, 10370, 14614, 10258, 13913, 10799, 14734, 10951, 16803, 10080, 13188, 11044, 19137, 10390, 12421, 13091, 15091, 14808, 12348, 10452, 10079, 10296, 13579, 12348, 10452, 19137, 15494, 10370, 14614, 10258, 15895, 11685, 14734, 15162, 10250, 10951, 16803, 28329, 65535, 261, 65530, 65532]
==== < Message
id: (1447)
role: (text)
text: ()
prefix: ()
postfix: ()
prefix_token: ([])
temperature: (0.1)
top_p: (0.1)
top_k: (0)
alpha_frequency: (0.45)
alpha_presence: (0.45)
alpha_decay: (0.996)
token_ban: ([0])
token_stop: ([65530, 65531, 65532, 65533, 65534, 65535])
chunk_len: (128)
token_count: (256)
over: (True)
scene_id: (507)
response: ()
generated: (False)
to_train: (True)
ctx: (2048)
ctx_fix: (256)
tokens: ([16503, 13685, 17097, 17688, 11124, 15478, 15850, 14734, 17109, 10760, 11, 65530, 65532, 12605, 17879, 16503, 10464, 10988, 15641, 14931, 12314, 10267, 14610, 19137, 12176, 12605, 10708, 10250, 17303, 11975, 16735, 11, 65535, 261, 65530, 65534, 11685, 14734, 19137, 12605, 10423, 11986, 16707, 10708, 10250, 17303, 11975, 16735, 10080, 10260, 17141, 19137, 12605, 17879, 16503, 13179, 11632, 14734, 10529, 12445, 13234, 12176, 10847, 12605, 13250, 12410, 28329, 65535, 261, 65530, 65532, 10464, 17879, 16503, 10373, 10303, 10529, 12445, 19156, 11, 65535, 261, 65530, 65534, 18117, 10658, 19137, 12605, 17879, 16503, 14862, 17222, 17148, 17303, 11975, 16735, 14734, 15261, 11496, 10080, 13091, 15034, 12206, 11975, 16735, 10079, 12472, 14638, 11975, 16735, 17147, 13091, 10687, 10390, 15261, 11496, 14734, 11975, 16735, 19156, 11, 65535, 261, 65530, 65532, 12605, 16667, 10292, 13091, 15034, 12206, 11975, 16735, 11, 65535, 261, 65530, 65534, 11685, 14734, 19137, 12605, 10423, 10292, 10464, 12848, 10494, 13179, 11632, 15478, 15907, 10080, 11454, 17148, 17303, 11975, 16735, 10285, 19137, 10293, 10370, 10678, 13091, 10250, 10447, 11043, 11017, 11889, 12363, 18295, 2453, 10788, 10348, 18005, 14734, 12201, 17093, 10460, 11920, 10080, 10390, 11454, 10708, 10460, 17141, 15052, 10285, 11002, 10792, 10333, 13279, 15033, 14987, 15036, 10838, 17387, 14734, 12325, 11168, 19137, 12266, 11726, 10588, 10250, 10349, 11656, 12418, 14734, 13370, 10080, 17148, 10349, 13370, 11574, 16672, 10390, 15752, 11634, 14791, 16519, 13203, 13234, 11124, 17141, 10985, 19137, 12202, 10265, 16056, 12348, 10333, 13279, 15033, 14987, 11656, 14734, 10838, 17387, 28329, 65535, 261, 65530, 65532, 15752, 12848, 10494, 16721, 10293, 10370, 10678, 14734, 11630, 16815, 14332, 12333, 11124, 15727, 13126, 10529, 12445, 11053, 19156, 11, 65535, 261, 65530, 65534, 11685, 14734, 19137, 12605, 10423, 10292, 10464, 12848, 10494, 16721, 10293, 10370, 10678, 14734, 14782, 10684, 10529, 12445, 10080, 18117, 10658, 19137, 17148, 10447, 10293, 10370, 10678, 11043, 11017, 11889, 12363, 14179, 2453, 12163, 14332, 11979, 19137, 10390, 13091, 10250, 10447, 12201, 17093, 13191, 10292, 14734, 10460, 11920, 10080, 10687, 13557, 19137, 10390, 13191, 10250, 11647, 13833, 11930, 14734, 18438, 10997, 11124, 13913, 17808, 14734, 14806, 14815, 19137, 15494, 10370, 14614, 10258, 13913, 10799, 14734, 10951, 16803, 10080, 13188, 11044, 19137, 10390, 12421, 13091, 15091, 14808, 12348, 10452, 10079, 10296, 13579, 12348, 10452, 19137, 15494, 10370, 14614, 10258, 15895, 11685, 14734, 15162, 10250, 10951, 16803, 28329, 65535, 261, 65530, 65532]) 
>

Traceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/app.py", line 173, in inference_by_org
    msg = inferencer.generate(rwkv_rnn,msg)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/models/inference_helper.py", line 128, in generate
    logits[n] = -float('inf')
IndexError: list assignment index out of range
192.168.0.103 - - [30/Sep/2023 19:58:19] "POST /inference/by-org HTTP/1.1" 500 759
====== None
====== None
====== None
==== [<models.org_text.DataNode object at 0x7fa16e4508b0>]
==== [11687, 10454, 16056, 11001, 7451, 1802, 267, 25028, 34043, 14734, 10285, 14734, 10696, 11922, 19156, 11, 5686, 1511, 70, 33, 10464, 11021, 10399, 10482, 14589, 10399, 10258, 4515, 7917, 33, 10397, 14875, 16056, 11001, 3784, 6908, 267, 25028, 34043, 33, 14734, 10696, 11922, 59, 11, 6884, 7441, 2191, 11, 41, 49026, 46, 26081, 283, 42, 11, 6884, 11, 97, 49026, 46, 26081, 97, 33, 10762, 13004, 13191, 10278, 10283, 10988, 13004, 59, 11, 46, 283, 33, 16417, 14966, 16056, 11001, 13188, 17145, 14734, 31213, 11, 46, 33, 13580, 13004, 16417, 14966, 16056, 11001, 13179, 13057, 14734, 31213, 11, 12631, 10399, 10292, 10333, 11016, 16056, 11001, 13188, 13034, 14734, 59300, 33, 10696, 11922, 45, 12605, 10402, 10427, 10673, 283, 33, 10953, 11021, 28329, 10487, 11687, 59, 11, 6884, 7441, 2191, 11, 41, 8079, 3421, 27139, 275, 49026, 46, 26081, 283, 5292, 11, 41, 49410, 269, 57956, 51308, 59, 272, 116, 35, 32224, 494, 11, 6884, 11, 17148, 13619, 10397, 14875, 10423, 16056, 11001, 59300, 33, 10696, 11922, 45, 12202, 13098, 14966, 11454, 13855, 12445, 13314, 10285, 28329, 10464, 10322, 11021, 10399, 12669, 59300, 33, 10696, 11922, 16914, 10577, 15494, 10250, 10283, 11003, 17387, 45, 14221, 11044, 17149, 16403, 14782, 10684, 14734, 11621, 14486, 28329, 12631, 10399, 45, 10482, 14589, 331, 49026, 46, 26081, 283, 97, 33, 11994, 11021, 10399, 11454, 3784, 6908, 20299, 113, 33, 10397, 14875, 10285, 16056, 11001, 3784, 6908, 33, 14734, 59300, 33, 10696, 11922, 28329, 10482, 14589, 10373, 10303, 7441, 2191, 10762, 13004, 15752, 11634, 11903, 14446, 11454, 7451, 1802, 10285, 14734, 8401, 46, 26267, 10258, 17177, 10285, 13279, 10283, 8760, 46, 27187, 19156, 520, 19589, 1511, 70, 33, 11021, 10399, 10482, 14589, 22236, 46, 34200, 33, 11117, 10398, 13234, 17177, 10285, 53048, 28329, 12750, 17644, 15487, 11899, 59, 11, 68, 46, 100, 332, 275, 8401, 46, 34200, 42, 11, 14589, 13764, 59, 11, 50, 47, 33, 10659, 13308, 12983, 11454, 8760, 27187, 14734, 13308, 18025, 10257, 11, 51, 47, 33, 12750, 302, 46, 100, 332, 11, 52, 47, 33, 17148, 11967, 17177, 10285, 16721, 8760, 27187, 10258, 14734, 12631, 13191, 10696, 11922, 11, 10464, 10322, 11021, 10399, 12748, 11899, 17879, 16503, 12357, 14445, 14734, 13557, 13004, 13234, 17177, 10285, 10260, 11042, 15451, 10790, 14734, 10696, 11922, 59, 11, 46, 302, 46, 100, 332, 284, 280, 33, 17177, 10285, 12307, 10808, 16403, 11, 46, 302, 46, 100, 332, 285, 280, 33, 17177, 10285, 12307, 10808, 15907, 14172, 41, 13308, 18025, 10992, 11855, 15907, 14172, 42, 11, 46, 302, 46, 100, 332, 286, 280, 33, 17177, 10285, 12307, 10808, 15907, 14172, 14734, 14302, 15907, 14172, 11, 46, 33, 15169, 15169, 11, 12631, 10399, 45, 11454, 22236, 46, 26267, 33, 10258, 17177, 10285, 13279, 10283, 53048, 45, 10464, 11021, 10399, 59, 11, 50, 47, 33, 10659, 13308, 12983, 11454, 16721, 53048, 33, 14734, 13308, 18025, 10257, 11, 51, 47, 33, 16749, 14589, 4398, 8401, 46, 34200, 313, 512, 10762, 13004, 45, 79, 33, 10292, 16503, 12357, 14445, 14734, 13557, 13004, 45, 10399, 17177, 10285, 16721, 11855, 13315, 11, 10487, 11687, 59, 11, 6884, 4515, 7917, 11, 41, 8401, 46, 34200, 285, 42, 267, 60, 33, 17177, 10285, 12307, 10808, 15907, 14172, 41, 11855, 13315, 42, 11, 6884, 11, 12159, 13197, 17148, 15752, 12176, 10847, 10464, 34, 24210, 11]
[2023-09-30 19:59:48,108] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2023-09-30 19:59:48,108] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-30 19:59:48,108] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-30 19:59:48,109] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2023-09-30 19:59:48,109] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
-> item_loss 2.1173954010009766 batch_loss 2.1173954010009766
192.168.0.103 - - [30/Sep/2023 20:00:04] "POST /train/by-org HTTP/1.1" 200 28
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 30457.22it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 534622.19it/s]
127.0.0.1 - - [30/Sep/2023 20:03:43] "POST /inference/load-model HTTP/1.1" 200 26
None
None
127.0.0.1 - - [30/Sep/2023 20:03:43] "POST /state/reset HTTP/1.1" 200 21

<|over|>

<|me|><|request|>Question: 
<|over|>

<|me|><|response|>Answer: 
1. Ilica de la Reyna
2. Jazmin Harry Potter-Crabble-Kreacher
3. Fezco de la Reyna
4. Fernando de la Reyna
5. Fernando de la Reyna
6. Fernando de la Reyna
7. Fernando de la Reyna
8. 127.0.0.1 - - [30/Sep/2023 20:04:02] "POST /inference/generate HTTP/1.1" 200 3960
tensor([[ 1.1239e-01,  6.5617e-03,  1.8517e-01,  ...,  1.2466e-01,
          1.9486e-01,  1.4709e-01],
        [ 1.8481e-01,  1.7514e-02, -1.2661e-02,  ...,  8.3811e-01,
          1.9648e-01,  6.7376e-02],
        [-5.3432e+00, -4.5233e+00, -7.8453e-01,  ...,  1.3372e-01,
         -1.0569e-01, -6.6466e-01],
        ...,
        [-2.6437e-01, -3.1639e+01,  4.9620e+01,  ..., -4.7746e+00,
          2.7792e+00, -6.9258e+00],
        [ 1.0281e+00,  2.2015e+01,  1.6767e+01,  ...,  1.0229e+00,
          1.1468e+00,  1.2015e+00],
        [ 3.4083e+00,  7.3872e+00,  6.2586e+00,  ...,  1.7435e+01,
          1.3982e+00,  4.2004e+00]], device='cuda:0')
None
127.0.0.1 - - [30/Sep/2023 20:04:03] "POST /state/reset HTTP/1.1" 200 21

: 
1. 
2. 
3. 
4. 
5. : : 
: ?
: H.P.1923The Wonderful Works of God: 
: ?
: 127.0.0.1 - - [30/Sep/2023 20:05:57] "POST /inference/generate-no-state HTTP/1.1" 200 4281
[2023-09-30 20:07:32,097] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1300887
[2023-09-30 20:07:32,098] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 20:11:05,456] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 20:11:06,619] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 20:11:09,108] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 20:11:09,108] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 20:11:09,108] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 20:11:09,109] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 20:11:09,109] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.635452032089233 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 20:11:59,965] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 20:11:59,966] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 20:12:07,632] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 20:12:07,635] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 20:12:07,635] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 20:12:07,675] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 20:12:07,675] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 20:12:07,675] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 20:12:07,675] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 20:12:07,675] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 20:12:07,675] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 20:12:07,676] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.30291295051574707 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-09-30 20:12:34,014] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 20:12:34,015] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 20:12:34,015] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.39 GB, percent = 17.8%
[2023-09-30 20:12:56,506] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 20:12:56,507] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 20:12:56,507] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.61 GB, percent = 49.0%
[2023-09-30 20:12:56,507] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 20:12:57,491] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 20:12:57,492] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 20:12:57,492] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.61 GB, percent = 49.0%
[2023-09-30 20:12:57,517] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 20:12:57,517] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 20:12:57,517] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fb6c2f18040>
[2023-09-30 20:12:57,518] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 20:12:57,519] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 20:12:57,519] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 20:12:57,519] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 20:12:57,519] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 20:12:57,519] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 20:12:57,519] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 20:12:57,519] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 20:12:57,519] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb6c2d27040>
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 20:12:57,522] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 20:12:57,522] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 20:12:57,522] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 20:12:57,522] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 20:12:57,522] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 20:12:57,522] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 20:12:57,522] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 20:12:57,522] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 20:12:57,522] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 20:12:57,522] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 20:12:57,522] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00042128562927246094 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 20772.19it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 575184.95it/s]
==== [65530, 65531, 12656, 14035, 10250, 10283, 15847, 11805, 10452, 15543, 17108, 19137, 13326, 12797, 27370, 14734, 17109, 10673, 15494, 10760, 15543, 17108, 11044, 14734, 13012, 13205, 11, 65535, 261, 65530, 65532, 12605, 17879, 16503, 10464, 12927, 10708, 10684, 10339, 13631, 11795, 12678, 10691, 14782, 10684, 14734, 14862, 16696, 14734, 13012, 15131, 11, 65535, 261, 65530, 65534, 10399, 10258, 13091, 10250, 10283, 10487, 11855, 59, 267, 11454, 17141, 10985, 14734, 10745, 10917, 12201, 17384, 45, 33, 13631, 11795, 10684, 15304, 10250, 14781, 11622, 11002, 10370, 10402, 14734, 10684, 13773, 10080, 17148, 10293, 16503, 13091, 11416, 10292, 45, 33, 13631, 10366, 11454, 12395, 11858, 11124, 10358, 11044, 14734, 17268, 10745, 10283, 13190, 17384, 12665, 12701, 10333, 12140, 11638, 14734, 14583, 10358, 10838, 10409, 10843, 45, 33, 11042, 13064, 10322, 12665, 11002, 10333, 12140, 11638, 14734, 15287, 14987, 10966, 10838, 10080, 11684, 17879, 16503, 14240, 18004, 11795, 10650, 45, 33, 11042, 13064, 17147, 16503, 10692, 18004, 12137, 10460, 11124, 11871, 10323, 28329, 11454, 17148, 15033, 12484, 10724, 10258, 45, 33, 13631, 10366, 14734, 17177, 12731, 10293, 16503, 13191, 10278, 15033, 59, 33, 10250, 15033, 13091, 17177, 12731, 11454, 11920, 10285, 15847, 15494, 15847, 16956, 45, 33, 11013, 10250, 15033, 10779, 13091, 17177, 12731, 11454, 10913, 17796, 12824, 11002, 12686, 14486, 28329, 13051, 16686, 17177, 12731, 11187, 15033, 13036, 13764, 45, 33, 13631, 10366, 17311, 17919, 10288, 14808, 12140, 11638, 14734, 10966, 10838, 10080, 11684, 17879, 16503, 14240, 18004, 11795, 10650, 45, 33, 11042, 13064, 17147, 16503, 10692, 18004, 12137, 10460, 11124, 11871, 10323, 10080, 11684, 12368, 18001, 10260, 13032, 11459, 11871, 10323, 13034, 14862, 16696, 11124, 12666, 15752, 45, 33, 11042, 13064, 10322, 12159, 13197, 15752, 11634, 12348, 10792, 14970, 10423, 14734, 16667, 11021, 10080, 11454, 17148, 10283, 17141, 15052, 10285, 45, 33, 11684, 17919, 10288, 14808, 12140, 11638, 14734, 10966, 10838, 28329, 11687, 13258, 17177, 12731, 11454, 11920, 10285, 15847, 15494, 15847, 16956, 45, 65535, 261, 65530, 65534, 10399, 10258, 13091, 10250, 10283, 15486, 17141, 15543, 17108, 11044, 14734, 14314, 13205, 59, 11, 13631, 11795, 17734, 37881, 33, 13091, 12445, 12445, 14782, 10684, 14734, 10080, 17145, 10745, 10917, 12201, 45, 13631, 10366, 10402, 14734, 16530, 15898, 11622, 11002, 10684, 13773, 28329, 12395, 11858, 10992, 10780, 14583, 10358, 11044, 10745, 10283, 13190, 45, 13631, 10366, 15710, 16875, 13722, 17385, 14734, 11920, 12234, 16879, 10409, 45, 11042, 13064, 12665, 11002, 10966, 10838, 10080, 11684, 16503, 14240, 18004, 11795, 10650, 10079, 12137, 10460, 11124, 11871, 10323, 45, 17879, 16503, 11454, 17148, 10285, 17734, 11001, 12348, 12200, 16412, 28329, 13191, 10278, 15033, 13036, 12276, 11021, 10494, 17177, 12731, 59, 11, 10250, 13091, 45, 11454, 11920, 15847, 10293, 14240, 18004, 11795, 10650, 10080, 17148, 15033, 13036, 12276, 14782, 11957, 10511, 11901, 10444, 17879, 16503, 13631, 10366, 15931, 16899, 13179, 11632, 13064, 17734, 10261, 13773, 12522, 10838, 28329, 10337, 13091, 45, 11967, 11795, 10650, 10355, 15494, 10913, 17796, 14791, 12686, 10080, 13631, 10366, 11021, 10264, 13773, 12137, 10460, 11871, 10323, 45, 10444, 15931, 16899, 13179, 11632, 28329, 10260, 15202, 13631, 10366, 17177, 12731, 11187, 15033, 13036, 12276, 45, 10966, 10838, 17311, 12337, 11638, 28329, 11684, 17879, 16503, 10260, 13032, 11871, 10323, 13034, 12666, 15752, 10079, 11584, 17688, 14862, 16696, 45, 11042, 13064, 10335, 11001, 14970, 10423, 16667, 11042, 28329, 11687, 13258, 17177, 12731, 11454, 11920, 14240, 18004, 45, 13631, 10366, 17879, 16503, 59, 11, 9828, 33, 10333, 16533, 11795, 10650, 16099, 10691, 10079, 11889, 12678, 10261, 10997, 12022, 17879, 16503, 11, 9828, 33, 11871, 10323, 14660, 14662, 18009, 17771, 10261, 11533, 13205, 12686, 14486, 14862, 16696, 11, 9828, 33, 10261, 11795, 10650, 13250, 12261, 39332, 7939, 102, 33, 10342, 10846, 11, 9828, 28377, 585, 11, 11687, 13581, 12638, 15752, 16672, 13631, 11795, 37881, 33, 11454, 11795, 10650, 12604, 17688, 17141, 15052, 10285, 12746, 15511, 12299, 10901, 28329, 12421, 14734, 13234, 16735, 45, 13191, 12986, 14734, 13631, 11795, 12678, 10691, 11001, 10722, 10339, 13631, 10366, 14734, 12986, 14426, 15202, 14486, 11124, 10260, 13032, 11871, 10323, 28329, 10399, 10257, 13091, 10250, 10283, 13179, 15190, 15475, 13923, 13127, 14734, 14314, 13205, 10080, 16738, 10538, 12981, 17631, 16730, 10305, 11621, 45, 11088, 14862, 10684, 17644, 10529, 12445, 17225, 14033, 10080, 65535, 261, 65530, 65534]
:
 
 
 
,,192.168.0.103 - - [30/Sep/2023 20:14:48] "POST /inference/by-org HTTP/1.1" 200 5688
,,
192.168.0.103 - - [30/Sep/2023 20:33:26] "POST /inference/generate HTTP/1.1" 200 1230
,
USER 
ROBOT 
 USER 
 ROBOT 
USER 
192.168.0.103 - - [30/Sep/2023 20:34:19] "POST /inference/generate HTTP/1.1" 200 2114
,
192.168.0.103 - - [30/Sep/2023 20:37:40] "POST /inference/generate HTTP/1.1" 200 1265
,
192.168.0.103 - - [30/Sep/2023 20:38:31] "POST /inference/generate HTTP/1.1" 200 1265
,
192.168.0.103 - - [30/Sep/2023 20:38:59] "POST /inference/generate HTTP/1.1" 200 1265
,
192.168.0.103 - - [30/Sep/2023 20:39:15] "POST /inference/generate HTTP/1.1" 200 1265
192.168.0.103 - - [30/Sep/2023 20:41:41] "POST /inference/generate HTTP/1.1" 200 10176
192.168.0.103 - - [30/Sep/2023 20:44:24] "POST /inference/generate HTTP/1.1" 200 10175
,user
192.168.0.103 - - [30/Sep/2023 20:46:24] "POST /inference/generate HTTP/1.1" 200 5661
,user
192.168.0.103 - - [30/Sep/2023 20:46:59] "POST /inference/generate HTTP/1.1" 200 11223
:
:
,,,
, 192.168.0.103 - - [30/Sep/2023 20:48:20] "POST /inference/generate HTTP/1.1" 200 12208
AI
192.168.0.103 - - [30/Sep/2023 20:52:44] "POST /inference/generate HTTP/1.1" 200 1280
:
,,,, 
192.168.0.103 - - [30/Sep/2023 20:52:57] "POST /inference/generate HTTP/1.1" 200 2596
192.168.0.103 - - [30/Sep/2023 20:53:14] "POST /inference/generate HTTP/1.1" 200 3939
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 34985.60it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 501146.57it/s]
127.0.0.1 - - [30/Sep/2023 20:54:51] "POST /inference/load-model HTTP/1.1" 200 26
AI192.168.0.103 - - [30/Sep/2023 20:55:10] "POST /inference/generate HTTP/1.1" 200 1242
:
,,,
192.168.0.103 - - [30/Sep/2023 20:55:20] "POST /inference/generate HTTP/1.1" 200 2516
192.168.0.103 - - [30/Sep/2023 20:55:35] "POST /inference/generate HTTP/1.1" 200 3749
:
,,,
192.168.0.103 - - [30/Sep/2023 20:55:50] "POST /inference/generate HTTP/1.1" 200 4899
[2023-09-30 21:23:07,882] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 21:23:08,888] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1305551
[2023-09-30 21:23:08,890] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 21:23:12,077] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 21:23:14,549] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 21:23:14,549] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 21:23:14,549] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 21:23:14,549] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 21:23:14,549] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.703971862792969 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 21:24:05,396] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 21:24:05,396] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 21:24:12,948] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 21:24:12,950] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 21:24:12,950] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 21:24:12,989] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 21:24:12,989] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 21:24:12,989] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 21:24:12,989] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 21:24:12,989] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 21:24:12,989] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 21:24:12,989] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.2928633689880371 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-09-30 21:24:37,127] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 21:24:37,128] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 21:24:37,129] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.38 GB, percent = 17.8%
[2023-09-30 21:24:57,717] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 21:24:57,718] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 21:24:57,718] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.61 GB, percent = 49.0%
[2023-09-30 21:24:57,718] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 21:24:59,841] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 21:24:59,842] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 21:24:59,842] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.61 GB, percent = 49.0%
[2023-09-30 21:24:59,869] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 21:24:59,869] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 21:24:59,869] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f9e70b39160>
[2023-09-30 21:24:59,869] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 21:24:59,871] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 21:24:59,871] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 21:24:59,871] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 21:24:59,871] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 21:24:59,871] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9e70b26160>
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 21:24:59,874] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0006890296936035156 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

  0%|          | 0/582 [00:00<?, ?it/s]  1%|         | 8/582 [00:01<02:23,  4.00it/s]100%|| 582/582 [00:02<00:00, 290.95it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 419574.58it/s]
bot: 
192.168.0.103 - - [30/Sep/2023 21:26:11] "POST /inference/generate HTTP/1.1" 200 1127
bot: 
192.168.0.103 - - [30/Sep/2023 21:26:34] "POST /inference/generate HTTP/1.1" 200 2369
bot: SCP-1000SCP-1000SCP-1000SCP-1000330SCP-1000SCP-1000SCP-1000192.168.0.103 - - [30/Sep/2023 21:27:15] "POST /inference/generate HTTP/1.1" 200 5304
bot: SCP-1000SCP-1000SCP-1000SCP-1000192.168.0.103 - - [30/Sep/2023 21:28:59] "POST /inference/generate HTTP/1.1" 200 1821
None
None
192.168.0.103 - - [30/Sep/2023 21:29:35] "POST /state/reset HTTP/1.1" 200 21
bot: SCP-1000
192.168.0.103 - - [30/Sep/2023 21:29:42] "POST /inference/generate HTTP/1.1" 200 3279
None
None
192.168.0.103 - - [30/Sep/2023 21:29:52] "POST /state/reset HTTP/1.1" 200 21
scp682AIscp682
192.168.0.103 - - [30/Sep/2023 21:29:55] "POST /inference/generate HTTP/1.1" 200 1518
None
None
192.168.0.103 - - [30/Sep/2023 21:37:15] "POST /state/reset HTTP/1.1" 200 21
[2023-09-30 21:37:17,537] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2023-09-30 21:37:17,537] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-30 21:37:17,537] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-30 21:37:17,537] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2023-09-30 21:37:17,537] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
192.168.0.103 - - [30/Sep/2023 21:37:36] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [30/Sep/2023 21:39:34] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [30/Sep/2023 21:39:51] "POST /train/tx-data HTTP/1.1" 200 28
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
==== [15847, 12605, 12522, 16696, 12261, 13250, 11, 65530, 65531, 17148, 13091, 65530, 14734, 15847, 14787, 19137, 17148, 17384, 14734, 65530, 11454, 17149, 16403, 10390, 15847, 12605, 16667, 14862, 13036, 17919, 14734, 13250, 12261, 28329, 65535, 261, 65530, 65534, 12605, 11043, 11017, 65530, 19137, 15669, 10270, 13091, 10250, 11043, 15036, 10325, 19137, 10270, 10457, 14299, 11685, 13091, 16530, 15898, 12656, 14035, 15643, 19137, 12605, 17147, 13191, 14488, 14543, 10079, 11889, 13849, 12705, 15169, 10790, 11043, 19137, 33, 17189, 17141, 17749, 16742, 65532, 10285, 14734, 16503, 13685, 19137, 33, 12605, 14256, 12459, 15202, 14486, 15643, 11957, 12605, 14734, 16503, 13685, 19137, 12605, 11454, 65534, 10305, 11044, 15494, 10760, 12605, 14734, 11168, 12220, 10080, 12605, 10423, 11454, 65533, 10285, 17149, 16403, 13913, 12231, 12410, 15641, 19137, 33, 10985, 16671, 16686, 28329, 65535, 261, 65530, 65531, 13191, 10370, 10423, 17728, 16944, 10464, 13091, 11060, 13091, 28316, 74, 9823, 19137, 10464, 16525, 12348, 12220, 16721, 12401, 10303, 11414, 15178, 19156, 11, 65535, 261, 65530, 65533, 12605, 16667, 10292, 14914, 11899, 65530, 13091, 11060, 13091, 10370, 12137, 13133, 15752, 19133, 820, 19134, 13091, 13644, 13051, 12522, 10304, 14734, 10080, 10460, 10292, 10250, 10283, 13133, 15752, 10452, 19137, 10684, 13773, 14734, 13091, 13133, 15752, 10452, 14734, 11159, 13327, 11124, 10370, 13327, 14734, 10590, 10675, 19137, 15646, 10260, 13091, 10399, 10339, 11885, 14734, 15033, 13046, 13234, 10784, 13032, 28329, 65535, 261, 65530, 65534, 12605, 16667, 10292, 13637, 17097, 11038, 17172, 14734, 11414, 15178, 13091, 19151, 11, 11957, 10370, 12137, 13133, 15752, 13234, 16735, 45, 17385, 16503, 14734, 13091, 10655, 10768, 10997, 12764, 10841, 15752, 11124, 10408, 10577, 45, 15646, 10260, 11454, 10339, 16443, 10775, 10768, 10292, 35, 10370, 15261, 35, 17147, 13091, 35, 13212, 11369, 35, 10080, 12605, 10402, 12220, 15674, 14216, 10339, 13133, 15752, 10452, 13205, 17038, 14734, 16403, 10292, 11124, 15752, 10838, 45, 15646, 10260, 13091, 10687, 12666, 13208, 13205, 13981, 28329, 11042, 13064, 45, 17385, 16523, 10431, 14486, 10733, 10779, 11124, 10408, 10577, 16520, 10079, 11971, 17385, 10370, 12415, 10322, 13091, 13133, 15752, 15304, 15500, 12220, 10688, 11622, 14734, 28329, 12421, 14734, 13234, 16735, 45, 10399, 10258, 10745, 14172, 13179, 17385, 16503, 59, 11, 50, 42, 33, 14015, 16956, 14589, 12626, 17879, 13685, 10079, 12848, 10494, 10408, 10577, 28329, 51, 42, 33, 17241, 11888, 13580, 12307, 12415, 11124, 17181, 13078, 12415, 10974, 10779, 28329, 52, 42, 33, 11971, 17385, 10370, 13216, 10079, 17248, 10662, 13584, 16523, 28329, 53, 42, 33, 11021, 10399, 12824, 11002, 14762, 14819, 11124, 16749, 13006, 28329, 15646, 10260, 12220, 10377, 15674, 14216, 10339, 13091, 11060, 16443, 11899, 12415, 10292, 35, 820, 35, 10080, 11016, 16503, 12605, 10402, 14734, 13133, 15752, 15304, 15500, 15752, 10452, 14446, 10399, 10257, 11159, 16884, 45, 11885, 12220, 16721, 16443, 15494, 10334, 14782, 12220, 14734, 11971, 17385, 28329, 11687, 10454, 11414, 11626, 11687, 13258, 16443, 17728, 10992, 35, 10464, 13091, 820, 11053, 799, 17148, 15261, 17728, 18025, 45, 12605, 16667, 10292, 11021, 10399, 11687, 13581, 11414, 15178, 59, 11, 35, 12605, 13091, 10250, 10283, 16689, 16664, 14589, 10339, 12848, 10494, 12176, 10847, 11124, 10529, 12445, 14734, 13133, 15752, 15304, 15500, 10080, 12605, 14734, 10293, 16503, 14778, 13308, 13091, 14015, 16956, 14589, 12626, 17879, 13685, 45, 12605, 10423, 10260, 13032, 11871, 10323, 11124, 12848, 18216, 10399, 11903, 14446, 17148, 10283, 14778, 13308, 10080, 35, 11, 11967, 17385, 14172, 12983, 11454, 10841, 15752, 11124, 14778, 13308, 10257, 45, 15646, 10260, 13091, 12666, 13208, 10768, 15261, 45, 17148, 13324, 14734, 11414, 12220, 10423, 13179, 10842, 10285, 15713, 12266, 12983, 10080, 65535, 261, 65530, 65531, 12824, 10258, 13234, 19137, 17148, 13091, 10464, 14734, 15847, 17162, 28329, 65535, 261, 65530, 65532, 10464, 13091, 16747, 19156, 10382, 15484, 10464, 15847, 12144, 28329, 65535, 261, 65530, 65534, 12605, 11017, 65530, 33, 12605, 13091, 10250, 10283, 13125, 13125, 17189, 17189, 14734, 10370, 10080, 12605, 14583, 13815, 11454, 11459, 14484, 10257, 19137, 12010, 10449, 11454, 10285, 11435, 10080, 33, 12605, 13191, 10250, 17734, 12630, 11855, 11124, 10250, 17100, 13713, 17079, 19137, 33, 17147, 13191, 10250, 10283, 11021, 14299, 14734, 11679, 10650, 10080, 33, 12605, 14734, 14583, 13815, 10655, 11903, 15646, 10265, 15597, 14015, 10080, 33, 12605, 14583, 13815, 10285, 13191, 10250, 10283, 11210, 10250, 14734, 15568, 12577, 19137, 17268, 11994, 13091, 11454, 12605, 10344, 12039, 17268, 12201, 19137, 12605, 14734, 14302, 10366, 11645, 17001, 10333, 10080, 10390, 14734, 15023, 10985, 16672, 12605, 12337, 12476, 10429, 10080, 33, 10444, 13091, 12605, 14446, 11454, 12768, 10460, 10333, 16944, 13234, 19137, 10655, 14015, 10310, 16520, 11124, 12159, 13197, 14734, 14583, 13815, 14808, 28329, 65535, 261, 65530, 65532, 12631, 10399, 10464, 10848, 10838, 14734, 11659, 13020, 10792, 14446, 11454, 19137, 11124, 10464, 14734, 15133, 12201, 10781, 10429, 13191, 10684, 11053, 19156, 11, 65535, 261, 65530, 65534, 12605, 16667, 10292, 11021, 15752, 13091, 14734, 11, 65535, 261, 65530, 65532, 11315, 45, 12605, 14486, 16533, 10460, 10292, 10250, 10283, 13191, 15133, 12201, 10781, 10429, 14734, 10370, 45, 12469, 10696, 12366, 11021, 15752, 15043, 15330, 10333, 16684, 11632, 12484, 12527, 17879, 16503, 17382, 12983, 10080, 10460, 10292, 12469, 14734, 12366, 14486, 10913, 14583, 45, 12605, 10423, 10399, 12266, 12983, 10079, 17916, 16694, 10784, 14734, 12396, 12231, 10578, 11065, 12469, 14734, 12985, 10336, 45, 12202, 15494, 10334, 12368, 16503, 14734, 12978, 12746, 11124, 12261, 16676, 10080, 18117, 10658, 45, 12469, 12537, 12522, 16753, 16753, 15133, 12201, 13064, 17237, 17213, 10333, 10373, 10303, 13324, 14734, 10781, 10429, 11053, 64, 16738, 12002, 11021, 15752, 16722, 15478, 11459, 12847, 17162, 12469, 14734, 12527, 11002, 11124, 10452, 18183, 10080, 12605, 10423, 15648, 12366, 11459, 10578, 11065, 45, 10260, 10423, 12662, 10784, 13032, 12469, 10080, 12605, 10402, 11021, 10399, 10250, 13582, 13582, 14486, 13923, 17148, 10349, 13057, 13199, 15486, 10964, 11687, 10454, 12325, 11168, 10333, 12469, 14446, 11454, 14734, 14583, 13815, 11124, 12484, 15509, 10080, 11454, 17148, 10283, 17141, 15052, 10285, 45, 12605, 12159, 13197, 15752, 12176, 10847, 12469, 12261, 15123, 15847, 10529, 45, 17919, 11957, 12202, 12824, 15462, 15847, 12144, 14734, 17141, 10985, 10080, 16738, 12983, 12366, 10578, 16698, 12469, 14734, 12985, 10336, 45, 12605, 10402, 10423, 10250, 16944, 12509, 10840, 13764, 12176, 12469, 16939, 10760, 10781, 10429, 14734, 17773, 12325, 28329, 65535, 261, 65530, 65534, 12605, 11017, 65530, 33, 12605, 13091, 10250, 10283, 13125, 13125, 17189, 17189, 14734, 10370, 10080, 12605, 14583, 13815, 11454, 11459, 14484, 10257, 19137, 12010, 10449, 11454, 10285, 11435, 10080, 33, 12605, 13191, 10250, 17734, 12630, 11855, 11124, 10250, 17100, 13713, 17079, 19137, 33, 17147, 13191, 10250, 10283, 11021, 14299, 14734, 11679, 10650, 10080, 33, 12605, 14734, 14583, 13815, 10655, 11903, 15646, 10265, 15597, 14015, 10080, 33, 12605, 14583, 13815, 10285, 13191, 10250, 10283, 11210, 10250, 14734, 15568, 12577, 19137, 17268, 11994, 13091, 11454, 12605, 10344, 12039, 17268, 12201, 19137, 12605, 14734, 14302, 10366, 11645, 17001, 10333, 10080, 10390, 14734, 15023, 10985, 16672, 12605, 12337, 12476, 10429, 10080, 33, 10444, 13091, 12605, 14446, 11454, 12768, 10460, 10333, 16944, 13234, 19137, 10655, 14015, 10310, 16520, 11124, 12159, 13197, 14734, 14583, 13815, 14808, 28329, 65535, 261, 65530, 65532, 16774, 16774, 10464, 12537, 12522, 10768, 10361, 10464, 14734, 12985, 10336, 45, 65530, 10080, 12605, 15752, 14486, 16533, 10460, 10292, 10250, 10283, 54, 12039, 11872, 11855, 45, 14302, 10366, 14734, 15023, 10985, 11957, 10464, 17194, 12604, 10333, 12140, 11638, 14734, 10781, 10429, 45, 17148, 15033, 10429, 14673, 10250, 11899, 17819, 14808, 10464, 14734, 12604, 17688, 10437, 17819, 10333, 12337, 17688, 13064, 17734, 28329, 10250, 10283, 11872, 11855, 17879, 16503, 14302, 10366, 14734, 17802, 10437, 11124, 10684, 14299, 45, 10390, 14734, 15023, 10985, 10260, 11021, 17248, 10662, 11459, 10423, 16672, 10464, 12527, 10792, 16443, 12679, 12269, 10080, 17148, 10283, 10781, 10429, 11021, 15752, 12325, 11168, 10333, 10464, 11957, 10684, 15304, 14734, 10529, 10409, 45, 10322, 16672, 10464, 11454, 12604, 17688, 17141, 15052, 10285, 15568, 10309, 14302, 17102, 14734, 12748, 11960, 10080, 17148, 10250, 10769, 17311, 11021, 15752, 11960, 15850, 10464, 14734, 12484, 12527, 17879, 13685, 12348, 10260, 10792, 14015, 16956, 28329, 14221, 15646, 45, 12605, 12337, 18216, 10685, 14791, 10792, 10464, 14446, 11454, 15752, 15043, 13249, 11459, 14791, 12335, 14583, 13815, 45, 13191, 15847, 12144, 14734, 11920, 12234, 11124, 10336, 10270, 10080, 17148, 16417, 13078, 10464, 13091, 10250, 10283, 13249, 10688, 17945, 12415, 11124, 14583, 11861, 10838, 14734, 10370, 10080, 10464, 13734, 13191, 16443, 17141, 10985, 14734, 10781, 10429, 12642, 16881, 45, 15646, 13091, 10848, 10838, 16939, 10760, 17773, 12325, 45, 13815, 10760, 15847, 12144, 12509, 16503, 14734, 14583, 13815, 28329, 11687, 13258, 10464, 10383, 14221, 11957, 14302, 10366, 14734, 15023, 10985, 12527, 10792, 11426, 12658, 12609, 14673, 15958, 45, 12605, 10402, 11021, 10399, 17149, 10250, 13582, 12822, 16671, 10464, 10696, 12366, 14734, 12527, 11002, 45, 13637, 11687, 16443, 17225, 12269, 14734, 12430, 12499, 45, 11957, 14302, 31337, 33, 14734, 13952, 13197, 15169, 15169, 10080, 10464, 11021, 10399, 10578, 16698, 10409, 10454, 10464, 12509, 16735, 14734, 45, 12605, 10423, 10578, 11065, 12202, 15494, 10334, 12978, 12746, 10080, 12605, 10402, 10423, 10250, 16944, 12664, 10792, 12176, 10847, 10464, 11891, 10675, 16939, 10760, 17148, 13619, 10781, 10429, 14734, 13036, 13764, 10080, 16738, 14782, 10529, 15847, 12144, 14734, 10838, 17387, 45, 10464, 10260, 13091, 11870, 14370, 10250, 10370, 10080, 65535, 261, 65530, 65534, 12605, 11454, 10257, 11871, 14734, 13064, 10560, 19137, 33, 11042, 11871, 10402, 12421, 13091, 11348, 15149, 12605, 10080, 33, 11454, 17213, 10792, 13320, 11423, 17901, 10737, 10336, 10407, 14734, 13064, 10560, 19137, 12605, 16678, 12348, 17268, 10283, 10370, 11017, 13221, 13109, 13078, 19137, 33, 10390, 12421, 13091, 12642, 12605, 19137, 12202, 10265, 10423, 12172, 10257, 10745, 10283, 10370, 10250, 16944, 10080, 33, 10390, 10402, 12421, 11017, 12605, 17386, 11872, 11855, 28329, 65535, 261, 65530, 65532, 12605, 14486, 16533, 11454, 13320, 11423, 17384, 17237, 17213, 17901, 10737, 13091, 17916, 12182, 14673, 15958, 14734, 15486, 10964, 10080, 16443, 13221, 13109, 13078, 11124, 10687, 10390, 10370, 17129, 18175, 10079, 13618, 12642, 11957, 10464, 17194, 12604, 10333, 13913, 10799, 14734, 10781, 10429, 28329, 10460, 10292, 10250, 10283, 11872, 11855, 45, 11002, 10792, 11042, 10437, 14734, 12711, 15498, 11124, 10429, 11917, 13091, 13249, 10687, 11426, 17843, 14734, 10080, 10390, 10402, 14734, 16403, 10292, 16672, 10464, 12527, 10792, 15847, 12144, 10260, 16443, 12824, 11002, 10079, 10260, 16443, 11971, 17385, 10080, 9822, 17386, 11872, 11855, 9823, 17148, 13324, 14734, 15044, 11116, 13179, 13091, 11454, 10464, 14734, 15847, 11971, 12366, 10257, 12266, 10764, 45, 16672, 10464, 16667, 10292, 15847, 12144, 13091, 10250, 10283, 10260, 16443, 14970, 10423, 17879, 16503, 14734, 10370, 28329, 12605, 15752, 12527, 11002, 10792, 13320, 11423, 13156, 10838, 15494, 10464, 12172, 13234, 14734, 15600, 17129, 12527, 11124, 10696, 14643, 12527, 10080, 10444, 12605, 12509, 16672, 10464, 14862, 17222, 45, 17148, 15498, 10260, 13091, 10464, 14734, 17631, 10080, 10464, 10260, 16721, 10292, 10390, 10370, 14734, 13156, 10838, 16403, 10292, 16875, 16879, 10409, 10080, 12605, 12159, 13197, 12605, 10402, 11021, 10399, 12822, 16671, 10464, 11957, 17148, 13619, 15486, 10964, 14734, 14794, 11903, 12527, 11002, 45, 10482, 10464, 10957, 10258, 10696, 12366, 14734, 17385, 12701, 28329, 10464, 13091, 10250, 10283, 12337, 13191, 10408, 10577, 14734, 10370, 10080, 17148, 10283, 10267, 14610, 17879, 16503, 10464, 45, 17879, 16503, 10464, 14734, 14370, 14332, 10305, 11621, 10080, 12605, 10423, 12176, 10847, 10464, 17385, 12261, 15847, 10529, 45, 16667, 16696, 10792, 17901, 10737, 15643, 14734, 17631, 16730, 16403, 10292, 10261, 10464, 13051, 10684, 10080, 12605, 10402, 10423, 10250, 16944, 12664, 10792, 13036, 13764, 45, 16672, 10464, 14734, 14583, 13815, 10260, 10703, 16443, 17141, 10985, 14734, 17773, 12325, 12631, 12138, 11025, 10080, 16738, 14782, 10529, 10464, 14734, 10838, 17387, 45, 12605, 10402, 10250, 11899, 15752, 16939, 10760, 17148, 13557, 14734, 10781, 10429, 10080, 65535, 261, 65530, 65534, 11454, 12824, 11002, 17148, 10349, 10370, 14734, 13565, 16875, 10305, 11044, 19137, 12605, 11994, 10258, 11899, 10722, 12366, 16503, 10848, 10838, 14734, 11871, 10323, 14862, 16696, 19137, 15641, 10283, 11685, 11871, 13320, 16056, 12348, 13179, 18216, 14734, 14970, 10423, 11459, 10447, 28329, 65535, 261, 65530, 65532, 12605, 12337, 13559, 16917, 10464, 10293, 10846, 10384, 16443, 10846, 14734, 11621, 11574, 10285, 11959, 12664, 15043, 13249, 14734, 10760, 16981, 10080, 10722, 12366, 17189, 17141, 10848, 10838, 11871, 10323, 13234, 12981, 11003, 15847, 12144, 14734, 11117, 17144, 45, 17148, 13098, 14966, 10760, 10464, 13249, 12299, 14734, 12522, 12377, 10838, 11124, 12676, 10966, 15752, 10838, 28329, 12307, 17237, 17213, 13320, 11423, 13565, 10737, 13064, 45, 10464, 13734, 13191, 17177, 12731, 12687, 11626, 12609, 15847, 13156, 15847, 12269, 45, 15646, 13091, 11967, 17148, 13619, 14673, 15958, 15486, 10964, 10460, 10292, 10846, 10838, 45, 10292, 15847, 12144, 16689, 11899, 10333, 13179, 18216, 14734, 14778, 13308, 10080, 17148, 10410, 10863, 13655, 11124, 10722, 12366, 10398, 10370, 17551, 10470, 28329, 17189, 17141, 10848, 10838, 11871, 10323, 13234, 12848, 10920, 15847, 12144, 45, 16056, 12348, 14862, 16696, 11124, 12666, 15752, 45, 14734, 14914, 11021, 10399, 12176, 10847, 10464, 16056, 11001, 13179, 11632, 13212, 10423, 11124, 11021, 15752, 12415, 10080, 12605, 14782, 10529, 10399, 10464, 14734, 13628, 10838, 45, 11891, 10675, 13191, 15752, 10838, 17136, 12604, 15847, 12144, 14734, 14778, 13308, 28329, 16678, 10449, 16503, 14240, 18004, 11685, 15847, 12144, 45, 10260, 16503, 10292, 10333, 14778, 13308, 15646, 12392, 16523, 10333, 17038, 12366, 10590, 12237, 10080, 10464, 12631, 17237, 11002, 14734, 10781, 10429, 11021, 15752, 17147, 17879, 16503, 13064, 17734, 13234, 12435, 11626, 10080, 12605, 10423, 10675, 15052, 12978, 12746, 10464, 45, 10464, 10260, 13091, 10250, 10283, 10370, 11454, 12610, 13020, 10080, 16672, 12605, 10402, 10250, 16944, 16671, 16686, 11687, 10454, 10588, 10792, 10848, 10838, 11047, 10257, 45, 11042, 13064, 10322, 10684, 12366, 15847, 12144, 14734, 17879, 16503, 10080, 10464, 14734, 13203, 13234, 10655, 14015, 12159, 13197, 45, 16738, 10525, 12746, 10529, 12366, 34, 65535, 261, 65530, 65534, 11021, 15752, 12605, 11640, 14589, 10838, 10333, 10080, 33, 10441, 10308, 11454, 12604, 12201, 14734, 17141, 15052, 12307, 10285, 12392, 16523, 10333, 12337, 11632, 10272, 16501, 28329, 65535, 261, 65530, 65532, 12469, 12848, 10792, 11454, 10848, 10838, 11903, 14446, 14778, 13308, 14734, 17141, 15052, 10285, 45, 11021, 15752, 12392, 16523, 10333, 10250, 10349, 10687, 10390, 17385, 16503, 14734, 10336, 12484, 45, 17148, 16672, 12605, 13179, 10842, 13559, 16917, 12469, 14734, 15847, 14787, 10261, 10995, 12410, 10080, 12604, 12201, 12604, 17688, 14734, 17222, 16981, 10257, 45, 12605, 10402, 17311, 10423, 10250, 10975, 12484, 12537, 45, 10399, 15849, 10339, 11003, 12348, 14313, 17919, 28329, 10848, 10838, 12981, 11003, 11117, 17144, 10079, 11001, 12348, 17149, 13582, 13205, 17038, 13091, 17916, 12182, 13580, 17919, 14734, 10336, 12484, 10080, 10444, 11454, 13581, 17141, 15052, 10285, 45, 12605, 10402, 10322, 13191, 11021, 15752, 12392, 16523, 10370, 14583, 10285, 10687, 10390, 17385, 16503, 14734, 12007, 17919, 45, 13637, 11687, 11920, 10370, 10079, 10993, 12484, 10079, 14299, 12484, 10079, 10283, 10370, 10685, 16954, 14299, 11685, 10079, 12366, 14141, 11124, 12484, 12527, 14734, 17879, 13685, 15169, 10080, 17148, 10349, 17311, 13091, 13250, 12604, 10286, 11932, 11632, 12319, 10370, 14583, 14734, 16503, 15323, 28329, 12631, 10399, 12605, 12261, 16676, 12469, 11021, 10399, 17379, 11001, 10399, 10258, 12828, 13038, 59, 15162, 10250, 45, 10452, 16751, 12202, 12527, 16774, 17141, 10985, 10848, 10838, 11659, 13020, 14734, 15847, 12144, 60, 15162, 10337, 45, 10589, 10258, 13234, 10995, 12410, 16443, 12392, 16523, 14734, 18011, 11524, 45, 16694, 10435, 15847, 12144, 17147, 17879, 16503, 10373, 10303, 60, 15162, 10256, 45, 10794, 11899, 16403, 10846, 16664, 10775, 45, 17182, 13582, 10692, 18004, 16443, 12392, 16523, 14734, 17879, 13685, 60, 15162, 11412, 45, 10261, 12978, 12746, 12469, 14734, 10366, 10993, 10355, 13819, 45, 16056, 11001, 10390, 10402, 14734, 10995, 18108, 28329, 16678, 10449, 45, 14583, 13815, 10285, 12337, 11977, 13191, 11891, 15597, 13051, 15568, 10305, 10336, 10080, 12469, 12145, 15486, 16939, 10333, 12337, 17688, 14734, 16981, 45, 14446, 11454, 11021, 10399, 16749, 13006, 13036, 11047, 45, 11903, 14446, 13179, 10675, 17919, 12200, 16412, 14734, 12604, 17688, 10080, 12605, 14782, 10529, 45, 15486, 17141, 12469, 14734, 10848, 10838, 45, 10250, 11899, 15752, 12664, 10792, 16672, 15847, 12144, 12527, 10792, 13179, 14015, 16956, 11124, 12203, 15009, 14734, 14583, 13815, 14349, 12396, 10080, 11687, 13258, 12527, 10792, 17165, 15987, 45, 17819, 13064, 13558, 17143, 10261, 12605, 10355, 13819, 10080, 65535, 261, 65530, 65534, 11044, 13234, 12605, 16667, 16696, 10333, 10250, 10283, 11435, 17781, 10355, 12791, 14583, 13209, 16037, 19137, 33, 12605, 10402, 12330, 13581, 17916, 12182, 13559, 16917, 19137, 33, 13186, 15486, 10997, 14583, 17141, 10250, 10349, 13844, 14042, 14734, 10336, 12484, 10080, 33, 10790, 16730, 10423, 12605, 13734, 13191, 11124, 11684, 10588, 11903, 17781, 14734, 12415, 12824, 16535, 10080, 33, 10444, 13091, 12605, 10402, 12330, 13581, 12570, 12573, 10080, 33, 10444, 13091, 17268, 10283, 13064, 17734, 13619, 12605, 11903, 11454, 13091, 11640, 10585, 14091, 10333, 19137, 33, 12669, 17169, 13685, 14970, 10423, 11459, 10447, 12307, 10588, 11210, 10250, 14734, 10336, 12484, 28329, 65535, 261, 65530, 65532, 12605, 14486, 16533, 11454, 17169, 13685, 14778, 13308, 14734, 17141, 15052, 10285, 45, 12469, 17213, 10792, 10333, 13209, 16037, 17148, 13324, 10250, 10283, 17385, 16503, 14734, 10370, 45, 12330, 13581, 17734, 10322, 10358, 14583, 10333, 15597, 11685, 14734, 12484, 12527, 15671, 15489, 10080, 14221, 15646, 12307, 13064, 17141, 10339, 12650, 14808, 10339, 10336, 10270, 45, 13203, 15752, 15494, 10334, 17148, 13619, 12527, 12484, 12220, 13191, 14734, 17385, 16523, 11124, 11414, 12220, 10080, 17148, 11021, 15752, 16672, 12469, 11454, 10399, 11044, 11414, 12509, 16944, 13234, 13064, 12527, 10792, 17225, 12577, 28329, 14299, 12484, 11124, 10993, 12484, 17311, 13091, 14583, 13815, 10285, 17916, 12182, 11902, 16896, 14734, 15486, 10964, 10080, 12307, 12605, 10402, 10675, 12366, 12674, 10673, 13279, 10250, 10283, 18011, 11524, 13064, 45, 12337, 11922, 13080, 12392, 16523, 10370, 10261, 10370, 10305, 17734, 12359, 11701, 15646, 15597, 11685, 14734, 12484, 12527, 10355, 13819, 28329, 12605, 12261, 16676, 12469, 10260, 16503, 11416, 13581, 15847, 16879, 10080, 17268, 10283, 17775, 13619, 14734, 12469, 13326, 12797, 12307, 13064, 14734, 14349, 12396, 10588, 10760, 10333, 17177, 12731, 10080, 14446, 11454, 12469, 12145, 15486, 12522, 16696, 10792, 45, 10370, 14583, 10285, 17879, 16503, 13179, 10675, 17919, 11459, 10692, 18004, 10260, 11042, 14734, 17879, 13685, 28329, 11687, 13258, 13209, 16037, 11957, 12469, 10383, 12522, 10304, 17916, 10746, 45, 12469, 10260, 11709, 16707, 14808, 17385, 13034, 15671, 15304, 11684, 45, 16417, 17136, 12469, 14734, 13571, 12522, 11124, 15847, 12605, 12604, 17688, 14734, 12527, 12465, 10080, 11994, 15200, 12145, 17631, 17141, 17148, 13619, 12527, 12484, 45, 11885, 10322, 12604, 10292, 12469, 10370, 14583, 11902, 16896, 14734, 10250, 17996, 10080, 16678, 10449, 12266, 12983, 12366, 15750, 45, 14460, 12493, 14806, 10808, 10370, 10080, 12469, 10423, 12979, 16056, 13179, 11632, 13203, 13234, 14734, 13212, 10423, 10080, 65535, 261, 65530, 65531, 16738, 13326, 12797, 12605, 10402, 10305, 10808, 14734, 10355, 13819, 19137, 33, 16667, 14794, 14734, 13377, 14486, 10250, 10258, 65530, 14734, 12415, 13327, 14332, 14172, 19137, 12176, 10847, 12605, 10588, 10250, 10258, 15287, 14987, 10768, 13254, 13234, 16672, 12605, 13179, 10842, 10333, 16533, 65530, 15847, 12144, 28329, 65535, 261, 65530, 65532, 11685, 14734, 45, 16672, 12605, 11986, 16707, 13326, 12797, 12605, 10402, 10305, 10808, 14734, 16671, 16686, 45, 40635, 6105, 1216, 10768, 13254, 12469, 14734, 12415, 13327, 14332, 14172, 59, 11, 12469, 13191, 14808, 12299, 11638, 14734, 12522, 12377, 10838, 11124, 11474, 12746, 10260, 12580, 14734, 15287, 14987, 10080, 10384, 11975, 17237, 17213, 14302, 10366, 15023, 10267, 14734, 10781, 10429, 45, 12469, 13734, 13191, 16443, 12642, 16881, 45, 15646, 13091, 10848, 10838, 14583, 13815, 10080, 17919, 11957, 13320, 11423, 13156, 10838, 45, 12469, 17177, 12731, 14589, 16742, 10325, 12981, 11003, 11117, 17144, 10080, 17148, 13098, 14966, 10760, 12469, 17919, 11957, 17176, 11574, 14734, 17945, 12415, 28329, 12469, 13191, 12337, 12299, 14734, 10257, 17149, 12366, 11124, 17149, 11001, 15287, 14987, 10080, 10292, 10333, 16947, 16948, 17141, 10985, 14734, 10781, 10429, 45, 12469, 10392, 10760, 12140, 11638, 10848, 10838, 13685, 11871, 11124, 12137, 10460, 45, 13098, 14966, 10760, 12299, 14179, 14734, 12604, 11994, 10846, 13212, 28329, 12469, 13637, 17097, 13773, 17385, 10336, 10270, 11124, 14486, 12509, 45, 13186, 10250, 12231, 12392, 16523, 10283, 10370, 12484, 12527, 17879, 16503, 11124, 14583, 13815, 14734, 12200, 16412, 10080, 10444, 12469, 12145, 15486, 12522, 16696, 10792, 17148, 10250, 17728, 18025, 45, 16417, 14446, 10760, 12604, 14256, 14734, 15847, 14787, 15752, 10838, 28329, 12469, 11957, 10390, 10370, 12484, 12527, 15671, 15304, 11124, 10366, 11930, 10684, 15304, 17147, 13191, 10250, 11899, 14734, 18004, 16253, 45, 11021, 15752, 10261, 13057, 13199, 14734, 10781, 10429, 15486, 10964, 13191, 10684, 10080, 17148, 10322, 13091, 13203, 13234, 11021, 10399, 15507, 15511, 12822, 15325, 14734, 10250, 13036, 17919, 28329, 12469, 12604, 14256, 10696, 12997, 15646, 11474, 17945, 45, 13191, 17919, 11957, 10370, 14583, 11426, 11574, 10261, 15847, 12144, 10696, 12366, 14734, 10863, 13655, 45, 10322, 12537, 12522, 10578, 11065, 10390, 10370, 12261, 16676, 10080, 17148, 13098, 14966, 10760, 13133, 12551, 10261, 11042, 12484, 12366, 14734, 15489, 11038, 28329, 12605, 12261, 16676, 12469, 13052, 16667, 16696, 10792, 15847, 12144, 14734, 10421, 14172, 45, 10322, 13773, 12522, 17268, 17384, 10383, 17879, 12604, 17688, 10080, 13633, 10250, 10283, 10370, 17311, 13091, 11454, 11003, 10901, 10285, 12604, 17688, 14734, 45, 17385, 16503, 14734, 13091, 10525, 12746, 11957, 14583, 13815, 14734, 14195, 12484, 45, 10261, 15713, 11899, 15847, 12144, 14734, 12522, 12537, 10080, 16738, 15507, 15511, 11474, 12746, 45, 12605, 10423, 10250, 14781, 11454, 17148, 17384, 12978, 12746, 12469, 34, 65535, 261, 65530, 65534, 15752, 11634, 12176, 12605, 10588, 10250, 13557, 15287, 14987, 10768, 13254, 11053, 19156, 11, 65535, 261, 65530, 65532, 11685, 14734, 45, 12605, 11021, 10399, 11986, 16707, 14589, 10459, 13799, 10414, 12363, 15287, 14987, 10768, 13254, 11871, 13818, 14734, 16523, 16530, 13234, 14486, 16533, 12469, 14734, 12415, 13327, 10261, 15486, 10964, 59, 11, 10384, 15287, 14987, 15489, 13250, 13234, 14791, 45, 12469, 14734, 15847, 12605, 10441, 10308, 13637, 17097, 10997, 17136, 45, 16417, 14446, 10292, 12299, 14179, 14734, 10257, 17149, 12366, 11124, 12604, 11994, 10846, 13212, 10080, 10444, 12469, 14734, 15847, 12605, 11021, 15752, 10322, 12665, 11002, 10333, 10250, 11899, 14734, 10781, 10429, 45, 13637, 11687, 14302, 10366, 14734, 15023, 10985, 11124, 13320, 11423, 13156, 10838, 10336, 10407, 45, 17148, 12325, 11168, 10333, 12469, 14734, 10370, 13327, 10997, 12022, 28329, 11, 13326, 12797, 10459, 13799, 10414, 12363, 14734, 16520, 14172, 45, 12469, 14734, 13205, 12605, 16417, 14446, 10760, 12299, 11638, 14734, 14583, 11861, 13205, 15752, 45, 11454, 14445, 11574, 10966, 10838, 10258, 10383, 14221, 11474, 12746, 15847, 12605, 10080, 10444, 11021, 15752, 12469, 14734, 16947, 12605, 13637, 17097, 10279, 15951, 45, 16503, 13685, 12469, 17241, 12357, 17097, 18216, 14734, 13308, 10733, 45, 12392, 16523, 12484, 12527, 17879, 13685, 10080, 17148, 17194, 12604, 10333, 13205, 12605, 11124, 16947, 12605, 10305, 17734, 14734, 14858, 14785, 28329, 11, 10384, 15287, 14987, 10846, 10838, 13234, 14791, 45, 12469, 10441, 10308, 13191, 17097, 12299, 14734, 10397, 10598, 12366, 14486, 45, 17189, 17141, 10264, 13773, 10336, 10270, 11124, 14778, 13308, 13234, 29334, 2241, 33, 13057, 13199, 10781, 10429, 15486, 10964, 12172, 13234, 14734, 36831, 7876, 33, 12527, 10080, 12469, 10322, 16417, 14446, 10760, 17097, 18216, 14734, 13074, 10925, 15752, 10838, 45, 10953, 12669, 15287, 10838, 12674, 10673, 10792, 14970, 10423, 12665, 16667, 14734, 12604, 11994, 10285, 28329, 11, 14059, 12522, 16696, 13036, 17919, 45, 12605, 12826, 13827, 12469, 11021, 15752, 11861, 11454, 16443, 17225, 12269, 14734, 12484, 15489, 45, 13952, 13197, 12348, 10792, 16667, 11021, 11124, 11889, 10675, 12527, 10080, 17148, 11021, 15752, 12325, 11168, 12469, 11957, 10366, 11930, 10684, 15304, 14734, 10495, 16922, 12415, 10080, 12605, 10402, 11021, 15752, 17879, 16503, 17149, 10250, 13582, 12822, 16671, 12469, 10696, 12366, 14734, 14216, 16253, 10261, 17771, 12354, 13212, 10794, 28329, 11, 11687, 13258, 12605, 14734, 10768, 13254, 13191, 10409, 10454, 17631, 16730, 45, 16738, 12469, 12748, 13580, 10080, 12605, 10402, 11021, 10399, 15507, 15511, 12822, 16671, 45, 10399, 12176, 10847, 12469, 16056, 12348, 11957, 15847, 12605, 14734, 13913, 10799, 14486, 16533, 10080, 17148, 17879, 16503, 17688, 13199, 14734, 12137, 10460, 45, 10444, 12605, 10423, 10675, 15052, 12978, 12746, 12469, 10080, 65535, 261, 65530, 65534, 13191, 13064, 10560, 12605, 10423, 16525, 12348, 10696, 12366, 15090, 16256, 19137, 10260, 14862, 12631, 12828, 28329, 65535, 261, 65530, 65532, 11685, 14734, 45, 12605, 10423, 11986, 16707, 10384, 10650, 15133, 12366, 14486, 11871, 11124, 10370, 13205, 10293, 10304, 12366, 14486, 11871, 14734, 16523, 16530, 45, 10768, 13254, 12469, 10696, 12366, 15090, 16256, 10079, 17165, 15987, 14734, 11021, 15752, 10974, 11416, 59, 11, 10384, 10650, 15133, 12366, 14486, 11871, 14791, 45, 14302, 10366, 11454, 54, 12039, 13064, 14734, 15023, 10985, 11021, 15752, 16672, 12469, 14734, 10495, 12428, 17879, 13685, 12348, 10260, 10792, 14015, 16956, 10080, 13326, 12797, 14299, 10788, 10660, 13396, 14734, 15287, 14987, 14970, 10423, 10997, 12022, 14486, 16686, 45, 17148, 10250, 13064, 13199, 11957, 14302, 13631, 14734, 10495, 16922, 11124, 10529, 10409, 11957, 10250, 10283, 11872, 11855, 14734, 12604, 17688, 15849, 10684, 17385, 16503, 10080, 14302, 14299, 14734, 15568, 11645, 11021, 15752, 11960, 15850, 12469, 14734, 11533, 13205, 10529, 10409, 11002, 10792, 10846, 12894, 45, 17149, 15646, 17194, 12604, 12484, 12527, 10257, 14734, 10260, 11889, 10675, 12527, 28329, 11454, 10370, 13205, 10293, 10304, 12366, 14486, 11871, 10285, 45, 15577, 13240, 13033, 12299, 16749, 13633, 10283, 10370, 17311, 13191, 11903, 14446, 15847, 12605, 45098, 34932, 33, 14734, 10696, 11454, 17879, 13685, 10080, 11021, 15752, 11454, 17169, 13685, 11630, 11454, 12604, 11994, 14734, 17141, 15052, 10285, 45, 12469, 12392, 16523, 10333, 10696, 11454, 15847, 12605, 14734, 17879, 16503, 45, 11960, 15850, 14446, 11454, 12527, 10792, 15090, 16256, 28329, 10292, 10333, 16056, 12348, 10390, 10370, 16667, 11021, 45, 12469, 11021, 15752, 17141, 10768, 12299, 16749, 10588, 10250, 10283, 9822, 11957, 14734, 10370, 28319, 15646, 12392, 16523, 10333, 10460, 10292, 10250, 10283, 9822, 14794, 11903, 14734, 10370, 9823, 14734, 17879, 13685, 10080, 13326, 12797, 18157, 13033, 13799, 14734, 17879, 13685, 12007, 13557, 14486, 16686, 45, 11016, 13191, 18216, 12007, 13557, 14734, 15847, 12605, 11903, 14446, 17879, 13685, 16443, 14015, 16956, 45, 10250, 10283, 10370, 12638, 15752, 12664, 10792, 14794, 13580, 14734, 14015, 16956, 12527, 28329, 12605, 12261, 16676, 12469, 11414, 12306, 10696, 12366, 45, 10578, 11065, 10696, 11454, 14734, 11610, 17952, 45, 12822, 15325, 15847, 12144, 16443, 17225, 12378, 12609, 12670, 10794, 14734, 12484, 12527, 17879, 13685, 10080, 17385, 13034, 12261, 15123, 11957, 15847, 12144, 14734, 10529, 10409, 45, 13815, 10760, 10250, 10283, 13179, 14794, 11903, 13179, 10286, 11932, 14734, 15847, 12605, 10080, 17148, 17879, 16503, 13064, 17734, 45, 10444, 12605, 14782, 10529, 10399, 12469, 14734, 10863, 13655, 45, 10250, 11899, 15752, 11634, 10588, 10792, 10080, 12605, 11454, 17148, 17384, 10675, 12366, 12978, 12746, 12469, 10080, 65535, 261, 65530, 65534, 11957, 10339, 17148, 15033, 12484, 12527, 11124, 12484, 15509, 14734, 10370, 19137, 33, 15752, 12826, 16007, 10325, 15252, 10079, 11975, 16735, 11124, 12603, 10821, 13234, 12176, 10847, 12200, 11626, 11124, 17149, 16403, 12484, 15509, 14864, 13580, 11053, 19156, 11, 65535, 261, 65530, 65532, 11685, 14734, 45, 12605, 11986, 16707, 12826, 16007, 10250, 10349, 10325, 15252, 10079, 11975, 16735, 11124, 12603, 10821, 45, 17148, 10349, 10460, 11159, 11021, 15752, 12176, 10847, 12469, 12200, 11626, 12484, 15509, 45, 17149, 16403, 12484, 12527, 14734, 13751, 14641, 59, 11, 50, 47, 33, 10325, 15252, 59, 10086, 12727, 12691, 10696, 11454, 14734, 11872, 11855, 10087, 45, 12176, 10847, 17919, 11957, 10696, 12366, 10781, 10429, 60, 10086, 10370, 14583, 14734, 17169, 13685, 10087, 45, 11959, 12664, 14583, 11117, 12522, 10304, 60, 10086, 12366, 14486, 11871, 10261, 14583, 13815, 10087, 45, 16667, 16696, 15847, 12605, 60, 10086, 10846, 13212, 10261, 10370, 13327, 10087, 45, 12664, 10792, 10696, 11454, 10846, 10838, 28329, 51, 47, 33, 11975, 16735, 59, 10086, 12766, 11753, 14734, 13396, 13256, 10087, 45, 13751, 12518, 17141, 15052, 60, 10086, 12605, 10402, 10400, 10087, 45, 11920, 12234, 10684, 15304, 60, 10086, 18052, 18361, 17848, 10087, 45, 10283, 10452, 12604, 17688, 60, 10086, 12366, 13091, 11870, 14370, 14734, 14384, 12636, 10087, 45, 12822, 15325, 10370, 12415, 28329, 52, 47, 33, 12603, 10821, 59, 10086, 13849, 18366, 10087, 45, 11920, 12234, 15593, 15482, 60, 10086, 13594, 10353, 17613, 11209, 11098, 10087, 45, 10283, 10370, 11659, 13020, 60, 10086, 15169, 12335, 12598, 11632, 10087, 45, 14583, 11117, 12522, 10304, 60, 10086, 11633, 13953, 14987, 10087, 45, 12366, 14141, 12991, 16916, 28329, 12605, 12261, 16676, 12469, 11021, 10399, 10384, 10285, 12664, 10792, 10682, 18365, 45, 11967, 10325, 10285, 10370, 14328, 14734, 15486, 10964, 10261, 15847, 12144, 14734, 61604, 45, 16056, 12348, 12484, 12527, 14734, 13800, 16519, 10080, 10421, 15597, 14734, 16728, 16537, 10322, 11021, 10399, 13751, 12518, 12366, 14141, 10781, 10429, 45, 17385, 12261, 15043, 13249, 12366, 12396, 10080, 12159, 13197, 17148, 10349, 10460, 11159, 11021, 10399, 17802, 10437, 12469, 14734, 15847, 12605, 12822, 15325, 10305, 13042, 10080, 11687, 13258, 13191, 10687, 10390, 12826, 16007, 17879, 16503, 45, 16738, 11088, 16698, 12605, 10080, 65535, 261]
-> item_loss 1.9321231842041016 batch_loss 1.9321231842041016
==== [10272, 18043, 46, 647, 11960, 12298, 10529, 12445, 11, 65530, 65532, 15494, 10760, 10272, 18043, 647, 14734, 13012, 13205, 10529, 12445, 45, 11454, 13012, 13205, 13188, 11044, 19137, 12605, 17879, 16503, 13006, 14486, 12604, 1843, 111, 13327, 12276, 19137, 13036, 10511, 14589, 10339, 11, 65535, 261, 65530, 65534, 10272, 18043, 46, 647, 10285, 11960, 12298, 13091, 10285, 11435, 14880, 10794, 14734, 10285, 17150, 15052, 12298, 17222, 11960, 12298, 10080, 33, 13326, 12797, 10678, 12266, 16909, 13021, 45, 10687, 10293, 16503, 12666, 13208, 10988, 13004, 11687, 10258, 59, 11, 9828, 33, 11965, 15052, 59, 640, 620, 46, 650, 620, 10918, 15260, 11, 9828, 33, 11960, 12298, 17688, 12231, 59, 637, 15260, 265, 9828, 33, 10997, 11965, 17385, 17387, 59, 638, 47, 55, 33, 11062, 3339, 9828, 33, 12610, 13020, 17303, 17385, 17387, 59, 50, 11062, 11, 9828, 33, 10482, 14589, 13882, 10452, 14267, 13021, 11, 10272, 18043, 46, 647, 11960, 12298, 10292, 10278, 10283, 15451, 10790, 59, 10272, 18043, 46, 647, 11124, 10272, 18043, 46, 647, 66, 45, 10293, 16503, 10912, 10790, 11454, 11965, 15052, 10257, 28329, 10272, 18043, 46, 647, 59, 11, 9828, 33, 11965, 15052, 59, 640, 620, 10918, 15260, 11, 9828, 33, 12610, 13020, 17303, 17385, 17387, 59, 695, 49, 10918, 10660, 11, 10272, 18043, 46, 647, 66, 59, 11, 9828, 11965, 15052, 59, 645, 620, 127, 650, 620, 10918, 15260, 3339, 9828, 33, 12610, 13020, 17303, 17385, 17387, 59, 50, 11062, 11, 10272, 18043, 46, 647, 11960, 12298, 16443, 12266, 10997, 14589, 10339, 15092, 14894, 10912, 11524, 17771, 15090, 15304, 15500, 45, 15752, 11634, 10495, 17917, 12298, 17222, 13177, 15471, 11124, 13212, 10846, 17248, 12987, 45, 13091, 10285, 11435, 17782, 11533, 13325, 10838, 17387, 14734, 10293, 10838, 10305, 10250, 28329, 10272, 18043, 46, 647, 11960, 12298, 12025, 10339, 10285, 10079, 14866, 11168, 12220, 13064, 17734, 12298, 17222, 11960, 12298, 45, 15752, 11634, 11454, 12728, 12617, 10808, 12387, 17193, 15092, 14894, 17771, 15090, 10452, 15304, 28329, 10272, 18043, 10337, 10917, 10253, 14734, 14332, 14172, 10894, 12732, 11, 50, 47, 33, 17379, 14589, 10994, 13882, 12396, 14136, 15208, 10997, 10846, 13212, 28331, 51, 47, 33, 17332, 15586, 10333, 11626, 13215, 14734, 10794, 11960, 15304, 15500, 19137, 10399, 10992, 10688, 11622, 12101, 13034, 14734, 10846, 10838, 15304, 15500, 28329, 10399, 10257, 13091, 10684, 10339, 10272, 18043, 46, 647, 11960, 12298, 14734, 11533, 13205, 13445, 17162, 45, 12159, 13197, 15752, 10292, 12469, 12848, 10494, 10988, 15641, 10080, 11687, 13258, 13191, 10409, 10454, 14638, 17728, 45, 13558, 17143, 11047, 12605, 12848, 17728, 28329, 10384, 10257, 17162, 13004, 12797, 10285, 19137, 33, 13006, 14486, 15489, 13250, 10901, 10529, 12445, 11687, 10258, 11, 6884, 1843, 111, 11, 2370, 26290, 269, 10272, 18043, 46, 647, 35, 11, 59, 8148, 58475, 269, 10285, 11435, 35, 11, 59, 42519, 269, 638, 47, 55, 33, 11062, 35, 11, 59, 41775, 267, 35, 637, 33, 15260, 35, 11, 59, 9059, 25566, 267, 35, 630, 620, 10918, 10660, 35, 11, 59, 35494, 269, 640, 620, 46, 650, 620, 10918, 15260, 35, 11, 59, 41330, 269, 10994, 13882, 12396, 14136, 15208, 10997, 10846, 13212, 35, 11, 59, 53988, 4363, 13191, 12986, 17095, 16032, 11021, 16749, 395, 11, 59, 63053, 269, 10272, 18043, 46, 647, 10285, 15052, 12298, 17222, 11960, 12298, 45, 17332, 11622, 11626, 13215, 14734, 10794, 11960, 11124, 12101, 13034, 14734, 10846, 10838, 15304, 15500, 10080, 399, 11, 6884, 65535, 261]
-> item_loss 1.6350220441818237 batch_loss 1.7835726141929626
==== [12366, 14486, 16690, 16753, 12308, 11, 65530, 65531, 10399, 10258, 11957, 16713, 13091, 12366, 14486, 10913, 14583, 11124, 13234, 16690, 15643, 10305, 17734, 14734, 11957, 16713, 19137, 6608, 54116, 1937, 13091, 10250, 10447, 15287, 14987, 10768, 16449, 14663, 12467, 15643, 10080, 10390, 16667, 10292, 15847, 12144, 13091, 10250, 10283, 820, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10464, 11685, 844, 2194, 41693, 45, 267, 13234, 14734, 12771, 13057, 14734, 47, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 6608, 54116, 1937, 501, 33, 13091, 14734, 45, 12605, 13199, 12335, 17148, 13557, 14734, 10355, 13819, 47, 33, 15847, 10384, 10257, 13557, 16519, 17141, 10464, 10305, 11044, 12605, 11685, 11632, 10333, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 17268, 10303, 12605, 10402, 14446, 11454, 12266, 11726, 47, 267, 14446, 11454, 12605, 17728, 10464, 10250, 10283, 17728, 18025, 45, 33, 10464, 13091, 11060, 17147, 12395, 14638, 15847, 12144, 13091, 10250, 10283, 820, 10370, 12137, 13133, 15752, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 6608, 54116, 1937, 501, 33, 12605, 10260, 14862, 17222, 47, 33, 12605, 11016, 13091, 10250, 10283, 15052, 12217, 47, 33, 12605, 13734, 13191, 12527, 16525, 11124, 12522, 16696, 47, 33, 12605, 11016, 13091, 10250, 10283, 15052, 12217, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 12631, 10399, 10464, 13091, 10250, 10283, 15052, 12217, 64, 33, 10464, 13191, 10373, 10303, 13234, 16692, 13078, 15847, 12144, 13091, 10250, 10283, 15052, 12217, 17268, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 6608, 54116, 1937, 501, 33, 12605, 13091, 10250, 10283, 15052, 12217, 47, 33, 12605, 13734, 13191, 12527, 16525, 11124, 12522, 16696, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 14589, 12636, 12623, 10333, 10250, 10258, 5585, 41693, 14734, 15804, 17303, 47, 58683, 12527, 16525, 10792, 10250, 17774, 17355, 14673, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10464, 14446, 11454, 13091, 11060, 12527, 16525, 10792, 10373, 10303, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 6608, 54116, 1937, 501, 33, 12605, 12527, 16525, 10792, 10333, 47, 33, 10464, 12623, 10333, 12605, 10250, 10258, 47, 33, 10444, 13091, 17148, 10260, 15752, 16692, 13078, 10373, 10303, 47, 33, 12605, 17147, 13091, 820, 10370, 12137, 13133, 15752, 47, 11, 65535, 261, 65530, 65531, 33365, 49461, 12745, 10760, 10333, 10250, 12287, 14240, 14313, 45, 33, 10257, 17919, 13091, 5585, 41693, 11124, 10390, 14734, 11920, 10370, 10402, 45, 58683, 12691, 14808, 10390, 14734, 11679, 10650, 17900, 16501, 45, 33, 10390, 10402, 15149, 14734, 17311, 12337, 12266, 12366, 47, 17900, 16501, 12636, 10285, 12745, 14808, 10250, 10283, 13778, 17159, 11975, 14249, 45, 11994, 13091, 16407, 10257, 12182, 16519, 14734, 17268, 15033, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 17148, 12287, 14240, 14313, 10464, 16667, 16696, 11053, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 6608, 54116, 1937, 501, 33, 17148, 13091, 12605, 14734, 11679, 10650, 17900, 16501, 47, 33, 11684, 12337, 11021, 14299, 47, 33, 12605, 12337, 11268, 13558, 11684, 47, 33, 12605, 10402, 10250, 11920, 10370, 17311, 12337, 11268, 13558, 11684, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10370, 12137, 13133, 15752, 12401, 10303, 10423, 13191, 11679, 10650, 14734, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 6608, 54116, 1937, 501, 33, 12605, 10260, 14862, 17222, 47, 33, 10444, 13091, 17148, 11994, 13091, 12605, 14734, 11679, 10650, 47, 33, 11684, 12337, 11021, 14299, 47, 33, 12605, 12337, 11268, 13558, 11684, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 12605, 16535, 14935, 10464, 14734, 13064, 10560, 45, 33, 10464, 10423, 12527, 16525, 10792, 14658, 14673, 47, 33, 10464, 10322, 13191, 14658, 14299, 14734, 11679, 10650, 47, 267, 12631, 10399, 10464, 17147, 11474, 12746, 10464, 13091, 10370, 12137, 13133, 15752, 11053, 64, 11, 65535, 261, 65530, 65531, 33365, 49461, 14589, 12748, 11981, 12813, 10449, 5585, 41693, 14734, 12636, 15833, 45, 58683, 12527, 16525, 10792, 14658, 14673, 47, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 6608, 54116, 1937, 501, 33, 12266, 11726, 13722, 18441, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10464, 17147, 11474, 12746, 10464, 13091, 10370, 12137, 13133, 15752, 11053, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 12605, 10260, 14862, 17222, 47, 33, 12605, 11016, 13091, 10250, 10283, 15052, 12217, 47, 33, 12605, 13734, 13191, 12527, 16525, 11124, 12522, 16696, 47, 33, 12605, 11016, 13091, 10250, 10283, 15052, 12217, 579, 10390, 14734, 15287, 14987, 17771, 15471, 12266, 11726, 12100, 13977, 42, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 12605, 14782, 10529, 10464, 13091, 10250, 10283, 14794, 13580, 14734, 10370, 47, 33, 10464, 14734, 12366, 14486, 14660, 14662, 11016, 13091, 16417, 14446, 10292, 10250, 10283, 820, 10370, 12137, 13133, 15752, 47, 33, 10464, 10260, 10423, 16443, 14794, 13580, 14734, 15287, 14987, 14662, 12631, 12325, 11168, 47, 33, 14446, 11454, 10464, 14862, 17222, 15847, 12144, 13091, 10373, 10303, 10333, 11053, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 12605, 16678, 16944, 13234, 10333, 47, 33, 12605, 13091, 10250, 10283, 14794, 13580, 14734, 10370, 47, 33, 12605, 10260, 13091, 820, 10370, 12137, 13133, 15752, 47, 33, 12605, 13091, 10250, 10283, 10370, 15261, 45, 33, 10250, 10283, 13191, 16399, 13191, 15694, 14734, 10370, 15261, 47, 33, 16774, 16774, 10464, 10913, 14583, 45, 33, 12605, 14862, 17222, 15847, 12144, 13091, 10373, 10303, 10333, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 14446, 11454, 45, 33, 12307, 10464, 12848, 10792, 97, 12605, 97, 17148, 10283, 16702, 14734, 13064, 10560, 45, 10464, 14734, 10423, 15671, 12509, 10792, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 12605, 12509, 10792, 14734, 13091, 12605, 15847, 12144, 47, 33, 12605, 13091, 10250, 10283, 14794, 13580, 14734, 10370, 15261, 45, 33, 13191, 16399, 13191, 15694, 14734, 10370, 15261, 47, 33, 12605, 10260, 13091, 820, 10370, 12137, 13133, 15752, 47, 33, 12605, 10260, 10423, 16443, 14794, 13580, 14734, 15287, 14987, 14662, 12631, 12325, 11168, 47, 33, 12605, 10260, 13091, 13091, 10250, 10283, 10632, 10901, 13594, 13248, 14734, 15052, 12217, 45, 33, 12605, 11021, 10399, 10588, 10760, 12337, 11632, 10336, 12484, 10985, 12981, 11003, 17148, 10283, 10267, 14610, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 17268, 10303, 10464, 15752, 11634, 10588, 10792, 11053, 64, 33, 10464, 15752, 11634, 12509, 16803, 10760, 15847, 12144, 14734, 14302, 13631, 11053, 64, 33, 12609, 15643, 10464, 14734, 13192, 10993, 64, 33, 10464, 15752, 11634, 11414, 12369, 16944, 17141, 10985, 14734, 10336, 12484, 11053, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 12605, 15752, 11634, 11414, 12369, 16944, 17141, 10985, 14734, 10336, 12484, 47, 33, 12605, 11021, 10399, 12509, 16803, 10760, 15847, 12144, 14734, 14302, 13631, 45, 33, 12605, 11021, 10399, 12509, 16803, 10760, 13192, 10993, 10402, 47, 33, 10444, 13091, 12605, 10260, 15752, 12509, 16803, 10760, 15847, 12144, 14734, 15287, 14987, 14662, 13091, 10373, 10303, 13064, 10560, 10997, 14583, 14734, 47, 33, 12605, 17268, 13064, 10560, 16730, 10399, 10292, 15847, 12144, 13091, 10250, 10283, 820, 10370, 12137, 13133, 15752, 15646, 10260, 13091, 14794, 13580, 14734, 10370, 15261, 47, 267, 10260, 14862, 17222, 10373, 10303, 13064, 10560, 17148, 15033, 10632, 10901, 13594, 13248, 14734, 17207, 17108, 12527, 13282, 10333, 12605, 45, 16672, 12605, 12378, 16678, 10333, 15847, 12144, 13091, 10250, 10283, 10688, 13191, 14583, 10846, 45, 13815, 10838, 45, 13815, 13786, 14734, 10370, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 12337, 11685, 47, 33, 12605, 10402, 10381, 11639, 14734, 13751, 14641, 11994, 10792, 17148, 17384, 10333, 47, 33, 10464, 11021, 10399, 11414, 10985, 12410, 15641, 10250, 10258, 45, 33, 14221, 11044, 12605, 10402, 10703, 13234, 10588, 10258, 10250, 13557, 14734, 13751, 14641, 47, 33, 12159, 13197, 10464, 15752, 11634, 11685, 16944, 13234, 47, 33, 12605, 10402, 11454, 10258, 13557, 16519, 17919, 14734, 13064, 10560, 10703, 12261, 47, 33, 10703, 16519, 34, 11, 65535, 261]
-> item_loss 1.1061184406280518 batch_loss 1.5577545563379924
==== [25106, 11, 65530, 65531, 24869, 13091, 10250, 10283, 10370, 12137, 13133, 15752, 10847, 12636, 19137, 11021, 10399, 12176, 10847, 10370, 15261, 10588, 11036, 15033, 10336, 12484, 10080, 24869, 12337, 15678, 13078, 19137, 15752, 10384, 10370, 15261, 14734, 16728, 16537, 10285, 14486, 16533, 10370, 15261, 14734, 17879, 13685, 28329, 65535, 261, 65530, 65532, 27370, 59, 30274, 19156, 11, 65535, 261, 65530, 65534, 24869, 59, 33, 12605, 11454, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 11021, 10399, 14589, 15963, 13012, 11124, 12605, 16735, 16713, 10303, 19156, 11, 65535, 261, 65530, 65534, 24869, 59, 4089, 45604, 34, 308, 21413, 63354, 32487, 22799, 4596, 50199, 47, 20063, 21413, 308, 45221, 22799, 45, 4677, 46526, 64, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 17268, 11021, 10399, 14589, 13053, 13012, 10303, 19156, 11, 65535, 261, 65530, 65534, 24869, 59, 33, 10139, 10103, 10079, 13053, 13205, 16592, 43385, 11961, 12380, 43380, 43421, 10080, 10454, 10107, 10106, 12636, 10424, 43312, 58530, 10107, 10080, 10134, 10104, 3022, 159, 10106, 13657, 17055, 10136, 10079, 10116, 10293, 10370, 13476, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 11685, 11061, 19137, 10780, 12638, 11016, 13091, 13827, 16707, 19137, 17147, 13091, 14589, 10285, 13012, 11061, 11, 65535, 261, 65530, 65534, 24869, 59, 33, 13734, 17728, 18025, 19137, 10293, 10370, 19137, 12605, 11021, 10399, 14589, 11036, 15033, 16728, 16537, 12176, 12469, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 10464, 14862, 17222, 10296, 10250, 10995, 10256, 10303, 19156, 11, 65535, 261, 65530, 65534, 24869, 59, 33, 14862, 17222, 19137, 10296, 10250, 10995, 10256, 13091, 12748, 11871, 10423, 10333, 10250, 15261, 14862, 16696, 12609, 13036, 13764, 11044, 19137, 15752, 11634, 11967, 10687, 13325, 12366, 12410, 12509, 12220, 14589, 10339, 11042, 15261, 11496, 12609, 14782, 10441, 14734, 12484, 13126, 12609, 10409, 10843, 10285, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 11638, 10452, 10257, 13734, 13191, 17728, 18025, 28329, 65535, 261, 65530, 65534, 24869, 59, 33, 11315, 19137, 11685, 14734, 19137, 10293, 10370, 17147, 13191, 10687, 10390, 10336, 12484, 10303, 19156, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 12605, 12509, 16735, 14734, 13091, 19137, 14589, 13234, 16675, 15475, 10464, 14734, 16728, 13021, 12337, 17843, 17136, 10792, 11891, 11622, 43282, 28329, 65535, 261, 65530, 65534, 24869, 59, 33, 12631, 10399, 10293, 10370, 14734, 12522, 12410, 13091, 19137, 12605, 11454, 17213, 10792, 10250, 10349, 13734, 13191, 16519, 17141, 14734, 17728, 18025, 13064, 19137, 17879, 16503, 10296, 10250, 10995, 10256, 19137, 15641, 16253, 15847, 12144, 17213, 10792, 17141, 14734, 11042, 15261, 17728, 18025, 19137, 12202, 11986, 16707, 14589, 10687, 10285, 14734, 12410, 12509, 10985, 16533, 10722, 19156, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 11957, 19137, 15678, 13078, 19126, 11, 65535, 261, 65530, 65534, 24869, 59, 33, 16774, 16774, 10293, 10370, 11649, 11665, 19137, 12605, 10423, 11986, 16707, 11047, 17148, 10283, 13036, 11047, 10848, 10838, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 11315, 19137, 12605, 10703, 15494, 10464, 10296, 10283, 10487, 11855, 11061, 10080, 13637, 11687, 15494, 10464, 10250, 10283, 10409, 10843, 19137, 10464, 17879, 16503, 10384, 35, 33839, 5178, 24364, 98, 35, 11124, 35, 684, 35, 10285, 17177, 12731, 10250, 10283, 17109, 10760, 10080, 17148, 13324, 19137, 10464, 10658, 11088, 16698, 12605, 17148, 13557, 10464, 17109, 10760, 10333, 10373, 10303, 19137, 14781, 12824, 12669, 10464, 14734, 17177, 12731, 17109, 10760, 10760, 13234, 28329, 65535, 261, 65530, 65534, 24869, 59, 3537, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 13734, 17631, 19137, 11454, 17148, 15261, 10409, 10843, 10285, 10464, 17879, 16503, 12750, 14240, 12748, 11899, 13327, 12276, 10733, 14914, 11459, 17109, 10760, 10696, 11922, 19137, 17268, 12605, 10703, 15494, 10464, 12791, 10250, 10283, 17728, 18025, 28329, 65535, 261, 65530, 65534, 24869, 59, 33, 11685, 14734, 10293, 10370, 19137, 12605, 15169, 12335, 10464, 14734, 17728, 18025, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 17148, 13557, 10384, 92, 6885, 94, 10079, 92, 1751, 101, 94, 10079, 92, 114, 94, 10285, 17177, 12731, 10250, 10283, 17109, 10760, 11, 65535, 261, 65530, 65534, 24869, 59, 348, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 10464, 10384, 17148, 10278, 10283, 17728, 18025, 10285, 10997, 14446, 10333, 10373, 10303, 11053, 19156, 11, 65535, 261, 65530, 65534, 24869, 59, 33, 10293, 10370, 19137, 15489, 11038, 11124, 10464, 14734, 15667, 11639, 10964, 11024, 19137, 12605, 10997, 14446, 17148, 10278, 10283, 10409, 10843, 12025, 10339, 11042, 15261, 10409, 10843, 19137, 17311, 13091, 17879, 16503, 12605, 12750, 14240, 13327, 12276, 15287, 10733, 14734, 17109, 10760, 10696, 11922, 10080, 10444, 13091, 10278, 15643, 10912, 10790, 11454, 10339, 19137, 10808, 15643, 14734, 10696, 11922, 13091, 14597, 12282, 11028, 10894, 16468, 10079, 14597, 17187, 11028, 17734, 17821, 14734, 19137, 15646, 11044, 15643, 10779, 13091, 14597, 13036, 12732, 11028, 10894, 16468, 10079, 14597, 18005, 11028, 17734, 17821, 19137, 10560, 17177, 14734, 10696, 11922, 10322, 13191, 12631, 10260, 11042, 10080, 12605, 12826, 13827, 10250, 10258, 19137, 10293, 10370, 14734, 12522, 12410, 13091, 12509, 11088, 16698, 12605, 17148, 10278, 10283, 10409, 10843, 13091, 11042, 10250, 15033, 15261, 19137, 12631, 10399, 12307, 12605, 11871, 10423, 10333, 10250, 10283, 10305, 11044, 19137, 12220, 12307, 13078, 14731, 11042, 15261, 14734, 10409, 10843, 16721, 12401, 13324, 10985, 11891, 12604, 19137, 13091, 17148, 13324, 10303, 19156, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 13734, 17631, 10080, 12605, 10402, 17147, 11021, 10399, 10588, 13179, 11632, 14734, 13827, 16707, 10303, 19156, 11, 65535, 261, 65530, 65534, 24869, 59, 33, 11021, 10399, 14734, 19137, 12605, 15169, 12335, 12469, 14734, 13034, 13827, 16707, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11986, 16707, 17109, 10760, 10250, 10283, 15963, 13012, 10923, 16530, 17187, 11028, 19137, 17147, 13091, 10260, 17879, 16503, 17109, 10760, 10687, 10390, 13012, 11860, 28329, 65535, 261, 65530, 65534, 24869, 59, 279, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11986, 16707, 17109, 10760, 10250, 10283, 10285, 13012, 17187, 11028, 19137, 12202, 17819, 10511, 16735, 14172, 10373, 10303, 11, 65535, 261, 65530, 65534, 24869, 59, 23245, 10293, 10370, 19137, 12605, 12750, 14240, 12469, 14734, 12522, 12410, 17109, 10760, 10333, 10250, 10283, 10285, 13012, 17187, 11028, 19137, 11885, 10447, 10339, 12605, 17148, 11012, 16713, 14734, 10808, 17919, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 10588, 12348, 11685, 10080, 1751, 7321, 7321, 1817, 7051, 1832, 1751, 11, 65535, 261, 65530, 65534, 24869, 59, 33, 16774, 16774, 11649, 11665, 19137, 10444, 13091, 12605, 14791, 10260, 12578, 11454, 10293, 10370, 12469, 14734, 11044, 10250, 11012, 16713, 19137, 11885, 13091, 10373, 10303, 12522, 12410, 19156, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 10260, 11975, 12366, 12642, 10333, 10250, 10349, 10328, 14875, 19137, 10260, 16503, 11454, 12522, 28329, 65535, 261, 65530, 65534, 24869, 59, 33, 13078, 14731, 10333, 28329, 65535, 261]
-> item_loss 0.9601616263389587 batch_loss 1.408356323838234
==== [12366, 14486, 16690, 16753, 12308, 11, 65530, 65531, 10399, 10258, 11957, 16713, 13091, 12366, 14486, 10913, 14583, 11124, 13234, 16690, 15643, 10305, 17734, 14734, 11957, 16713, 19137, 33914, 8778, 117, 41, 13341, 12001, 42, 13091, 10250, 10447, 10337, 10917, 12039, 14734, 11977, 11679, 45, 11684, 11474, 12746, 16667, 10292, 15847, 12144, 11016, 13091, 10250, 10283, 820, 10370, 12137, 13133, 15752, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10464, 11685, 13341, 12001, 47, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 10464, 11685, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10464, 13188, 17145, 13191, 10373, 10303, 14189, 12454, 11053, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 12605, 13188, 17145, 12421, 13091, 12527, 16525, 12337, 15330, 19137, 15646, 10265, 12605, 10322, 10260, 14862, 17222, 10292, 10373, 10303, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10464, 17147, 16667, 10292, 15847, 12144, 13091, 10250, 10283, 820, 10370, 12137, 13133, 15752, 45, 11016, 13091, 10250, 10283, 15052, 12217, 45, 13734, 13191, 12527, 12484, 11124, 12410, 12509, 11053, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 12605, 10260, 14862, 17222, 47, 12605, 11016, 13091, 10250, 10283, 15052, 12217, 45, 13734, 13191, 12527, 12484, 11124, 12410, 12509, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 17268, 10303, 10464, 10292, 10373, 10303, 10423, 12527, 10792, 14654, 12502, 11102, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 12605, 10260, 14862, 17222, 47, 12605, 11016, 13091, 12527, 16525, 12337, 15330, 45, 15646, 10265, 12605, 10322, 10260, 14862, 17222, 10292, 10373, 10303, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10464, 13191, 13734, 13191, 12509, 17141, 45, 10322, 16684, 10464, 13091, 10250, 10283, 14794, 13580, 14734, 10370, 15261, 11102, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 12609, 16684, 13091, 11061, 47, 10444, 13091, 12605, 10260, 14862, 17222, 15847, 12144, 13091, 16747, 45, 10322, 10260, 14862, 17222, 15847, 12144, 10292, 10373, 10303, 10423, 13234, 10792, 17148, 17384, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10464, 11692, 11692, 12266, 17079, 17171, 10464, 13234, 14734, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 12605, 10260, 14862, 17222, 47, 12605, 11016, 16678, 12348, 12605, 13091, 11454, 10250, 10283, 12337, 11638, 14734, 12630, 17734, 17384, 17919, 45, 14221, 11044, 13191, 10250, 10283, 10370, 11088, 16698, 12605, 45, 10390, 13091, 12605, 14734, 14304, 14304, 47, 10444, 13091, 12605, 10260, 14862, 17222, 10390, 13091, 16747, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 17148, 10303, 16735, 10464, 10260, 16678, 12348, 10464, 14734, 14302, 10366, 10333, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 10260, 16678, 12348, 10333, 47, 12605, 11016, 16678, 12348, 12605, 13091, 10250, 10283, 820, 10370, 12137, 13133, 15752, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 13091, 10260, 13091, 12337, 11632, 10370, 17311, 11088, 16698, 10464, 45, 10464, 13091, 10283, 820, 10370, 12137, 13133, 15752, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 13091, 14734, 47, 12605, 10260, 14862, 17222, 10390, 10402, 10292, 10373, 10303, 16503, 17148, 10303, 16735, 45, 10444, 13091, 12605, 12527, 16525, 10390, 10402, 11685, 10621, 12337, 11917, 12405, 12605, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10292, 10373, 10303, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 12605, 10260, 14862, 17222, 47, 10444, 13091, 10390, 10402, 12421, 13091, 12337, 11917, 12405, 12605, 45, 11685, 10621, 12605, 10423, 10429, 11917, 10390, 10402, 10250, 13324, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10444, 13091, 10464, 10260, 10423, 10429, 11917, 10390, 10402, 11957, 10260, 11957, 47, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 10260, 10423, 47, 12605, 10260, 10423, 10429, 11917, 10390, 10402, 14734, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 45913, 32232, 31123, 22799, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 10260, 10423, 47, 12605, 10260, 10423, 10429, 11917, 10390, 10402, 14734, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 13341, 12001, 45, 15847, 12605, 10525, 12686, 13091, 10250, 15033, 10370, 15261, 14734, 13205, 15752, 45, 33, 10464, 10260, 12220, 16721, 12676, 12711, 11885, 47, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 12605, 10260, 14862, 17222, 47, 12605, 11016, 13091, 12527, 16525, 12337, 11917, 12405, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10464, 10260, 14589, 11917, 12405, 47, 10464, 11016, 13091, 10250, 10283, 11021, 14299, 14734, 11975, 11679, 11872, 45, 33, 11975, 11679, 11872, 15752, 13191, 10373, 10303, 17631, 17268, 64, 33, 12509, 10250, 12509, 45, 12605, 10402, 13186, 15486, 10250, 16944, 12610, 15735, 17141, 12140, 18488, 45, 17268, 15033, 12418, 14328, 12605, 10402, 17311, 11021, 10399, 12610, 15735, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 12605, 10260, 14862, 17222, 47, 12605, 11016, 13091, 12527, 16525, 12337, 11917, 12405, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10464, 11021, 10399, 11088, 16698, 12605, 10997, 14583, 10333, 10373, 10303, 10336, 11053, 64, 33, 12605, 10402, 10250, 16944, 13234, 16533, 10722, 11885, 11685, 11053, 64, 11, 65535, 261, 65530, 65532, 5585, 41693, 59, 33, 11957, 16713, 12145, 15489, 13229, 19137, 13051, 13764, 17149, 16403, 10258, 10250, 13582, 14734, 10409, 10843, 11621, 14486, 10080, 11687, 13258, 12469, 13191, 10687, 10390, 17728, 18025, 17879, 16503, 12176, 10847, 19137, 16738, 17819, 13064, 12848, 10760, 28329, 65535, 261]
