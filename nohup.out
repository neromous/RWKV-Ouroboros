[2023-09-30 14:43:29,895] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 14:43:29,915] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 14:43:32,381] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 14:43:32,381] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 14:43:32,381] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 14:43:32,381] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 14:43:32,381] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.616151332855225 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 14:44:20,319] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 14:44:20,320] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 14:44:23,626] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 14:44:23,628] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 14:44:23,628] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 14:44:23,667] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 14:44:23,667] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 14:44:23,667] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 14:44:23,667] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 14:44:23,667] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 14:44:23,668] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 14:44:23,668] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.23317265510559082 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-09-30 14:44:50,011] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 14:44:50,012] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 14:44:50,012] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.04 GB, percent = 17.5%
[2023-09-30 14:45:09,469] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 14:45:09,470] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 14:45:09,470] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.25 GB, percent = 48.7%
[2023-09-30 14:45:09,471] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 14:45:11,081] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 14:45:11,082] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 14:45:11,082] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.27 GB, percent = 48.7%
[2023-09-30 14:45:11,106] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 14:45:11,106] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 14:45:11,106] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fb77d5f10d0>
[2023-09-30 14:45:11,106] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 14:45:11,107] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 14:45:11,108] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 14:45:11,108] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 14:45:11,108] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 14:45:11,108] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 14:45:11,108] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 14:45:11,108] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 14:45:11,108] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 14:45:11,108] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb77d5c80d0>
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 14:45:11,109] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 14:45:11,110] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 14:45:11,111] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 14:45:11,111] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00045418739318847656 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

[2023-09-30 16:38:02,153] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 16:38:02,173] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 16:38:04,699] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 16:38:04,699] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 16:38:04,699] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 16:38:04,699] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 16:38:04,699] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
[2023-09-30 16:38:22,008] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1207084
[2023-09-30 16:38:22,009] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.4222633838653564 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 16:38:27,349] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 16:38:27,350] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 16:38:27,999] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 16:38:28,001] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 16:38:28,001] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 16:38:28,025] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 16:38:28,025] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 16:38:28,025] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 16:38:28,025] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 16:38:28,025] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 16:38:28,025] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 16:38:28,025] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.22358965873718262 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(461598720, False), (24576, False), (24576, False)] 
[2023-09-30 16:38:32,351] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 16:38:32,351] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:38:32,352] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 8.85 GB, percent = 7.0%
[2023-09-30 16:38:35,089] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 16:38:35,090] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:38:35,090] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 14.33 GB, percent = 11.4%
[2023-09-30 16:38:35,090] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 16:38:35,750] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 16:38:35,751] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:38:35,751] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 14.33 GB, percent = 11.4%
[2023-09-30 16:38:35,766] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 16:38:35,766] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 16:38:35,766] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f9c5e773cd0>
[2023-09-30 16:38:35,766] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 16:38:35,767] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9c7ce4d040>
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 16:38:35,768] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 16:38:35,769] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 16:38:35,770] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 16:38:35,771] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0004134178161621094 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

Traceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/app.py", line 155, in train
    for token in data_iter:
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/models/scene.py", line 76, in yield_tokens
    output = tokens[ctx:]
NameError: name 'ctx' is not defined
127.0.0.1 - - [30/Sep/2023 16:39:41] "POST /train/tx-data HTTP/1.1" 500 752
[2023-09-30 16:40:45,891] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1239336
[2023-09-30 16:40:45,892] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 16:40:54,128] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 16:40:57,657] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 16:41:00,151] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 16:41:00,151] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 16:41:00,151] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 16:41:00,151] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 16:41:00,152] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.167691230773926 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 16:41:21,608] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 16:41:21,609] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 16:41:22,237] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 16:41:22,238] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 16:41:22,238] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 16:41:22,262] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 16:41:22,262] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 16:41:22,262] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 16:41:22,262] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 16:41:22,262] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 16:41:22,262] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 16:41:22,262] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.24089789390563965 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(461598720, False), (24576, False), (24576, False)] 
[2023-09-30 16:41:26,679] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 16:41:26,680] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:41:26,680] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 8.85 GB, percent = 7.0%
[2023-09-30 16:41:29,388] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 16:41:29,389] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:41:29,389] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 14.34 GB, percent = 11.4%
[2023-09-30 16:41:29,389] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 16:41:30,060] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 16:41:30,061] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:41:30,061] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 14.34 GB, percent = 11.4%
[2023-09-30 16:41:30,083] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 16:41:30,084] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 16:41:30,084] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f0bf8a70220>
[2023-09-30 16:41:30,084] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 16:41:30,085] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 16:41:30,085] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 16:41:30,085] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 16:41:30,085] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 16:41:30,085] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 16:41:30,085] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f0bf0d06b80>
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 16:41:30,086] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 16:41:30,087] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 16:41:30,088] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 16:41:30,088] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00039887428283691406 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

[2023-09-30 16:41:51,328] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2023-09-30 16:41:51,328] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-30 16:41:51,328] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-30 16:41:51,328] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2023-09-30 16:41:51,328] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
Traceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/app.py", line 159, in train
    m = model_engine.training_step(batch, model_engine=model_engine)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 790, in training_step
    logits = self(idx)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 765, in forward
    x = deepspeed.checkpointing.checkpoint(block, x)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/runtime/activation_checkpointing/checkpointing.py", line 713, in checkpoint
    CheckpointFunction.apply(function, all_outputs, *args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/runtime/activation_checkpointing/checkpointing.py", line 555, in forward
    outputs = run_function(*inputs_cuda)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 518, in forward
    x = x + self.att(self.ln1(x))
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 422, in forward
    sr, k, v = self.jit_func(x)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 410, in jit_func
    @MyFunction
    def jit_func(self, x):
        xx = self.time_shift(x) # Mix x with the previous timestep to produce xk, xv, xr
             ~~~~~~~~~~~~~~~ <--- HERE
        xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)
        xv = x * self.time_mix_v + xx * (1 - self.time_mix_v)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/padding.py", line 25, in forward
    def forward(self, input: Tensor) -> Tensor:
        return F.pad(input, self.padding, 'constant', self.value)
               ~~~~~ <--- HERE
RuntimeError: narrow(): length must be non-negative.

127.0.0.1 - - [30/Sep/2023 16:41:51] "POST /train/tx-data HTTP/1.1" 500 752
[2023-09-30 16:42:56,292] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1240743
[2023-09-30 16:42:56,293] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 16:43:00,215] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 16:43:00,235] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 16:43:02,761] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 16:43:02,761] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 16:43:02,761] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 16:43:02,761] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 16:43:02,761] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.2136683464050293 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 16:43:24,447] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 16:43:24,447] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 16:43:25,064] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 16:43:25,065] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 16:43:25,065] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 16:43:25,089] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 16:43:25,089] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 16:43:25,089] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 16:43:25,089] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 16:43:25,089] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 16:43:25,089] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 16:43:25,089] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.2330338954925537 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(461598720, False), (24576, False), (24576, False)] 
[2023-09-30 16:43:29,446] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 16:43:29,447] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:43:29,447] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 8.85 GB, percent = 7.0%
[2023-09-30 16:43:36,109] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 16:43:36,110] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:43:36,110] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 14.32 GB, percent = 11.4%
[2023-09-30 16:43:36,110] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 16:43:36,763] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 16:43:36,763] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:43:36,764] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 14.33 GB, percent = 11.4%
[2023-09-30 16:43:36,779] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 16:43:36,779] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 16:43:36,779] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f8381a33c70>
[2023-09-30 16:43:36,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 16:43:36,780] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f83940771f0>
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 16:43:36,781] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 16:43:36,782] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 16:43:36,783] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 16:43:36,783] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0003781318664550781 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

{'input_ids': [], 'attention_mask': None}
[2023-09-30 16:43:53,902] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2023-09-30 16:43:53,902] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-30 16:43:53,902] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-30 16:43:53,902] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2023-09-30 16:43:53,902] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
Traceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/app.py", line 160, in train
    m = model_engine.training_step(batch, model_engine=model_engine)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 790, in training_step
    logits = self(idx)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 765, in forward
    x = deepspeed.checkpointing.checkpoint(block, x)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/runtime/activation_checkpointing/checkpointing.py", line 713, in checkpoint
    CheckpointFunction.apply(function, all_outputs, *args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/runtime/activation_checkpointing/checkpointing.py", line 555, in forward
    outputs = run_function(*inputs_cuda)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 518, in forward
    x = x + self.att(self.ln1(x))
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 422, in forward
    sr, k, v = self.jit_func(x)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 410, in jit_func
    @MyFunction
    def jit_func(self, x):
        xx = self.time_shift(x) # Mix x with the previous timestep to produce xk, xv, xr
             ~~~~~~~~~~~~~~~ <--- HERE
        xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)
        xv = x * self.time_mix_v + xx * (1 - self.time_mix_v)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/padding.py", line 25, in forward
    def forward(self, input: Tensor) -> Tensor:
        return F.pad(input, self.padding, 'constant', self.value)
               ~~~~~ <--- HERE
RuntimeError: narrow(): length must be non-negative.

127.0.0.1 - - [30/Sep/2023 16:43:54] "POST /train/tx-data HTTP/1.1" 500 752
[2023-09-30 16:44:33,875] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1241445
[2023-09-30 16:44:33,876] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 16:44:35,625] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 16:44:35,645] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 16:44:38,155] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 16:44:38,155] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 16:44:38,155] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 16:44:38,155] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 16:44:38,155] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.2163853645324707 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 16:44:59,755] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 16:44:59,755] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 16:45:00,370] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 16:45:00,372] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 16:45:00,372] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 16:45:00,395] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 16:45:00,395] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 16:45:00,395] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 16:45:00,395] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 16:45:00,395] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 16:45:00,395] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 16:45:00,395] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.2518012523651123 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(461598720, False), (24576, False), (24576, False)] 
[2023-09-30 16:45:04,773] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 16:45:04,774] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:45:04,774] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 8.85 GB, percent = 7.0%
[2023-09-30 16:45:07,478] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 16:45:07,479] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:45:07,479] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 14.33 GB, percent = 11.4%
[2023-09-30 16:45:07,479] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 16:45:08,136] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 16:45:08,136] [INFO] [utils.py:786:see_memory_usage] MA 1.97 GB         Max_MA 1.97 GB         CA 1.97 GB         Max_CA 2 GB 
[2023-09-30 16:45:08,137] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 14.33 GB, percent = 11.4%
[2023-09-30 16:45:08,152] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 16:45:08,152] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 16:45:08,152] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f9b22e35b50>
[2023-09-30 16:45:08,152] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 16:45:08,153] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 16:45:08,153] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 16:45:08,153] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 16:45:08,153] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 16:45:08,153] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9b2d706d30>
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 16:45:08,154] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 16:45:08,155] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 16:45:08,156] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 16:45:08,156] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0003788471221923828 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

{'input_ids': [65535, 10444, 65530, 65532, 11164, 11164, 6134, 2058, 59, 33, 11640, 17772, 13091, 15446, 15898, 14734, 19137, 11957, 10260, 11957, 19156, 12605, 12605, 12605, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 13091, 14734, 19137, 11640, 17772, 13091, 15446, 15898, 14734, 19137, 12605, 16503, 12848, 17363, 10464, 11640, 17772, 13191, 13064, 10560, 17147, 11086, 14446, 10250, 10349, 18431, 15898, 10080, 65535, 261, 11111, 11111, 65535], 'attention_mask': None}
[2023-09-30 16:45:46,789] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2023-09-30 16:45:46,789] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-30 16:45:46,789] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-30 16:45:46,790] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2023-09-30 16:45:46,790] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
127.0.0.1 - - [30/Sep/2023 16:45:54] "POST /train/tx-data HTTP/1.1" 200 27
[2023-09-30 17:14:02,124] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1242055
[2023-09-30 17:14:02,126] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 18:36:34,507] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 18:36:34,527] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 18:36:37,056] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 18:36:37,056] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 18:36:37,056] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 18:36:37,056] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 18:36:37,056] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.662330150604248 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 18:37:32,089] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 18:37:32,089] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 18:37:35,898] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 18:37:35,901] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 18:37:35,901] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 18:37:35,940] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 18:37:35,940] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 18:37:35,940] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 18:37:35,940] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 18:37:35,940] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 18:37:35,940] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 18:37:35,940] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.2947425842285156 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-09-30 18:38:03,297] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 18:38:03,298] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 18:38:03,299] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.43 GB, percent = 17.8%
[2023-09-30 18:38:24,130] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 18:38:24,131] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 18:38:24,131] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.6 GB, percent = 49.0%
[2023-09-30 18:38:24,131] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 18:38:26,232] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 18:38:26,233] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 18:38:26,234] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.61 GB, percent = 49.0%
[2023-09-30 18:38:26,268] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 18:38:26,268] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 18:38:26,268] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f55adb2c640>
[2023-09-30 18:38:26,268] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 18:38:26,269] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 18:38:26,270] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 18:38:26,270] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 18:38:26,270] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 18:38:26,270] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 18:38:26,270] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 18:38:26,270] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 18:38:26,270] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 18:38:26,270] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 18:38:26,270] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f55adb1e970>
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 18:38:26,271] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 18:38:26,272] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 18:38:26,273] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 18:38:26,273] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0005602836608886719 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

[2023-09-30 19:20:04,963] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1276601
[2023-09-30 19:20:04,964] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 19:20:43,073] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 19:20:43,093] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 19:20:45,640] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 19:20:45,640] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 19:20:45,640] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 19:20:45,640] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 19:20:45,640] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.627546072006226 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 19:21:36,486] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 19:21:36,486] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 19:21:40,429] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 19:21:40,431] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 19:21:40,431] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 19:21:40,470] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 19:21:40,471] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 19:21:40,471] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 19:21:40,471] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 19:21:40,471] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 19:21:40,471] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 19:21:40,471] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.29407310485839844 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-09-30 19:22:15,587] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 19:22:15,588] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:22:15,588] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.37 GB, percent = 17.8%
[2023-09-30 19:22:36,301] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 19:22:36,302] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:22:36,303] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.59 GB, percent = 49.0%
[2023-09-30 19:22:36,303] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 19:22:38,390] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 19:22:38,391] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:22:38,391] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.59 GB, percent = 49.0%
[2023-09-30 19:22:38,420] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 19:22:38,421] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 19:22:38,421] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fbd700ea460>
[2023-09-30 19:22:38,421] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 19:22:38,422] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 19:22:38,422] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 19:22:38,422] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fbd6ff99460>
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 19:22:38,423] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 19:22:38,424] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 19:22:38,425] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 19:22:38,425] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0005447864532470703 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

192.168.0.103 - - [30/Sep/2023 19:23:50] "POST /inference/by-inf HTTP/1.1" 404 754
====== None



<|me|><|request|>
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>
<|over|>


Traceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/app.py", line 162, in inference_by_org
    msg = inferencer.generate(rwkv_rnn,msg)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/models/inference_helper.py", line 125, in generate
    logits , self.state = model(token, self.state)
TypeError: 'NoneType' object is not callable
192.168.0.103 - - [30/Sep/2023 19:24:33] "POST /inference/by-org HTTP/1.1" 500 759
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 12134.26it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 325782.05it/s]
127.0.0.1 - - [30/Sep/2023 19:25:55] "POST /inference/load-model HTTP/1.1" 200 26
====== None



<|me|><|request|>
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>
<|over|>


```python
def is_palindrome(string):
    """Check if a string is a palindrome."""
    return string == string[::-1]
```

## 


```console
True
```

## 

*   [Python Cookbook](https://docs.python.org/3/library/index.html)
*   [Learn Python the Hard Way](https://www. ZhaoxuQiu.com/Books/LearnPythonTheHardWay/)
*   [Automate the Boring Stuff with Python](https://automatetheboringstudio.withsm.com/)
*   [Automate the Boring Stuff with Python, Second Edition](https://automatetheboringstudio.withsm.com/)
*   [Automate the Boring Stuff with Python, Third Edition](https://automatetheboringstudio.withsm.com/)
*   [Automate the Boring Stuff with Python, Fourth Edition](https://automatetheboringstudio.withsm.com/)
*   [Automate the Boring Stuff with Python, FifthTraceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1765, in wrapper
    json_response = dumps(rv)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/json/__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type Message is not JSON serializable
192.168.0.103 - - [30/Sep/2023 19:26:23] "POST /inference/by-org HTTP/1.1" 500 759
[2023-09-30 19:29:14,213] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1289811
[2023-09-30 19:29:14,214] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 19:30:06,300] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 19:30:06,320] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 19:30:08,816] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 19:30:08,816] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 19:30:08,816] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 19:30:08,816] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 19:30:08,816] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.694763660430908 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 19:30:59,515] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 19:30:59,515] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 19:31:06,311] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 19:31:06,313] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 19:31:06,313] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 19:31:06,352] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 19:31:06,352] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 19:31:06,352] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 19:31:06,352] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 19:31:06,352] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 19:31:06,352] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 19:31:06,352] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.29123592376708984 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-09-30 19:31:30,257] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 19:31:30,258] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:31:30,258] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.4 GB, percent = 17.8%
[2023-09-30 19:31:50,752] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 19:31:50,753] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:31:50,753] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.59 GB, percent = 49.0%
[2023-09-30 19:31:50,753] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 19:31:52,863] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 19:31:52,864] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:31:52,864] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.59 GB, percent = 49.0%
[2023-09-30 19:31:52,890] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 19:31:52,891] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 19:31:52,891] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f648cba3700>
[2023-09-30 19:31:52,891] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 19:31:52,892] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 19:31:52,892] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 19:31:52,892] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f648ca081c0>
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 19:31:52,893] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 19:31:52,894] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 19:31:52,895] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 19:31:52,896] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 19:31:52,896] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00044608116149902344 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

====== None
Traceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/app.py", line 161, in inference_by_org
    while token[-1] in [0,65535,261,11]:
NameError: name 'token' is not defined
192.168.0.103 - - [30/Sep/2023 19:32:08] "POST /inference/by-org HTTP/1.1" 500 759
[2023-09-30 19:32:54,013] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1292675
[2023-09-30 19:32:54,014] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 19:33:36,996] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 19:33:37,016] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 19:33:39,543] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 19:33:39,543] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 19:33:39,543] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 19:33:39,543] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 19:33:39,543] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.655505657196045 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 19:34:32,972] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 19:34:32,973] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 19:34:36,763] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 19:34:36,765] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 19:34:36,766] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 19:34:36,805] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 19:34:36,805] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 19:34:36,805] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 19:34:36,805] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 19:34:36,805] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 19:34:36,805] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 19:34:36,806] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.3257639408111572 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-09-30 19:35:03,929] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 19:35:03,930] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:35:03,930] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.38 GB, percent = 17.8%
[2023-09-30 19:35:24,625] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 19:35:24,626] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:35:24,626] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.61 GB, percent = 49.0%
[2023-09-30 19:35:24,627] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 19:35:25,506] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 19:35:25,507] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:35:25,507] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.61 GB, percent = 49.0%
[2023-09-30 19:35:25,541] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 19:35:25,541] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 19:35:25,541] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f03a8cfe6a0>
[2023-09-30 19:35:25,541] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 19:35:25,542] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 19:35:25,543] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 19:35:25,543] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 19:35:25,543] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 19:35:25,543] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 19:35:25,543] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 19:35:25,543] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 19:35:25,543] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 19:35:25,543] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 19:35:25,543] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 19:35:25,543] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f03a8b83460>
[2023-09-30 19:35:25,543] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 19:35:25,544] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 19:35:25,545] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 19:35:25,546] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 19:35:25,546] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 19:35:25,546] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 19:35:25,546] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 19:35:25,546] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 19:35:25,546] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 19:35:25,546] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 19:35:25,546] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 19:35:25,546] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 19:35:25,546] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 19:35:25,546] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0004334449768066406 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

====== None
Traceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/app.py", line 165, in inference_by_org
    msg = inferencer.generate(rwkv_rnn,msg)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/models/inference_helper.py", line 125, in generate
    logits , self.state = model(token, self.state)
TypeError: 'NoneType' object is not callable
192.168.0.103 - - [30/Sep/2023 19:36:30] "POST /inference/by-org HTTP/1.1" 500 759
  0%|          | 0/582 [00:00<?, ?it/s]  1%|         | 8/582 [00:02<02:24,  3.97it/s]100%|| 582/582 [00:02<00:00, 288.18it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 567166.57it/s]
127.0.0.1 - - [30/Sep/2023 19:37:00] "POST /inference/load-model HTTP/1.1" 200 26
====== None
```python
def is_palindrome(string):
    """Check if a string is a palindrome."""
    return string == string[::-1]
```

## 


```console
True
```

## 

*   [PyCharm](https://www.jetbrains.com/pycharm/): Integrated Development Environment for Python, with syntax highlighting and debugging tools.
*   [PyCharm IDE](https://www.jetbrains.com/pycharm/): Integrated development environment for Python, with syntax highlighting and debugging tools, built on top of [JetBrains](https://www.jetbrains.com/)' IntelliJ IDEA productivity suite.
*   [PyCharm IDE tutorial](https://www.jetbrains.com/pycharm/tutorial/): Learn how to use PyCharm to write plugins for IntelliJ IDEA, including plugin architecture and plugin API reference documentation.
*   [PyCharm tutorial](https://www.jetbrains.com/pycharm/tutorial/): Learn how to use PyCharm to write plugins for IntelliJ IDEA,192.168.0.103 - - [30/Sep/2023 19:38:07] "POST /inference/by-org HTTP/1.1" 200 4068
====== None
## @ @

[@TimZhangLLL](https://github.com/TimZhangLLL)

[@TimZhangLLL](https://github.com/TimZhangLLL)

[@TimZhangLLL](https://github.com/TimZhangLLL)

[@TimZhangLLL](https://github.com/TimZhangLLL)

[@TimZhangLLL](https://github.com/TimZhangLLL)

[@TimZhangLLL](https://github.com/TimZhangLLL)192.168.0.103 - - [30/Sep/2023 19:38:38] "POST /inference/by-org HTTP/1.1" 200 4511
192.168.0.103 - - [30/Sep/2023 19:44:24] "POST /inference/by-org-no-state HTTP/1.1" 404 772
[2023-09-30 19:45:07,340] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1293729
[2023-09-30 19:45:07,341] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 19:46:07,284] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 19:46:10,131] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 19:46:12,598] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 19:46:12,598] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 19:46:12,598] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 19:46:12,598] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 19:46:12,598] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.585330009460449 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 19:47:03,163] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 19:47:03,164] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 19:47:09,893] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 19:47:09,895] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 19:47:09,895] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 19:47:09,934] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 19:47:09,934] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 19:47:09,934] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 19:47:09,935] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 19:47:09,935] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 19:47:09,935] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 19:47:09,935] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.2807796001434326 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-09-30 19:47:33,836] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 19:47:33,837] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:47:33,837] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.39 GB, percent = 17.8%
[2023-09-30 19:47:54,737] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 19:47:54,738] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:47:54,738] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.61 GB, percent = 49.0%
[2023-09-30 19:47:54,738] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 19:47:56,842] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 19:47:56,843] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:47:56,843] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.61 GB, percent = 49.0%
[2023-09-30 19:47:56,871] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 19:47:56,871] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 19:47:56,871] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fca0f8de8e0>
[2023-09-30 19:47:56,871] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 19:47:56,872] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fca0f6da9d0>
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 19:47:56,873] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 19:47:56,874] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 19:47:56,875] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 19:47:56,876] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 19:47:56,876] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 19:47:56,876] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 19:47:56,876] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0005488395690917969 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

====== None
org-mode:
1.  org-mode , C-c C-x 
2.  Shift+C-x  org mode
3.  Ctrl+C (Windows)  Command+C (Mac OS X)  cmd window,  org mode :
``` elisp
   (printf "\\n\\nOrg mode: \\n%s\\n"\\org_mode)
 Ctrl+V (Windows)  Command+V (Mac OS X) 
192.168.0.103 - - [30/Sep/2023 19:49:21] "POST /inference/by-org-no-state HTTP/1.1" 200 4261
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 32374.70it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 553157.70it/s]
====== None
Traceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/app.py", line 171, in inference_by_org
    msg = inferencer.generate(rwkv_rnn,msg)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/models/inference_helper.py", line 128, in generate
    logits[n] = -float('inf')
IndexError: list assignment index out of range
192.168.0.103 - - [30/Sep/2023 19:49:38] "POST /inference/by-org HTTP/1.1" 500 759
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 28787.74it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 564282.23it/s]
127.0.0.1 - - [30/Sep/2023 19:51:31] "POST /inference/load-model HTTP/1.1" 200 26
====== None
Traceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/app.py", line 171, in inference_by_org
    msg = inferencer.generate(rwkv_rnn,msg)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/models/inference_helper.py", line 128, in generate
    logits[n] = -float('inf')
IndexError: list assignment index out of range
192.168.0.103 - - [30/Sep/2023 19:51:42] "POST /inference/by-org HTTP/1.1" 500 759
192.168.0.103 - - [30/Sep/2023 19:52:29] "POST /train/by-org HTTP/1.1" 404 746
[2023-09-30 19:55:49,257] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1297923
[2023-09-30 19:55:49,258] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 19:56:03,785] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 19:56:03,805] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 19:56:06,318] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 19:56:06,319] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 19:56:06,319] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 19:56:06,319] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 19:56:06,319] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.607452154159546 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 19:56:59,396] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 19:56:59,396] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 19:57:02,062] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 19:57:02,064] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 19:57:02,064] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 19:57:02,103] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 19:57:02,103] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 19:57:02,103] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 19:57:02,103] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 19:57:02,103] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 19:57:02,103] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 19:57:02,103] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.3096616268157959 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-09-30 19:57:24,706] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 19:57:24,707] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:57:24,708] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.38 GB, percent = 17.8%
[2023-09-30 19:57:44,875] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 19:57:44,876] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:57:44,876] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.6 GB, percent = 49.0%
[2023-09-30 19:57:44,876] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 19:57:45,743] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 19:57:45,744] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 19:57:45,744] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.6 GB, percent = 49.0%
[2023-09-30 19:57:45,765] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 19:57:45,765] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 19:57:45,765] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fa1fb839f40>
[2023-09-30 19:57:45,765] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 19:57:45,766] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 19:57:45,766] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 19:57:45,766] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa1fb839640>
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 19:57:45,767] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 19:57:45,768] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 19:57:45,769] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 19:57:45,769] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.000385284423828125 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

  0%|          | 0/582 [00:00<?, ?it/s]  1%|         | 8/582 [00:00<00:46, 12.41it/s]100%|| 582/582 [00:00<00:00, 899.80it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 526663.41it/s]
====== None
=== [16503, 13685, 17097, 17688, 11124, 15478, 15850, 14734, 17109, 10760, 11, 65530, 65532, 12605, 17879, 16503, 10464, 10988, 15641, 14931, 12314, 10267, 14610, 19137, 12176, 12605, 10708, 10250, 17303, 11975, 16735, 11, 65535, 261, 65530, 65534, 11685, 14734, 19137, 12605, 10423, 11986, 16707, 10708, 10250, 17303, 11975, 16735, 10080, 10260, 17141, 19137, 12605, 17879, 16503, 13179, 11632, 14734, 10529, 12445, 13234, 12176, 10847, 12605, 13250, 12410, 28329, 65535, 261, 65530, 65532, 10464, 17879, 16503, 10373, 10303, 10529, 12445, 19156, 11, 65535, 261, 65530, 65534, 18117, 10658, 19137, 12605, 17879, 16503, 14862, 17222, 17148, 17303, 11975, 16735, 14734, 15261, 11496, 10080, 13091, 15034, 12206, 11975, 16735, 10079, 12472, 14638, 11975, 16735, 17147, 13091, 10687, 10390, 15261, 11496, 14734, 11975, 16735, 19156, 11, 65535, 261, 65530, 65532, 12605, 16667, 10292, 13091, 15034, 12206, 11975, 16735, 11, 65535, 261, 65530, 65534, 11685, 14734, 19137, 12605, 10423, 10292, 10464, 12848, 10494, 13179, 11632, 15478, 15907, 10080, 11454, 17148, 17303, 11975, 16735, 10285, 19137, 10293, 10370, 10678, 13091, 10250, 10447, 11043, 11017, 11889, 12363, 18295, 2453, 10788, 10348, 18005, 14734, 12201, 17093, 10460, 11920, 10080, 10390, 11454, 10708, 10460, 17141, 15052, 10285, 11002, 10792, 10333, 13279, 15033, 14987, 15036, 10838, 17387, 14734, 12325, 11168, 19137, 12266, 11726, 10588, 10250, 10349, 11656, 12418, 14734, 13370, 10080, 17148, 10349, 13370, 11574, 16672, 10390, 15752, 11634, 14791, 16519, 13203, 13234, 11124, 17141, 10985, 19137, 12202, 10265, 16056, 12348, 10333, 13279, 15033, 14987, 11656, 14734, 10838, 17387, 28329, 65535, 261, 65530, 65532, 15752, 12848, 10494, 16721, 10293, 10370, 10678, 14734, 11630, 16815, 14332, 12333, 11124, 15727, 13126, 10529, 12445, 11053, 19156, 11, 65535, 261, 65530, 65534, 11685, 14734, 19137, 12605, 10423, 10292, 10464, 12848, 10494, 16721, 10293, 10370, 10678, 14734, 14782, 10684, 10529, 12445, 10080, 18117, 10658, 19137, 17148, 10447, 10293, 10370, 10678, 11043, 11017, 11889, 12363, 14179, 2453, 12163, 14332, 11979, 19137, 10390, 13091, 10250, 10447, 12201, 17093, 13191, 10292, 14734, 10460, 11920, 10080, 10687, 13557, 19137, 10390, 13191, 10250, 11647, 13833, 11930, 14734, 18438, 10997, 11124, 13913, 17808, 14734, 14806, 14815, 19137, 15494, 10370, 14614, 10258, 13913, 10799, 14734, 10951, 16803, 10080, 13188, 11044, 19137, 10390, 12421, 13091, 15091, 14808, 12348, 10452, 10079, 10296, 13579, 12348, 10452, 19137, 15494, 10370, 14614, 10258, 15895, 11685, 14734, 15162, 10250, 10951, 16803, 28329, 65535, 261, 65530, 65532]
==== < Message
id: (1447)
role: (text)
text: ()
prefix: ()
postfix: ()
prefix_token: ([])
temperature: (0.1)
top_p: (0.1)
top_k: (0)
alpha_frequency: (0.45)
alpha_presence: (0.45)
alpha_decay: (0.996)
token_ban: ([0])
token_stop: ([65530, 65531, 65532, 65533, 65534, 65535])
chunk_len: (128)
token_count: (256)
over: (True)
scene_id: (507)
response: ()
generated: (False)
to_train: (True)
ctx: (2048)
ctx_fix: (256)
tokens: ([16503, 13685, 17097, 17688, 11124, 15478, 15850, 14734, 17109, 10760, 11, 65530, 65532, 12605, 17879, 16503, 10464, 10988, 15641, 14931, 12314, 10267, 14610, 19137, 12176, 12605, 10708, 10250, 17303, 11975, 16735, 11, 65535, 261, 65530, 65534, 11685, 14734, 19137, 12605, 10423, 11986, 16707, 10708, 10250, 17303, 11975, 16735, 10080, 10260, 17141, 19137, 12605, 17879, 16503, 13179, 11632, 14734, 10529, 12445, 13234, 12176, 10847, 12605, 13250, 12410, 28329, 65535, 261, 65530, 65532, 10464, 17879, 16503, 10373, 10303, 10529, 12445, 19156, 11, 65535, 261, 65530, 65534, 18117, 10658, 19137, 12605, 17879, 16503, 14862, 17222, 17148, 17303, 11975, 16735, 14734, 15261, 11496, 10080, 13091, 15034, 12206, 11975, 16735, 10079, 12472, 14638, 11975, 16735, 17147, 13091, 10687, 10390, 15261, 11496, 14734, 11975, 16735, 19156, 11, 65535, 261, 65530, 65532, 12605, 16667, 10292, 13091, 15034, 12206, 11975, 16735, 11, 65535, 261, 65530, 65534, 11685, 14734, 19137, 12605, 10423, 10292, 10464, 12848, 10494, 13179, 11632, 15478, 15907, 10080, 11454, 17148, 17303, 11975, 16735, 10285, 19137, 10293, 10370, 10678, 13091, 10250, 10447, 11043, 11017, 11889, 12363, 18295, 2453, 10788, 10348, 18005, 14734, 12201, 17093, 10460, 11920, 10080, 10390, 11454, 10708, 10460, 17141, 15052, 10285, 11002, 10792, 10333, 13279, 15033, 14987, 15036, 10838, 17387, 14734, 12325, 11168, 19137, 12266, 11726, 10588, 10250, 10349, 11656, 12418, 14734, 13370, 10080, 17148, 10349, 13370, 11574, 16672, 10390, 15752, 11634, 14791, 16519, 13203, 13234, 11124, 17141, 10985, 19137, 12202, 10265, 16056, 12348, 10333, 13279, 15033, 14987, 11656, 14734, 10838, 17387, 28329, 65535, 261, 65530, 65532, 15752, 12848, 10494, 16721, 10293, 10370, 10678, 14734, 11630, 16815, 14332, 12333, 11124, 15727, 13126, 10529, 12445, 11053, 19156, 11, 65535, 261, 65530, 65534, 11685, 14734, 19137, 12605, 10423, 10292, 10464, 12848, 10494, 16721, 10293, 10370, 10678, 14734, 14782, 10684, 10529, 12445, 10080, 18117, 10658, 19137, 17148, 10447, 10293, 10370, 10678, 11043, 11017, 11889, 12363, 14179, 2453, 12163, 14332, 11979, 19137, 10390, 13091, 10250, 10447, 12201, 17093, 13191, 10292, 14734, 10460, 11920, 10080, 10687, 13557, 19137, 10390, 13191, 10250, 11647, 13833, 11930, 14734, 18438, 10997, 11124, 13913, 17808, 14734, 14806, 14815, 19137, 15494, 10370, 14614, 10258, 13913, 10799, 14734, 10951, 16803, 10080, 13188, 11044, 19137, 10390, 12421, 13091, 15091, 14808, 12348, 10452, 10079, 10296, 13579, 12348, 10452, 19137, 15494, 10370, 14614, 10258, 15895, 11685, 14734, 15162, 10250, 10951, 16803, 28329, 65535, 261, 65530, 65532]) 
>

Traceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/app.py", line 173, in inference_by_org
    msg = inferencer.generate(rwkv_rnn,msg)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/models/inference_helper.py", line 128, in generate
    logits[n] = -float('inf')
IndexError: list assignment index out of range
192.168.0.103 - - [30/Sep/2023 19:58:19] "POST /inference/by-org HTTP/1.1" 500 759
====== None
====== None
====== None
==== [<models.org_text.DataNode object at 0x7fa16e4508b0>]
==== [11687, 10454, 16056, 11001, 7451, 1802, 267, 25028, 34043, 14734, 10285, 14734, 10696, 11922, 19156, 11, 5686, 1511, 70, 33, 10464, 11021, 10399, 10482, 14589, 10399, 10258, 4515, 7917, 33, 10397, 14875, 16056, 11001, 3784, 6908, 267, 25028, 34043, 33, 14734, 10696, 11922, 59, 11, 6884, 7441, 2191, 11, 41, 49026, 46, 26081, 283, 42, 11, 6884, 11, 97, 49026, 46, 26081, 97, 33, 10762, 13004, 13191, 10278, 10283, 10988, 13004, 59, 11, 46, 283, 33, 16417, 14966, 16056, 11001, 13188, 17145, 14734, 31213, 11, 46, 33, 13580, 13004, 16417, 14966, 16056, 11001, 13179, 13057, 14734, 31213, 11, 12631, 10399, 10292, 10333, 11016, 16056, 11001, 13188, 13034, 14734, 59300, 33, 10696, 11922, 45, 12605, 10402, 10427, 10673, 283, 33, 10953, 11021, 28329, 10487, 11687, 59, 11, 6884, 7441, 2191, 11, 41, 8079, 3421, 27139, 275, 49026, 46, 26081, 283, 5292, 11, 41, 49410, 269, 57956, 51308, 59, 272, 116, 35, 32224, 494, 11, 6884, 11, 17148, 13619, 10397, 14875, 10423, 16056, 11001, 59300, 33, 10696, 11922, 45, 12202, 13098, 14966, 11454, 13855, 12445, 13314, 10285, 28329, 10464, 10322, 11021, 10399, 12669, 59300, 33, 10696, 11922, 16914, 10577, 15494, 10250, 10283, 11003, 17387, 45, 14221, 11044, 17149, 16403, 14782, 10684, 14734, 11621, 14486, 28329, 12631, 10399, 45, 10482, 14589, 331, 49026, 46, 26081, 283, 97, 33, 11994, 11021, 10399, 11454, 3784, 6908, 20299, 113, 33, 10397, 14875, 10285, 16056, 11001, 3784, 6908, 33, 14734, 59300, 33, 10696, 11922, 28329, 10482, 14589, 10373, 10303, 7441, 2191, 10762, 13004, 15752, 11634, 11903, 14446, 11454, 7451, 1802, 10285, 14734, 8401, 46, 26267, 10258, 17177, 10285, 13279, 10283, 8760, 46, 27187, 19156, 520, 19589, 1511, 70, 33, 11021, 10399, 10482, 14589, 22236, 46, 34200, 33, 11117, 10398, 13234, 17177, 10285, 53048, 28329, 12750, 17644, 15487, 11899, 59, 11, 68, 46, 100, 332, 275, 8401, 46, 34200, 42, 11, 14589, 13764, 59, 11, 50, 47, 33, 10659, 13308, 12983, 11454, 8760, 27187, 14734, 13308, 18025, 10257, 11, 51, 47, 33, 12750, 302, 46, 100, 332, 11, 52, 47, 33, 17148, 11967, 17177, 10285, 16721, 8760, 27187, 10258, 14734, 12631, 13191, 10696, 11922, 11, 10464, 10322, 11021, 10399, 12748, 11899, 17879, 16503, 12357, 14445, 14734, 13557, 13004, 13234, 17177, 10285, 10260, 11042, 15451, 10790, 14734, 10696, 11922, 59, 11, 46, 302, 46, 100, 332, 284, 280, 33, 17177, 10285, 12307, 10808, 16403, 11, 46, 302, 46, 100, 332, 285, 280, 33, 17177, 10285, 12307, 10808, 15907, 14172, 41, 13308, 18025, 10992, 11855, 15907, 14172, 42, 11, 46, 302, 46, 100, 332, 286, 280, 33, 17177, 10285, 12307, 10808, 15907, 14172, 14734, 14302, 15907, 14172, 11, 46, 33, 15169, 15169, 11, 12631, 10399, 45, 11454, 22236, 46, 26267, 33, 10258, 17177, 10285, 13279, 10283, 53048, 45, 10464, 11021, 10399, 59, 11, 50, 47, 33, 10659, 13308, 12983, 11454, 16721, 53048, 33, 14734, 13308, 18025, 10257, 11, 51, 47, 33, 16749, 14589, 4398, 8401, 46, 34200, 313, 512, 10762, 13004, 45, 79, 33, 10292, 16503, 12357, 14445, 14734, 13557, 13004, 45, 10399, 17177, 10285, 16721, 11855, 13315, 11, 10487, 11687, 59, 11, 6884, 4515, 7917, 11, 41, 8401, 46, 34200, 285, 42, 267, 60, 33, 17177, 10285, 12307, 10808, 15907, 14172, 41, 11855, 13315, 42, 11, 6884, 11, 12159, 13197, 17148, 15752, 12176, 10847, 10464, 34, 24210, 11]
[2023-09-30 19:59:48,108] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2023-09-30 19:59:48,108] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-30 19:59:48,108] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-30 19:59:48,109] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2023-09-30 19:59:48,109] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
-> item_loss 2.1173954010009766 batch_loss 2.1173954010009766
192.168.0.103 - - [30/Sep/2023 20:00:04] "POST /train/by-org HTTP/1.1" 200 28
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 30457.22it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 534622.19it/s]
127.0.0.1 - - [30/Sep/2023 20:03:43] "POST /inference/load-model HTTP/1.1" 200 26
None
None
127.0.0.1 - - [30/Sep/2023 20:03:43] "POST /state/reset HTTP/1.1" 200 21

<|over|>

<|me|><|request|>Question: 
<|over|>

<|me|><|response|>Answer: 
1. Ilica de la Reyna
2. Jazmin Harry Potter-Crabble-Kreacher
3. Fezco de la Reyna
4. Fernando de la Reyna
5. Fernando de la Reyna
6. Fernando de la Reyna
7. Fernando de la Reyna
8. 127.0.0.1 - - [30/Sep/2023 20:04:02] "POST /inference/generate HTTP/1.1" 200 3960
tensor([[ 1.1239e-01,  6.5617e-03,  1.8517e-01,  ...,  1.2466e-01,
          1.9486e-01,  1.4709e-01],
        [ 1.8481e-01,  1.7514e-02, -1.2661e-02,  ...,  8.3811e-01,
          1.9648e-01,  6.7376e-02],
        [-5.3432e+00, -4.5233e+00, -7.8453e-01,  ...,  1.3372e-01,
         -1.0569e-01, -6.6466e-01],
        ...,
        [-2.6437e-01, -3.1639e+01,  4.9620e+01,  ..., -4.7746e+00,
          2.7792e+00, -6.9258e+00],
        [ 1.0281e+00,  2.2015e+01,  1.6767e+01,  ...,  1.0229e+00,
          1.1468e+00,  1.2015e+00],
        [ 3.4083e+00,  7.3872e+00,  6.2586e+00,  ...,  1.7435e+01,
          1.3982e+00,  4.2004e+00]], device='cuda:0')
None
127.0.0.1 - - [30/Sep/2023 20:04:03] "POST /state/reset HTTP/1.1" 200 21

: 
1. 
2. 
3. 
4. 
5. : : 
: ?
: H.P.1923The Wonderful Works of God: 
: ?
: 127.0.0.1 - - [30/Sep/2023 20:05:57] "POST /inference/generate-no-state HTTP/1.1" 200 4281
[2023-09-30 20:07:32,097] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1300887
[2023-09-30 20:07:32,098] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 20:11:05,456] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 20:11:06,619] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 20:11:09,108] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 20:11:09,108] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 20:11:09,108] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 20:11:09,109] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 20:11:09,109] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.635452032089233 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 20:11:59,965] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 20:11:59,966] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 20:12:07,632] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 20:12:07,635] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 20:12:07,635] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 20:12:07,675] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 20:12:07,675] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 20:12:07,675] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 20:12:07,675] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 20:12:07,675] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 20:12:07,675] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 20:12:07,676] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.30291295051574707 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-09-30 20:12:34,014] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 20:12:34,015] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 20:12:34,015] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.39 GB, percent = 17.8%
[2023-09-30 20:12:56,506] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 20:12:56,507] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 20:12:56,507] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.61 GB, percent = 49.0%
[2023-09-30 20:12:56,507] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 20:12:57,491] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 20:12:57,492] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 20:12:57,492] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.61 GB, percent = 49.0%
[2023-09-30 20:12:57,517] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 20:12:57,517] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 20:12:57,517] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fb6c2f18040>
[2023-09-30 20:12:57,518] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 20:12:57,519] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 20:12:57,519] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 20:12:57,519] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 20:12:57,519] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 20:12:57,519] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 20:12:57,519] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 20:12:57,519] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 20:12:57,519] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb6c2d27040>
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 20:12:57,520] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 20:12:57,521] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 20:12:57,522] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 20:12:57,522] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 20:12:57,522] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 20:12:57,522] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 20:12:57,522] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 20:12:57,522] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 20:12:57,522] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 20:12:57,522] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 20:12:57,522] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 20:12:57,522] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 20:12:57,522] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00042128562927246094 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 20772.19it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 575184.95it/s]
==== [65530, 65531, 12656, 14035, 10250, 10283, 15847, 11805, 10452, 15543, 17108, 19137, 13326, 12797, 27370, 14734, 17109, 10673, 15494, 10760, 15543, 17108, 11044, 14734, 13012, 13205, 11, 65535, 261, 65530, 65532, 12605, 17879, 16503, 10464, 12927, 10708, 10684, 10339, 13631, 11795, 12678, 10691, 14782, 10684, 14734, 14862, 16696, 14734, 13012, 15131, 11, 65535, 261, 65530, 65534, 10399, 10258, 13091, 10250, 10283, 10487, 11855, 59, 267, 11454, 17141, 10985, 14734, 10745, 10917, 12201, 17384, 45, 33, 13631, 11795, 10684, 15304, 10250, 14781, 11622, 11002, 10370, 10402, 14734, 10684, 13773, 10080, 17148, 10293, 16503, 13091, 11416, 10292, 45, 33, 13631, 10366, 11454, 12395, 11858, 11124, 10358, 11044, 14734, 17268, 10745, 10283, 13190, 17384, 12665, 12701, 10333, 12140, 11638, 14734, 14583, 10358, 10838, 10409, 10843, 45, 33, 11042, 13064, 10322, 12665, 11002, 10333, 12140, 11638, 14734, 15287, 14987, 10966, 10838, 10080, 11684, 17879, 16503, 14240, 18004, 11795, 10650, 45, 33, 11042, 13064, 17147, 16503, 10692, 18004, 12137, 10460, 11124, 11871, 10323, 28329, 11454, 17148, 15033, 12484, 10724, 10258, 45, 33, 13631, 10366, 14734, 17177, 12731, 10293, 16503, 13191, 10278, 15033, 59, 33, 10250, 15033, 13091, 17177, 12731, 11454, 11920, 10285, 15847, 15494, 15847, 16956, 45, 33, 11013, 10250, 15033, 10779, 13091, 17177, 12731, 11454, 10913, 17796, 12824, 11002, 12686, 14486, 28329, 13051, 16686, 17177, 12731, 11187, 15033, 13036, 13764, 45, 33, 13631, 10366, 17311, 17919, 10288, 14808, 12140, 11638, 14734, 10966, 10838, 10080, 11684, 17879, 16503, 14240, 18004, 11795, 10650, 45, 33, 11042, 13064, 17147, 16503, 10692, 18004, 12137, 10460, 11124, 11871, 10323, 10080, 11684, 12368, 18001, 10260, 13032, 11459, 11871, 10323, 13034, 14862, 16696, 11124, 12666, 15752, 45, 33, 11042, 13064, 10322, 12159, 13197, 15752, 11634, 12348, 10792, 14970, 10423, 14734, 16667, 11021, 10080, 11454, 17148, 10283, 17141, 15052, 10285, 45, 33, 11684, 17919, 10288, 14808, 12140, 11638, 14734, 10966, 10838, 28329, 11687, 13258, 17177, 12731, 11454, 11920, 10285, 15847, 15494, 15847, 16956, 45, 65535, 261, 65530, 65534, 10399, 10258, 13091, 10250, 10283, 15486, 17141, 15543, 17108, 11044, 14734, 14314, 13205, 59, 11, 13631, 11795, 17734, 37881, 33, 13091, 12445, 12445, 14782, 10684, 14734, 10080, 17145, 10745, 10917, 12201, 45, 13631, 10366, 10402, 14734, 16530, 15898, 11622, 11002, 10684, 13773, 28329, 12395, 11858, 10992, 10780, 14583, 10358, 11044, 10745, 10283, 13190, 45, 13631, 10366, 15710, 16875, 13722, 17385, 14734, 11920, 12234, 16879, 10409, 45, 11042, 13064, 12665, 11002, 10966, 10838, 10080, 11684, 16503, 14240, 18004, 11795, 10650, 10079, 12137, 10460, 11124, 11871, 10323, 45, 17879, 16503, 11454, 17148, 10285, 17734, 11001, 12348, 12200, 16412, 28329, 13191, 10278, 15033, 13036, 12276, 11021, 10494, 17177, 12731, 59, 11, 10250, 13091, 45, 11454, 11920, 15847, 10293, 14240, 18004, 11795, 10650, 10080, 17148, 15033, 13036, 12276, 14782, 11957, 10511, 11901, 10444, 17879, 16503, 13631, 10366, 15931, 16899, 13179, 11632, 13064, 17734, 10261, 13773, 12522, 10838, 28329, 10337, 13091, 45, 11967, 11795, 10650, 10355, 15494, 10913, 17796, 14791, 12686, 10080, 13631, 10366, 11021, 10264, 13773, 12137, 10460, 11871, 10323, 45, 10444, 15931, 16899, 13179, 11632, 28329, 10260, 15202, 13631, 10366, 17177, 12731, 11187, 15033, 13036, 12276, 45, 10966, 10838, 17311, 12337, 11638, 28329, 11684, 17879, 16503, 10260, 13032, 11871, 10323, 13034, 12666, 15752, 10079, 11584, 17688, 14862, 16696, 45, 11042, 13064, 10335, 11001, 14970, 10423, 16667, 11042, 28329, 11687, 13258, 17177, 12731, 11454, 11920, 14240, 18004, 45, 13631, 10366, 17879, 16503, 59, 11, 9828, 33, 10333, 16533, 11795, 10650, 16099, 10691, 10079, 11889, 12678, 10261, 10997, 12022, 17879, 16503, 11, 9828, 33, 11871, 10323, 14660, 14662, 18009, 17771, 10261, 11533, 13205, 12686, 14486, 14862, 16696, 11, 9828, 33, 10261, 11795, 10650, 13250, 12261, 39332, 7939, 102, 33, 10342, 10846, 11, 9828, 28377, 585, 11, 11687, 13581, 12638, 15752, 16672, 13631, 11795, 37881, 33, 11454, 11795, 10650, 12604, 17688, 17141, 15052, 10285, 12746, 15511, 12299, 10901, 28329, 12421, 14734, 13234, 16735, 45, 13191, 12986, 14734, 13631, 11795, 12678, 10691, 11001, 10722, 10339, 13631, 10366, 14734, 12986, 14426, 15202, 14486, 11124, 10260, 13032, 11871, 10323, 28329, 10399, 10257, 13091, 10250, 10283, 13179, 15190, 15475, 13923, 13127, 14734, 14314, 13205, 10080, 16738, 10538, 12981, 17631, 16730, 10305, 11621, 45, 11088, 14862, 10684, 17644, 10529, 12445, 17225, 14033, 10080, 65535, 261, 65530, 65534]
:
 
 
 
,,192.168.0.103 - - [30/Sep/2023 20:14:48] "POST /inference/by-org HTTP/1.1" 200 5688
,,
192.168.0.103 - - [30/Sep/2023 20:33:26] "POST /inference/generate HTTP/1.1" 200 1230
,
USER 
ROBOT 
 USER 
 ROBOT 
USER 
192.168.0.103 - - [30/Sep/2023 20:34:19] "POST /inference/generate HTTP/1.1" 200 2114
,
192.168.0.103 - - [30/Sep/2023 20:37:40] "POST /inference/generate HTTP/1.1" 200 1265
,
192.168.0.103 - - [30/Sep/2023 20:38:31] "POST /inference/generate HTTP/1.1" 200 1265
,
192.168.0.103 - - [30/Sep/2023 20:38:59] "POST /inference/generate HTTP/1.1" 200 1265
,
192.168.0.103 - - [30/Sep/2023 20:39:15] "POST /inference/generate HTTP/1.1" 200 1265
192.168.0.103 - - [30/Sep/2023 20:41:41] "POST /inference/generate HTTP/1.1" 200 10176
192.168.0.103 - - [30/Sep/2023 20:44:24] "POST /inference/generate HTTP/1.1" 200 10175
,user
192.168.0.103 - - [30/Sep/2023 20:46:24] "POST /inference/generate HTTP/1.1" 200 5661
,user
192.168.0.103 - - [30/Sep/2023 20:46:59] "POST /inference/generate HTTP/1.1" 200 11223
:
:
,,,
, 192.168.0.103 - - [30/Sep/2023 20:48:20] "POST /inference/generate HTTP/1.1" 200 12208
AI
192.168.0.103 - - [30/Sep/2023 20:52:44] "POST /inference/generate HTTP/1.1" 200 1280
:
,,,, 
192.168.0.103 - - [30/Sep/2023 20:52:57] "POST /inference/generate HTTP/1.1" 200 2596
192.168.0.103 - - [30/Sep/2023 20:53:14] "POST /inference/generate HTTP/1.1" 200 3939
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 34985.60it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 501146.57it/s]
127.0.0.1 - - [30/Sep/2023 20:54:51] "POST /inference/load-model HTTP/1.1" 200 26
AI192.168.0.103 - - [30/Sep/2023 20:55:10] "POST /inference/generate HTTP/1.1" 200 1242
:
,,,
192.168.0.103 - - [30/Sep/2023 20:55:20] "POST /inference/generate HTTP/1.1" 200 2516
192.168.0.103 - - [30/Sep/2023 20:55:35] "POST /inference/generate HTTP/1.1" 200 3749
:
,,,
192.168.0.103 - - [30/Sep/2023 20:55:50] "POST /inference/generate HTTP/1.1" 200 4899
[2023-09-30 21:23:07,882] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 21:23:08,888] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1305551
[2023-09-30 21:23:08,890] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 21:23:12,077] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 21:23:14,549] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 21:23:14,549] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 21:23:14,549] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 21:23:14,549] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 21:23:14,549] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.703971862792969 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 21:24:05,396] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 21:24:05,396] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 21:24:12,948] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 21:24:12,950] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 21:24:12,950] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 21:24:12,989] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 21:24:12,989] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 21:24:12,989] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 21:24:12,989] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 21:24:12,989] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 21:24:12,989] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 21:24:12,989] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.2928633689880371 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-09-30 21:24:37,127] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 21:24:37,128] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 21:24:37,129] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.38 GB, percent = 17.8%
[2023-09-30 21:24:57,717] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 21:24:57,718] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 21:24:57,718] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.61 GB, percent = 49.0%
[2023-09-30 21:24:57,718] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 21:24:59,841] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 21:24:59,842] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 21:24:59,842] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.61 GB, percent = 49.0%
[2023-09-30 21:24:59,869] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 21:24:59,869] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 21:24:59,869] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f9e70b39160>
[2023-09-30 21:24:59,869] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0002, 0.00030000000000000003], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 21:24:59,871] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 21:24:59,871] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 21:24:59,871] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 21:24:59,871] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 21:24:59,871] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9e70b26160>
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 21:24:59,872] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 21:24:59,873] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 21:24:59,874] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 21:24:59,874] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0006890296936035156 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

  0%|          | 0/582 [00:00<?, ?it/s]  1%|         | 8/582 [00:01<02:23,  4.00it/s]100%|| 582/582 [00:02<00:00, 290.95it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 419574.58it/s]
bot: 
192.168.0.103 - - [30/Sep/2023 21:26:11] "POST /inference/generate HTTP/1.1" 200 1127
bot: 
192.168.0.103 - - [30/Sep/2023 21:26:34] "POST /inference/generate HTTP/1.1" 200 2369
bot: SCP-1000SCP-1000SCP-1000SCP-1000330SCP-1000SCP-1000SCP-1000192.168.0.103 - - [30/Sep/2023 21:27:15] "POST /inference/generate HTTP/1.1" 200 5304
bot: SCP-1000SCP-1000SCP-1000SCP-1000192.168.0.103 - - [30/Sep/2023 21:28:59] "POST /inference/generate HTTP/1.1" 200 1821
None
None
192.168.0.103 - - [30/Sep/2023 21:29:35] "POST /state/reset HTTP/1.1" 200 21
bot: SCP-1000
192.168.0.103 - - [30/Sep/2023 21:29:42] "POST /inference/generate HTTP/1.1" 200 3279
None
None
192.168.0.103 - - [30/Sep/2023 21:29:52] "POST /state/reset HTTP/1.1" 200 21
scp682AIscp682
192.168.0.103 - - [30/Sep/2023 21:29:55] "POST /inference/generate HTTP/1.1" 200 1518
None
None
192.168.0.103 - - [30/Sep/2023 21:37:15] "POST /state/reset HTTP/1.1" 200 21
[2023-09-30 21:37:17,537] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2023-09-30 21:37:17,537] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-30 21:37:17,537] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-30 21:37:17,537] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2023-09-30 21:37:17,537] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
192.168.0.103 - - [30/Sep/2023 21:37:36] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [30/Sep/2023 21:39:34] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [30/Sep/2023 21:39:51] "POST /train/tx-data HTTP/1.1" 200 28
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
==== [15847, 12605, 12522, 16696, 12261, 13250, 11, 65530, 65531, 17148, 13091, 65530, 14734, 15847, 14787, 19137, 17148, 17384, 14734, 65530, 11454, 17149, 16403, 10390, 15847, 12605, 16667, 14862, 13036, 17919, 14734, 13250, 12261, 28329, 65535, 261, 65530, 65534, 12605, 11043, 11017, 65530, 19137, 15669, 10270, 13091, 10250, 11043, 15036, 10325, 19137, 10270, 10457, 14299, 11685, 13091, 16530, 15898, 12656, 14035, 15643, 19137, 12605, 17147, 13191, 14488, 14543, 10079, 11889, 13849, 12705, 15169, 10790, 11043, 19137, 33, 17189, 17141, 17749, 16742, 65532, 10285, 14734, 16503, 13685, 19137, 33, 12605, 14256, 12459, 15202, 14486, 15643, 11957, 12605, 14734, 16503, 13685, 19137, 12605, 11454, 65534, 10305, 11044, 15494, 10760, 12605, 14734, 11168, 12220, 10080, 12605, 10423, 11454, 65533, 10285, 17149, 16403, 13913, 12231, 12410, 15641, 19137, 33, 10985, 16671, 16686, 28329, 65535, 261, 65530, 65531, 13191, 10370, 10423, 17728, 16944, 10464, 13091, 11060, 13091, 28316, 74, 9823, 19137, 10464, 16525, 12348, 12220, 16721, 12401, 10303, 11414, 15178, 19156, 11, 65535, 261, 65530, 65533, 12605, 16667, 10292, 14914, 11899, 65530, 13091, 11060, 13091, 10370, 12137, 13133, 15752, 19133, 820, 19134, 13091, 13644, 13051, 12522, 10304, 14734, 10080, 10460, 10292, 10250, 10283, 13133, 15752, 10452, 19137, 10684, 13773, 14734, 13091, 13133, 15752, 10452, 14734, 11159, 13327, 11124, 10370, 13327, 14734, 10590, 10675, 19137, 15646, 10260, 13091, 10399, 10339, 11885, 14734, 15033, 13046, 13234, 10784, 13032, 28329, 65535, 261, 65530, 65534, 12605, 16667, 10292, 13637, 17097, 11038, 17172, 14734, 11414, 15178, 13091, 19151, 11, 11957, 10370, 12137, 13133, 15752, 13234, 16735, 45, 17385, 16503, 14734, 13091, 10655, 10768, 10997, 12764, 10841, 15752, 11124, 10408, 10577, 45, 15646, 10260, 11454, 10339, 16443, 10775, 10768, 10292, 35, 10370, 15261, 35, 17147, 13091, 35, 13212, 11369, 35, 10080, 12605, 10402, 12220, 15674, 14216, 10339, 13133, 15752, 10452, 13205, 17038, 14734, 16403, 10292, 11124, 15752, 10838, 45, 15646, 10260, 13091, 10687, 12666, 13208, 13205, 13981, 28329, 11042, 13064, 45, 17385, 16523, 10431, 14486, 10733, 10779, 11124, 10408, 10577, 16520, 10079, 11971, 17385, 10370, 12415, 10322, 13091, 13133, 15752, 15304, 15500, 12220, 10688, 11622, 14734, 28329, 12421, 14734, 13234, 16735, 45, 10399, 10258, 10745, 14172, 13179, 17385, 16503, 59, 11, 50, 42, 33, 14015, 16956, 14589, 12626, 17879, 13685, 10079, 12848, 10494, 10408, 10577, 28329, 51, 42, 33, 17241, 11888, 13580, 12307, 12415, 11124, 17181, 13078, 12415, 10974, 10779, 28329, 52, 42, 33, 11971, 17385, 10370, 13216, 10079, 17248, 10662, 13584, 16523, 28329, 53, 42, 33, 11021, 10399, 12824, 11002, 14762, 14819, 11124, 16749, 13006, 28329, 15646, 10260, 12220, 10377, 15674, 14216, 10339, 13091, 11060, 16443, 11899, 12415, 10292, 35, 820, 35, 10080, 11016, 16503, 12605, 10402, 14734, 13133, 15752, 15304, 15500, 15752, 10452, 14446, 10399, 10257, 11159, 16884, 45, 11885, 12220, 16721, 16443, 15494, 10334, 14782, 12220, 14734, 11971, 17385, 28329, 11687, 10454, 11414, 11626, 11687, 13258, 16443, 17728, 10992, 35, 10464, 13091, 820, 11053, 799, 17148, 15261, 17728, 18025, 45, 12605, 16667, 10292, 11021, 10399, 11687, 13581, 11414, 15178, 59, 11, 35, 12605, 13091, 10250, 10283, 16689, 16664, 14589, 10339, 12848, 10494, 12176, 10847, 11124, 10529, 12445, 14734, 13133, 15752, 15304, 15500, 10080, 12605, 14734, 10293, 16503, 14778, 13308, 13091, 14015, 16956, 14589, 12626, 17879, 13685, 45, 12605, 10423, 10260, 13032, 11871, 10323, 11124, 12848, 18216, 10399, 11903, 14446, 17148, 10283, 14778, 13308, 10080, 35, 11, 11967, 17385, 14172, 12983, 11454, 10841, 15752, 11124, 14778, 13308, 10257, 45, 15646, 10260, 13091, 12666, 13208, 10768, 15261, 45, 17148, 13324, 14734, 11414, 12220, 10423, 13179, 10842, 10285, 15713, 12266, 12983, 10080, 65535, 261, 65530, 65531, 12824, 10258, 13234, 19137, 17148, 13091, 10464, 14734, 15847, 17162, 28329, 65535, 261, 65530, 65532, 10464, 13091, 16747, 19156, 10382, 15484, 10464, 15847, 12144, 28329, 65535, 261, 65530, 65534, 12605, 11017, 65530, 33, 12605, 13091, 10250, 10283, 13125, 13125, 17189, 17189, 14734, 10370, 10080, 12605, 14583, 13815, 11454, 11459, 14484, 10257, 19137, 12010, 10449, 11454, 10285, 11435, 10080, 33, 12605, 13191, 10250, 17734, 12630, 11855, 11124, 10250, 17100, 13713, 17079, 19137, 33, 17147, 13191, 10250, 10283, 11021, 14299, 14734, 11679, 10650, 10080, 33, 12605, 14734, 14583, 13815, 10655, 11903, 15646, 10265, 15597, 14015, 10080, 33, 12605, 14583, 13815, 10285, 13191, 10250, 10283, 11210, 10250, 14734, 15568, 12577, 19137, 17268, 11994, 13091, 11454, 12605, 10344, 12039, 17268, 12201, 19137, 12605, 14734, 14302, 10366, 11645, 17001, 10333, 10080, 10390, 14734, 15023, 10985, 16672, 12605, 12337, 12476, 10429, 10080, 33, 10444, 13091, 12605, 14446, 11454, 12768, 10460, 10333, 16944, 13234, 19137, 10655, 14015, 10310, 16520, 11124, 12159, 13197, 14734, 14583, 13815, 14808, 28329, 65535, 261, 65530, 65532, 12631, 10399, 10464, 10848, 10838, 14734, 11659, 13020, 10792, 14446, 11454, 19137, 11124, 10464, 14734, 15133, 12201, 10781, 10429, 13191, 10684, 11053, 19156, 11, 65535, 261, 65530, 65534, 12605, 16667, 10292, 11021, 15752, 13091, 14734, 11, 65535, 261, 65530, 65532, 11315, 45, 12605, 14486, 16533, 10460, 10292, 10250, 10283, 13191, 15133, 12201, 10781, 10429, 14734, 10370, 45, 12469, 10696, 12366, 11021, 15752, 15043, 15330, 10333, 16684, 11632, 12484, 12527, 17879, 16503, 17382, 12983, 10080, 10460, 10292, 12469, 14734, 12366, 14486, 10913, 14583, 45, 12605, 10423, 10399, 12266, 12983, 10079, 17916, 16694, 10784, 14734, 12396, 12231, 10578, 11065, 12469, 14734, 12985, 10336, 45, 12202, 15494, 10334, 12368, 16503, 14734, 12978, 12746, 11124, 12261, 16676, 10080, 18117, 10658, 45, 12469, 12537, 12522, 16753, 16753, 15133, 12201, 13064, 17237, 17213, 10333, 10373, 10303, 13324, 14734, 10781, 10429, 11053, 64, 16738, 12002, 11021, 15752, 16722, 15478, 11459, 12847, 17162, 12469, 14734, 12527, 11002, 11124, 10452, 18183, 10080, 12605, 10423, 15648, 12366, 11459, 10578, 11065, 45, 10260, 10423, 12662, 10784, 13032, 12469, 10080, 12605, 10402, 11021, 10399, 10250, 13582, 13582, 14486, 13923, 17148, 10349, 13057, 13199, 15486, 10964, 11687, 10454, 12325, 11168, 10333, 12469, 14446, 11454, 14734, 14583, 13815, 11124, 12484, 15509, 10080, 11454, 17148, 10283, 17141, 15052, 10285, 45, 12605, 12159, 13197, 15752, 12176, 10847, 12469, 12261, 15123, 15847, 10529, 45, 17919, 11957, 12202, 12824, 15462, 15847, 12144, 14734, 17141, 10985, 10080, 16738, 12983, 12366, 10578, 16698, 12469, 14734, 12985, 10336, 45, 12605, 10402, 10423, 10250, 16944, 12509, 10840, 13764, 12176, 12469, 16939, 10760, 10781, 10429, 14734, 17773, 12325, 28329, 65535, 261, 65530, 65534, 12605, 11017, 65530, 33, 12605, 13091, 10250, 10283, 13125, 13125, 17189, 17189, 14734, 10370, 10080, 12605, 14583, 13815, 11454, 11459, 14484, 10257, 19137, 12010, 10449, 11454, 10285, 11435, 10080, 33, 12605, 13191, 10250, 17734, 12630, 11855, 11124, 10250, 17100, 13713, 17079, 19137, 33, 17147, 13191, 10250, 10283, 11021, 14299, 14734, 11679, 10650, 10080, 33, 12605, 14734, 14583, 13815, 10655, 11903, 15646, 10265, 15597, 14015, 10080, 33, 12605, 14583, 13815, 10285, 13191, 10250, 10283, 11210, 10250, 14734, 15568, 12577, 19137, 17268, 11994, 13091, 11454, 12605, 10344, 12039, 17268, 12201, 19137, 12605, 14734, 14302, 10366, 11645, 17001, 10333, 10080, 10390, 14734, 15023, 10985, 16672, 12605, 12337, 12476, 10429, 10080, 33, 10444, 13091, 12605, 14446, 11454, 12768, 10460, 10333, 16944, 13234, 19137, 10655, 14015, 10310, 16520, 11124, 12159, 13197, 14734, 14583, 13815, 14808, 28329, 65535, 261, 65530, 65532, 16774, 16774, 10464, 12537, 12522, 10768, 10361, 10464, 14734, 12985, 10336, 45, 65530, 10080, 12605, 15752, 14486, 16533, 10460, 10292, 10250, 10283, 54, 12039, 11872, 11855, 45, 14302, 10366, 14734, 15023, 10985, 11957, 10464, 17194, 12604, 10333, 12140, 11638, 14734, 10781, 10429, 45, 17148, 15033, 10429, 14673, 10250, 11899, 17819, 14808, 10464, 14734, 12604, 17688, 10437, 17819, 10333, 12337, 17688, 13064, 17734, 28329, 10250, 10283, 11872, 11855, 17879, 16503, 14302, 10366, 14734, 17802, 10437, 11124, 10684, 14299, 45, 10390, 14734, 15023, 10985, 10260, 11021, 17248, 10662, 11459, 10423, 16672, 10464, 12527, 10792, 16443, 12679, 12269, 10080, 17148, 10283, 10781, 10429, 11021, 15752, 12325, 11168, 10333, 10464, 11957, 10684, 15304, 14734, 10529, 10409, 45, 10322, 16672, 10464, 11454, 12604, 17688, 17141, 15052, 10285, 15568, 10309, 14302, 17102, 14734, 12748, 11960, 10080, 17148, 10250, 10769, 17311, 11021, 15752, 11960, 15850, 10464, 14734, 12484, 12527, 17879, 13685, 12348, 10260, 10792, 14015, 16956, 28329, 14221, 15646, 45, 12605, 12337, 18216, 10685, 14791, 10792, 10464, 14446, 11454, 15752, 15043, 13249, 11459, 14791, 12335, 14583, 13815, 45, 13191, 15847, 12144, 14734, 11920, 12234, 11124, 10336, 10270, 10080, 17148, 16417, 13078, 10464, 13091, 10250, 10283, 13249, 10688, 17945, 12415, 11124, 14583, 11861, 10838, 14734, 10370, 10080, 10464, 13734, 13191, 16443, 17141, 10985, 14734, 10781, 10429, 12642, 16881, 45, 15646, 13091, 10848, 10838, 16939, 10760, 17773, 12325, 45, 13815, 10760, 15847, 12144, 12509, 16503, 14734, 14583, 13815, 28329, 11687, 13258, 10464, 10383, 14221, 11957, 14302, 10366, 14734, 15023, 10985, 12527, 10792, 11426, 12658, 12609, 14673, 15958, 45, 12605, 10402, 11021, 10399, 17149, 10250, 13582, 12822, 16671, 10464, 10696, 12366, 14734, 12527, 11002, 45, 13637, 11687, 16443, 17225, 12269, 14734, 12430, 12499, 45, 11957, 14302, 31337, 33, 14734, 13952, 13197, 15169, 15169, 10080, 10464, 11021, 10399, 10578, 16698, 10409, 10454, 10464, 12509, 16735, 14734, 45, 12605, 10423, 10578, 11065, 12202, 15494, 10334, 12978, 12746, 10080, 12605, 10402, 10423, 10250, 16944, 12664, 10792, 12176, 10847, 10464, 11891, 10675, 16939, 10760, 17148, 13619, 10781, 10429, 14734, 13036, 13764, 10080, 16738, 14782, 10529, 15847, 12144, 14734, 10838, 17387, 45, 10464, 10260, 13091, 11870, 14370, 10250, 10370, 10080, 65535, 261, 65530, 65534, 12605, 11454, 10257, 11871, 14734, 13064, 10560, 19137, 33, 11042, 11871, 10402, 12421, 13091, 11348, 15149, 12605, 10080, 33, 11454, 17213, 10792, 13320, 11423, 17901, 10737, 10336, 10407, 14734, 13064, 10560, 19137, 12605, 16678, 12348, 17268, 10283, 10370, 11017, 13221, 13109, 13078, 19137, 33, 10390, 12421, 13091, 12642, 12605, 19137, 12202, 10265, 10423, 12172, 10257, 10745, 10283, 10370, 10250, 16944, 10080, 33, 10390, 10402, 12421, 11017, 12605, 17386, 11872, 11855, 28329, 65535, 261, 65530, 65532, 12605, 14486, 16533, 11454, 13320, 11423, 17384, 17237, 17213, 17901, 10737, 13091, 17916, 12182, 14673, 15958, 14734, 15486, 10964, 10080, 16443, 13221, 13109, 13078, 11124, 10687, 10390, 10370, 17129, 18175, 10079, 13618, 12642, 11957, 10464, 17194, 12604, 10333, 13913, 10799, 14734, 10781, 10429, 28329, 10460, 10292, 10250, 10283, 11872, 11855, 45, 11002, 10792, 11042, 10437, 14734, 12711, 15498, 11124, 10429, 11917, 13091, 13249, 10687, 11426, 17843, 14734, 10080, 10390, 10402, 14734, 16403, 10292, 16672, 10464, 12527, 10792, 15847, 12144, 10260, 16443, 12824, 11002, 10079, 10260, 16443, 11971, 17385, 10080, 9822, 17386, 11872, 11855, 9823, 17148, 13324, 14734, 15044, 11116, 13179, 13091, 11454, 10464, 14734, 15847, 11971, 12366, 10257, 12266, 10764, 45, 16672, 10464, 16667, 10292, 15847, 12144, 13091, 10250, 10283, 10260, 16443, 14970, 10423, 17879, 16503, 14734, 10370, 28329, 12605, 15752, 12527, 11002, 10792, 13320, 11423, 13156, 10838, 15494, 10464, 12172, 13234, 14734, 15600, 17129, 12527, 11124, 10696, 14643, 12527, 10080, 10444, 12605, 12509, 16672, 10464, 14862, 17222, 45, 17148, 15498, 10260, 13091, 10464, 14734, 17631, 10080, 10464, 10260, 16721, 10292, 10390, 10370, 14734, 13156, 10838, 16403, 10292, 16875, 16879, 10409, 10080, 12605, 12159, 13197, 12605, 10402, 11021, 10399, 12822, 16671, 10464, 11957, 17148, 13619, 15486, 10964, 14734, 14794, 11903, 12527, 11002, 45, 10482, 10464, 10957, 10258, 10696, 12366, 14734, 17385, 12701, 28329, 10464, 13091, 10250, 10283, 12337, 13191, 10408, 10577, 14734, 10370, 10080, 17148, 10283, 10267, 14610, 17879, 16503, 10464, 45, 17879, 16503, 10464, 14734, 14370, 14332, 10305, 11621, 10080, 12605, 10423, 12176, 10847, 10464, 17385, 12261, 15847, 10529, 45, 16667, 16696, 10792, 17901, 10737, 15643, 14734, 17631, 16730, 16403, 10292, 10261, 10464, 13051, 10684, 10080, 12605, 10402, 10423, 10250, 16944, 12664, 10792, 13036, 13764, 45, 16672, 10464, 14734, 14583, 13815, 10260, 10703, 16443, 17141, 10985, 14734, 17773, 12325, 12631, 12138, 11025, 10080, 16738, 14782, 10529, 10464, 14734, 10838, 17387, 45, 12605, 10402, 10250, 11899, 15752, 16939, 10760, 17148, 13557, 14734, 10781, 10429, 10080, 65535, 261, 65530, 65534, 11454, 12824, 11002, 17148, 10349, 10370, 14734, 13565, 16875, 10305, 11044, 19137, 12605, 11994, 10258, 11899, 10722, 12366, 16503, 10848, 10838, 14734, 11871, 10323, 14862, 16696, 19137, 15641, 10283, 11685, 11871, 13320, 16056, 12348, 13179, 18216, 14734, 14970, 10423, 11459, 10447, 28329, 65535, 261, 65530, 65532, 12605, 12337, 13559, 16917, 10464, 10293, 10846, 10384, 16443, 10846, 14734, 11621, 11574, 10285, 11959, 12664, 15043, 13249, 14734, 10760, 16981, 10080, 10722, 12366, 17189, 17141, 10848, 10838, 11871, 10323, 13234, 12981, 11003, 15847, 12144, 14734, 11117, 17144, 45, 17148, 13098, 14966, 10760, 10464, 13249, 12299, 14734, 12522, 12377, 10838, 11124, 12676, 10966, 15752, 10838, 28329, 12307, 17237, 17213, 13320, 11423, 13565, 10737, 13064, 45, 10464, 13734, 13191, 17177, 12731, 12687, 11626, 12609, 15847, 13156, 15847, 12269, 45, 15646, 13091, 11967, 17148, 13619, 14673, 15958, 15486, 10964, 10460, 10292, 10846, 10838, 45, 10292, 15847, 12144, 16689, 11899, 10333, 13179, 18216, 14734, 14778, 13308, 10080, 17148, 10410, 10863, 13655, 11124, 10722, 12366, 10398, 10370, 17551, 10470, 28329, 17189, 17141, 10848, 10838, 11871, 10323, 13234, 12848, 10920, 15847, 12144, 45, 16056, 12348, 14862, 16696, 11124, 12666, 15752, 45, 14734, 14914, 11021, 10399, 12176, 10847, 10464, 16056, 11001, 13179, 11632, 13212, 10423, 11124, 11021, 15752, 12415, 10080, 12605, 14782, 10529, 10399, 10464, 14734, 13628, 10838, 45, 11891, 10675, 13191, 15752, 10838, 17136, 12604, 15847, 12144, 14734, 14778, 13308, 28329, 16678, 10449, 16503, 14240, 18004, 11685, 15847, 12144, 45, 10260, 16503, 10292, 10333, 14778, 13308, 15646, 12392, 16523, 10333, 17038, 12366, 10590, 12237, 10080, 10464, 12631, 17237, 11002, 14734, 10781, 10429, 11021, 15752, 17147, 17879, 16503, 13064, 17734, 13234, 12435, 11626, 10080, 12605, 10423, 10675, 15052, 12978, 12746, 10464, 45, 10464, 10260, 13091, 10250, 10283, 10370, 11454, 12610, 13020, 10080, 16672, 12605, 10402, 10250, 16944, 16671, 16686, 11687, 10454, 10588, 10792, 10848, 10838, 11047, 10257, 45, 11042, 13064, 10322, 10684, 12366, 15847, 12144, 14734, 17879, 16503, 10080, 10464, 14734, 13203, 13234, 10655, 14015, 12159, 13197, 45, 16738, 10525, 12746, 10529, 12366, 34, 65535, 261, 65530, 65534, 11021, 15752, 12605, 11640, 14589, 10838, 10333, 10080, 33, 10441, 10308, 11454, 12604, 12201, 14734, 17141, 15052, 12307, 10285, 12392, 16523, 10333, 12337, 11632, 10272, 16501, 28329, 65535, 261, 65530, 65532, 12469, 12848, 10792, 11454, 10848, 10838, 11903, 14446, 14778, 13308, 14734, 17141, 15052, 10285, 45, 11021, 15752, 12392, 16523, 10333, 10250, 10349, 10687, 10390, 17385, 16503, 14734, 10336, 12484, 45, 17148, 16672, 12605, 13179, 10842, 13559, 16917, 12469, 14734, 15847, 14787, 10261, 10995, 12410, 10080, 12604, 12201, 12604, 17688, 14734, 17222, 16981, 10257, 45, 12605, 10402, 17311, 10423, 10250, 10975, 12484, 12537, 45, 10399, 15849, 10339, 11003, 12348, 14313, 17919, 28329, 10848, 10838, 12981, 11003, 11117, 17144, 10079, 11001, 12348, 17149, 13582, 13205, 17038, 13091, 17916, 12182, 13580, 17919, 14734, 10336, 12484, 10080, 10444, 11454, 13581, 17141, 15052, 10285, 45, 12605, 10402, 10322, 13191, 11021, 15752, 12392, 16523, 10370, 14583, 10285, 10687, 10390, 17385, 16503, 14734, 12007, 17919, 45, 13637, 11687, 11920, 10370, 10079, 10993, 12484, 10079, 14299, 12484, 10079, 10283, 10370, 10685, 16954, 14299, 11685, 10079, 12366, 14141, 11124, 12484, 12527, 14734, 17879, 13685, 15169, 10080, 17148, 10349, 17311, 13091, 13250, 12604, 10286, 11932, 11632, 12319, 10370, 14583, 14734, 16503, 15323, 28329, 12631, 10399, 12605, 12261, 16676, 12469, 11021, 10399, 17379, 11001, 10399, 10258, 12828, 13038, 59, 15162, 10250, 45, 10452, 16751, 12202, 12527, 16774, 17141, 10985, 10848, 10838, 11659, 13020, 14734, 15847, 12144, 60, 15162, 10337, 45, 10589, 10258, 13234, 10995, 12410, 16443, 12392, 16523, 14734, 18011, 11524, 45, 16694, 10435, 15847, 12144, 17147, 17879, 16503, 10373, 10303, 60, 15162, 10256, 45, 10794, 11899, 16403, 10846, 16664, 10775, 45, 17182, 13582, 10692, 18004, 16443, 12392, 16523, 14734, 17879, 13685, 60, 15162, 11412, 45, 10261, 12978, 12746, 12469, 14734, 10366, 10993, 10355, 13819, 45, 16056, 11001, 10390, 10402, 14734, 10995, 18108, 28329, 16678, 10449, 45, 14583, 13815, 10285, 12337, 11977, 13191, 11891, 15597, 13051, 15568, 10305, 10336, 10080, 12469, 12145, 15486, 16939, 10333, 12337, 17688, 14734, 16981, 45, 14446, 11454, 11021, 10399, 16749, 13006, 13036, 11047, 45, 11903, 14446, 13179, 10675, 17919, 12200, 16412, 14734, 12604, 17688, 10080, 12605, 14782, 10529, 45, 15486, 17141, 12469, 14734, 10848, 10838, 45, 10250, 11899, 15752, 12664, 10792, 16672, 15847, 12144, 12527, 10792, 13179, 14015, 16956, 11124, 12203, 15009, 14734, 14583, 13815, 14349, 12396, 10080, 11687, 13258, 12527, 10792, 17165, 15987, 45, 17819, 13064, 13558, 17143, 10261, 12605, 10355, 13819, 10080, 65535, 261, 65530, 65534, 11044, 13234, 12605, 16667, 16696, 10333, 10250, 10283, 11435, 17781, 10355, 12791, 14583, 13209, 16037, 19137, 33, 12605, 10402, 12330, 13581, 17916, 12182, 13559, 16917, 19137, 33, 13186, 15486, 10997, 14583, 17141, 10250, 10349, 13844, 14042, 14734, 10336, 12484, 10080, 33, 10790, 16730, 10423, 12605, 13734, 13191, 11124, 11684, 10588, 11903, 17781, 14734, 12415, 12824, 16535, 10080, 33, 10444, 13091, 12605, 10402, 12330, 13581, 12570, 12573, 10080, 33, 10444, 13091, 17268, 10283, 13064, 17734, 13619, 12605, 11903, 11454, 13091, 11640, 10585, 14091, 10333, 19137, 33, 12669, 17169, 13685, 14970, 10423, 11459, 10447, 12307, 10588, 11210, 10250, 14734, 10336, 12484, 28329, 65535, 261, 65530, 65532, 12605, 14486, 16533, 11454, 17169, 13685, 14778, 13308, 14734, 17141, 15052, 10285, 45, 12469, 17213, 10792, 10333, 13209, 16037, 17148, 13324, 10250, 10283, 17385, 16503, 14734, 10370, 45, 12330, 13581, 17734, 10322, 10358, 14583, 10333, 15597, 11685, 14734, 12484, 12527, 15671, 15489, 10080, 14221, 15646, 12307, 13064, 17141, 10339, 12650, 14808, 10339, 10336, 10270, 45, 13203, 15752, 15494, 10334, 17148, 13619, 12527, 12484, 12220, 13191, 14734, 17385, 16523, 11124, 11414, 12220, 10080, 17148, 11021, 15752, 16672, 12469, 11454, 10399, 11044, 11414, 12509, 16944, 13234, 13064, 12527, 10792, 17225, 12577, 28329, 14299, 12484, 11124, 10993, 12484, 17311, 13091, 14583, 13815, 10285, 17916, 12182, 11902, 16896, 14734, 15486, 10964, 10080, 12307, 12605, 10402, 10675, 12366, 12674, 10673, 13279, 10250, 10283, 18011, 11524, 13064, 45, 12337, 11922, 13080, 12392, 16523, 10370, 10261, 10370, 10305, 17734, 12359, 11701, 15646, 15597, 11685, 14734, 12484, 12527, 10355, 13819, 28329, 12605, 12261, 16676, 12469, 10260, 16503, 11416, 13581, 15847, 16879, 10080, 17268, 10283, 17775, 13619, 14734, 12469, 13326, 12797, 12307, 13064, 14734, 14349, 12396, 10588, 10760, 10333, 17177, 12731, 10080, 14446, 11454, 12469, 12145, 15486, 12522, 16696, 10792, 45, 10370, 14583, 10285, 17879, 16503, 13179, 10675, 17919, 11459, 10692, 18004, 10260, 11042, 14734, 17879, 13685, 28329, 11687, 13258, 13209, 16037, 11957, 12469, 10383, 12522, 10304, 17916, 10746, 45, 12469, 10260, 11709, 16707, 14808, 17385, 13034, 15671, 15304, 11684, 45, 16417, 17136, 12469, 14734, 13571, 12522, 11124, 15847, 12605, 12604, 17688, 14734, 12527, 12465, 10080, 11994, 15200, 12145, 17631, 17141, 17148, 13619, 12527, 12484, 45, 11885, 10322, 12604, 10292, 12469, 10370, 14583, 11902, 16896, 14734, 10250, 17996, 10080, 16678, 10449, 12266, 12983, 12366, 15750, 45, 14460, 12493, 14806, 10808, 10370, 10080, 12469, 10423, 12979, 16056, 13179, 11632, 13203, 13234, 14734, 13212, 10423, 10080, 65535, 261, 65530, 65531, 16738, 13326, 12797, 12605, 10402, 10305, 10808, 14734, 10355, 13819, 19137, 33, 16667, 14794, 14734, 13377, 14486, 10250, 10258, 65530, 14734, 12415, 13327, 14332, 14172, 19137, 12176, 10847, 12605, 10588, 10250, 10258, 15287, 14987, 10768, 13254, 13234, 16672, 12605, 13179, 10842, 10333, 16533, 65530, 15847, 12144, 28329, 65535, 261, 65530, 65532, 11685, 14734, 45, 16672, 12605, 11986, 16707, 13326, 12797, 12605, 10402, 10305, 10808, 14734, 16671, 16686, 45, 40635, 6105, 1216, 10768, 13254, 12469, 14734, 12415, 13327, 14332, 14172, 59, 11, 12469, 13191, 14808, 12299, 11638, 14734, 12522, 12377, 10838, 11124, 11474, 12746, 10260, 12580, 14734, 15287, 14987, 10080, 10384, 11975, 17237, 17213, 14302, 10366, 15023, 10267, 14734, 10781, 10429, 45, 12469, 13734, 13191, 16443, 12642, 16881, 45, 15646, 13091, 10848, 10838, 14583, 13815, 10080, 17919, 11957, 13320, 11423, 13156, 10838, 45, 12469, 17177, 12731, 14589, 16742, 10325, 12981, 11003, 11117, 17144, 10080, 17148, 13098, 14966, 10760, 12469, 17919, 11957, 17176, 11574, 14734, 17945, 12415, 28329, 12469, 13191, 12337, 12299, 14734, 10257, 17149, 12366, 11124, 17149, 11001, 15287, 14987, 10080, 10292, 10333, 16947, 16948, 17141, 10985, 14734, 10781, 10429, 45, 12469, 10392, 10760, 12140, 11638, 10848, 10838, 13685, 11871, 11124, 12137, 10460, 45, 13098, 14966, 10760, 12299, 14179, 14734, 12604, 11994, 10846, 13212, 28329, 12469, 13637, 17097, 13773, 17385, 10336, 10270, 11124, 14486, 12509, 45, 13186, 10250, 12231, 12392, 16523, 10283, 10370, 12484, 12527, 17879, 16503, 11124, 14583, 13815, 14734, 12200, 16412, 10080, 10444, 12469, 12145, 15486, 12522, 16696, 10792, 17148, 10250, 17728, 18025, 45, 16417, 14446, 10760, 12604, 14256, 14734, 15847, 14787, 15752, 10838, 28329, 12469, 11957, 10390, 10370, 12484, 12527, 15671, 15304, 11124, 10366, 11930, 10684, 15304, 17147, 13191, 10250, 11899, 14734, 18004, 16253, 45, 11021, 15752, 10261, 13057, 13199, 14734, 10781, 10429, 15486, 10964, 13191, 10684, 10080, 17148, 10322, 13091, 13203, 13234, 11021, 10399, 15507, 15511, 12822, 15325, 14734, 10250, 13036, 17919, 28329, 12469, 12604, 14256, 10696, 12997, 15646, 11474, 17945, 45, 13191, 17919, 11957, 10370, 14583, 11426, 11574, 10261, 15847, 12144, 10696, 12366, 14734, 10863, 13655, 45, 10322, 12537, 12522, 10578, 11065, 10390, 10370, 12261, 16676, 10080, 17148, 13098, 14966, 10760, 13133, 12551, 10261, 11042, 12484, 12366, 14734, 15489, 11038, 28329, 12605, 12261, 16676, 12469, 13052, 16667, 16696, 10792, 15847, 12144, 14734, 10421, 14172, 45, 10322, 13773, 12522, 17268, 17384, 10383, 17879, 12604, 17688, 10080, 13633, 10250, 10283, 10370, 17311, 13091, 11454, 11003, 10901, 10285, 12604, 17688, 14734, 45, 17385, 16503, 14734, 13091, 10525, 12746, 11957, 14583, 13815, 14734, 14195, 12484, 45, 10261, 15713, 11899, 15847, 12144, 14734, 12522, 12537, 10080, 16738, 15507, 15511, 11474, 12746, 45, 12605, 10423, 10250, 14781, 11454, 17148, 17384, 12978, 12746, 12469, 34, 65535, 261, 65530, 65534, 15752, 11634, 12176, 12605, 10588, 10250, 13557, 15287, 14987, 10768, 13254, 11053, 19156, 11, 65535, 261, 65530, 65532, 11685, 14734, 45, 12605, 11021, 10399, 11986, 16707, 14589, 10459, 13799, 10414, 12363, 15287, 14987, 10768, 13254, 11871, 13818, 14734, 16523, 16530, 13234, 14486, 16533, 12469, 14734, 12415, 13327, 10261, 15486, 10964, 59, 11, 10384, 15287, 14987, 15489, 13250, 13234, 14791, 45, 12469, 14734, 15847, 12605, 10441, 10308, 13637, 17097, 10997, 17136, 45, 16417, 14446, 10292, 12299, 14179, 14734, 10257, 17149, 12366, 11124, 12604, 11994, 10846, 13212, 10080, 10444, 12469, 14734, 15847, 12605, 11021, 15752, 10322, 12665, 11002, 10333, 10250, 11899, 14734, 10781, 10429, 45, 13637, 11687, 14302, 10366, 14734, 15023, 10985, 11124, 13320, 11423, 13156, 10838, 10336, 10407, 45, 17148, 12325, 11168, 10333, 12469, 14734, 10370, 13327, 10997, 12022, 28329, 11, 13326, 12797, 10459, 13799, 10414, 12363, 14734, 16520, 14172, 45, 12469, 14734, 13205, 12605, 16417, 14446, 10760, 12299, 11638, 14734, 14583, 11861, 13205, 15752, 45, 11454, 14445, 11574, 10966, 10838, 10258, 10383, 14221, 11474, 12746, 15847, 12605, 10080, 10444, 11021, 15752, 12469, 14734, 16947, 12605, 13637, 17097, 10279, 15951, 45, 16503, 13685, 12469, 17241, 12357, 17097, 18216, 14734, 13308, 10733, 45, 12392, 16523, 12484, 12527, 17879, 13685, 10080, 17148, 17194, 12604, 10333, 13205, 12605, 11124, 16947, 12605, 10305, 17734, 14734, 14858, 14785, 28329, 11, 10384, 15287, 14987, 10846, 10838, 13234, 14791, 45, 12469, 10441, 10308, 13191, 17097, 12299, 14734, 10397, 10598, 12366, 14486, 45, 17189, 17141, 10264, 13773, 10336, 10270, 11124, 14778, 13308, 13234, 29334, 2241, 33, 13057, 13199, 10781, 10429, 15486, 10964, 12172, 13234, 14734, 36831, 7876, 33, 12527, 10080, 12469, 10322, 16417, 14446, 10760, 17097, 18216, 14734, 13074, 10925, 15752, 10838, 45, 10953, 12669, 15287, 10838, 12674, 10673, 10792, 14970, 10423, 12665, 16667, 14734, 12604, 11994, 10285, 28329, 11, 14059, 12522, 16696, 13036, 17919, 45, 12605, 12826, 13827, 12469, 11021, 15752, 11861, 11454, 16443, 17225, 12269, 14734, 12484, 15489, 45, 13952, 13197, 12348, 10792, 16667, 11021, 11124, 11889, 10675, 12527, 10080, 17148, 11021, 15752, 12325, 11168, 12469, 11957, 10366, 11930, 10684, 15304, 14734, 10495, 16922, 12415, 10080, 12605, 10402, 11021, 15752, 17879, 16503, 17149, 10250, 13582, 12822, 16671, 12469, 10696, 12366, 14734, 14216, 16253, 10261, 17771, 12354, 13212, 10794, 28329, 11, 11687, 13258, 12605, 14734, 10768, 13254, 13191, 10409, 10454, 17631, 16730, 45, 16738, 12469, 12748, 13580, 10080, 12605, 10402, 11021, 10399, 15507, 15511, 12822, 16671, 45, 10399, 12176, 10847, 12469, 16056, 12348, 11957, 15847, 12605, 14734, 13913, 10799, 14486, 16533, 10080, 17148, 17879, 16503, 17688, 13199, 14734, 12137, 10460, 45, 10444, 12605, 10423, 10675, 15052, 12978, 12746, 12469, 10080, 65535, 261, 65530, 65534, 13191, 13064, 10560, 12605, 10423, 16525, 12348, 10696, 12366, 15090, 16256, 19137, 10260, 14862, 12631, 12828, 28329, 65535, 261, 65530, 65532, 11685, 14734, 45, 12605, 10423, 11986, 16707, 10384, 10650, 15133, 12366, 14486, 11871, 11124, 10370, 13205, 10293, 10304, 12366, 14486, 11871, 14734, 16523, 16530, 45, 10768, 13254, 12469, 10696, 12366, 15090, 16256, 10079, 17165, 15987, 14734, 11021, 15752, 10974, 11416, 59, 11, 10384, 10650, 15133, 12366, 14486, 11871, 14791, 45, 14302, 10366, 11454, 54, 12039, 13064, 14734, 15023, 10985, 11021, 15752, 16672, 12469, 14734, 10495, 12428, 17879, 13685, 12348, 10260, 10792, 14015, 16956, 10080, 13326, 12797, 14299, 10788, 10660, 13396, 14734, 15287, 14987, 14970, 10423, 10997, 12022, 14486, 16686, 45, 17148, 10250, 13064, 13199, 11957, 14302, 13631, 14734, 10495, 16922, 11124, 10529, 10409, 11957, 10250, 10283, 11872, 11855, 14734, 12604, 17688, 15849, 10684, 17385, 16503, 10080, 14302, 14299, 14734, 15568, 11645, 11021, 15752, 11960, 15850, 12469, 14734, 11533, 13205, 10529, 10409, 11002, 10792, 10846, 12894, 45, 17149, 15646, 17194, 12604, 12484, 12527, 10257, 14734, 10260, 11889, 10675, 12527, 28329, 11454, 10370, 13205, 10293, 10304, 12366, 14486, 11871, 10285, 45, 15577, 13240, 13033, 12299, 16749, 13633, 10283, 10370, 17311, 13191, 11903, 14446, 15847, 12605, 45098, 34932, 33, 14734, 10696, 11454, 17879, 13685, 10080, 11021, 15752, 11454, 17169, 13685, 11630, 11454, 12604, 11994, 14734, 17141, 15052, 10285, 45, 12469, 12392, 16523, 10333, 10696, 11454, 15847, 12605, 14734, 17879, 16503, 45, 11960, 15850, 14446, 11454, 12527, 10792, 15090, 16256, 28329, 10292, 10333, 16056, 12348, 10390, 10370, 16667, 11021, 45, 12469, 11021, 15752, 17141, 10768, 12299, 16749, 10588, 10250, 10283, 9822, 11957, 14734, 10370, 28319, 15646, 12392, 16523, 10333, 10460, 10292, 10250, 10283, 9822, 14794, 11903, 14734, 10370, 9823, 14734, 17879, 13685, 10080, 13326, 12797, 18157, 13033, 13799, 14734, 17879, 13685, 12007, 13557, 14486, 16686, 45, 11016, 13191, 18216, 12007, 13557, 14734, 15847, 12605, 11903, 14446, 17879, 13685, 16443, 14015, 16956, 45, 10250, 10283, 10370, 12638, 15752, 12664, 10792, 14794, 13580, 14734, 14015, 16956, 12527, 28329, 12605, 12261, 16676, 12469, 11414, 12306, 10696, 12366, 45, 10578, 11065, 10696, 11454, 14734, 11610, 17952, 45, 12822, 15325, 15847, 12144, 16443, 17225, 12378, 12609, 12670, 10794, 14734, 12484, 12527, 17879, 13685, 10080, 17385, 13034, 12261, 15123, 11957, 15847, 12144, 14734, 10529, 10409, 45, 13815, 10760, 10250, 10283, 13179, 14794, 11903, 13179, 10286, 11932, 14734, 15847, 12605, 10080, 17148, 17879, 16503, 13064, 17734, 45, 10444, 12605, 14782, 10529, 10399, 12469, 14734, 10863, 13655, 45, 10250, 11899, 15752, 11634, 10588, 10792, 10080, 12605, 11454, 17148, 17384, 10675, 12366, 12978, 12746, 12469, 10080, 65535, 261, 65530, 65534, 11957, 10339, 17148, 15033, 12484, 12527, 11124, 12484, 15509, 14734, 10370, 19137, 33, 15752, 12826, 16007, 10325, 15252, 10079, 11975, 16735, 11124, 12603, 10821, 13234, 12176, 10847, 12200, 11626, 11124, 17149, 16403, 12484, 15509, 14864, 13580, 11053, 19156, 11, 65535, 261, 65530, 65532, 11685, 14734, 45, 12605, 11986, 16707, 12826, 16007, 10250, 10349, 10325, 15252, 10079, 11975, 16735, 11124, 12603, 10821, 45, 17148, 10349, 10460, 11159, 11021, 15752, 12176, 10847, 12469, 12200, 11626, 12484, 15509, 45, 17149, 16403, 12484, 12527, 14734, 13751, 14641, 59, 11, 50, 47, 33, 10325, 15252, 59, 10086, 12727, 12691, 10696, 11454, 14734, 11872, 11855, 10087, 45, 12176, 10847, 17919, 11957, 10696, 12366, 10781, 10429, 60, 10086, 10370, 14583, 14734, 17169, 13685, 10087, 45, 11959, 12664, 14583, 11117, 12522, 10304, 60, 10086, 12366, 14486, 11871, 10261, 14583, 13815, 10087, 45, 16667, 16696, 15847, 12605, 60, 10086, 10846, 13212, 10261, 10370, 13327, 10087, 45, 12664, 10792, 10696, 11454, 10846, 10838, 28329, 51, 47, 33, 11975, 16735, 59, 10086, 12766, 11753, 14734, 13396, 13256, 10087, 45, 13751, 12518, 17141, 15052, 60, 10086, 12605, 10402, 10400, 10087, 45, 11920, 12234, 10684, 15304, 60, 10086, 18052, 18361, 17848, 10087, 45, 10283, 10452, 12604, 17688, 60, 10086, 12366, 13091, 11870, 14370, 14734, 14384, 12636, 10087, 45, 12822, 15325, 10370, 12415, 28329, 52, 47, 33, 12603, 10821, 59, 10086, 13849, 18366, 10087, 45, 11920, 12234, 15593, 15482, 60, 10086, 13594, 10353, 17613, 11209, 11098, 10087, 45, 10283, 10370, 11659, 13020, 60, 10086, 15169, 12335, 12598, 11632, 10087, 45, 14583, 11117, 12522, 10304, 60, 10086, 11633, 13953, 14987, 10087, 45, 12366, 14141, 12991, 16916, 28329, 12605, 12261, 16676, 12469, 11021, 10399, 10384, 10285, 12664, 10792, 10682, 18365, 45, 11967, 10325, 10285, 10370, 14328, 14734, 15486, 10964, 10261, 15847, 12144, 14734, 61604, 45, 16056, 12348, 12484, 12527, 14734, 13800, 16519, 10080, 10421, 15597, 14734, 16728, 16537, 10322, 11021, 10399, 13751, 12518, 12366, 14141, 10781, 10429, 45, 17385, 12261, 15043, 13249, 12366, 12396, 10080, 12159, 13197, 17148, 10349, 10460, 11159, 11021, 10399, 17802, 10437, 12469, 14734, 15847, 12605, 12822, 15325, 10305, 13042, 10080, 11687, 13258, 13191, 10687, 10390, 12826, 16007, 17879, 16503, 45, 16738, 11088, 16698, 12605, 10080, 65535, 261]
-> item_loss 1.9321231842041016 batch_loss 1.9321231842041016
==== [10272, 18043, 46, 647, 11960, 12298, 10529, 12445, 11, 65530, 65532, 15494, 10760, 10272, 18043, 647, 14734, 13012, 13205, 10529, 12445, 45, 11454, 13012, 13205, 13188, 11044, 19137, 12605, 17879, 16503, 13006, 14486, 12604, 1843, 111, 13327, 12276, 19137, 13036, 10511, 14589, 10339, 11, 65535, 261, 65530, 65534, 10272, 18043, 46, 647, 10285, 11960, 12298, 13091, 10285, 11435, 14880, 10794, 14734, 10285, 17150, 15052, 12298, 17222, 11960, 12298, 10080, 33, 13326, 12797, 10678, 12266, 16909, 13021, 45, 10687, 10293, 16503, 12666, 13208, 10988, 13004, 11687, 10258, 59, 11, 9828, 33, 11965, 15052, 59, 640, 620, 46, 650, 620, 10918, 15260, 11, 9828, 33, 11960, 12298, 17688, 12231, 59, 637, 15260, 265, 9828, 33, 10997, 11965, 17385, 17387, 59, 638, 47, 55, 33, 11062, 3339, 9828, 33, 12610, 13020, 17303, 17385, 17387, 59, 50, 11062, 11, 9828, 33, 10482, 14589, 13882, 10452, 14267, 13021, 11, 10272, 18043, 46, 647, 11960, 12298, 10292, 10278, 10283, 15451, 10790, 59, 10272, 18043, 46, 647, 11124, 10272, 18043, 46, 647, 66, 45, 10293, 16503, 10912, 10790, 11454, 11965, 15052, 10257, 28329, 10272, 18043, 46, 647, 59, 11, 9828, 33, 11965, 15052, 59, 640, 620, 10918, 15260, 11, 9828, 33, 12610, 13020, 17303, 17385, 17387, 59, 695, 49, 10918, 10660, 11, 10272, 18043, 46, 647, 66, 59, 11, 9828, 11965, 15052, 59, 645, 620, 127, 650, 620, 10918, 15260, 3339, 9828, 33, 12610, 13020, 17303, 17385, 17387, 59, 50, 11062, 11, 10272, 18043, 46, 647, 11960, 12298, 16443, 12266, 10997, 14589, 10339, 15092, 14894, 10912, 11524, 17771, 15090, 15304, 15500, 45, 15752, 11634, 10495, 17917, 12298, 17222, 13177, 15471, 11124, 13212, 10846, 17248, 12987, 45, 13091, 10285, 11435, 17782, 11533, 13325, 10838, 17387, 14734, 10293, 10838, 10305, 10250, 28329, 10272, 18043, 46, 647, 11960, 12298, 12025, 10339, 10285, 10079, 14866, 11168, 12220, 13064, 17734, 12298, 17222, 11960, 12298, 45, 15752, 11634, 11454, 12728, 12617, 10808, 12387, 17193, 15092, 14894, 17771, 15090, 10452, 15304, 28329, 10272, 18043, 10337, 10917, 10253, 14734, 14332, 14172, 10894, 12732, 11, 50, 47, 33, 17379, 14589, 10994, 13882, 12396, 14136, 15208, 10997, 10846, 13212, 28331, 51, 47, 33, 17332, 15586, 10333, 11626, 13215, 14734, 10794, 11960, 15304, 15500, 19137, 10399, 10992, 10688, 11622, 12101, 13034, 14734, 10846, 10838, 15304, 15500, 28329, 10399, 10257, 13091, 10684, 10339, 10272, 18043, 46, 647, 11960, 12298, 14734, 11533, 13205, 13445, 17162, 45, 12159, 13197, 15752, 10292, 12469, 12848, 10494, 10988, 15641, 10080, 11687, 13258, 13191, 10409, 10454, 14638, 17728, 45, 13558, 17143, 11047, 12605, 12848, 17728, 28329, 10384, 10257, 17162, 13004, 12797, 10285, 19137, 33, 13006, 14486, 15489, 13250, 10901, 10529, 12445, 11687, 10258, 11, 6884, 1843, 111, 11, 2370, 26290, 269, 10272, 18043, 46, 647, 35, 11, 59, 8148, 58475, 269, 10285, 11435, 35, 11, 59, 42519, 269, 638, 47, 55, 33, 11062, 35, 11, 59, 41775, 267, 35, 637, 33, 15260, 35, 11, 59, 9059, 25566, 267, 35, 630, 620, 10918, 10660, 35, 11, 59, 35494, 269, 640, 620, 46, 650, 620, 10918, 15260, 35, 11, 59, 41330, 269, 10994, 13882, 12396, 14136, 15208, 10997, 10846, 13212, 35, 11, 59, 53988, 4363, 13191, 12986, 17095, 16032, 11021, 16749, 395, 11, 59, 63053, 269, 10272, 18043, 46, 647, 10285, 15052, 12298, 17222, 11960, 12298, 45, 17332, 11622, 11626, 13215, 14734, 10794, 11960, 11124, 12101, 13034, 14734, 10846, 10838, 15304, 15500, 10080, 399, 11, 6884, 65535, 261]
-> item_loss 1.6350220441818237 batch_loss 1.7835726141929626
==== [12366, 14486, 16690, 16753, 12308, 11, 65530, 65531, 10399, 10258, 11957, 16713, 13091, 12366, 14486, 10913, 14583, 11124, 13234, 16690, 15643, 10305, 17734, 14734, 11957, 16713, 19137, 6608, 54116, 1937, 13091, 10250, 10447, 15287, 14987, 10768, 16449, 14663, 12467, 15643, 10080, 10390, 16667, 10292, 15847, 12144, 13091, 10250, 10283, 820, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10464, 11685, 844, 2194, 41693, 45, 267, 13234, 14734, 12771, 13057, 14734, 47, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 6608, 54116, 1937, 501, 33, 13091, 14734, 45, 12605, 13199, 12335, 17148, 13557, 14734, 10355, 13819, 47, 33, 15847, 10384, 10257, 13557, 16519, 17141, 10464, 10305, 11044, 12605, 11685, 11632, 10333, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 17268, 10303, 12605, 10402, 14446, 11454, 12266, 11726, 47, 267, 14446, 11454, 12605, 17728, 10464, 10250, 10283, 17728, 18025, 45, 33, 10464, 13091, 11060, 17147, 12395, 14638, 15847, 12144, 13091, 10250, 10283, 820, 10370, 12137, 13133, 15752, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 6608, 54116, 1937, 501, 33, 12605, 10260, 14862, 17222, 47, 33, 12605, 11016, 13091, 10250, 10283, 15052, 12217, 47, 33, 12605, 13734, 13191, 12527, 16525, 11124, 12522, 16696, 47, 33, 12605, 11016, 13091, 10250, 10283, 15052, 12217, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 12631, 10399, 10464, 13091, 10250, 10283, 15052, 12217, 64, 33, 10464, 13191, 10373, 10303, 13234, 16692, 13078, 15847, 12144, 13091, 10250, 10283, 15052, 12217, 17268, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 6608, 54116, 1937, 501, 33, 12605, 13091, 10250, 10283, 15052, 12217, 47, 33, 12605, 13734, 13191, 12527, 16525, 11124, 12522, 16696, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 14589, 12636, 12623, 10333, 10250, 10258, 5585, 41693, 14734, 15804, 17303, 47, 58683, 12527, 16525, 10792, 10250, 17774, 17355, 14673, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10464, 14446, 11454, 13091, 11060, 12527, 16525, 10792, 10373, 10303, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 6608, 54116, 1937, 501, 33, 12605, 12527, 16525, 10792, 10333, 47, 33, 10464, 12623, 10333, 12605, 10250, 10258, 47, 33, 10444, 13091, 17148, 10260, 15752, 16692, 13078, 10373, 10303, 47, 33, 12605, 17147, 13091, 820, 10370, 12137, 13133, 15752, 47, 11, 65535, 261, 65530, 65531, 33365, 49461, 12745, 10760, 10333, 10250, 12287, 14240, 14313, 45, 33, 10257, 17919, 13091, 5585, 41693, 11124, 10390, 14734, 11920, 10370, 10402, 45, 58683, 12691, 14808, 10390, 14734, 11679, 10650, 17900, 16501, 45, 33, 10390, 10402, 15149, 14734, 17311, 12337, 12266, 12366, 47, 17900, 16501, 12636, 10285, 12745, 14808, 10250, 10283, 13778, 17159, 11975, 14249, 45, 11994, 13091, 16407, 10257, 12182, 16519, 14734, 17268, 15033, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 17148, 12287, 14240, 14313, 10464, 16667, 16696, 11053, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 6608, 54116, 1937, 501, 33, 17148, 13091, 12605, 14734, 11679, 10650, 17900, 16501, 47, 33, 11684, 12337, 11021, 14299, 47, 33, 12605, 12337, 11268, 13558, 11684, 47, 33, 12605, 10402, 10250, 11920, 10370, 17311, 12337, 11268, 13558, 11684, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10370, 12137, 13133, 15752, 12401, 10303, 10423, 13191, 11679, 10650, 14734, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 6608, 54116, 1937, 501, 33, 12605, 10260, 14862, 17222, 47, 33, 10444, 13091, 17148, 11994, 13091, 12605, 14734, 11679, 10650, 47, 33, 11684, 12337, 11021, 14299, 47, 33, 12605, 12337, 11268, 13558, 11684, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 12605, 16535, 14935, 10464, 14734, 13064, 10560, 45, 33, 10464, 10423, 12527, 16525, 10792, 14658, 14673, 47, 33, 10464, 10322, 13191, 14658, 14299, 14734, 11679, 10650, 47, 267, 12631, 10399, 10464, 17147, 11474, 12746, 10464, 13091, 10370, 12137, 13133, 15752, 11053, 64, 11, 65535, 261, 65530, 65531, 33365, 49461, 14589, 12748, 11981, 12813, 10449, 5585, 41693, 14734, 12636, 15833, 45, 58683, 12527, 16525, 10792, 14658, 14673, 47, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 6608, 54116, 1937, 501, 33, 12266, 11726, 13722, 18441, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10464, 17147, 11474, 12746, 10464, 13091, 10370, 12137, 13133, 15752, 11053, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 12605, 10260, 14862, 17222, 47, 33, 12605, 11016, 13091, 10250, 10283, 15052, 12217, 47, 33, 12605, 13734, 13191, 12527, 16525, 11124, 12522, 16696, 47, 33, 12605, 11016, 13091, 10250, 10283, 15052, 12217, 579, 10390, 14734, 15287, 14987, 17771, 15471, 12266, 11726, 12100, 13977, 42, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 12605, 14782, 10529, 10464, 13091, 10250, 10283, 14794, 13580, 14734, 10370, 47, 33, 10464, 14734, 12366, 14486, 14660, 14662, 11016, 13091, 16417, 14446, 10292, 10250, 10283, 820, 10370, 12137, 13133, 15752, 47, 33, 10464, 10260, 10423, 16443, 14794, 13580, 14734, 15287, 14987, 14662, 12631, 12325, 11168, 47, 33, 14446, 11454, 10464, 14862, 17222, 15847, 12144, 13091, 10373, 10303, 10333, 11053, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 12605, 16678, 16944, 13234, 10333, 47, 33, 12605, 13091, 10250, 10283, 14794, 13580, 14734, 10370, 47, 33, 12605, 10260, 13091, 820, 10370, 12137, 13133, 15752, 47, 33, 12605, 13091, 10250, 10283, 10370, 15261, 45, 33, 10250, 10283, 13191, 16399, 13191, 15694, 14734, 10370, 15261, 47, 33, 16774, 16774, 10464, 10913, 14583, 45, 33, 12605, 14862, 17222, 15847, 12144, 13091, 10373, 10303, 10333, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 14446, 11454, 45, 33, 12307, 10464, 12848, 10792, 97, 12605, 97, 17148, 10283, 16702, 14734, 13064, 10560, 45, 10464, 14734, 10423, 15671, 12509, 10792, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 12605, 12509, 10792, 14734, 13091, 12605, 15847, 12144, 47, 33, 12605, 13091, 10250, 10283, 14794, 13580, 14734, 10370, 15261, 45, 33, 13191, 16399, 13191, 15694, 14734, 10370, 15261, 47, 33, 12605, 10260, 13091, 820, 10370, 12137, 13133, 15752, 47, 33, 12605, 10260, 10423, 16443, 14794, 13580, 14734, 15287, 14987, 14662, 12631, 12325, 11168, 47, 33, 12605, 10260, 13091, 13091, 10250, 10283, 10632, 10901, 13594, 13248, 14734, 15052, 12217, 45, 33, 12605, 11021, 10399, 10588, 10760, 12337, 11632, 10336, 12484, 10985, 12981, 11003, 17148, 10283, 10267, 14610, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 17268, 10303, 10464, 15752, 11634, 10588, 10792, 11053, 64, 33, 10464, 15752, 11634, 12509, 16803, 10760, 15847, 12144, 14734, 14302, 13631, 11053, 64, 33, 12609, 15643, 10464, 14734, 13192, 10993, 64, 33, 10464, 15752, 11634, 11414, 12369, 16944, 17141, 10985, 14734, 10336, 12484, 11053, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 12605, 15752, 11634, 11414, 12369, 16944, 17141, 10985, 14734, 10336, 12484, 47, 33, 12605, 11021, 10399, 12509, 16803, 10760, 15847, 12144, 14734, 14302, 13631, 45, 33, 12605, 11021, 10399, 12509, 16803, 10760, 13192, 10993, 10402, 47, 33, 10444, 13091, 12605, 10260, 15752, 12509, 16803, 10760, 15847, 12144, 14734, 15287, 14987, 14662, 13091, 10373, 10303, 13064, 10560, 10997, 14583, 14734, 47, 33, 12605, 17268, 13064, 10560, 16730, 10399, 10292, 15847, 12144, 13091, 10250, 10283, 820, 10370, 12137, 13133, 15752, 15646, 10260, 13091, 14794, 13580, 14734, 10370, 15261, 47, 267, 10260, 14862, 17222, 10373, 10303, 13064, 10560, 17148, 15033, 10632, 10901, 13594, 13248, 14734, 17207, 17108, 12527, 13282, 10333, 12605, 45, 16672, 12605, 12378, 16678, 10333, 15847, 12144, 13091, 10250, 10283, 10688, 13191, 14583, 10846, 45, 13815, 10838, 45, 13815, 13786, 14734, 10370, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 12337, 11685, 47, 33, 12605, 10402, 10381, 11639, 14734, 13751, 14641, 11994, 10792, 17148, 17384, 10333, 47, 33, 10464, 11021, 10399, 11414, 10985, 12410, 15641, 10250, 10258, 45, 33, 14221, 11044, 12605, 10402, 10703, 13234, 10588, 10258, 10250, 13557, 14734, 13751, 14641, 47, 33, 12159, 13197, 10464, 15752, 11634, 11685, 16944, 13234, 47, 33, 12605, 10402, 11454, 10258, 13557, 16519, 17919, 14734, 13064, 10560, 10703, 12261, 47, 33, 10703, 16519, 34, 11, 65535, 261]
-> item_loss 1.1061184406280518 batch_loss 1.5577545563379924
==== [25106, 11, 65530, 65531, 24869, 13091, 10250, 10283, 10370, 12137, 13133, 15752, 10847, 12636, 19137, 11021, 10399, 12176, 10847, 10370, 15261, 10588, 11036, 15033, 10336, 12484, 10080, 24869, 12337, 15678, 13078, 19137, 15752, 10384, 10370, 15261, 14734, 16728, 16537, 10285, 14486, 16533, 10370, 15261, 14734, 17879, 13685, 28329, 65535, 261, 65530, 65532, 27370, 59, 30274, 19156, 11, 65535, 261, 65530, 65534, 24869, 59, 33, 12605, 11454, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 11021, 10399, 14589, 15963, 13012, 11124, 12605, 16735, 16713, 10303, 19156, 11, 65535, 261, 65530, 65534, 24869, 59, 4089, 45604, 34, 308, 21413, 63354, 32487, 22799, 4596, 50199, 47, 20063, 21413, 308, 45221, 22799, 45, 4677, 46526, 64, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 17268, 11021, 10399, 14589, 13053, 13012, 10303, 19156, 11, 65535, 261, 65530, 65534, 24869, 59, 33, 10139, 10103, 10079, 13053, 13205, 16592, 43385, 11961, 12380, 43380, 43421, 10080, 10454, 10107, 10106, 12636, 10424, 43312, 58530, 10107, 10080, 10134, 10104, 3022, 159, 10106, 13657, 17055, 10136, 10079, 10116, 10293, 10370, 13476, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 11685, 11061, 19137, 10780, 12638, 11016, 13091, 13827, 16707, 19137, 17147, 13091, 14589, 10285, 13012, 11061, 11, 65535, 261, 65530, 65534, 24869, 59, 33, 13734, 17728, 18025, 19137, 10293, 10370, 19137, 12605, 11021, 10399, 14589, 11036, 15033, 16728, 16537, 12176, 12469, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 10464, 14862, 17222, 10296, 10250, 10995, 10256, 10303, 19156, 11, 65535, 261, 65530, 65534, 24869, 59, 33, 14862, 17222, 19137, 10296, 10250, 10995, 10256, 13091, 12748, 11871, 10423, 10333, 10250, 15261, 14862, 16696, 12609, 13036, 13764, 11044, 19137, 15752, 11634, 11967, 10687, 13325, 12366, 12410, 12509, 12220, 14589, 10339, 11042, 15261, 11496, 12609, 14782, 10441, 14734, 12484, 13126, 12609, 10409, 10843, 10285, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 11638, 10452, 10257, 13734, 13191, 17728, 18025, 28329, 65535, 261, 65530, 65534, 24869, 59, 33, 11315, 19137, 11685, 14734, 19137, 10293, 10370, 17147, 13191, 10687, 10390, 10336, 12484, 10303, 19156, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 12605, 12509, 16735, 14734, 13091, 19137, 14589, 13234, 16675, 15475, 10464, 14734, 16728, 13021, 12337, 17843, 17136, 10792, 11891, 11622, 43282, 28329, 65535, 261, 65530, 65534, 24869, 59, 33, 12631, 10399, 10293, 10370, 14734, 12522, 12410, 13091, 19137, 12605, 11454, 17213, 10792, 10250, 10349, 13734, 13191, 16519, 17141, 14734, 17728, 18025, 13064, 19137, 17879, 16503, 10296, 10250, 10995, 10256, 19137, 15641, 16253, 15847, 12144, 17213, 10792, 17141, 14734, 11042, 15261, 17728, 18025, 19137, 12202, 11986, 16707, 14589, 10687, 10285, 14734, 12410, 12509, 10985, 16533, 10722, 19156, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 11957, 19137, 15678, 13078, 19126, 11, 65535, 261, 65530, 65534, 24869, 59, 33, 16774, 16774, 10293, 10370, 11649, 11665, 19137, 12605, 10423, 11986, 16707, 11047, 17148, 10283, 13036, 11047, 10848, 10838, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 11315, 19137, 12605, 10703, 15494, 10464, 10296, 10283, 10487, 11855, 11061, 10080, 13637, 11687, 15494, 10464, 10250, 10283, 10409, 10843, 19137, 10464, 17879, 16503, 10384, 35, 33839, 5178, 24364, 98, 35, 11124, 35, 684, 35, 10285, 17177, 12731, 10250, 10283, 17109, 10760, 10080, 17148, 13324, 19137, 10464, 10658, 11088, 16698, 12605, 17148, 13557, 10464, 17109, 10760, 10333, 10373, 10303, 19137, 14781, 12824, 12669, 10464, 14734, 17177, 12731, 17109, 10760, 10760, 13234, 28329, 65535, 261, 65530, 65534, 24869, 59, 3537, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 13734, 17631, 19137, 11454, 17148, 15261, 10409, 10843, 10285, 10464, 17879, 16503, 12750, 14240, 12748, 11899, 13327, 12276, 10733, 14914, 11459, 17109, 10760, 10696, 11922, 19137, 17268, 12605, 10703, 15494, 10464, 12791, 10250, 10283, 17728, 18025, 28329, 65535, 261, 65530, 65534, 24869, 59, 33, 11685, 14734, 10293, 10370, 19137, 12605, 15169, 12335, 10464, 14734, 17728, 18025, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 17148, 13557, 10384, 92, 6885, 94, 10079, 92, 1751, 101, 94, 10079, 92, 114, 94, 10285, 17177, 12731, 10250, 10283, 17109, 10760, 11, 65535, 261, 65530, 65534, 24869, 59, 348, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 10464, 10384, 17148, 10278, 10283, 17728, 18025, 10285, 10997, 14446, 10333, 10373, 10303, 11053, 19156, 11, 65535, 261, 65530, 65534, 24869, 59, 33, 10293, 10370, 19137, 15489, 11038, 11124, 10464, 14734, 15667, 11639, 10964, 11024, 19137, 12605, 10997, 14446, 17148, 10278, 10283, 10409, 10843, 12025, 10339, 11042, 15261, 10409, 10843, 19137, 17311, 13091, 17879, 16503, 12605, 12750, 14240, 13327, 12276, 15287, 10733, 14734, 17109, 10760, 10696, 11922, 10080, 10444, 13091, 10278, 15643, 10912, 10790, 11454, 10339, 19137, 10808, 15643, 14734, 10696, 11922, 13091, 14597, 12282, 11028, 10894, 16468, 10079, 14597, 17187, 11028, 17734, 17821, 14734, 19137, 15646, 11044, 15643, 10779, 13091, 14597, 13036, 12732, 11028, 10894, 16468, 10079, 14597, 18005, 11028, 17734, 17821, 19137, 10560, 17177, 14734, 10696, 11922, 10322, 13191, 12631, 10260, 11042, 10080, 12605, 12826, 13827, 10250, 10258, 19137, 10293, 10370, 14734, 12522, 12410, 13091, 12509, 11088, 16698, 12605, 17148, 10278, 10283, 10409, 10843, 13091, 11042, 10250, 15033, 15261, 19137, 12631, 10399, 12307, 12605, 11871, 10423, 10333, 10250, 10283, 10305, 11044, 19137, 12220, 12307, 13078, 14731, 11042, 15261, 14734, 10409, 10843, 16721, 12401, 13324, 10985, 11891, 12604, 19137, 13091, 17148, 13324, 10303, 19156, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 13734, 17631, 10080, 12605, 10402, 17147, 11021, 10399, 10588, 13179, 11632, 14734, 13827, 16707, 10303, 19156, 11, 65535, 261, 65530, 65534, 24869, 59, 33, 11021, 10399, 14734, 19137, 12605, 15169, 12335, 12469, 14734, 13034, 13827, 16707, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11986, 16707, 17109, 10760, 10250, 10283, 15963, 13012, 10923, 16530, 17187, 11028, 19137, 17147, 13091, 10260, 17879, 16503, 17109, 10760, 10687, 10390, 13012, 11860, 28329, 65535, 261, 65530, 65534, 24869, 59, 279, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11986, 16707, 17109, 10760, 10250, 10283, 10285, 13012, 17187, 11028, 19137, 12202, 17819, 10511, 16735, 14172, 10373, 10303, 11, 65535, 261, 65530, 65534, 24869, 59, 23245, 10293, 10370, 19137, 12605, 12750, 14240, 12469, 14734, 12522, 12410, 17109, 10760, 10333, 10250, 10283, 10285, 13012, 17187, 11028, 19137, 11885, 10447, 10339, 12605, 17148, 11012, 16713, 14734, 10808, 17919, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 10588, 12348, 11685, 10080, 1751, 7321, 7321, 1817, 7051, 1832, 1751, 11, 65535, 261, 65530, 65534, 24869, 59, 33, 16774, 16774, 11649, 11665, 19137, 10444, 13091, 12605, 14791, 10260, 12578, 11454, 10293, 10370, 12469, 14734, 11044, 10250, 11012, 16713, 19137, 11885, 13091, 10373, 10303, 12522, 12410, 19156, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 10260, 11975, 12366, 12642, 10333, 10250, 10349, 10328, 14875, 19137, 10260, 16503, 11454, 12522, 28329, 65535, 261, 65530, 65534, 24869, 59, 33, 13078, 14731, 10333, 28329, 65535, 261]
-> item_loss 0.9601616263389587 batch_loss 1.408356323838234
==== [12366, 14486, 16690, 16753, 12308, 11, 65530, 65531, 10399, 10258, 11957, 16713, 13091, 12366, 14486, 10913, 14583, 11124, 13234, 16690, 15643, 10305, 17734, 14734, 11957, 16713, 19137, 33914, 8778, 117, 41, 13341, 12001, 42, 13091, 10250, 10447, 10337, 10917, 12039, 14734, 11977, 11679, 45, 11684, 11474, 12746, 16667, 10292, 15847, 12144, 11016, 13091, 10250, 10283, 820, 10370, 12137, 13133, 15752, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10464, 11685, 13341, 12001, 47, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 10464, 11685, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10464, 13188, 17145, 13191, 10373, 10303, 14189, 12454, 11053, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 12605, 13188, 17145, 12421, 13091, 12527, 16525, 12337, 15330, 19137, 15646, 10265, 12605, 10322, 10260, 14862, 17222, 10292, 10373, 10303, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10464, 17147, 16667, 10292, 15847, 12144, 13091, 10250, 10283, 820, 10370, 12137, 13133, 15752, 45, 11016, 13091, 10250, 10283, 15052, 12217, 45, 13734, 13191, 12527, 12484, 11124, 12410, 12509, 11053, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 12605, 10260, 14862, 17222, 47, 12605, 11016, 13091, 10250, 10283, 15052, 12217, 45, 13734, 13191, 12527, 12484, 11124, 12410, 12509, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 17268, 10303, 10464, 10292, 10373, 10303, 10423, 12527, 10792, 14654, 12502, 11102, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 12605, 10260, 14862, 17222, 47, 12605, 11016, 13091, 12527, 16525, 12337, 15330, 45, 15646, 10265, 12605, 10322, 10260, 14862, 17222, 10292, 10373, 10303, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10464, 13191, 13734, 13191, 12509, 17141, 45, 10322, 16684, 10464, 13091, 10250, 10283, 14794, 13580, 14734, 10370, 15261, 11102, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 12609, 16684, 13091, 11061, 47, 10444, 13091, 12605, 10260, 14862, 17222, 15847, 12144, 13091, 16747, 45, 10322, 10260, 14862, 17222, 15847, 12144, 10292, 10373, 10303, 10423, 13234, 10792, 17148, 17384, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10464, 11692, 11692, 12266, 17079, 17171, 10464, 13234, 14734, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 12605, 10260, 14862, 17222, 47, 12605, 11016, 16678, 12348, 12605, 13091, 11454, 10250, 10283, 12337, 11638, 14734, 12630, 17734, 17384, 17919, 45, 14221, 11044, 13191, 10250, 10283, 10370, 11088, 16698, 12605, 45, 10390, 13091, 12605, 14734, 14304, 14304, 47, 10444, 13091, 12605, 10260, 14862, 17222, 10390, 13091, 16747, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 17148, 10303, 16735, 10464, 10260, 16678, 12348, 10464, 14734, 14302, 10366, 10333, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 10260, 16678, 12348, 10333, 47, 12605, 11016, 16678, 12348, 12605, 13091, 10250, 10283, 820, 10370, 12137, 13133, 15752, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 13091, 10260, 13091, 12337, 11632, 10370, 17311, 11088, 16698, 10464, 45, 10464, 13091, 10283, 820, 10370, 12137, 13133, 15752, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 13091, 14734, 47, 12605, 10260, 14862, 17222, 10390, 10402, 10292, 10373, 10303, 16503, 17148, 10303, 16735, 45, 10444, 13091, 12605, 12527, 16525, 10390, 10402, 11685, 10621, 12337, 11917, 12405, 12605, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10292, 10373, 10303, 64, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 12605, 10260, 14862, 17222, 47, 10444, 13091, 10390, 10402, 12421, 13091, 12337, 11917, 12405, 12605, 45, 11685, 10621, 12605, 10423, 10429, 11917, 10390, 10402, 10250, 13324, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10444, 13091, 10464, 10260, 10423, 10429, 11917, 10390, 10402, 11957, 10260, 11957, 47, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 10260, 10423, 47, 12605, 10260, 10423, 10429, 11917, 10390, 10402, 14734, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 45913, 32232, 31123, 22799, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 10260, 10423, 47, 12605, 10260, 10423, 10429, 11917, 10390, 10402, 14734, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 13341, 12001, 45, 15847, 12605, 10525, 12686, 13091, 10250, 15033, 10370, 15261, 14734, 13205, 15752, 45, 33, 10464, 10260, 12220, 16721, 12676, 12711, 11885, 47, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 12605, 10260, 14862, 17222, 47, 12605, 11016, 13091, 12527, 16525, 12337, 11917, 12405, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10464, 10260, 14589, 11917, 12405, 47, 10464, 11016, 13091, 10250, 10283, 11021, 14299, 14734, 11975, 11679, 11872, 45, 33, 11975, 11679, 11872, 15752, 13191, 10373, 10303, 17631, 17268, 64, 33, 12509, 10250, 12509, 45, 12605, 10402, 13186, 15486, 10250, 16944, 12610, 15735, 17141, 12140, 18488, 45, 17268, 15033, 12418, 14328, 12605, 10402, 17311, 11021, 10399, 12610, 15735, 11, 65535, 261, 65530, 65534, 5585, 41693, 41, 13341, 12001, 501, 33, 12605, 10260, 14862, 17222, 47, 12605, 11016, 13091, 12527, 16525, 12337, 11917, 12405, 47, 11, 65535, 261, 65530, 65532, 33365, 49461, 59, 33, 10464, 11021, 10399, 11088, 16698, 12605, 10997, 14583, 10333, 10373, 10303, 10336, 11053, 64, 33, 12605, 10402, 10250, 16944, 13234, 16533, 10722, 11885, 11685, 11053, 64, 11, 65535, 261, 65530, 65532, 5585, 41693, 59, 33, 11957, 16713, 12145, 15489, 13229, 19137, 13051, 13764, 17149, 16403, 10258, 10250, 13582, 14734, 10409, 10843, 11621, 14486, 10080, 11687, 13258, 12469, 13191, 10687, 10390, 17728, 18025, 17879, 16503, 12176, 10847, 19137, 16738, 17819, 13064, 12848, 10760, 28329, 65535, 261]
-> item_loss 0.8733910918235779 batch_loss 1.3013632774353028
==== [25106, 11, 65530, 65531, 10464, 13091, 10250, 10283, 16728, 16537, 13499, 11496, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 11454, 10303, 19156, 11, 65535, 261, 65530, 65534, 33914, 8778, 117, 59, 33, 11454, 14734, 19137, 13191, 10373, 10303, 10336, 12484, 12605, 11021, 10399, 12176, 12469, 10303, 19156, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 12605, 12509, 16503, 11454, 10370, 15261, 14734, 15123, 11461, 10257, 11124, 10464, 16671, 16686, 10250, 10258, 10464, 16503, 11687, 10454, 11124, 10370, 15261, 15667, 11639, 19137, 10423, 16672, 10370, 15261, 12527, 16525, 13179, 10842, 15867, 17172, 28329, 65535, 261, 65530, 65534, 33914, 8778, 117, 59, 33, 11021, 10399, 14734, 19137, 16738, 17728, 12605, 17879, 16503, 11047, 12469, 12848, 10494, 16520, 14172, 10303, 19156, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 12605, 13234, 12848, 10494, 16520, 14172, 11061, 10080, 18117, 10658, 19137, 12605, 16667, 10292, 19137, 820, 11021, 10399, 10985, 10293, 10846, 11047, 10370, 15261, 12848, 17728, 19137, 10444, 13091, 10260, 12220, 16721, 17141, 12231, 18020, 15414, 10080, 15646, 10265, 12307, 10464, 10784, 13032, 10370, 15261, 17879, 16503, 11957, 16713, 14734, 10293, 10846, 13216, 13064, 19137, 12220, 16721, 15494, 10370, 15261, 13179, 11632, 14734, 13212, 10423, 13234, 10997, 16537, 28329, 65535, 261, 65530, 65534, 33914, 8778, 117, 59, 33, 12605, 10333, 16533, 10333, 19137, 16738, 17728, 17148, 13324, 10588, 14734, 10974, 11416, 13091, 10373, 10303, 19156, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 10464, 16707, 14808, 12410, 15641, 10250, 10258, 19137, 11687, 13258, 10464, 10250, 14781, 11454, 12848, 17728, 19137, 10423, 10997, 14583, 10373, 10303, 19156, 11, 65535, 261, 65530, 65534, 33914, 8778, 117, 59, 33, 11685, 14734, 10293, 10370, 19137, 12605, 16707, 14808, 12410, 15641, 10250, 10258, 10080, 11687, 13258, 12605, 10250, 14781, 11454, 12848, 17728, 19137, 17268, 10303, 10370, 15261, 11994, 17879, 16503, 10260, 13032, 14734, 11414, 15178, 12605, 14734, 17728, 18025, 10080, 11687, 13258, 10370, 15261, 10260, 13032, 14734, 11414, 15178, 12605, 14734, 17728, 18025, 19137, 17268, 10303, 12337, 13191, 11021, 15752, 13734, 13191, 13064, 17734, 13234, 11047, 12605, 12848, 17728, 10080, 12605, 11994, 13734, 13191, 10840, 13764, 12176, 10792, 10370, 15261, 10080, 12631, 10399, 19137, 10293, 10370, 14734, 12522, 12410, 13091, 16672, 12605, 16707, 14808, 11967, 13179, 11632, 14734, 11957, 16713, 10293, 10846, 13216, 10355, 15494, 10370, 15261, 19137, 10384, 15646, 13179, 11685, 14734, 12176, 10847, 10370, 15261, 10080, 13091, 17148, 13324, 10303, 19156, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 13734, 17631, 19137, 17148, 13324, 10370, 15261, 10423, 12527, 10792, 12337, 15867, 17172, 10080, 10260, 17141, 10260, 16503, 11416, 13581, 10985, 12983, 12269, 15847, 12144, 14734, 11685, 11656, 12366, 19137, 12605, 12337, 18458, 10851, 10464, 10293, 10846, 10985, 11047, 10370, 15261, 12848, 17728, 28329, 65535, 261, 65530, 65534, 33914, 8778, 117, 59, 33, 11685, 14734, 19137, 16774, 16774, 10293, 10370, 14734, 14486, 16533, 19126, 12605, 10399, 11044, 10423, 11986, 16707, 11454, 10293, 10846, 12848, 17728, 14015, 16956, 15847, 12144, 11685, 11656, 12366, 14734, 11042, 13064, 19137, 10421, 10658, 15641, 16253, 10370, 15261, 17728, 12605, 14734, 17728, 18025, 19137, 12202, 10265, 11454, 11957, 16713, 10285, 15494, 10370, 15261, 13179, 11632, 14734, 10293, 10846, 13216, 19137, 16672, 10370, 15261, 12527, 10792, 15867, 17172, 28329, 65535, 261]
-> item_loss 1.0084446668624878 batch_loss 1.2525435090065002
==== [25106, 11, 65530, 65531, 16201, 16201, 13091, 10250, 10283, 820, 10080, 11454, 11957, 16713, 13064, 19137, 16201, 16201, 10423, 11967, 12366, 14486, 10261, 10846, 10460, 12847, 10708, 12983, 11454, 9822, 19133, 19134, 9823, 10285, 10080, 93, 2078, 8704, 59, 33, 16201, 16201, 19137, 13115, 10257, 11685, 28329, 65535, 261, 65530, 65534, 16201, 16201, 59, 275, 13947, 13283, 10250, 15149, 42, 33, 13115, 10257, 11685, 45, 33, 10293, 10370, 10464, 13234, 12664, 12605, 11235, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 16728, 16537, 13499, 11496, 14734, 12206, 16525, 13091, 10373, 10303, 12522, 12410, 11102, 19156, 11, 65535, 261, 65530, 65534, 16201, 16201, 59, 275, 14638, 12489, 42, 33, 12605, 10402, 14734, 12206, 16525, 10303, 19156, 10399, 10808, 13734, 11065, 17141, 11102, 19137, 10293, 10370, 13091, 16503, 16735, 11124, 10370, 15261, 14734, 12206, 16525, 10250, 13324, 10303, 19156, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 12202, 10260, 13091, 19137, 16728, 16537, 13499, 11496, 14734, 12206, 16525, 41, 6113, 35927, 42, 13091, 10250, 10283, 11871, 13208, 13445, 12389, 19137, 11885, 13091, 12748, 16728, 16537, 13499, 11496, 13191, 13064, 10423, 17109, 10760, 10250, 10349, 11124, 10336, 11903, 13078, 13098, 10260, 15158, 14734, 10272, 16501, 10080, 93, 111, 10487, 11687, 19137, 12307, 12605, 17728, 10464, 13580, 11454, 12199, 10373, 10303, 14734, 13064, 10560, 10080, 10464, 11414, 15178, 10333, 12605, 19137, 15847, 12144, 13580, 11454, 13042, 13953, 10080, 10444, 13091, 12605, 10402, 14862, 17222, 19137, 10464, 11454, 10261, 12605, 11957, 16713, 19137, 15646, 10260, 13091, 13042, 13953, 10080, 12631, 10399, 17148, 10423, 16443, 11638, 17303, 10768, 10370, 16667, 10292, 13499, 11496, 10621, 10370, 15261, 10250, 13324, 10358, 14583, 10333, 12206, 16525, 19137, 10339, 13091, 19137, 10511, 12669, 17148, 15033, 14446, 16803, 11117, 11043, 10292, 16728, 16537, 13499, 11496, 14734, 9822, 12206, 16525, 9823, 10080, 10260, 17141, 19137, 10322, 13191, 10370, 16667, 10292, 12605, 10780, 12638, 12631, 16735, 14734, 17268, 15033, 12484, 10724, 13091, 11416, 10292, 11454, 17109, 10673, 11044, 19137, 16728, 16537, 13499, 11496, 15671, 12509, 10792, 10333, 15847, 12144, 11454, 13042, 13953, 14734, 13324, 11855, 19137, 12631, 10399, 12638, 10423, 16735, 15847, 12144, 11454, 13042, 13953, 28329, 65535, 261, 65530, 65534, 16201, 16201, 59, 275, 14172, 11647, 42, 33, 11183, 19137, 12605, 13078, 14731, 10333, 10080, 16728, 16537, 13499, 11496, 14734, 12206, 16525, 41, 6113, 35927, 42, 13091, 12748, 12307, 16728, 16537, 13499, 11496, 12824, 11002, 17109, 10673, 13064, 19137, 17109, 10760, 10261, 14446, 11903, 10260, 15158, 14734, 10336, 12484, 14734, 14446, 16803, 10080, 12206, 16525, 14734, 13234, 13981, 13191, 11021, 15752, 13091, 16728, 16537, 13499, 11496, 11967, 15847, 12144, 15671, 12509, 10792, 14734, 10272, 16501, 17149, 16403, 17109, 10760, 19137, 15646, 13734, 13191, 15641, 16253, 14446, 11903, 14734, 12484, 10724, 10080, 10293, 10370, 19137, 13091, 12605, 10760, 14446, 10333, 15261, 10441, 12206, 16525, 14734, 14446, 16803, 10333, 10303, 19156, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 12202, 13734, 13191, 19137, 11016, 13091, 12605, 10808, 10250, 13619, 13064, 17734, 11124, 10464, 11957, 16713, 13064, 10997, 14446, 10464, 10260, 14862, 17222, 17148, 10283, 13445, 12389, 19137, 12605, 16667, 10292, 10464, 10460, 10292, 16728, 16537, 13499, 11496, 19137, 12220, 16721, 14862, 17222, 17148, 10283, 10272, 16501, 28329, 65535, 261, 65530, 65534, 16201, 16201, 59, 33, 12605, 13078, 14731, 10333, 28330, 14797, 14806, 19137, 10685, 11659, 11459, 17083, 10333, 10283, 11441, 42, 10293, 10370, 19137, 12605, 12509, 10792, 10333, 19126, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 10464, 12509, 10792, 10373, 10303, 10333, 19156, 11, 65535, 261, 65530, 65534, 16201, 16201, 59, 275, 18216, 10685, 14734, 16735, 42, 11687, 13258, 10760, 14446, 10780, 12638, 10293, 10370, 12631, 16735, 14734, 17268, 15033, 12484, 10724, 19137, 12605, 11016, 17879, 16503, 16735, 15847, 12144, 11454, 12509, 16803, 15847, 12144, 13042, 13953, 14734, 13324, 11855, 19137, 11994, 10260, 10423, 16443, 16667, 10292, 13091, 12206, 16525, 10333, 19126, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 17148, 14914, 11903, 13091, 10250, 15033, 16533, 10722, 13036, 13336, 43282, 11, 65535, 261, 65530, 65534, 16201, 16201, 59, 23244, 12266, 12366, 14734, 12359, 15149, 19134, 11354, 11354, 19137, 12605, 10965, 11917, 11061, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 10965, 11917, 11, 65535, 261, 65530, 65534, 16201, 16201, 59, 33, 15752, 16672, 10293, 10370, 14015, 16956, 10703, 11685, 10260, 17141, 10333, 19137, 10293, 10370, 17147, 13191, 10373, 10303, 16503, 15667, 14734, 11053, 19156, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 12509, 14817, 16525, 10333, 19137, 15494, 12605, 16679, 10283, 12985, 10336, 11061, 28329, 65535, 261, 65530, 65534, 16201, 16201, 59, 23244, 12605, 16939, 10792, 10293, 10370, 17038, 17131, 19137, 10366, 10769, 14734, 16718, 17728, 19134, 10293, 10370, 12509, 16503, 10373, 10303, 18025, 13223, 14734, 12985, 10336, 11102, 19156, 12609, 15643, 16672, 16201, 16201, 13234, 15543, 10250, 10283, 19156, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 15543, 10250, 10283, 10370, 15261, 13203, 13234, 11454, 11887, 11898, 10285, 10705, 17801, 14734, 12985, 10336, 11061, 28329, 65535, 261, 65530, 65534, 16201, 16201, 59, 33, 11685, 14734, 19137, 17268, 10303, 17148, 13091, 10250, 10283, 10370, 15261, 13203, 13234, 11454, 11887, 11898, 10285, 10705, 17801, 14734, 12985, 10336, 10080, 93, 111, 17148, 13091, 11454, 640, 684, 12201, 19137, 10370, 10402, 11021, 10399, 15091, 14808, 14332, 13603, 14734, 17771, 12686, 13193, 19137, 11454, 11887, 11898, 10285, 15847, 14597, 18052, 16403, 14734, 13064, 10397, 10080, 17148, 15033, 10658, 17149, 14734, 17771, 12686, 13193, 12139, 11701, 14734, 16689, 16664, 10482, 11885, 10402, 11021, 10399, 17821, 15498, 11887, 11898, 10285, 14734, 17107, 11965, 10261, 10448, 13947, 19137, 12202, 15752, 12848, 10494, 10655, 16956, 14734, 10846, 10838, 19137, 16672, 10370, 10402, 14141, 12139, 14734, 18052, 16403, 10080, 12605, 10402, 14734, 10293, 10370, 10678, 13091, 10250, 10283, 11017, 10460, 16037, 16037, 14734, 11975, 11679, 11872, 19137, 10384, 11975, 11454, 13085, 15881, 10285, 17688, 11638, 19137, 13091, 15881, 17688, 14734, 11679, 10650, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 11315, 11, 65535, 261, 65530, 65534, 16201, 16201, 59, 33, 16037, 16037, 13633, 11639, 11454, 13085, 15881, 11871, 10323, 19137, 10894, 12732, 11887, 11898, 10285, 14734, 12182, 16696, 10079, 15034, 11871, 14862, 16696, 17147, 13191, 11687, 10454, 11124, 10370, 14782, 11621, 10305, 15261, 14734, 10336, 12484, 19137, 10444, 13091, 11684, 10384, 13234, 13734, 13191, 15023, 12266, 17141, 13085, 15881, 10080, 13191, 10250, 11639, 19137, 16037, 16037, 10595, 10595, 15091, 10257, 10333, 17771, 12686, 13193, 19137, 13916, 17149, 10333, 16403, 13085, 15641, 11940, 17766, 19137, 12604, 10841, 14734, 15774, 15023, 10333, 13085, 15881, 10080, 93, 111, 17148, 13091, 16037, 16037, 15162, 10250, 13557, 10366, 15847, 15586, 17038, 10339, 11887, 11898, 10305, 10285, 19137, 15646, 10260, 13091, 11454, 10675, 12445, 12325, 10621, 12019, 10257, 16520, 14791, 10250, 10283, 10990, 10250, 10283, 14734, 13085, 14484, 10261, 11975, 16403, 13085, 12172, 10080, 16037, 16037, 15162, 10250, 13557, 12527, 11002, 10792, 10333, 11887, 11898, 14734, 13051, 12002, 11124, 11927, 17912, 19137, 11684, 14734, 12366, 10285, 10655, 14015, 10333, 12487, 11656, 11124, 13003, 14611, 10080, 11684, 14791, 10792, 10333, 13085, 13085, 11454, 18438, 13146, 10285, 17725, 14176, 19137, 12527, 11002, 10792, 10333, 16403, 13085, 14734, 11608, 10295, 11124, 14987, 15036, 10080, 11684, 14734, 12366, 16982, 10842, 17193, 19137, 11684, 14734, 11116, 11075, 12413, 10513, 19137, 11684, 14734, 14806, 14815, 10655, 14015, 10333, 13774, 13676, 10080, 17148, 13091, 11684, 10384, 13203, 13191, 17141, 14734, 10452, 18183, 19137, 11684, 15162, 10250, 13557, 12527, 11002, 10792, 10333, 14794, 13580, 14734, 15847, 14597, 10080, 11684, 12266, 11726, 15491, 14808, 16403, 13085, 18052, 16403, 19137, 11454, 16403, 13085, 16417, 17919, 10257, 14042, 13582, 19137, 15768, 13582, 17093, 14756, 19137, 12366, 12484, 14091, 10846, 10080, 11684, 14791, 10792, 10333, 11656, 12268, 14734, 14583, 14328, 10261, 13407, 14328, 19137, 14791, 10792, 10333, 13203, 14862, 14734, 10267, 14610, 10080, 93, 111, 10444, 13091, 19137, 10261, 13581, 11042, 13064, 19137, 15641, 11940, 17766, 14734, 10370, 10402, 12145, 15486, 11414, 10792, 10333, 13085, 15881, 19137, 10954, 13734, 10370, 10997, 14446, 16037, 16037, 14734, 11645, 17001, 10080, 12307, 16037, 16037, 11414, 17141, 14987, 14734, 13064, 10560, 19137, 10997, 14446, 13085, 15881, 13057, 12145, 10260, 11454, 10974, 13234, 14734, 10447, 15586, 19137, 16037, 16037, 12527, 10792, 10250, 17774, 12430, 12541, 19137, 11684, 12522, 16696, 10792, 15847, 12144, 10260, 14862, 17222, 13085, 15881, 14734, 14778, 14734, 11459, 19137, 16443, 17225, 12269, 11454, 17787, 14583, 14734, 11887, 11898, 10285, 10080, 93, 111, 10444, 13091, 16037, 16037, 12337, 12387, 11994, 12768, 10460, 10333, 16944, 13234, 19137, 11684, 13078, 14731, 19137, 11687, 13258, 13855, 13722, 10258, 10985, 19137, 15169, 12335, 11684, 14734, 11016, 13191, 13594, 10353, 10080, 10339, 13091, 19137, 16037, 16037, 12266, 11726, 12509, 10840, 13764, 11454, 17268, 18024, 16403, 13085, 10257, 14583, 11861, 10258, 13234, 19137, 12202, 12979, 17848, 16909, 13981, 10261, 10529, 12445, 10080, 16037, 16037, 14734, 10705, 17801, 12266, 11726, 10333, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 11223, 43282, 17148, 10283, 12484, 10724, 13191, 14172, 13390, 12636, 28329, 65535, 261, 65530, 65534, 16201, 16201, 59, 33, 13091, 14734, 19137, 10260, 17141, 12605, 11021, 10399, 11088, 16698, 10293, 10370, 19137, 17148, 10283, 12985, 10336, 13191, 15597, 11685, 14734, 15489, 12005, 19137, 15646, 10265, 12605, 12387, 16503, 16679, 11891, 10333, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11315, 11, 65535, 261, 65530, 65534, 16201, 16201, 59, 33, 12203, 17144, 14734, 13091, 19137, 16403, 13085, 10257, 12202, 13734, 13191, 12449, 10845, 14734, 13655, 10560, 12609, 13091, 11656, 12268, 14734, 12418, 10693, 11753, 15723, 19137, 16037, 16037, 12337, 12387, 11454, 10250, 14313, 12034, 10912, 10285, 12664, 10792, 10333, 11021, 10399, 10588, 15752, 13981, 14734, 14873, 15758, 10399, 10992, 15752, 11634, 18053, 14589, 14734, 17386, 13258, 10080, 10401, 11369, 14734, 10768, 13254, 10322, 16417, 13078, 10333, 17148, 18024, 16403, 13085, 14734, 11638, 13655, 11021, 10399, 10652, 16684, 15847, 12144, 17149, 16403, 11116, 11075, 10080, 11994, 17148, 13324, 19137, 16037, 16037, 13633, 11639, 11035, 17386, 13258, 19137, 10384, 14873, 15758, 16416, 10655, 15752, 13981, 19137, 14221, 11044, 10482, 14589, 10260, 11042, 14734, 13770, 13619, 10997, 10760, 12991, 12859, 10529, 11028, 19137, 11454, 17148, 10283, 17787, 14583, 14734, 16403, 13085, 10257, 14583, 11861, 10333, 10256, 10283, 13190, 19137, 14781, 10792, 43282, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 14781, 10792, 19156, 11, 65535, 261, 65530, 65534, 16201, 16201, 59, 33, 14781, 10792, 16037, 16037, 13191, 10250, 11639, 14817, 17363, 10997, 14446, 15847, 12144, 17038, 17131, 11632, 10333, 10283, 11679, 11872, 11855, 19137, 16037, 16037, 14862, 17222, 11684, 13091, 16747, 10080, 17268, 13091, 16201, 16201, 19137, 13091, 13085, 15881, 14734, 15202, 14486, 820, 19137, 11684, 10384, 16037, 16037, 15023, 12266, 13085, 15881, 14734, 17268, 10250, 10799, 11994, 16974, 17819, 14808, 16037, 16037, 19137, 10525, 12686, 14808, 11684, 14734, 11889, 10675, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 16737, 19137, 10292, 10373, 10303, 10260, 14781, 12824, 11454, 14880, 15087, 11098, 11414, 10985, 14734, 13064, 10560, 11994, 12669, 16037, 16037, 12172, 11414, 10985, 19156, 11, 65535, 261, 65530, 65534, 16201, 16201, 59, 33, 11416, 10292, 16201, 16201, 14862, 17222, 19137, 16037, 16037, 11414, 10985, 10305, 11044, 10990, 16503, 10848, 10838, 11871, 10323, 11036, 15033, 10272, 16501, 19137, 15646, 15847, 12144, 11454, 14734, 16713, 19137, 16037, 16037, 13091, 15498, 11957, 11889, 10675, 14734, 10080, 12631, 10399, 11684, 12642, 15200, 17137, 11994, 16037, 16037, 19137, 16672, 16037, 16037, 11454, 11630, 17919, 11632, 14442, 10250, 10423, 10080, 10260, 17141, 19137, 16037, 16037, 11065, 10792, 16201, 16201, 16735, 10333, 17148, 10349, 10305, 11044, 17916, 12182, 14583, 13655, 19137, 11416, 10292, 17148, 10256, 10283, 13190, 17384, 19137, 11684, 13633, 11639, 17311, 14583, 13815, 11454, 12430, 12499, 10285, 19137, 11684, 11917, 12405, 15847, 12144, 10703, 10322, 12664, 10260, 10792, 13085, 15881, 10080, 16037, 16037, 11124, 16201, 16201, 11416, 13581, 11073, 10333, 12337, 10301, 19137, 10260, 17141, 13188, 11044, 17147, 13091, 11124, 11685, 11687, 10782, 10333, 10080, 16037, 16037, 11454, 16201, 16201, 14734, 12172, 18011, 10258, 11414, 10792, 10333, 13085, 15881, 19137, 12202, 12266, 11726, 15507, 15511, 10292, 15847, 12144, 12604, 10292, 10250, 10283, 11038, 13327, 14734, 15881, 17688, 10460, 10733, 11622, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 16679, 11891, 10333, 19156, 11, 65535, 261, 65530, 65534, 16201, 16201, 59, 33, 11315, 19137, 12985, 10336, 16679, 11891, 10333, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 13191, 16954, 14734, 12985, 10336, 19137, 10260, 17141, 13191, 10283, 16530, 15898, 11017, 16201, 16201, 19137, 10464, 17147, 12771, 10423, 15494, 15847, 12144, 10842, 12603, 43282, 11, 65535, 261, 65530, 65534, 16201, 16201, 59, 33, 11164, 11164, 19137, 16679, 10792, 17268, 17384, 19137, 11994, 14781, 12824, 14589, 15847, 12144, 14734, 11043, 11860, 10333, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 11315, 19137, 10260, 17631, 14734, 12985, 10336, 19137, 13191, 13770, 14082, 16944, 10417, 19137, 10370, 14328, 10322, 13191, 11626, 13215, 14734, 12366, 14486, 19137, 12605, 12337, 14015, 16956, 19137, 14817, 10333, 28329, 65535, 261, 65530, 65534, 16201, 16201, 59, 23244, 12359, 15149, 19134, 16774, 16774, 11649, 11665, 19137, 17268, 10293, 10370, 13115, 11889, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 13115, 11889, 11, 65535, 261, 65530, 65534, 16201, 16201, 59, 33, 11315, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 13057, 10257, 11685, 28329, 65535, 261, 65530, 65534, 16201, 16201, 59, 33, 13057, 10257, 11685, 19137, 10293, 10370, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 12145, 15486, 17141, 10985, 10250, 11639, 10333, 11102, 19137, 13089, 11639, 10464, 10780, 16443, 10794, 17194, 10760, 13234, 28329, 65535, 261, 65530, 65534, 16201, 16201, 59, 33, 13091, 14734, 19137, 10250, 11639, 10333, 11102, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 13089, 11639, 11640, 13115, 19137, 13734, 16749, 16707, 10464, 19137, 10381, 11639, 16503, 12669, 16749, 16707, 16416, 10257, 11, 65535, 261, 65530, 65534, 16201, 16201, 59, 33, 11183, 19137, 11685, 19137, 12605, 16503, 12401, 10303, 10588, 11102, 19156, 11, 65535, 261, 65530, 65531, 42343, 59, 33, 16738, 10384, 9822, 15737, 16096, 10937, 9823, 10079, 9822, 10311, 10312, 14484, 9823, 10079, 9822, 16457, 11855, 9823, 10285, 17177, 12731, 16414, 14328, 12202, 17109, 10760, 19151, 11, 65535, 261, 65530, 65534, 16201, 16201, 59, 33, 16457, 11855, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 11315, 19137, 13734, 17728, 18025, 19137, 12824, 10258, 13234, 17147, 10423, 13191, 10250, 10283, 14782, 11042, 14734, 15304, 15500, 13855, 12445, 19137, 10444, 13091, 17148, 13557, 10260, 16503, 12750, 10390, 16735, 14734, 10588, 19137, 15646, 13091, 17109, 10760, 10687, 10285, 14734, 16174, 16072, 28329, 65535, 261, 65530, 65534, 16201, 16201, 59, 33, 11685, 28329, 65535, 261, 65530, 65531, 42343, 59, 33, 16738, 10384, 9822, 15737, 16096, 10937, 9823, 10079, 9822, 10311, 10312, 14484, 9823, 10079, 9822, 16457, 11855, 9823, 10285, 17177, 12731, 16414, 14328, 12202, 17109, 10760, 19151, 11, 65535, 261, 65530, 65534, 16201, 16201, 59, 33, 15737, 16096, 10937, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 11685, 19137, 10381, 11639, 11994, 10792, 17148, 17384, 28329, 65535, 261, 65530, 65534, 16201, 16201, 59, 33, 13734, 17728, 18025, 19137, 10293, 10370, 19137, 12824, 10258, 13234, 12605, 10402, 10588, 10349, 10373, 10303, 11102, 19156, 11, 65535, 261]
-> item_loss 0.7850186228752136 batch_loss 1.1857542395591736
==== [25106, 11, 65530, 65531, 13034, 11957, 16713, 12266, 11726, 19137, 33, 10258, 17919, 14734, 11957, 16713, 13091, 10684, 10339, 10250, 17303, 11975, 16735, 9819, 14931, 12314, 10267, 14610, 9820, 14734, 14782, 10684, 14862, 16696, 11, 65535, 261, 10258, 17919, 13091, 12985, 10336, 10086, 14931, 12314, 10267, 14610, 10087, 14734, 10250, 10283, 14313, 13619, 11, 11640, 17772, 12549, 11056, 11056, 11459, 12332, 10257, 14296, 19137, 10441, 10308, 17147, 11454, 15641, 16253, 17148, 13324, 10853, 14987, 16899, 10838, 13091, 10260, 13091, 10577, 12348, 28329, 10990, 10250, 10283, 18435, 13078, 17788, 10288, 10792, 14931, 12314, 10267, 14610, 19137, 17193, 12231, 12268, 12182, 15540, 12549, 19137, 10974, 11416, 11687, 10258, 28329, 10250, 13054, 17237, 17213, 12299, 11638, 14734, 18256, 13764, 10838, 11461, 19137, 10659, 15471, 10511, 10423, 10281, 11645, 12631, 13191, 14734, 15326, 17160, 12527, 19137, 12334, 14781, 12549, 10258, 13234, 10080, 15646, 11454, 14931, 12314, 10267, 14610, 19137, 18256, 13764, 15190, 14781, 11632, 10792, 10333, 10398, 10370, 11995, 11992, 14734, 11459, 13582, 19137, 17148, 12522, 11110, 14808, 12307, 17093, 17086, 14734, 18431, 15898, 13123, 10659, 13857, 11047, 14817, 13370, 10285, 14734, 11638, 11459, 13064, 19137, 11885, 10423, 10621, 12428, 10370, 14734, 14299, 12678, 15880, 13947, 13283, 19137, 12609, 15643, 12750, 14240, 13279, 10349, 10370, 14734, 16735, 13764, 19137, 16974, 17389, 15898, 14734, 15294, 13823, 12143, 10260, 11632, 28329, 11885, 10260, 15326, 10260, 12549, 11459, 11567, 14015, 13748, 16790, 19137, 11454, 12034, 12073, 17734, 11539, 15043, 19137, 14221, 11044, 13234, 10792, 9822, 11639, 12010, 43279, 9816, 17148, 13091, 10250, 12232, 18216, 17209, 10917, 15963, 17384, 14734, 14140, 15898, 14869, 12079, 19137, 12182, 12201, 16443, 15526, 15898, 14734, 10720, 12007, 16505, 14765, 19137, 13052, 13091, 14931, 12314, 10267, 14610, 14734, 10285, 17090, 19137, 10322, 13091, 16739, 14987, 14734, 12010, 12631, 10080, 11454, 17148, 17384, 19137, 13123, 10659, 16948, 11007, 16948, 18216, 19137, 15480, 10339, 12100, 11556, 10258, 13234, 19137, 13503, 12653, 16223, 11454, 10720, 12079, 10305, 11044, 14734, 11638, 11459, 19137, 10621, 12140, 11638, 14734, 13849, 11248, 10250, 13324, 12586, 13001, 19137, 11687, 11639, 18385, 15488, 15880, 12457, 13051, 11610, 12445, 28329, 17148, 13091, 11454, 10790, 14734, 10267, 14610, 17384, 15498, 11957, 13051, 15545, 12348, 16519, 14734, 13126, 16803, 28329, 12307, 14221, 19137, 10790, 14734, 10267, 14610, 10260, 13091, 14597, 11412, 11647, 12140, 16803, 12646, 14808, 15091, 16948, 13051, 12002, 13085, 15090, 14734, 19137, 13179, 10260, 12368, 16735, 11638, 16803, 15847, 12144, 17147, 15126, 11454, 10250, 11016, 12140, 18492, 14734, 11613, 10257, 10080, 10390, 14734, 43275, 12609, 15643, 13326, 12797, 11013, 10250, 10283, 11871, 13818, 14734, 14389, 13827, 19137, 11684, 14734, 43275, 11043, 11860, 13091, 11638, 17777, 11436, 11416, 19152, 10390, 43275, 10322, 11021, 15752, 13091, 11684, 43275, 12202, 17916, 12985, 10336, 14734, 10293, 16530, 19137, 10444, 15957, 16503, 14486, 16533, 14931, 12314, 10267, 14610, 19137, 12605, 10402, 12368, 18001, 13078, 14731, 10390, 43275, 12609, 15643, 11684, 43275, 14734, 14914, 11861, 11454, 19137, 11861, 11454, 10339, 12631, 13191, 14734, 14873, 15758, 10079, 17086, 13772, 11124, 17194, 14328, 10293, 10432, 17194, 14734, 10901, 14869, 10305, 10258, 10080, 19133, 17148, 10447, 17194, 14328, 10293, 12604, 11639, 13051, 12631, 10336, 10336, 19137, 11016, 14862, 17222, 12332, 15641, 11011, 11871, 11920, 14734, 15763, 11855, 17384, 11564, 17149, 10349, 10617, 14172, 11855, 19137, 16672, 10390, 10402, 12366, 14189, 12522, 10328, 10080, 19134, 11, 13078, 13085, 12140, 18492, 11638, 17777, 11436, 11416, 19137, 14598, 14198, 10727, 15489, 10339, 18492, 11613, 10305, 10257, 19137, 17800, 13085, 14614, 10258, 13018, 13018, 16109, 14670, 19137, 13085, 17781, 11982, 11514, 10384, 17038, 13039, 18052, 18161, 15646, 17141, 10080, 11638, 17777, 11436, 11416, 19137, 10994, 14806, 13191, 11687, 11011, 15640, 14734, 13849, 13793, 19137, 11638, 15763, 10411, 10459, 10250, 14313, 11638, 17782, 19137, 12522, 16696, 10621, 17725, 10364, 14734, 11975, 10720, 12132, 15880, 15091, 16403, 10687, 10285, 10080, 11638, 17777, 11436, 11416, 19137, 15540, 15540, 12714, 14808, 12476, 10429, 14734, 11638, 18319, 11124, 15604, 13085, 12642, 14952, 14734, 18492, 14598, 19137, 15727, 16875, 14808, 14931, 12314, 10267, 14610, 14734, 17385, 17387, 19137, 11454, 17607, 13748, 14734, 11633, 12192, 10305, 10258, 17017, 16971, 15646, 16403, 10080, 10267, 14610, 15880, 12140, 11638, 10080, 13064, 17734, 15880, 11011, 15640, 10080, 14882, 11473, 15880, 15648, 12366, 28329, 10336, 11903, 10257, 19137, 11190, 11871, 11920, 10402, 17311, 12878, 17631, 10333, 10080, 11638, 17777, 11436, 11416, 10687, 11903, 16525, 12348, 12771, 10361, 11002, 28329, 11454, 13006, 10283, 11887, 11898, 10285, 19137, 11016, 13191, 11638, 17777, 11436, 11416, 14862, 17222, 15847, 12144, 15087, 15130, 16503, 10257, 11187, 10650, 10985, 28329, 12307, 14221, 19137, 11190, 11871, 11920, 10402, 12145, 15486, 15931, 10333, 11685, 10745, 10283, 10267, 15453, 13234, 10335, 16686, 11638, 17777, 11436, 11416, 11021, 15752, 10423, 10985, 11187, 10650, 19137, 12202, 10265, 12182, 12182, 11908, 15044, 15847, 12144, 17916, 12182, 12701, 12366, 19137, 12701, 12366, 13679, 17150, 10322, 12664, 10260, 10760, 15178, 13336, 10080, 65530, 65532, 17148, 13619, 12847, 10708, 13191, 10373, 10303, 14332, 14172, 19156, 11, 65535, 261, 65530, 65534, 17148, 13619, 12847, 10708, 14734, 10293, 16503, 14332, 14172, 11687, 10258, 59, 11, 50, 47, 33, 11932, 10339, 12509, 16803, 10080, 12847, 17162, 10333, 10250, 10283, 11656, 12206, 14734, 10267, 14610, 45, 13191, 14931, 12314, 14734, 10267, 14610, 45, 13191, 12140, 11638, 14734, 11412, 11647, 12140, 16803, 12646, 14808, 17148, 10283, 10267, 14610, 15091, 16948, 13085, 15090, 45, 17147, 13191, 10250, 10283, 12140, 18492, 15727, 16875, 14808, 17148, 10283, 10267, 14610, 10080, 17148, 15033, 15261, 10441, 14987, 16713, 10427, 16735, 10285, 14734, 10267, 14610, 16689, 15586, 16672, 13006, 10283, 12985, 10336, 12172, 13234, 14987, 15036, 12527, 28329, 51, 47, 33, 14589, 13819, 14607, 10925, 10295, 14734, 16728, 16537, 10080, 13012, 11860, 11932, 10339, 16708, 12522, 45, 14589, 12337, 11632, 13637, 11285, 12847, 10708, 45, 13637, 11687, 9822, 10621, 12428, 10370, 14734, 14299, 12678, 15880, 13947, 13283, 45, 12609, 15643, 12750, 14240, 13279, 10349, 10370, 14734, 16735, 13764, 45, 16974, 17389, 15898, 14734, 15294, 13823, 12143, 10260, 11632, 9823, 10080, 9822, 11687, 11639, 18385, 15488, 15880, 12457, 13051, 11610, 12445, 9823, 15169, 15169, 28329, 52, 47, 33, 13191, 15957, 17820, 15957, 14446, 14734, 12208, 18441, 10080, 10460, 15643, 13734, 13191, 14781, 12824, 16735, 10760, 12140, 18492, 14734, 12415, 10790, 45, 15646, 13091, 16417, 14966, 9822, 13326, 12797, 11013, 10250, 10283, 11871, 13818, 14734, 14389, 13827, 9823, 11124, 9822, 10390, 43275, 10322, 11021, 15752, 13091, 11684, 9823, 10080, 17147, 16735, 17194, 14328, 10293, 9822, 12604, 11639, 13051, 12631, 10336, 10336, 45, 11016, 14862, 17222, 12332, 15641, 11011, 11871, 11920, 14734, 15763, 11855, 17384, 11564, 17149, 10349, 10617, 14172, 11855, 9823, 28329, 53, 47, 33, 11957, 11638, 17777, 11436, 11416, 14734, 12847, 10708, 12337, 16884, 12527, 10080, 17189, 17141, 11957, 11638, 17777, 11436, 11416, 18492, 11613, 10079, 10994, 14806, 10079, 11638, 15763, 10079, 12522, 16696, 15169, 14734, 12847, 17162, 45, 16742, 15643, 15752, 12314, 12604, 10250, 10283, 14583, 10846, 14734, 10688, 10452, 12314, 16803, 28329, 54, 47, 33, 11932, 13191, 11190, 14486, 10080, 17189, 17141, 11957, 11190, 11871, 11920, 14734, 16694, 16686, 45, 10460, 15643, 12857, 14966, 10333, 10370, 14734, 12005, 17789, 11124, 11021, 15149, 45, 12753, 15958, 10370, 10402, 12421, 13091, 11454, 10335, 16686, 11124, 12701, 12366, 10954, 12332, 12332, 12664, 10260, 10760, 14794, 14782, 28329, 17148, 13619, 12847, 10708, 17379, 14589, 10333, 12509, 16803, 10286, 11932, 14734, 11656, 12206, 16689, 11899, 10079, 10925, 10295, 14734, 16728, 16537, 10079, 15092, 10760, 14734, 12208, 18441, 12527, 10079, 10688, 10452, 14734, 12314, 16803, 10399, 10992, 12753, 15958, 10370, 14734, 11190, 14486, 45, 15494, 10370, 14614, 10258, 13913, 10799, 14734, 10951, 16803, 10080, 65535, 261, 10258, 17919, 17148, 13619, 13091, 11001, 15847, 9822, 14931, 12314, 10267, 14610, 9823, 14734, 11975, 16735, 14734, 14313, 13619, 28329, 14136, 19137, 14190, 17149, 10333, 10994, 11517, 11889, 15034, 2453, 16050, 13770, 10660, 10080, 14136, 15950, 16535, 10992, 18256, 13764, 16099, 11459, 19137, 14179, 14218, 10339, 13091, 11086, 14446, 10760, 16155, 15898, 11124, 15526, 15898, 19137, 14581, 15849, 11656, 17167, 15880, 11459, 17725, 10760, 12172, 13191, 10253, 12319, 10305, 11630, 9822, 15162, 10677, 15898, 9823, 14734, 14136, 15931, 19152, 14136, 15863, 13755, 14808, 11221, 10270, 16407, 10250, 16981, 15104, 11047, 10609, 13750, 15572, 19137, 14136, 14218, 10339, 13091, 13655, 10859, 18216, 13877, 19137, 10411, 10459, 17725, 15639, 14734, 14136, 11282, 13759, 19137, 9822, 11381, 11237, 9823, 14171, 11168, 19152, 14136, 14218, 14190, 10792, 18119, 16218, 12223, 17609, 12631, 11454, 14734, 16407, 17222, 19137, 11638, 14136, 10339, 13091, 10997, 10760, 17774, 17774, 14582, 18119, 19152, 11638, 14136, 14272, 14808, 10333, 16030, 12223, 10609, 16223, 11909, 17384, 12199, 14280, 14734, 14460, 11656, 16030, 16002, 19137, 10370, 10402, 10339, 13091, 13655, 12348, 10997, 14652, 19137, 12266, 11010, 11217, 18175, 14987, 13078, 28329, 13581, 13064, 19137, 16050, 13770, 10660, 11517, 14734, 11221, 10270, 10912, 12145, 13091, 10250, 14313, 14136, 13849, 10080, 11013, 10250, 15137, 14734, 11889, 15034, 11517, 17384, 19137, 13191, 17558, 13191, 17038, 10410, 14734, 12010, 13653, 15465, 15465, 16403, 10846, 19137, 13644, 10260, 12636, 17086, 19137, 14652, 14356, 11459, 12702, 16944, 13352, 13234, 10080, 10444, 13091, 19137, 16050, 13770, 10660, 15887, 11478, 17384, 17268, 10349, 14015, 17095, 16790, 14328, 10079, 13384, 15931, 11124, 13202, 13223, 14734, 15878, 15887, 19137, 16417, 17919, 13852, 14808, 14216, 13750, 19137, 13057, 12145, 14175, 14267, 16944, 13234, 10080, 13760, 11459, 14190, 12604, 10333, 14140, 14194, 19137, 10250, 15891, 15891, 14136, 15887, 16945, 14808, 17170, 14066, 19137, 13755, 14808, 11889, 15034, 13748, 11047, 11638, 13849, 14030, 10985, 19137, 10411, 10459, 13996, 13676, 14734, 16098, 14136, 16263, 19137, 10250, 16981, 14172, 14267, 13755, 12061, 14734, 11913, 13624, 11124, 13224, 14970, 10080, 14136, 13085, 17819, 18043, 18048, 10792, 12061, 10257, 19137, 12640, 11047, 17150, 11621, 13913, 16223, 14734, 15931, 11423, 11124, 16002, 12015, 28329, 14179, 14218, 14583, 10760, 13833, 14187, 10254, 10255, 19137, 10411, 10459, 10250, 13326, 14356, 18043, 10956, 12604, 14734, 18438, 13295, 19137, 10953, 10511, 15126, 11454, 14931, 12314, 10267, 14610, 14734, 11013, 10250, 15137, 19137, 10322, 15752, 14791, 10283, 10250, 13923, 10337, 13425, 28329, 15957, 11454, 10745, 17384, 10305, 11630, 17773, 10735, 12208, 13146, 14734, 12034, 17997, 11471, 16520, 17148, 17774, 10859, 19137, 12527, 16525, 12368, 13091, 12647, 10370, 12366, 12290, 10080, 13581, 13064, 13580, 13191, 17148, 10303, 10278, 10447, 19137, 14791, 12348, 10685, 11110, 14759, 14221, 28329, 10687, 10285, 18216, 10283, 11855, 14734, 17268, 10447, 10561, 14808, 10250, 12669, 16956, 13191, 10250, 10370, 18216, 14734, 10811, 15126, 14808, 19137, 13580, 11638, 11401, 18363, 15809, 10080, 16503, 10260, 13091, 14791, 10390, 17181, 14808, 10250, 15706, 13212, 16650, 15678, 12551, 14734, 14141, 13655, 19137, 16519, 10333, 17148, 10588, 13818, 19137, 16747, 17311, 10423, 10399, 10292, 17148, 13091, 10384, 10285, 17090, 11459, 16009, 10974, 13234, 14734, 17386, 16306, 10370, 28329, 11013, 10250, 10447, 13098, 12348, 14867, 12348, 11632, 19137, 10384, 11647, 10792, 15768, 17311, 16136, 11454, 13388, 15898, 13020, 15232, 17384, 10080, 10594, 11979, 15053, 10846, 10250, 10846, 13064, 19137, 10846, 10460, 10305, 17093, 14352, 11687, 14398, 11145, 17004, 13582, 28329, 10305, 10808, 14734, 10337, 10917, 10768, 17544, 17384, 19137, 17148, 10278, 10447, 10745, 10308, 18441, 10260, 10460, 11610, 19137, 11016, 13191, 10250, 13619, 15190, 14866, 13051, 13258, 14734, 10335, 16686, 19137, 10336, 10684, 14136, 13849, 10285, 14734, 10250, 17774, 14388, 14179, 14288, 14171, 10792, 12221, 10997, 14583, 11454, 11861, 13750, 16883, 13309, 17147, 13091, 11454, 12142, 11606, 10660, 16052, 10936, 11979, 14734, 10460, 11467, 10080, 10278, 10370, 10292, 13581, 10258, 10333, 16915, 13773, 28329, 18216, 10283, 11855, 11219, 11891, 18363, 19137, 12669, 18200, 11647, 12643, 11454, 16002, 10271, 17384, 19137, 15149, 17384, 12172, 14808, 12409, 12474, 43899, 17268, 10349, 11975, 12148, 11855, 17311, 13626, 10333, 43282, 9823, 11, 10390, 16735, 19137, 9822, 12605, 12771, 11268, 13558, 11885, 10402, 14734, 10080, 9823, 11, 9822, 17147, 13191, 17268, 10349, 11902, 12219, 43282, 9823, 14867, 10283, 11855, 12509, 10333, 12509, 19137, 10990, 16416, 10333, 10250, 11012, 19137, 9822, 11902, 14869, 11021, 14267, 10303, 19156, 11065, 16735, 11885, 10402, 16974, 14237, 12143, 10260, 11632, 13091, 10250, 15261, 10272, 16501, 10080, 9823, 11, 9822, 12631, 13191, 17389, 11855, 19137, 17311, 14252, 10333, 19137, 18000, 14808, 13733, 13485, 13819, 13893, 10080, 9823, 11638, 10283, 11855, 16735, 14808, 19137, 13734, 13191, 14486, 10423, 14867, 10283, 11855, 14734, 17728, 18025, 19137, 9822, 12631, 13191, 15597, 17335, 19137, 17311, 11454, 13357, 17384, 13749, 15808, 10333, 10080, 9823, 11, 9822, 17147, 13191, 15640, 18459, 10080, 9823, 10250, 17038, 13388, 16435, 14734, 11042, 10437, 16735, 28329, 9822, 15640, 18459, 19137, 16735, 12348, 11957, 10080, 9823, 11, 9822, 14768, 11627, 13064, 15907, 19137, 13734, 11459, 13036, 11021, 17173, 10080, 9823, 11, 9822, 11042, 13324, 16735, 12348, 11957, 10080, 10444, 19137, 12421, 13091, 16525, 12348, 43282, 11315, 43282, 17148, 10423, 10650, 43282, 9823, 11, 10390, 11155, 10258, 13734, 16735, 11891, 14734, 16713, 19137, 17819, 10953, 12791, 10257, 17093, 12387, 14734, 11010, 13655, 43899, 12605, 10402, 17147, 13556, 9819, 15446, 13676, 16305, 9820, 17268, 10650, 14734, 15640, 12283, 16052, 11632, 10677, 10283, 17607, 12153, 11187, 10080, 9823, 14867, 10283, 11855, 14172, 10333, 14172, 11647, 28329, 10278, 10283, 10370, 10703, 13557, 18441, 10260, 10460, 11610, 10080, 11454, 17148, 12232, 13188, 11638, 14734, 11517, 12154, 11984, 13203, 16944, 14136, 14734, 11459, 13036, 19137, 10990, 10250, 17085, 14288, 14171, 12266, 11726, 10333, 19137, 11454, 11633, 12192, 10257, 14267, 16944, 10250, 17222, 15446, 10659, 28329, 18216, 10283, 11855, 10997, 16713, 10333, 43899, 18460, 11855, 43900, 11, 9822, 12469, 16735, 10080, 9823, 11, 9822, 12605, 12509, 14862, 17222, 13091, 16747, 12983, 14734, 14136, 10080, 9823, 11, 17148, 10283, 16443, 11206, 10460, 9822, 18460, 11855, 9823, 14734, 14867, 10283, 11855, 10811, 12636, 13734, 12220, 11610, 10080, 10390, 13580, 14791, 14808, 14136, 10659, 13086, 15446, 14734, 11638, 16981, 10080, 16981, 10257, 10250, 14781, 13734, 10373, 10303, 10370, 19137, 11416, 10292, 17159, 11672, 14524, 11517, 17723, 13091, 15162, 10250, 12662, 14190, 13626, 14734, 12261, 15176, 10080, 14249, 14249, 14267, 14190, 14734, 13359, 13295, 17865, 14172, 15880, 16109, 11459, 19137, 11517, 17723, 11994, 13581, 11468, 11556, 10080, 65530, 65532, 10257, 13012, 10760, 14446, 10333, 10745, 10283, 16530, 15898, 19156, 10390, 10402, 14734, 14332, 14172, 17311, 13091, 10373, 10303, 19156, 11, 65535, 261, 65530, 65534, 13326, 12797, 10257, 10258, 13012, 45, 14791, 13234, 17148, 12220, 16721, 13091, 10258, 17919, 10745, 10283, 16530, 15898, 59, 11, 50, 47, 33, 18216, 10283, 11855, 14734, 10811, 11606, 59, 10390, 10283, 11855, 18216, 11638, 45, 10296, 13579, 15269, 14350, 45, 10621, 10283, 17386, 16306, 10370, 45, 10444, 15752, 16520, 11940, 10792, 10390, 12366, 14141, 14141, 12990, 10080, 10390, 11957, 14894, 11470, 12527, 10792, 13191, 10349, 12476, 11158, 28329, 51, 47, 33, 14867, 10283, 11855, 41, 18460, 11855, 501, 10390, 13006, 10283, 10370, 17311, 16468, 11454, 13020, 15232, 17384, 45, 10846, 10460, 17093, 12139, 11687, 14398, 10080, 10390, 14734, 16713, 16728, 17097, 11977, 45, 10444, 12410, 15516, 12990, 12802, 28329, 52, 47, 33, 17147, 13191, 16443, 12642, 16954, 15640, 12283, 16052, 11632, 45, 10441, 10308, 13091, 10390, 10402, 13556, 13567, 14734, 10370, 28329, 53, 47, 33, 11013, 11630, 17147, 12848, 10792, 10333, 11889, 15034, 11517, 11124, 16050, 13770, 10660, 11517, 45, 10399, 10992, 10660, 16052, 10936, 11979, 15169, 10370, 10080, 10444, 11016, 11860, 13203, 16519, 10390, 10402, 14734, 14332, 14172, 28329, 12421, 15489, 10250, 10258, 19137, 17148, 13619, 13012, 11860, 10293, 16503, 12847, 17162, 14734, 13091, 10278, 10283, 16520, 13197, 11638, 14136, 14734, 10370, 14328, 10080, 18216, 10283, 11855, 10811, 11606, 16417, 14446, 10760, 13913, 12410, 11124, 11158, 12409, 10305, 12366, 45, 14867, 10283, 11855, 18460, 11855, 10779, 13078, 13098, 13179, 10726, 17912, 45, 12990, 12802, 28329, 11454, 12605, 14791, 13234, 45, 10390, 10402, 14734, 14332, 14172, 17311, 13091, 59, 12410, 15516, 12990, 12802, 10079, 16537, 16728, 11977, 10080, 10444, 18216, 10283, 11855, 10811, 11606, 12202, 10260, 15568, 10309, 12527, 12415, 11124, 10452, 16895, 10305, 12366, 10080, 65535, 261, 65530, 65532, 10464, 13179, 11268, 13558, 11187, 10283, 16530, 15898, 19156, 33, 10292, 10373, 10303, 19156, 11, 65535, 261, 65530, 65534, 13326, 12797, 10278, 10447, 16530, 15898, 14734, 12847, 17162, 45, 12605, 13179, 11268, 13558, 18216, 10283, 11855, 14734, 10811, 11606, 10080, 14486, 14597, 10293, 16503, 13191, 10399, 10258, 10745, 14172, 59, 11, 50, 47, 33, 10390, 12410, 15516, 14141, 12990, 45, 11042, 13064, 10692, 13191, 12527, 12415, 10080, 10390, 11957, 14894, 11470, 12527, 10792, 12476, 11158, 45, 17148, 13098, 14966, 10333, 10390, 13191, 13947, 12484, 11124, 11255, 15895, 28329, 51, 47, 33, 10390, 17038, 10292, 18216, 11638, 14734, 10811, 11606, 45, 10444, 10296, 13579, 15269, 14350, 10621, 17386, 16306, 10370, 45, 10954, 15752, 16520, 11940, 10792, 10390, 10696, 12366, 14734, 15287, 11701, 10080, 17148, 15033, 11630, 16417, 10261, 10696, 13325, 14734, 10995, 12143, 16672, 10370, 12527, 10685, 16954, 28329, 52, 47, 33, 10390, 18200, 11855, 17384, 17147, 12172, 13191, 15478, 15806, 11124, 10452, 16895, 10080, 13637, 11687, 10390, 11957, 11517, 12154, 14734, 11975, 12148, 11855, 10997, 15847, 10696, 12366, 14734, 11268, 14299, 45, 10399, 10992, 10684, 13773, 15597, 17335, 11124, 15640, 18459, 14734, 11117, 17144, 10080, 17148, 10349, 15478, 13259, 13204, 15907, 10995, 12220, 10760, 10390, 11957, 14583, 13815, 14734, 11268, 14299, 10261, 14195, 14299, 28329, 53, 47, 33, 14782, 13637, 10305, 10258, 45, 14867, 10283, 11855, 18460, 11855, 12527, 16525, 13179, 10842, 13212, 13133, 10444, 10726, 13903, 10080, 10390, 14734, 16713, 16728, 17097, 11977, 45, 12410, 15516, 12990, 12802, 10444, 15568, 11977, 12527, 12415, 10080, 13006, 10452, 13234, 16735, 15568, 10309, 18216, 10283, 11855, 10811, 11606, 17268, 10410, 13947, 12484, 28329, 12605, 16525, 12348, 45, 18216, 10283, 11855, 10811, 11606, 10692, 13191, 10696, 11454, 13133, 12551, 10079, 11630, 16417, 15269, 14350, 10079, 12527, 12415, 13947, 13283, 15169, 11632, 17385, 12415, 13327, 45, 13179, 15158, 11038, 12605, 14486, 12509, 10285, 14734, 16530, 15898, 12314, 16803, 10080, 10390, 11630, 16417, 14791, 10441, 15269, 14350, 10444, 10696, 12366, 15478, 15806, 45, 10452, 14446, 10333, 11626, 13215, 12415, 13327, 14734, 18248, 10838, 10080, 65535, 261, 65530, 65532, 10988, 15641, 10257, 17919, 14931, 12314, 10267, 14610, 14734, 14313, 13619, 19137, 10384, 10464, 14734, 16678, 12369, 17384, 12664, 10760, 10250, 13619, 15261, 10441, 14734, 11, 65535, 261, 65530, 65534, 13326, 12797, 10464, 12848, 10494, 14734, 14931, 12314, 10267, 14610, 14313, 13619, 45, 11885, 12847, 17162, 14734, 13091, 10250, 11461, 12140, 11638, 14734, 14136, 14146, 45, 10399, 10992, 10278, 11043, 16520, 13197, 11638, 14136, 14734, 10370, 14328, 10080, 17148, 16672, 12605, 12509, 10792, 10333, 10086, 11164, 11723, 17875, 14332, 10087, 17384, 13561, 12283, 13326, 16443, 14213, 14190, 14734, 11461, 13126, 28329, 11454, 11672, 16081, 10788, 10348, 17043, 10258, 10260, 10301, 10305, 11044, 45, 13561, 12283, 13326, 11994, 12172, 14808, 16684, 11632, 15874, 15643, 11124, 11269, 17335, 14734, 10370, 17729, 10673, 15874, 11022, 45, 17343, 14607, 11459, 14442, 10310, 16114, 10080, 11164, 11723, 17875, 14332, 12669, 10390, 11206, 10760, 13234, 45, 12857, 17900, 10390, 10260, 10768, 11461, 11038, 11459, 15463, 12484, 11610, 15898, 45, 10339, 13091, 11164, 11723, 17875, 14332, 14190, 10333, 10390, 14734, 15874, 11022, 28329, 13581, 13064, 45, 15874, 11022, 14213, 14190, 14734, 14136, 14218, 11454, 18438, 13146, 10285, 12719, 12894, 15639, 14806, 45, 12719, 13234, 16520, 10420, 14734, 11685, 11656, 59, 11, 9822, 14791, 11223, 45, 10442, 14434, 15658, 14508, 14267, 14190, 14734, 14152, 14142, 45, 10390, 17268, 12359, 15149, 14734, 11872, 11855, 45, 13561, 12283, 13326, 16443, 14179, 14136, 14144, 14190, 60, 14791, 14267, 14190, 14734, 14136, 14218, 10721, 10257, 11639, 15090, 45, 10621, 14141, 18245, 14975, 13685, 11885, 13034, 14734, 12010, 12631, 10080, 9823, 11, 15874, 11022, 10257, 14734, 14136, 10659, 11124, 13561, 12283, 13326, 14734, 17237, 17213, 45, 16672, 11164, 11723, 17875, 14332, 11105, 11430, 14734, 10370, 10260, 15002, 16676, 16686, 15465, 15465, 45, 10687, 10285, 18216, 10283, 14734, 12283, 13196, 16501, 13033, 15034, 11994, 16694, 16686, 17222, 23334, 11885, 10364, 12348, 11632, 10303, 15639, 14806, 11223, 23251, 11, 12421, 14734, 13234, 16735, 45, 17148, 10278, 13619, 13012, 11860, 14735, 10399, 11649, 12287, 14734, 14136, 10659, 10460, 10292, 13655, 13660, 14734, 10781, 17194, 45, 17181, 17141, 10370, 10402, 14734, 16520, 12527, 11124, 11957, 10687, 10260, 11042, 12527, 11002, 45, 12847, 15493, 10760, 10260, 11042, 12484, 12527, 28329, 10260, 11042, 11454, 10339, 10086, 14931, 12314, 10267, 14610, 10087, 13179, 10684, 13773, 14637, 15023, 16520, 13197, 14734, 12366, 12396, 45, 15646, 10086, 11164, 11723, 17875, 14332, 10087, 10779, 13179, 10688, 12603, 10821, 12986, 13258, 45, 12299, 16749, 14136, 14218, 11454, 15874, 11022, 10257, 14734, 12522, 10304, 10080, 10278, 10283, 12985, 10336, 14734, 14216, 14172, 10322, 10260, 12002, 14782, 11042, 28329, 10444, 10460, 10292, 10278, 13619, 12847, 17162, 14136, 14218, 10261, 10370, 12527, 11002, 12202, 11861, 14734, 13012, 11860, 45, 12421, 10452, 17147, 13091, 13191, 14782, 17189, 10305, 11621, 14734, 10080, 65535, 261, 65530, 65532, 12631, 10399, 10464, 10684, 13773, 14734, 13179, 11632, 14734, 13091, 14136, 14218, 11957, 10260, 11957, 19156, 11, 65535, 261, 65530, 65534, 12469, 12748, 10760, 12605, 10441, 10308, 13179, 10684, 13773, 17148, 10278, 13619, 13012, 11860, 10285, 14136, 14218, 14734, 12847, 17162, 45, 17148, 10441, 10308, 11021, 10399, 10995, 13086, 10760, 12605, 11957, 13012, 11860, 14734, 10250, 10349, 10585, 18012, 10080, 11042, 12522, 12469, 14734, 16520, 11940, 45, 12605, 14734, 14914, 11957, 14136, 14218, 14734, 12847, 10708, 13637, 17097, 13191, 10685, 16954, 10080, 10444, 12605, 12509, 12299, 16749, 14734, 13091, 59, 11, 50, 47, 33, 14136, 14218, 11016, 13091, 17148, 10278, 13619, 13012, 11860, 10799, 14604, 13660, 11430, 11124, 12484, 12527, 14734, 10250, 10283, 16803, 12333, 12636, 13619, 45, 15646, 10260, 13091, 12605, 10684, 13773, 14734, 14216, 14172, 10080, 12605, 13179, 10578, 11047, 10339, 10684, 13773, 14136, 14218, 15727, 11044, 10758, 13098, 10760, 14734, 10370, 14328, 12527, 11002, 11124, 10460, 15643, 12522, 11436, 28329, 51, 47, 33, 13637, 17097, 10278, 13619, 13012, 11860, 14734, 10260, 11042, 10482, 14589, 14136, 10659, 12636, 13764, 45, 13179, 15752, 10758, 13098, 10278, 10447, 10460, 15643, 10260, 11042, 14734, 12847, 10708, 18043, 13327, 11124, 12985, 10336, 10283, 12415, 10080, 17148, 13091, 12605, 13637, 17097, 13191, 10685, 16954, 10768, 13254, 14734, 28329, 52, 47, 33, 10370, 14328, 12527, 11002, 11124, 12484, 12527, 12220, 16721, 10260, 10348, 10339, 14136, 14218, 13205, 17038, 10080, 14136, 14218, 12847, 10708, 13179, 11632, 13091, 10292, 10333, 17363, 14778, 11459, 16698, 16735, 10370, 14328, 10696, 12366, 11124, 16417, 17136, 10460, 15643, 12509, 13764, 28329, 12421, 14734, 13234, 16735, 45, 10460, 10292, 10250, 10283, 12985, 10336, 16503, 15323, 45, 14136, 14218, 14914, 11903, 12348, 10792, 10333, 12605, 13637, 17097, 11632, 14734, 10684, 13773, 10080, 10444, 12605, 13179, 17385, 16523, 14734, 13091, 11885, 15727, 11044, 12631, 10758, 13098, 14734, 10370, 13012, 12522, 10304, 28329, 14136, 14218, 11016, 10260, 17141, 13091, 10250, 10283, 11805, 10382, 45, 15752, 17189, 17141, 11885, 12847, 15493, 10760, 13179, 10292, 18302, 13815, 14734, 10370, 14328, 12366, 14141, 10261, 10267, 14610, 16520, 28329, 12605, 14734, 10684, 13773, 14172, 17848, 10285, 11454, 10278, 10283, 13036, 17919, 59, 11, 10250, 13091, 10370, 14328, 12527, 11002, 17148, 10250, 12007, 17919, 45, 14136, 10659, 15727, 11044, 10758, 13098, 14734, 13091, 10260, 11042, 10370, 11957, 13581, 14734, 12527, 11002, 11124, 10995, 12220, 28329, 10337, 13091, 10460, 15643, 12636, 13764, 10261, 10283, 12415, 45, 10460, 15643, 17189, 17141, 10260, 11042, 13036, 12276, 10482, 14589, 14136, 10659, 45, 12022, 14446, 10760, 10283, 10370, 14734, 16728, 16537, 18043, 13327, 28329, 17148, 10278, 13036, 17919, 13179, 15752, 10452, 14446, 10460, 15643, 15904, 13208, 14589, 12522, 45, 13091, 12605, 13179, 12527, 10685, 16954, 10768, 13254, 10261, 13559, 16917, 14734, 10080, 65535, 261, 65530, 65532, 11957, 13637, 10086, 15597, 11435, 10420, 14987, 10087, 17148, 17303, 11975, 16735, 12848, 10494, 14734, 14987, 11871, 16803, 12333, 19137, 33, 12605, 10402, 15752, 10384, 10086, 14931, 12314, 10267, 14610, 10087, 17148, 17303, 11975, 16735, 17384, 17919, 12848, 11001, 10792, 17268, 10349, 16803, 12333, 12522, 10304, 14734, 10272, 16501, 19156, 11, 65535, 261, 65530, 65534, 11454, 11414, 15178, 17148, 10283, 17728, 18025, 10305, 10808, 45, 12605, 12509, 12299, 16749, 14734, 13091, 10260, 11042, 10460, 11159, 14734, 16803, 12333, 11064, 10304, 13091, 11036, 10260, 14782, 11042, 14734, 45, 10399, 10258, 13091, 12605, 11957, 10086, 14931, 12314, 10267, 14610, 10087, 17148, 17303, 11975, 16735, 14734, 10250, 10349, 16803, 12333, 16533, 16742, 59, 11, 50, 47, 33, 14931, 12314, 10267, 14610, 13205, 17038, 11994, 13091, 10250, 10283, 12299, 11638, 14734, 16803, 12333, 10080, 11885, 10397, 16417, 10460, 15643, 12509, 16803, 10838, 14734, 13051, 17789, 45, 16982, 10760, 14371, 11975, 14734, 14446, 11903, 13499, 11496, 28329, 51, 47, 33, 11888, 11633, 10370, 11124, 11888, 11633, 11691, 10370, 16803, 12333, 14808, 15516, 12746, 15042, 12217, 14734, 10838, 17387, 45, 10397, 16417, 14486, 12415, 11124, 13133, 12551, 28329, 52, 47, 33, 14320, 11647, 10370, 16803, 12333, 14808, 10974, 11726, 14734, 17386, 16306, 10838, 17387, 45, 17820, 11285, 10370, 12415, 10285, 11626, 13215, 14734, 10278, 17919, 12415, 28329, 53, 47, 33, 18256, 12996, 16803, 12333, 14808, 17271, 12449, 11124, 16009, 16783, 45, 10444, 10322, 12848, 17363, 12605, 10402, 12200, 16412, 13091, 17385, 16503, 14734, 28329, 54, 47, 33, 13042, 15052, 13205, 17038, 11994, 13091, 10250, 10283, 16803, 12333, 45, 10397, 16417, 10370, 14734, 12604, 17688, 11124, 16525, 17363, 28329, 55, 47, 33, 10260, 11042, 15033, 13046, 16803, 12333, 14808, 10370, 12415, 10285, 14734, 11036, 10283, 17919, 11047, 45, 12605, 10402, 17879, 16503, 13006, 11038, 11885, 10402, 12638, 15752, 16056, 12348, 12200, 16412, 28329, 12421, 14734, 13234, 16735, 45, 10086, 14931, 12314, 10267, 14610, 10087, 12848, 10494, 14734, 16803, 12333, 12522, 10304, 13179, 11632, 11459, 12299, 16749, 10370, 12415, 14734, 11626, 13215, 11632, 13324, 45, 12200, 16412, 10838, 17387, 10305, 17734, 14734, 17385, 16503, 12415, 10080, 11885, 13191, 12522, 16696, 11459, 12822, 16671, 10370, 12415, 14734, 11632, 17919, 12415, 45, 18458, 10851, 12605, 10402, 16947, 16948, 14371, 17824, 14734, 12410, 15516, 13499, 12276, 28329, 10261, 10086, 15597, 11435, 10420, 14987, 10087, 10285, 14734, 14987, 11871, 16803, 12333, 14782, 13637, 45, 10086, 14931, 12314, 10267, 14610, 10087, 12848, 10494, 14734, 16803, 12333, 12522, 10304, 13179, 13773, 17385, 10696, 11454, 15287, 14987, 12007, 17919, 14734, 12822, 15325, 10080, 11885, 12299, 16749, 10452, 18183, 11124, 12509, 16803, 45, 16947, 15774, 10333, 10688, 10452, 14734, 14987, 16713, 11887, 11898, 10080, 65535, 261, 65530, 65532, 12848, 10494, 9822, 14931, 12314, 10267, 14610, 9823, 17148, 17303, 11975, 16735, 14734, 14782, 10684, 10529, 12445, 19137, 10894, 12732, 10460, 15643, 19137, 10926, 10460, 18043, 13327, 19137, 10384, 10250, 10283, 17189, 10522, 11975, 16735, 10460, 11920, 14734, 16530, 12231, 12664, 10760, 11885, 14734, 17725, 10659, 14172, 11124, 10260, 16956, 11, 65535, 261, 65530, 65534, 10086, 14931, 12314, 10267, 14610, 10087, 17148, 17303, 11975, 16735, 10688, 13191, 12337, 18216, 14734, 10781, 10460, 10408, 10577, 45, 10384, 17189, 10522, 11975, 16735, 14734, 16530, 12231, 13234, 10768, 13254, 59, 11, 9828, 10460, 15643, 59, 14332, 17384, 2453, 13125, 12705, 10769, 14332, 28329, 9828, 10926, 10460, 18043, 13327, 59, 12208, 18441, 18043, 16954, 45, 11268, 10821, 10653, 15323, 10286, 11932, 45, 12172, 13191, 15034, 12206, 11124, 11656, 12206, 15898, 12319, 28329, 9828, 17725, 10659, 14172, 59, 11, 50, 42, 10286, 11932, 14734, 12509, 16803, 10838, 45, 16689, 16664, 10333, 14370, 14332, 14734, 14931, 12314, 10267, 14610, 52865, 45, 10894, 12732, 11412, 10283, 11869, 15907, 11435, 12231, 10079, 17997, 15137, 14734, 13961, 13819, 10079, 10452, 17385, 11003, 17093, 14734, 14328, 14486, 11899, 12340, 15169, 28329, 51, 42, 10286, 11932, 13191, 16954, 14734, 10370, 14328, 12314, 16803, 45, 11687, 13594, 14987, 10079, 18256, 13764, 12158, 10079, 12142, 12158, 11871, 12345, 10079, 12632, 12200, 11459, 14484, 11871, 10423, 10293, 12175, 15169, 28329, 52, 42, 14442, 11110, 14734, 12208, 18441, 18043, 16954, 45, 16688, 10798, 14970, 10423, 14446, 11903, 28329, 9828, 10260, 16956, 59, 11, 50, 42, 11638, 17387, 14734, 10328, 12657, 14486, 16686, 11124, 11043, 16702, 45, 11021, 15752, 15568, 10309, 15326, 10740, 12527, 28329, 51, 42, 15092, 14894, 12182, 14486, 14734, 15034, 12206, 16689, 11899, 45, 11957, 10250, 15880, 16742, 15643, 11021, 15752, 17843, 10399, 12824, 11002, 28329, 9828, 12421, 10452, 13234, 16735, 59, 11, 16269, 14221, 13191, 10250, 10349, 15568, 14172, 45, 10444, 10086, 14931, 12314, 10267, 14610, 10087, 10954, 10752, 10563, 14370, 10781, 14734, 12509, 16803, 10079, 12208, 18441, 16710, 16760, 14734, 11639, 18157, 16403, 15090, 14734, 12484, 15907, 45, 10399, 10992, 14370, 14332, 14734, 10267, 14610, 16520, 45, 16056, 12348, 10333, 12210, 13766, 14734, 16742, 15643, 16667, 11021, 10080, 17148, 13580, 13091, 10250, 17303, 17189, 10522, 11975, 16735, 12631, 17879, 16503, 10692, 11622, 14734, 10653, 15323, 10080, 65535, 261, 65530, 65532, 12848, 10494, 9822, 14931, 12314, 10267, 14610, 9823, 15304, 10777, 14734, 10460, 15643, 11043, 15044, 11124, 10529, 12445, 10080, 33, 17147, 13191, 16721, 10460, 15643, 14734, 14782, 10684, 10460, 11159, 17848, 11, 65535, 261, 65530, 65534, 10086, 14931, 12314, 10267, 14610, 10087, 15304, 10777, 14734, 10460, 15643, 13091, 15963, 11435, 10460, 11920, 13778, 14523, 2453, 13125, 12705, 10769, 14332, 41, 6696, 2174, 20670, 2204, 7686, 117, 497, 10390, 10322, 13191, 10687, 10390, 16114, 11043, 14734, 15304, 10777, 10427, 11656, 10460, 11159, 28329, 10293, 16503, 10529, 12445, 59, 11, 9828, 33, 10460, 15643, 59, 13778, 14523, 2453, 13125, 12705, 10769, 14332, 41, 6696, 2174, 20670, 2204, 7686, 117, 497, 639, 668, 12201, 53, 13190, 648, 13053, 10760, 14583, 45, 640, 635, 12201, 52, 13190, 632, 13053, 10985, 10267, 28329, 9828, 33, 11435, 15252, 59, 15963, 11435, 10460, 11920, 11, 9828, 33, 10397, 16417, 10460, 59, 10086, 14931, 12314, 10267, 14610, 10087, 15304, 10777, 41, 23570, 36017, 42, 11975, 16735, 17848, 11, 9828, 33, 10687, 10390, 10460, 11159, 59, 10086, 6699, 28608, 1859, 44696, 10087, 15304, 10777, 10079, 10086, 6699, 49955, 50425, 21265, 21823, 28821, 24738, 20762, 25291, 10087, 15304, 10777, 15169, 11, 10086, 14931, 12314, 10267, 14610, 10087, 15304, 10777, 10384, 639, 703, 12201, 12266, 11726, 14734, 45, 10682, 660, 11632, 13205, 10080, 10293, 16503, 12847, 17162, 10333, 14931, 12314, 10267, 14610, 10257, 14734, 10370, 10261, 14583, 10540, 13234, 14734, 18256, 13764, 10838, 17387, 10399, 10992, 11036, 15033, 11698, 12418, 14583, 14328, 10080, 16891, 15091, 11726, 15480, 14734, 10293, 18025, 13091, 11117, 17144, 10261, 15847, 14597, 12522, 12377, 10399, 10992, 13133, 12551, 10261, 12525, 16393, 14734, 11957, 15123, 28329, 13778, 14523, 2453, 13125, 12705, 10769, 14332, 14734, 16684, 11632, 10460, 11159, 17311, 10688, 13191, 13833, 17285, 14734, 15963, 11435, 12208, 18441, 18043, 13327, 45, 16728, 16537, 14583, 10846, 16710, 16760, 45, 10370, 14328, 18302, 13815, 16982, 15774, 45, 10399, 13249, 10687, 12208, 18441, 14734, 13036, 12276, 12847, 10708, 11626, 13215, 14734, 11190, 14486, 16520, 10080, 10390, 16443, 11971, 10292, 9822, 15963, 16728, 11268, 10821, 10305, 14302, 9823, 10080, 65535, 261, 65530, 65532, 10460, 10292, 10250, 11043, 15847, 11805, 10452, 15543, 17108, 19137, 33, 12848, 10494, 9822, 18256, 13764, 14734, 18029, 15898, 9823, 17148, 17303, 11975, 16735, 19133, 6696, 2174, 20670, 2204, 7686, 117, 16114, 19134, 14734, 12898, 16503, 10529, 12445, 19137, 33, 10894, 12732, 10821, 12484, 11124, 10684, 17644, 10370, 14328, 11, 65535, 261, 65530, 65534, 12605, 14446, 11454, 15494, 10760, 12605, 14734, 15162, 10250, 10283, 15178, 13336, 19137, 33, 17148, 13091, 12605, 16667, 10292, 13188, 11685, 14734, 19151, 11, 18256, 13764, 14734, 18029, 15898, 41, 6699, 44135, 4706, 36976, 42, 13091, 13778, 14523, 2453, 13125, 12705, 10769, 14332, 41, 6696, 2174, 20670, 2204, 7686, 117, 42, 14734, 18117, 17303, 16130, 15917, 12001, 11975, 16735, 41, 23570, 36017, 47234, 42, 28329, 10293, 16503, 10370, 14328, 59, 11, 9828, 1419, 8218, 27487, 59, 33, 10250, 10283, 15299, 15293, 14734, 18256, 13764, 12158, 10080, 10390, 10260, 11975, 12366, 17149, 10792, 10333, 18488, 33640, 12034, 10257, 45, 11454, 17268, 17384, 10390, 17213, 10792, 10333, 10278, 10283, 11630, 13234, 15643, 10080, 10390, 11994, 12604, 10333, 10390, 10402, 11024, 12645, 13799, 14057, 11638, 17782, 10305, 16403, 14734, 11047, 11960, 28329, 9828, 6720, 41426, 59, 33, 10250, 10283, 20683, 2162, 41, 23570, 36017, 10257, 14734, 10250, 10283, 11435, 11920, 42, 11932, 16455, 14734, 13042, 16403, 11221, 10370, 10080, 10390, 13234, 10792, 11024, 12645, 13799, 14057, 11638, 17782, 17149, 16403, 12231, 10580, 10080, 14597, 10339, 10260, 10333, 16533, 18256, 13764, 14734, 10267, 14610, 45, 10390, 12182, 12182, 17808, 10673, 10952, 17801, 10305, 10285, 28329, 9828, 6699, 20320, 25476, 59, 21042, 41426, 14734, 16403, 13221, 15209, 10080, 11885, 13191, 15847, 12144, 14734, 12410, 12527, 12484, 45, 13191, 13064, 10423, 10525, 12686, 11885, 14734, 10293, 10370, 28329, 10821, 12484, 59, 11, 10997, 14583, 11454, 18256, 13764, 10655, 13027, 14734, 11024, 12645, 13799, 14057, 11638, 17782, 10080, 6720, 41426, 13234, 10792, 17148, 17384, 13042, 16403, 45, 1419, 8218, 27487, 12604, 10292, 10390, 14734, 11047, 11960, 10080, 10390, 10402, 10339, 13091, 12022, 12266, 10250, 15304, 10777, 14734, 10705, 17801, 17149, 16403, 11024, 12645, 13799, 14057, 14734, 15091, 13373, 10305, 13042, 28329, 10390, 10402, 17213, 10792, 11036, 15033, 18256, 13764, 14583, 14328, 45, 13637, 11687, 18488, 10079, 15287, 14141, 10079, 18256, 13764, 12158, 15169, 10080, 10390, 10402, 10322, 17919, 10288, 18256, 13764, 12172, 13234, 14734, 11036, 15033, 10952, 17801, 11124, 12752, 12610, 10080, 65535, 261, 65530, 65534, 12605, 14446, 11454, 12791, 10283, 16530, 12231, 13234, 12410, 15641, 17148, 10283, 17728, 18025, 19137, 15494, 10760, 11013, 11630, 10250, 10283, 15178, 13336, 19151, 11, 11454, 17148, 17303, 11975, 16735, 10285, 45, 20754, 1788, 27487, 11124, 6720, 41426, 16443, 10956, 10673, 16684, 11632, 11426, 11574, 10305, 10285, 45, 13191, 13064, 13091, 11416, 10292, 15847, 12144, 14734, 13051, 14862, 11124, 10260, 16780, 12542, 45, 13191, 13064, 13091, 11416, 10292, 11013, 10250, 10283, 10722, 11899, 10390, 10402, 14734, 11630, 11454, 10838, 17387, 10080, 10390, 10402, 17919, 11957, 14808, 16739, 11687, 14136, 12034, 10079, 13849, 17843, 10399, 10992, 11024, 12645, 13799, 14057, 11638, 17782, 10285, 13188, 12299, 11638, 14734, 18256, 13764, 12158, 10402, 14734, 11753, 15723, 28329, 12002, 15202, 1419, 8218, 27487, 13091, 10250, 10283, 15299, 15293, 14734, 18256, 13764, 12158, 19137, 10444, 10390, 10250, 14781, 11454, 15136, 12002, 10675, 10838, 10525, 12686, 6720, 41426, 19137, 15646, 6720, 41426, 10779, 12172, 13234, 10333, 11957, 17148, 10283, 11656, 11701, 14734, 18256, 13764, 10267, 14610, 14734, 14195, 12484, 11124, 11685, 11656, 12366, 28329, 11454, 17148, 10283, 10655, 14015, 18256, 13764, 14734, 10267, 14610, 17384, 19137, 11210, 10250, 11474, 11687, 14947, 14869, 14734, 11957, 16803, 13091, 6720, 41426, 14734, 16403, 13221, 15209, 19137, 11885, 13091, 10250, 10283, 13815, 14583, 14583, 14734, 10079, 16399, 15446, 15898, 14734, 16403, 13221, 15209, 19137, 11021, 10399, 12669, 11885, 14734, 10293, 10370, 10525, 12686, 11454, 10409, 10454, 10952, 17801, 14445, 11574, 10285, 28329, 17148, 17303, 11975, 16735, 13091, 10250, 17303, 11701, 16954, 13503, 14583, 14734, 11656, 12206, 10705, 17801, 11975, 16735, 19137, 16672, 16742, 15643, 10452, 18183, 10792, 10333, 9822, 17169, 13685, 14862, 16696, 11124, 12822, 15325, 9823, 14734, 10310, 16954, 19137, 11042, 13064, 10322, 16672, 10370, 12509, 16944, 10333, 14583, 11117, 10285, 14734, 13947, 18121, 11124, 10993, 16754, 10080, 65535, 261]
[2023-09-30 21:43:15,223] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[0.0001, 0.0001, 0.0001], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.7131894826889038 batch_loss 1.1266836449503899
==== [12645, 13799, 14057, 11638, 17782, 17149, 16403, 12231, 10580, 10080, 14597, 10339, 10260, 10333, 16533, 18256, 13764, 14734, 10267, 14610, 45, 10390, 12182, 12182, 17808, 10673, 10952, 17801, 10305, 10285, 28329, 9828, 6699, 20320, 25476, 59, 21042, 41426, 14734, 16403, 13221, 15209, 10080, 11885, 13191, 15847, 12144, 14734, 12410, 12527, 12484, 45, 13191, 13064, 10423, 10525, 12686, 11885, 14734, 10293, 10370, 28329, 10821, 12484, 59, 11, 10997, 14583, 11454, 18256, 13764, 10655, 13027, 14734, 11024, 12645, 13799, 14057, 11638, 17782, 10080, 6720, 41426, 13234, 10792, 17148, 17384, 13042, 16403, 45, 1419, 8218, 27487, 12604, 10292, 10390, 14734, 11047, 11960, 10080, 10390, 10402, 10339, 13091, 12022, 12266, 10250, 15304, 10777, 14734, 10705, 17801, 17149, 16403, 11024, 12645, 13799, 14057, 14734, 15091, 13373, 10305, 13042, 28329, 10390, 10402, 17213, 10792, 11036, 15033, 18256, 13764, 14583, 14328, 45, 13637, 11687, 18488, 10079, 15287, 14141, 10079, 18256, 13764, 12158, 15169, 10080, 10390, 10402, 10322, 17919, 10288, 18256, 13764, 12172, 13234, 14734, 11036, 15033, 10952, 17801, 11124, 12752, 12610, 10080, 65535, 261, 65530, 65534, 12605, 14446, 11454, 12791, 10283, 16530, 12231, 13234, 12410, 15641, 17148, 10283, 17728, 18025, 19137, 15494, 10760, 11013, 11630, 10250, 10283, 15178, 13336, 19151, 11, 11454, 17148, 17303, 11975, 16735, 10285, 45, 20754, 1788, 27487, 11124, 6720, 41426, 16443, 10956, 10673, 16684, 11632, 11426, 11574, 10305, 10285, 45, 13191, 13064, 13091, 11416, 10292, 15847, 12144, 14734, 13051, 14862, 11124, 10260, 16780, 12542, 45, 13191, 13064, 13091, 11416, 10292, 11013, 10250, 10283, 10722, 11899, 10390, 10402, 14734, 11630, 11454, 10838, 17387, 10080, 10390, 10402, 17919, 11957, 14808, 16739, 11687, 14136, 12034, 10079, 13849, 17843, 10399, 10992, 11024, 12645, 13799, 14057, 11638, 17782, 10285, 13188, 12299, 11638, 14734, 18256, 13764, 12158, 10402, 14734, 11753, 15723, 28329, 12002, 15202, 1419, 8218, 27487, 13091, 10250, 10283, 15299, 15293, 14734, 18256, 13764, 12158, 19137, 10444, 10390, 10250, 14781, 11454, 15136, 12002, 10675, 10838, 10525, 12686, 6720, 41426, 19137, 15646, 6720, 41426, 10779, 12172, 13234, 10333, 11957, 17148, 10283, 11656, 11701, 14734, 18256, 13764, 10267, 14610, 14734, 14195, 12484, 11124, 11685, 11656, 12366, 28329, 11454, 17148, 10283, 10655, 14015, 18256, 13764, 14734, 10267, 14610, 17384, 19137, 11210, 10250, 11474, 11687, 14947, 14869, 14734, 11957, 16803, 13091, 6720, 41426, 14734, 16403, 13221, 15209, 19137, 11885, 13091, 10250, 10283, 13815, 14583, 14583, 14734, 10079, 16399, 15446, 15898, 14734, 16403, 13221, 15209, 19137, 11021, 10399, 12669, 11885, 14734, 10293, 10370, 10525, 12686, 11454, 10409, 10454, 10952, 17801, 14445, 11574, 10285, 28329, 17148, 17303, 11975, 16735, 13091, 10250, 17303, 11701, 16954, 13503, 14583, 14734, 11656, 12206, 10705, 17801, 11975, 16735, 19137, 16672, 16742, 15643, 10452, 18183, 10792, 10333, 9822, 17169, 13685, 14862, 16696, 11124, 12822, 15325, 9823, 14734, 10310, 16954, 19137, 11042, 13064, 10322, 16672, 10370, 12509, 16944, 10333, 14583, 11117, 10285, 14734, 13947, 18121, 11124, 10993, 16754, 10080, 65535, 261]
-> item_loss 1.085699200630188 batch_loss 1.1221298178037007
==== [25106, 11, 65530, 65531, 10464, 13091, 10250, 10283, 13637, 17097, 14332, 13603, 14734, 820, 19137, 11017, 10460, 1139, 1955, 10080, 10464, 16443, 16689, 16664, 10760, 13234, 12202, 10260, 13091, 10292, 10333, 10292, 10370, 15261, 12848, 10494, 13193, 10843, 19137, 15646, 13091, 13827, 16707, 820, 10792, 12221, 15752, 11003, 12348, 13191, 11632, 15678, 13078, 10080, 10464, 10423, 10621, 10250, 10283, 10370, 15261, 10250, 13324, 11124, 10687, 10390, 10370, 10355, 16753, 19137, 10464, 16735, 10760, 13234, 14734, 10696, 11922, 13091, 11891, 10675, 15847, 10293, 10722, 11899, 14734, 19137, 12202, 10265, 11001, 10722, 10339, 10464, 14734, 12509, 13764, 11124, 13499, 12722, 12366, 12484, 28329, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 11454, 10303, 11, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 10464, 11685, 19156, 11, 65535, 261, 65530, 65534, 1139, 1955, 59, 33, 10464, 11685, 19137, 12605, 13091, 1139, 1955, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 10464, 13191, 10373, 10303, 10841, 15752, 19156, 11, 65535, 261, 65530, 65534, 1139, 1955, 59, 33, 10460, 10292, 10250, 10283, 820, 19137, 12605, 14734, 10841, 15752, 13191, 12337, 11632, 19137, 13637, 11687, 10621, 14446, 11454, 17148, 13324, 11124, 10464, 15667, 11639, 19137, 10444, 13091, 12605, 12202, 10260, 13091, 10292, 10333, 13193, 10843, 10370, 15261, 15646, 16443, 10781, 17194, 10760, 13234, 14734, 19137, 12631, 10399, 11021, 15752, 16503, 16672, 10464, 11645, 13197, 10333, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 18030, 19137, 10260, 13091, 10292, 10333, 12176, 10847, 10370, 15261, 19137, 17268, 10464, 10292, 10373, 10303, 10423, 16443, 17194, 10760, 13234, 19156, 11, 65535, 261, 65530, 65534, 1139, 1955, 59, 33, 11111, 11111, 19137, 17268, 12605, 11994, 10260, 14862, 17222, 10333, 19137, 12609, 16684, 10464, 11021, 10399, 17728, 10258, 12605, 14734, 10794, 17194, 15643, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 10464, 17148, 10283, 12396, 12231, 43282, 11, 65535, 261, 65530, 65534, 1139, 1955, 59, 33, 12605, 16525, 12348, 12605, 14734, 12396, 12231, 13734, 10373, 10303, 17728, 18025, 19137, 11416, 10292, 10464, 17728, 17148, 10349, 17728, 18025, 13064, 10441, 10308, 13734, 13191, 15641, 16253, 12605, 10423, 12401, 10303, 12509, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 11685, 11061, 11957, 10260, 16944, 19137, 11021, 15752, 13091, 12605, 11124, 10687, 10390, 820, 15667, 11639, 11640, 11632, 10333, 19137, 10464, 10322, 16684, 13191, 10349, 10260, 10250, 13324, 28329, 65535, 261, 65530, 65534, 1139, 1955, 59, 33, 16774, 16774, 10464, 14734, 14486, 16533, 19137, 12605, 10322, 10292, 12605, 14734, 12396, 12231, 17222, 13571, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 12605, 16525, 12348, 10464, 13091, 10283, 13191, 16954, 14734, 820, 19137, 10260, 17141, 10780, 12638, 12605, 16707, 12822, 10464, 12202, 17916, 13091, 10760, 10339, 12449, 12522, 19137, 12605, 11016, 13091, 12509, 11957, 10464, 13191, 10250, 10283, 16722, 15478, 14734, 10333, 16533, 28329, 65535, 261, 65530, 65534, 1139, 1955, 59, 33, 11315, 19137, 12631, 10399, 10464, 12509, 10333, 16533, 10349, 10373, 10303, 19156, 12605, 10460, 10292, 10250, 10283, 13212, 11369, 13734, 10373, 10303, 15036, 11930, 19137, 11021, 10399, 11124, 10464, 16679, 16679, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 10464, 14734, 12221, 12007, 13499, 11496, 13091, 10373, 10303, 28329, 65535, 261, 65530, 65534, 1139, 1955, 59, 33, 18030, 19137, 17148, 10283, 12605, 11021, 15752, 13734, 13764, 11414, 15178, 10464, 19137, 12202, 10260, 13091, 11416, 10292, 17148, 13091, 13212, 11930, 19137, 15646, 13091, 11416, 10292, 12605, 11871, 10323, 14734, 16728, 13021, 11640, 11632, 10333, 19137, 10687, 10285, 10894, 12732, 11036, 15033, 10687, 10390, 16728, 16537, 13499, 11496, 14589, 17141, 14734, 16728, 13021, 19137, 12631, 10399, 12605, 10260, 11640, 10768, 12348, 13923, 43282, 11, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 10464, 10292, 10373, 10303, 16735, 16713, 10423, 17148, 10303, 15847, 14221, 19156, 13091, 11416, 10292, 10464, 16443, 16689, 11899, 12604, 17148, 13324, 10303, 19156, 11, 65535, 261, 65530, 65534, 1139, 1955, 59, 33, 13091, 17148, 13324, 14734, 19137, 10464, 10402, 10370, 15261, 12631, 16735, 14734, 16689, 11899, 10441, 10308, 13091, 12748, 11454, 12605, 14734, 17109, 10673, 12266, 11647, 17303, 10768, 14734, 13012, 13205, 10080, 35, 10464, 13091, 10250, 10283, 13637, 17097, 14332, 13603, 14734, 820, 19137, 11017, 10460, 1139, 1955, 10080, 10464, 16443, 16689, 16664, 10760, 13234, 12202, 10260, 13091, 10292, 10333, 10292, 10370, 15261, 12848, 10494, 13193, 10843, 19137, 15646, 13091, 13827, 16707, 820, 10792, 12221, 15752, 11003, 12348, 13191, 11632, 15678, 13078, 10080, 35, 11885, 10441, 10308, 13091, 17148, 13324, 10708, 14734, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 12631, 10399, 10464, 10377, 10377, 13091, 10250, 10283, 13212, 11369, 19137, 13734, 13191, 15847, 12605, 19156, 11, 65535, 261, 65530, 65534, 1139, 1955, 59, 33, 10464, 16525, 12348, 12605, 10621, 13091, 13734, 13191, 10303, 19156, 11685, 11061, 19137, 14791, 16944, 13234, 14914, 11903, 12337, 10621, 10080, 10444, 13091, 19137, 12605, 16269, 14221, 10423, 10279, 13327, 12750, 14240, 16689, 11899, 12314, 12276, 19137, 10444, 13091, 12605, 10322, 10423, 13326, 12797, 13499, 11496, 14734, 14862, 16696, 10261, 14332, 12415, 13234, 17109, 10760, 10696, 11922, 10080, 13637, 11687, 19137, 11454, 17148, 10283, 16689, 11899, 10258, 12605, 11021, 10399, 17177, 12731, 13091, 11060, 10261, 10464, 15507, 15511, 15667, 11639, 10080, 10464, 11021, 10399, 16667, 10292, 19137, 12605, 16269, 14221, 13091, 10283, 13212, 11369, 19137, 10444, 13091, 12727, 13191, 10250, 17303, 10768, 10370, 15261, 14734, 14332, 12415, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 11164, 11164, 19137, 10687, 11903, 12605, 17311, 14862, 17222, 19137, 12605, 13091, 10464, 14734, 10794, 17194, 15643, 19137, 1139, 1955, 28329, 65535, 261, 65530, 65534, 1139, 1955, 59, 33, 10322, 11994, 13091, 16735, 10464, 13091, 12605, 14734, 10293, 10370, 19156, 10293, 10370, 10464, 17148, 10260, 13091, 11454, 13565, 16875, 12605, 10303, 43282, 13078, 13078, 14862, 17222, 17147, 16503, 11124, 12605, 16679, 17148, 10349, 19137, 11100, 11100, 11100, 43282, 11, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 10790, 11188, 10333, 19137, 10687, 11903, 12605, 10402, 14734, 17148, 13619, 11957, 16713, 13091, 12605, 15543, 10708, 14734, 16728, 13021, 19137, 12202, 10260, 13091, 10464, 14794, 13580, 14734, 15486, 10964, 10080, 11454, 13203, 13234, 19137, 10464, 12609, 16684, 10423, 16443, 11017, 10460, 10687, 11885, 14734, 11043, 11860, 19137, 13637, 11687, 35, 32890, 35, 12609, 35, 994, 26405, 35, 10079, 14581, 15849, 13091, 15190, 10932, 14734, 12642, 10257, 35, 820, 5178, 33394, 35, 15169, 13308, 15189, 19137, 10444, 13091, 12605, 12159, 13197, 10464, 15752, 10384, 17148, 13619, 16728, 13021, 10285, 11871, 10323, 10792, 10250, 10349, 10272, 16501, 10080, 11685, 10333, 19137, 14446, 11454, 10589, 13579, 13499, 12722, 10370, 15261, 11061, 19137, 12605, 13179, 12981, 13188, 10782, 14734, 16689, 11899, 19137, 14446, 11454, 10464, 11003, 11414, 17268, 10283, 12176, 10847, 10370, 15261, 14734, 13212, 11369, 10370, 28329, 65535, 261, 65530, 65534, 1139, 1955, 59, 33, 11685, 14734, 19137, 12605, 14446, 11454, 11003, 12604, 12176, 10847, 10370, 15261, 14734, 13212, 11369, 10370, 10080, 10687, 11903, 19137, 10293, 10370, 10464, 14734, 11117, 10398, 10261, 9822, 16443, 16689, 16664, 10760, 13234, 10292, 10370, 15261, 13193, 10843, 14734, 16689, 11899, 9823, 14782, 12463, 19137, 10444, 13091, 12605, 12537, 12522, 11065, 12469, 14734, 11117, 10398, 10080, 12605, 10423, 16678, 10449, 17148, 13619, 16728, 13021, 19137, 10384, 10285, 11871, 10323, 14734, 10080, 16738, 17728, 10293, 10370, 17147, 13191, 10373, 10303, 17879, 16503, 12605, 12176, 10847, 14734, 11053, 19156, 11, 65535, 261]
-> item_loss 0.9231598973274231 batch_loss 1.102232825756073
==== [25106, 11, 65530, 65534, 17148, 13619, 13012, 11860, 13091, 9822, 14931, 12314, 10267, 14610, 9823, 14734, 10250, 10283, 14313, 13619, 19137, 12847, 17162, 10333, 10250, 10283, 10278, 10283, 10370, 11454, 11957, 16713, 14734, 11461, 13126, 19137, 33, 12605, 12509, 11454, 10258, 17919, 14734, 17749, 16742, 10285, 10333, 16533, 16721, 13012, 13205, 14734, 16530, 15898, 11124, 10370, 14328, 10529, 12445, 11, 14136, 19137, 14190, 17149, 10333, 10994, 11517, 11889, 15034, 2453, 16050, 13770, 10660, 10080, 14136, 15950, 16535, 10992, 18256, 13764, 16099, 11459, 19137, 14179, 14218, 10339, 13091, 11086, 14446, 10760, 16155, 15898, 11124, 15526, 15898, 19137, 14581, 15849, 11656, 17167, 15880, 11459, 17725, 10760, 12172, 13191, 10253, 12319, 10305, 11630, 9822, 15162, 10677, 15898, 9823, 14734, 14136, 15931, 19152, 14136, 15863, 13755, 14808, 11221, 10270, 16407, 10250, 16981, 15104, 11047, 10609, 13750, 15572, 19137, 14136, 14218, 10339, 13091, 13655, 10859, 18216, 13877, 19137, 10411, 10459, 17725, 15639, 14734, 14136, 11282, 13759, 19137, 9822, 11381, 11237, 9823, 14171, 11168, 19152, 14136, 14218, 14190, 10792, 18119, 16218, 12223, 17609, 12631, 11454, 14734, 16407, 17222, 19137, 11638, 14136, 10339, 13091, 10997, 10760, 17774, 17774, 14582, 18119, 19152, 11638, 14136, 14272, 14808, 10333, 16030, 12223, 10609, 16223, 11909, 17384, 12199, 14280, 14734, 14460, 11656, 16030, 16002, 19137, 10370, 10402, 10339, 13091, 13655, 12348, 10997, 14652, 19137, 12266, 11010, 11217, 18175, 14987, 13078, 28329, 13581, 13064, 19137, 16050, 13770, 10660, 11517, 14734, 11221, 10270, 10912, 12145, 13091, 10250, 14313, 14136, 13849, 10080, 11013, 10250, 15137, 14734, 11889, 15034, 11517, 17384, 19137, 13191, 17558, 13191, 17038, 10410, 14734, 12010, 13653, 15465, 15465, 16403, 10846, 19137, 13644, 10260, 12636, 17086, 19137, 14652, 14356, 11459, 12702, 16944, 13352, 13234, 10080, 10444, 13091, 19137, 16050, 13770, 10660, 15887, 11478, 17384, 17268, 10349, 14015, 17095, 16790, 14328, 10079, 13384, 15931, 11124, 13202, 13223, 14734, 15878, 15887, 19137, 16417, 17919, 13852, 14808, 14216, 13750, 19137, 13057, 12145, 14175, 14267, 16944, 13234, 10080, 13760, 11459, 14190, 12604, 10333, 14140, 14194, 19137, 10250, 15891, 15891, 14136, 15887, 16945, 14808, 17170, 14066, 19137, 13755, 14808, 11889, 15034, 13748, 11047, 11638, 13849, 14030, 10985, 19137, 10411, 10459, 13996, 13676, 14734, 16098, 14136, 16263, 19137, 10250, 16981, 14172, 14267, 13755, 12061, 14734, 11913, 13624, 11124, 13224, 14970, 10080, 14136, 13085, 17819, 18043, 18048, 10792, 12061, 10257, 19137, 12640, 11047, 17150, 11621, 13913, 16223, 14734, 15931, 11423, 11124, 16002, 12015, 28329, 14179, 14218, 14583, 10760, 13833, 14187, 10254, 10255, 19137, 10411, 10459, 10250, 13326, 14356, 18043, 10956, 12604, 14734, 18438, 13295, 19137, 10953, 10511, 15126, 11454, 14931, 12314, 10267, 14610, 14734, 11013, 10250, 15137, 19137, 10322, 15752, 14791, 10283, 10250, 13923, 10337, 13425, 28329, 15957, 11454, 10745, 17384, 10305, 11630, 17773, 10735, 12208, 13146, 14734, 12034, 17997, 11471, 16520, 17148, 17774, 10859, 19137, 12527, 16525, 12368, 13091, 12647, 10370, 12366, 12290, 10080, 13581, 13064, 13580, 13191, 17148, 10303, 10278, 10447, 19137, 14791, 12348, 10685, 11110, 14759, 14221, 28329, 10687, 10285, 18216, 10283, 11855, 14734, 17268, 10447, 10561, 14808, 10250, 12669, 16956, 13191, 10250, 10370, 18216, 14734, 10811, 15126, 14808, 19137, 13580, 11638, 11401, 18363, 15809, 10080, 16503, 10260, 13091, 14791, 10390, 17181, 14808, 10250, 15706, 13212, 16650, 15678, 12551, 14734, 14141, 13655, 19137, 16519, 10333, 17148, 10588, 13818, 19137, 16747, 17311, 10423, 10399, 10292, 17148, 13091, 10384, 10285, 17090, 11459, 16009, 10974, 13234, 14734, 17386, 16306, 10370, 28329, 11013, 10250, 10447, 13098, 12348, 14867, 12348, 11632, 19137, 10384, 11647, 10792, 15768, 17311, 16136, 11454, 13388, 15898, 13020, 15232, 17384, 10080, 10594, 11979, 15053, 10846, 10250, 10846, 13064, 19137, 10846, 10460, 10305, 17093, 14352, 11687, 14398, 11145, 17004, 13582, 28329, 10305, 10808, 14734, 10337, 10917, 10768, 17544, 17384, 19137, 17148, 10278, 10447, 10745, 10308, 18441, 10260, 10460, 11610, 19137, 11016, 13191, 10250, 13619, 15190, 14866, 13051, 13258, 14734, 10335, 16686, 19137, 10336, 10684, 14136, 13849, 10285, 14734, 10250, 17774, 14388, 14179, 14288, 14171, 10792, 12221, 10997, 14583, 11454, 11861, 13750, 16883, 13309, 17147, 13091, 11454, 12142, 11606, 10660, 16052, 10936, 11979, 14734, 10460, 11467, 10080, 10278, 10370, 10292, 13581, 10258, 10333, 16915, 13773, 28329, 18216, 10283, 11855, 11219, 11891, 18363, 19137, 12669, 18200, 11647, 12643, 11454, 16002, 10271, 17384, 19137, 15149, 17384, 12172, 14808, 12409, 12474, 43899, 17268, 10349, 11975, 12148, 11855, 17311, 13626, 10333, 43282, 9823, 11, 10390, 16735, 19137, 9822, 12605, 12771, 11268, 13558, 11885, 10402, 14734, 10080, 9823, 11, 9822, 17147, 13191, 17268, 10349, 11902, 12219, 43282, 9823, 14867, 10283, 11855, 12509, 10333, 12509, 19137, 10990, 16416, 10333, 10250, 11012, 19137, 9822, 11902, 14869, 11021, 14267, 10303, 19156, 11065, 16735, 11885, 10402, 16974, 14237, 12143, 10260, 11632, 13091, 10250, 15261, 10272, 16501, 10080, 9823, 11, 9822, 12631, 13191, 17389, 11855, 19137, 17311, 14252, 10333, 19137, 18000, 14808, 13733, 13485, 13819, 13893, 10080, 9823, 11638, 10283, 11855, 16735, 14808, 19137, 13734, 13191, 14486, 10423, 14867, 10283, 11855, 14734, 17728, 18025, 19137, 9822, 12631, 13191, 15597, 17335, 19137, 17311, 11454, 13357, 17384, 13749, 15808, 10333, 10080, 9823, 11, 9822, 17147, 13191, 15640, 18459, 10080, 9823, 10250, 17038, 13388, 16435, 14734, 11042, 10437, 16735, 28329, 9822, 15640, 18459, 19137, 16735, 12348, 11957, 10080, 9823, 11, 9822, 14768, 11627, 13064, 15907, 19137, 13734, 11459, 13036, 11021, 17173, 10080, 9823, 11, 9822, 11042, 13324, 16735, 12348, 11957, 10080, 10444, 19137, 12421, 13091, 16525, 12348, 43282, 11315, 43282, 17148, 10423, 10650, 43282, 9823, 11, 10390, 11155, 10258, 13734, 16735, 11891, 14734, 16713, 19137, 17819, 10953, 12791, 10257, 17093, 12387, 14734, 11010, 13655, 43899, 12605, 10402, 17147, 13556, 9819, 15446, 13676, 16305, 9820, 17268, 10650, 14734, 15640, 12283, 16052, 11632, 10677, 10283, 17607, 12153, 11187, 10080, 9823, 14867, 10283, 11855, 14172, 10333, 14172, 11647, 28329, 10278, 10283, 10370, 10703, 13557, 18441, 10260, 10460, 11610, 10080, 11454, 17148, 12232, 13188, 11638, 14734, 11517, 12154, 11984, 13203, 16944, 14136, 14734, 11459, 13036, 19137, 10990, 10250, 17085, 14288, 14171, 12266, 11726, 10333, 19137, 11454, 11633, 12192, 10257, 14267, 16944, 10250, 17222, 15446, 10659, 28329, 18216, 10283, 11855, 10997, 16713, 10333, 43899, 18460, 11855, 43900, 11, 9822, 12469, 16735, 10080, 9823, 11, 9822, 12605, 12509, 14862, 17222, 13091, 16747, 12983, 14734, 14136, 10080, 9823, 11, 17148, 10283, 16443, 11206, 10460, 9822, 18460, 11855, 9823, 14734, 14867, 10283, 11855, 10811, 12636, 13734, 12220, 11610, 10080, 10390, 13580, 14791, 14808, 14136, 10659, 13086, 15446, 14734, 11638, 16981, 10080, 16981, 10257, 10250, 14781, 13734, 10373, 10303, 10370, 19137, 11416, 10292, 17159, 11672, 14524, 11517, 17723, 13091, 15162, 10250, 12662, 14190, 13626, 14734, 12261, 15176, 10080, 14249, 14249, 14267, 14190, 14734, 13359, 13295, 17865, 14172, 15880, 16109, 11459, 19137, 11517, 17723, 11994, 13581, 11468, 11556, 10080, 65535, 261, 65530, 65532, 10460, 10292, 10250, 11043, 15287, 17189, 16530, 15898, 12656, 14035, 14734, 14442, 11920, 19137, 10399, 14867, 10283, 11855, 14734, 16523, 16530, 19137, 12847, 17162, 14867, 10283, 11855, 11454, 17148, 10283, 11461, 13126, 10285, 14734, 16403, 10292, 11124, 12366, 14486, 14349, 12396, 28329, 13773, 13078, 19151, 14867, 10283, 11855, 11957, 18216, 10283, 11855, 11861, 11454, 10250, 10349, 15847, 10928, 12484, 15509, 10080, 65535, 261, 65530, 65534, 14867, 10283, 11855, 14734, 16523, 16530, 19151, 11, 12605, 14791, 14808, 14249, 14249, 14267, 14190, 14734, 11517, 12154, 19137, 12366, 10285, 10655, 14015, 10333, 12476, 11158, 10080, 17148, 12232, 11517, 12154, 13091, 12605, 14734, 11920, 11423, 19137, 10444, 14446, 11454, 11885, 12604, 10333, 10250, 14313, 12228, 11585, 19137, 12605, 12631, 14256, 12459, 14734, 10250, 10769, 17311, 10392, 10305, 10250, 14165, 10080, 14136, 14218, 11454, 11633, 15090, 10285, 16982, 16962, 19137, 10411, 10459, 10278, 10447, 11679, 11606, 11454, 10335, 18043, 11035, 17359, 19137, 10260, 10768, 10434, 10405, 28329, 12605, 12509, 16944, 10333, 15640, 18459, 16735, 17141, 14734, 16713, 43899, 17148, 10303, 11638, 14734, 14136, 10859, 19137, 13734, 10370, 11098, 10429, 10353, 14794, 13091, 10254, 12203, 10333, 10080, 9823, 10390, 16735, 17148, 16713, 14734, 13064, 10560, 19137, 16728, 13655, 17384, 12172, 14808, 10250, 10273, 17093, 13247, 10080, 12609, 16684, 10390, 16525, 12348, 15847, 12144, 12145, 15486, 10323, 12506, 10333, 17148, 13324, 14734, 14146, 17843, 28329, 15646, 12605, 14862, 17222, 19137, 17148, 11016, 13091, 13156, 10838, 14734, 10720, 12034, 10250, 16530, 10080, 13006, 10283, 11517, 12154, 17311, 11021, 15752, 11454, 14179, 14218, 10285, 18034, 12675, 10079, 11115, 11057, 10079, 13188, 15480, 10901, 10292, 10306, 13191, 28329, 12605, 14791, 11047, 18216, 10283, 11855, 19137, 10390, 13580, 12379, 14808, 12669, 14188, 14256, 14734, 18293, 10384, 18293, 18200, 11647, 10257, 12831, 10258, 13234, 19137, 10277, 17149, 11349, 17384, 10080, 10390, 10441, 10308, 11891, 10675, 13734, 12669, 17148, 11461, 14146, 17843, 12307, 10250, 11414, 10336, 10080, 12609, 16684, 11454, 10390, 14791, 13234, 19137, 17148, 11016, 13091, 10250, 11461, 12522, 11630, 10080, 65535, 261, 65530, 65534, 14867, 10283, 11855, 14734, 16523, 16530, 19151, 11, 12605, 14791, 14808, 14249, 14249, 14267, 14190, 14734, 11517, 12154, 19137, 12366, 10285, 10655, 14015, 10333, 12476, 11158, 10080, 17148, 12232, 11517, 12154, 13091, 12605, 14734, 11920, 11423, 19137, 10444, 14446, 11454, 11885, 12604, 10333, 10250, 14313, 12228, 11585, 19137, 12605, 12631, 14460, 16523, 14734, 10272, 16501, 10392, 10305, 10250, 14165, 28329, 12605, 14791, 10792, 13191, 10349, 10370, 12604, 15604, 15489, 17766, 11459, 10384, 11638, 16981, 10257, 16939, 17141, 19137, 10411, 10459, 13734, 10336, 10370, 17268, 13324, 16676, 16686, 14808, 17148, 11461, 14146, 17843, 10080, 10390, 10402, 12609, 16684, 10423, 18169, 16956, 16520, 13197, 10250, 10258, 14249, 14249, 14267, 14190, 14734, 18216, 11559, 19137, 10444, 13179, 11021, 15752, 14734, 13091, 17177, 12731, 13051, 16523, 28329, 15646, 12307, 10390, 10402, 14778, 10659, 14782, 17213, 13064, 19137, 12605, 15752, 14791, 10792, 12476, 10429, 10079, 12409, 12474, 11124, 11042, 12484, 10080, 17148, 16672, 12605, 12527, 10792, 10250, 10273, 11889, 12556, 28329, 10444, 12337, 12387, 17148, 10349, 12484, 15509, 11994, 13855, 11645, 12348, 13051, 12325, 13051, 17001, 10080, 11416, 10292, 12605, 14862, 17222, 19137, 10390, 10402, 11016, 13091, 12669, 12476, 10429, 11124, 12409, 12474, 16223, 11454, 12366, 17384, 19137, 10260, 12509, 16672, 15847, 12144, 10322, 11003, 12348, 10448, 16893, 28329, 17148, 16672, 12605, 12509, 16944, 10333, 10399, 10808, 11454, 10324, 10258, 16519, 17141, 14734, 10250, 15033, 16273, 16272, 10080, 16273, 16272, 17213, 10792, 10952, 17801, 13064, 10423, 11967, 17038, 10452, 16335, 12604, 10250, 10283, 15326, 10740, 14734, 14484, 14349, 19137, 10399, 13581, 13234, 10739, 11977, 11957, 11630, 14610, 14734, 12824, 16535, 10080, 65535, 261, 65530, 65534, 14867, 10283, 11855, 14734, 16523, 16530, 59, 11, 12605, 18441, 10260, 10460, 11610, 11459, 14779, 14808, 14249, 14249, 14267, 14190, 14734, 14136, 10659, 45, 12366, 10285, 10655, 14015, 10333, 11626, 13215, 14734, 12484, 15509, 28329, 17148, 11461, 11638, 14136, 13091, 12605, 10402, 12457, 12457, 16664, 10775, 10333, 12337, 10301, 14734, 12687, 11626, 45, 11021, 10250, 14791, 10792, 11687, 13581, 12500, 14179, 14734, 13126, 16803, 45, 12605, 10383, 13191, 10349, 12430, 12499, 11124, 11044, 12462, 10305, 12522, 28329, 10444, 12605, 13179, 11632, 14734, 13091, 10966, 12670, 14808, 14734, 12348, 12522, 10080, 12605, 11124, 18216, 10283, 11855, 14734, 16664, 10775, 15480, 10339, 12348, 17192, 45, 17268, 10349, 13565, 10966, 12605, 10402, 16684, 10301, 14734, 15500, 13751, 15643, 11124, 13216, 16896, 10953, 11967, 10392, 10760, 10397, 10408, 10080, 17148, 10283, 13064, 10560, 12220, 12307, 12527, 10792, 13559, 11268, 12638, 13091, 28329, 14221, 15646, 45, 18216, 10283, 11855, 17268, 13324, 17093, 13247, 14734, 16728, 13655, 11124, 15149, 11922, 45, 17147, 13191, 10390, 11957, 14136, 14146, 11044, 14734, 10250, 10769, 12789, 11645, 13644, 10260, 11454, 10308, 14734, 13324, 11855, 45, 12421, 13091, 16672, 12605, 12527, 10792, 15847, 10928, 11124, 11645, 16109, 28329, 12605, 12202, 17916, 13734, 13191, 12509, 13764, 45, 10322, 15752, 16735, 10760, 10687, 10390, 16520, 14172, 45, 10444, 10390, 14734, 16713, 16728, 12421, 13091, 13187, 12605, 10460, 10293, 45, 15646, 12605, 10322, 11016, 13191, 18441, 18441, 14172, 11647, 28329, 12605, 12145, 15486, 10323, 12506, 10333, 10292, 10390, 17802, 16420, 11124, 10460, 17802, 10080, 11016, 13191, 11454, 17148, 13324, 13626, 14138, 10250, 10769, 14734, 13064, 10560, 45, 12605, 12638, 12527, 10792, 12605, 10402, 13091, 12200, 15169, 14734, 28329, 10444, 12605, 12145, 12509, 13078, 14731, 45, 12605, 11124, 10390, 15480, 15087, 10260, 11042, 10080, 10390, 13091, 10250, 10283, 15584, 14347, 45, 15646, 12605, 10383, 13191, 18004, 12370, 10261, 11255, 15895, 10080, 65535, 261, 65530, 65534, 18216, 10283, 11855, 14734, 12847, 17162, 16530, 12231, 11687, 10258, 59, 11, 12605, 41, 18216, 10283, 11855, 42, 11124, 12605, 14734, 11042, 10437, 18460, 11855, 41, 14867, 10283, 11855, 13020, 15217, 10285, 14734, 10370, 42, 13580, 11454, 17150, 11621, 14734, 12034, 17997, 16520, 14791, 10258, 13036, 10994, 11517, 11889, 15034, 11124, 16050, 13770, 10660, 11638, 14136, 14267, 14190, 14734, 13126, 16803, 10080, 12605, 10402, 10534, 18441, 10260, 10460, 11610, 14791, 10333, 10337, 10917, 11632, 10768, 17544, 45, 11016, 13191, 10250, 13557, 10335, 12650, 13091, 11060, 13091, 13279, 11621, 10387, 12219, 14288, 14171, 10358, 14583, 14734, 28329, 18460, 11855, 16269, 14221, 14867, 45, 10444, 14791, 10441, 12990, 12802, 10079, 13212, 12990, 10080, 10390, 14734, 13633, 10250, 10283, 10846, 10460, 17311, 12337, 17093, 12139, 28329, 12605, 11035, 11891, 18363, 15809, 11044, 16694, 10408, 17311, 12154, 17384, 14734, 11975, 12148, 11124, 11011, 11902, 16443, 14190, 13626, 45, 18460, 11855, 10779, 16718, 17728, 11902, 14869, 13091, 11060, 15752, 14267, 14190, 10080, 10444, 12605, 13734, 13191, 13580, 17919, 11414, 15178, 10390, 14734, 17728, 18025, 28329, 12605, 10402, 15507, 15511, 16520, 14136, 45, 18460, 11855, 12748, 10760, 14136, 14146, 10285, 15640, 18459, 14734, 10258, 11461, 45, 12605, 16417, 14966, 16928, 11042, 28329, 13006, 10452, 13234, 16735, 45, 12605, 11957, 18460, 11855, 13191, 15895, 11685, 14734, 16694, 10408, 10080, 10390, 13212, 12990, 15678, 12551, 45, 13633, 10250, 11012, 16713, 17311, 12663, 16503, 13191, 10838, 10080, 10390, 14734, 17728, 18025, 12609, 16694, 16686, 10322, 17311, 12337, 15287, 10733, 12202, 13191, 12261, 16689, 12415, 10080, 65535, 261, 65530, 65532, 10399, 15162, 10250, 10370, 15044, 19133, 18216, 10283, 11855, 43898, 12847, 17162, 18216, 10283, 11855, 11124, 14867, 10283, 11855, 14734, 10355, 13819, 19137, 16417, 14446, 18216, 10283, 11855, 14734, 12366, 14486, 13815, 10846, 11124, 10390, 11957, 14867, 10283, 11855, 14734, 16694, 10408, 28329, 65535, 261, 65530, 65534, 18216, 10283, 11855, 14734, 12847, 17162, 16530, 12231, 11687, 10258, 59, 11, 12605, 41, 18216, 10283, 11855, 42, 11124, 12605, 14734, 11042, 10437, 18460, 11855, 41, 14867, 10283, 11855, 13020, 15217, 10285, 14734, 10370, 42, 13580, 11454, 17150, 11621, 14734, 12034, 17997, 16520, 14791, 10258, 13036, 10994, 11517, 11889, 15034, 11124, 16050, 13770, 10660, 11638, 14136, 14267, 14190, 14734, 13126, 16803, 10080, 12605, 10402, 10534, 18441, 10260, 10460, 11610, 14791, 10333, 10337, 10917, 11632, 10768, 17544, 45, 11016, 13191, 10250, 13557, 10335, 12650, 13091, 11060, 13091, 13279, 11621, 10387, 12219, 14288, 14171, 10358, 14583, 14734, 28329, 18460, 11855, 16269, 14221, 14867, 45, 10444, 14791, 10441, 12990, 12802, 10079, 13212, 12990, 10080, 10390, 14734, 13633, 10250, 10283, 10846, 10460, 17311, 12337, 17093, 12139, 28329, 12605, 11035, 11891, 18363, 15809, 11044, 16694, 10408, 17311, 12154, 17384, 14734, 11975, 12148, 11124, 11011, 11902, 16443, 14190, 13626, 45, 18460, 11855, 10779, 16718, 17728, 11902, 14869, 13091, 11060, 15752, 14267, 14190, 10080, 10444, 12605, 13734, 13191, 13580, 17919, 11414, 15178, 10390, 14734, 17728, 18025, 28329, 12605, 10402, 15507, 15511, 16520, 14136, 45, 18460, 11855, 12748, 10760, 14136, 14146, 10285, 15640, 18459, 14734, 10258, 11461, 45, 12605, 16417, 14966, 16928, 11042, 28329, 13006, 10452, 13234, 16735, 45, 12605, 11957, 18460, 11855, 13191, 15895, 11685, 14734, 16694, 10408, 10080, 10390, 13212, 12990, 15678, 12551, 45, 13633, 10250, 11012, 16713, 17311, 12663, 16503, 13191, 10838, 10080, 10390, 14734, 17728, 18025, 12609, 16694, 16686, 10322, 17311, 12337, 15287, 10733, 12202, 13191, 12261, 16689, 12415, 10080, 65535, 261, 65530, 65532, 14313, 13619, 10285, 13191, 10745, 10447, 16530, 15898, 10760, 11461, 19156, 11, 65535, 261, 65530, 65534, 10377, 10384, 12631, 12848, 10494, 14734, 14313, 13619, 13234, 14791, 45, 14313, 13619, 10285, 10760, 11461, 14734, 10293, 16503, 16530, 15898, 10441, 10308, 11016, 13191, 10278, 10283, 59, 11, 50, 47, 33, 10250, 10283, 18216, 10283, 11855, 14734, 10811, 12636, 10080, 14313, 10285, 11967, 10390, 12847, 17162, 10292, 35, 18216, 10283, 11855, 384, 10390, 10561, 14808, 10250, 12669, 17688, 10811, 12202, 11638, 11401, 18363, 15809, 28329, 51, 47, 33, 10250, 10283, 14867, 10283, 11855, 14734, 11042, 10437, 45, 10675, 17038, 10894, 16468, 11454, 13020, 15217, 17384, 45, 15046, 10846, 13064, 10621, 14398, 10250, 13324, 17093, 14756, 28329, 14313, 13619, 10285, 13734, 13191, 17181, 17900, 17148, 10278, 10370, 14734, 10688, 10452, 17038, 10410, 12609, 15669, 10270, 45, 11016, 12847, 17162, 10390, 10402, 17912, 17912, 11459, 16520, 16917, 14808, 17150, 11621, 14179, 14218, 14249, 14249, 14734, 11517, 12154, 28329, 17148, 10278, 10370, 14734, 10342, 10846, 16417, 13078, 10390, 10402, 13091, 14782, 16696, 12145, 10301, 14734, 10422, 10437, 45, 10305, 17734, 11861, 11454, 10250, 11899, 14734, 18441, 11662, 28329, 12631, 10399, 11994, 14313, 13619, 13205, 17038, 12848, 10494, 14734, 10529, 12445, 13234, 14791, 45, 14313, 10285, 10760, 11461, 14734, 16530, 15898, 10293, 16503, 11016, 13191, 17148, 10278, 10447, 45, 10265, 13051, 13764, 12826, 13032, 10390, 10402, 14734, 10688, 10452, 15669, 10270, 28329, 12159, 13197, 10399, 10257, 10529, 12445, 15752, 12176, 10847, 10792, 12469, 10080, 15957, 13191, 10260, 16956, 10305, 11621, 17147, 16738, 12748, 13580, 10080, 65535, 261, 65530, 65532, 12847, 17162, 18216, 10283, 11855, 14734, 11630, 16815, 14332, 12333, 11124, 12415, 13327, 14332, 14172, 11, 65535, 261, 65530, 65534, 18216, 10283, 11855, 14734, 17385, 16503, 14332, 12333, 11124, 12415, 13327, 14332, 14172, 11687, 10258, 59, 11, 50, 47, 33, 11630, 16815, 18216, 11638, 10080, 14313, 13619, 10285, 11967, 10390, 12847, 17162, 10292, 9822, 18216, 10283, 11855, 28319, 13637, 10390, 14734, 11042, 10437, 14867, 10283, 11855, 13098, 16114, 18216, 11638, 28329, 51, 47, 33, 15752, 12199, 10843, 11903, 10080, 14313, 10285, 12848, 10792, 10390, 9822, 17181, 14808, 10250, 15706, 13212, 16650, 15678, 12551, 14734, 14141, 13655, 28319, 10261, 10390, 9822, 11638, 11401, 18363, 15809, 9823, 14734, 10588, 13818, 12314, 12604, 11957, 13637, 45, 16735, 13078, 10390, 12415, 13327, 11903, 17781, 15752, 12199, 28329, 52, 47, 33, 16792, 17136, 11638, 12231, 10080, 10390, 14791, 16944, 13234, 11957, 11638, 14136, 14190, 13626, 14734, 9822, 11975, 12148, 11855, 9823, 12202, 10260, 10382, 12522, 45, 10995, 15646, 16417, 14966, 9822, 12771, 11268, 13558, 11885, 10402, 14734, 9823, 28329, 53, 47, 33, 12560, 12552, 12503, 12522, 10080, 10390, 14331, 13856, 10792, 13556, 10258, 9822, 15446, 13676, 16305, 9823, 17335, 18107, 14734, 10576, 10843, 45, 13098, 14966, 10390, 14583, 13815, 17311, 14782, 12307, 11671, 10486, 28329, 54, 47, 33, 11957, 11042, 10437, 13191, 14862, 12144, 10305, 11685, 10080, 10390, 14589, 9822, 18460, 11855, 9823, 15044, 11116, 14867, 10283, 11855, 11042, 10437, 45, 13098, 14966, 10390, 10402, 12145, 13191, 13913, 10972, 10355, 12484, 28329, 55, 47, 33, 17819, 11124, 12208, 18441, 10080, 10390, 11016, 17819, 12522, 11414, 12220, 11042, 10437, 14734, 12848, 17728, 45, 12202, 12266, 14442, 15149, 16735, 9822, 12631, 13191, 15640, 18459, 9823, 17311, 13594, 11454, 11638, 14136, 10285, 28329, 56, 47, 33, 11957, 10336, 10407, 14794, 14782, 12337, 11454, 12522, 10080, 10390, 17728, 14867, 10283, 11855, 11042, 10437, 9822, 12605, 12509, 14862, 17222, 13091, 16747, 12983, 14734, 14136, 9823, 28329, 10399, 10257, 10293, 16503, 11533, 10339, 18216, 10283, 11855, 14734, 11630, 16815, 12847, 17162, 10079, 15707, 10452, 16728, 16537, 10399, 10992, 10261, 14867, 10283, 11855, 11042, 10437, 14734, 10342, 10846, 13234, 12826, 13032, 10390, 14734, 12415, 13327, 14332, 14172, 10080, 12159, 13197, 15752, 11634, 10733, 14914, 11414, 15178, 12469, 14734, 17728, 18025, 10080, 65535, 261, 65530, 65532, 12605, 17879, 16503, 14589, 8683, 24381, 59517, 15493, 10794, 14867, 10283, 11855, 14734, 11630, 16815, 19137, 33, 16738, 15494, 12605, 11957, 12220, 14734, 12848, 14966, 16702, 11, 65535, 261, 65530, 65534, 10399, 10258, 13091, 12722, 12261, 16676, 14734, 14867, 10283, 11855, 14734, 11630, 16815, 12847, 17162, 16702, 59, 11, 9828, 37347, 280, 33, 18216, 10283, 11855, 11967, 10390, 15044, 10292, 14867, 10283, 11855, 45, 16735, 13078, 10390, 17038, 13223, 12337, 14866, 11975, 28329, 9828, 29087, 1843, 280, 33, 10390, 10675, 17038, 17311, 10894, 16468, 11454, 13388, 15898, 14734, 13020, 15232, 17384, 45, 12848, 14966, 10390, 12182, 15091, 13191, 10671, 12184, 14734, 13020, 15232, 28329, 9828, 3630, 7834, 280, 33, 16269, 14221, 14867, 11975, 10444, 16403, 10846, 10621, 14398, 10250, 13324, 12990, 12802, 17140, 17193, 28329, 9828, 50469, 280, 33, 10390, 12337, 11977, 16735, 16713, 45, 15494, 10370, 14987, 15036, 14734, 12527, 16525, 28329, 9828, 36406, 38056, 280, 33, 10390, 15091, 14808, 13388, 15898, 13020, 15232, 45, 11021, 13091, 10250, 10283, 65129, 51725, 28329, 9828, 44656, 7015, 280, 33, 10390, 18441, 18441, 11459, 16520, 11940, 14808, 11638, 14136, 45, 16735, 13078, 10390, 12337, 14614, 12522, 11105, 11430, 14734, 15478, 15907, 28329, 9828, 28478, 101, 280, 33, 12002, 15202, 10260, 14914, 11899, 45, 10444, 14597, 10339, 10390, 14734, 14987, 15036, 11124, 17122, 16696, 12231, 45, 12337, 13191, 11021, 15752, 13191, 10250, 12669, 15737, 11855, 28329, 9828, 36838, 8091, 7473, 30806, 280, 33, 12002, 15202, 13734, 13191, 12847, 17162, 10390, 14734, 14806, 14815, 45, 10444, 15641, 16253, 10792, 10390, 14734, 15678, 13078, 11124, 16520, 11940, 10838, 45, 10390, 11021, 15752, 13191, 10250, 10994, 13133, 12551, 14734, 14806, 14815, 28329, 9828, 29087, 47249, 280, 33, 14597, 10339, 10390, 10675, 17038, 17311, 16468, 11454, 13020, 15232, 17384, 45, 10671, 12184, 14734, 17773, 12325, 11021, 15752, 17238, 12761, 17303, 10768, 10390, 14734, 17919, 17303, 14332, 12333, 28329, 9828, 29438, 7493, 8439, 39725, 280, 33, 12307, 18216, 10283, 11855, 12848, 10792, 35, 12631, 13191, 15640, 18459, 35, 13064, 45, 10390, 13191, 11021, 15752, 15149, 10333, 10250, 10258, 45, 10444, 15149, 11922, 17820, 16223, 11454, 13020, 15232, 10258, 45, 14987, 15036, 15646, 13191, 18248, 10838, 28329, 12159, 13197, 10399, 10257, 14734, 12847, 17162, 16702, 15752, 10847, 12469, 14583, 12604, 15158, 11038, 12985, 10336, 15478, 15907, 14734, 14867, 10283, 11855, 11630, 16815, 12314, 16803, 10080, 15957, 13191, 10260, 11038, 17172, 10305, 11621, 45, 13558, 17143, 12748, 13580, 28329, 36, 44, 24217, 59, 29945, 4164, 5608, 44816, 29850, 3662, 1313, 19589, 1511, 70, 359, 19566, 5675, 969, 65535, 261]
-> item_loss 1.3474713563919067 batch_loss 1.1245272376320579
==== [25106, 11, 65530, 65532, 13499, 11496, 17384, 13191, 10370, 11454, 10303, 19156, 11, 65535, 261, 65530, 65534, 12469, 11685, 19137, 12605, 13091, 13499, 11496, 10696, 14734, 11168, 12220, 10293, 10452, 10080, 12605, 11021, 10399, 14781, 12824, 11957, 12469, 17149, 16403, 11414, 11626, 19137, 10444, 13091, 12159, 13197, 12469, 15752, 12848, 10494, 13179, 10842, 11891, 11255, 14734, 42100, 19137, 17148, 13324, 15752, 10482, 12605, 13179, 11922, 13080, 10912, 10768, 10997, 16537, 10370, 12202, 12848, 18216, 12605, 14734, 17207, 17108, 28329, 65535, 261, 65530, 65532, 18030, 19137, 11685, 11061, 19137, 17148, 13557, 11016, 13091, 13827, 16707, 10250, 10258, 10080, 12605, 10985, 10708, 10250, 10258, 42100, 11, 65535, 261, 65530, 65534, 11685, 14734, 19137, 12605, 12337, 10310, 12522, 12824, 11002, 12469, 14734, 13827, 16707, 28329, 65535, 261, 65530, 65532, 16713, 16735, 19137, 10464, 13191, 10373, 10303, 12261, 16676, 10303, 19156, 11, 65535, 261, 65530, 65534, 12469, 10322, 16684, 11021, 10399, 11986, 16707, 10250, 10258, 11454, 15667, 11639, 10696, 11922, 10808, 10842, 10673, 10464, 15847, 12144, 14734, 16530, 15898, 19137, 13637, 11687, 35, 24281, 59, 358, 10997, 16537, 10696, 11922, 126, 33, 12609, 15643, 13091, 10293, 10370, 19151, 124, 10997, 16537, 10696, 11922, 2383, 19137, 12609, 15643, 11454, 15667, 11639, 10305, 10808, 10842, 10673, 10250, 10349, 12484, 13126, 12847, 17162, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 13091, 17148, 13324, 10303, 19156, 11, 65535, 261, 65530, 65534, 11168, 12220, 10293, 10452, 19151, 13734, 17631, 19137, 11584, 10842, 10997, 16537, 10370, 13308, 16696, 15158, 15752, 11634, 12176, 10847, 12605, 10402, 13179, 11685, 11459, 10912, 10768, 10997, 16537, 11957, 16803, 19137, 10384, 15646, 12848, 18216, 12605, 10402, 14734, 15667, 11639, 16884, 17387, 28329, 65535, 261, 65530, 65531, 40812, 59, 33, 13205, 13499, 11496, 12727, 13191, 15752, 11634, 14486, 16533, 13064, 17734, 14734, 15752, 10838, 10080, 12307, 13191, 13064, 17734, 17109, 10673, 13064, 19137, 12605, 11021, 10399, 14141, 13815, 14734, 10788, 14589, 11885, 10402, 13179, 11685, 11459, 13193, 10843, 14589, 12626, 28329, 65535, 261, 65530, 65531, 40812, 59, 33, 12307, 10808, 13064, 17734, 19151, 640, 643, 48, 54, 48, 51, 3486, 59, 622, 59, 643, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11454, 10303, 19156, 11, 65535, 261, 65530, 65534, 5645, 59, 33, 10293, 10370, 19137, 10258, 10921, 11685, 19137, 12605, 10250, 14781, 17311, 11454, 14734, 10080, 12469, 12664, 12605, 13191, 10373, 10303, 10336, 10303, 19156, 11, 65535, 261, 65530, 65531, 40812, 59, 33, 12307, 10808, 13064, 17734, 19151, 640, 643, 48, 54, 48, 51, 3486, 59, 623, 59, 621, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11315, 19137, 12605, 12159, 13197, 10464, 15752, 11454, 10305, 11044, 14734, 11957, 16713, 10285, 12656, 14035, 10250, 10283, 14398, 11764, 10080, 14398, 11764, 13091, 13191, 14808, 14398, 14332, 12333, 10261, 17303, 10768, 10323, 12415, 14734, 10370, 12314, 14583, 14328, 19137, 11684, 10402, 11454, 16735, 16713, 14734, 13064, 10560, 11268, 13558, 11454, 11012, 12003, 10842, 10250, 10283, 35, 11280, 127, 35, 11860, 28329, 65535, 261, 65530, 65534, 5645, 59, 33, 13078, 14731, 10333, 10293, 10370, 19137, 14446, 11454, 12605, 13091, 10250, 10283, 14398, 11764, 10333, 11280, 127, 11, 65535, 261, 65530, 65531, 40812, 59, 33, 12307, 10808, 13064, 17734, 19151, 640, 643, 48, 54, 48, 51, 3486, 59, 623, 59, 635, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 10464, 12509, 16503, 10921, 14817, 10303, 19156, 11, 65535, 261, 65530, 65534, 5645, 59, 33, 12605, 13091, 10283, 10653, 13655, 14015, 14015, 14734, 14398, 11764, 11280, 127, 14446, 11454, 17147, 10260, 12509, 10921, 14817, 19137, 11416, 10292, 12605, 12509, 12176, 10257, 10293, 10370, 14734, 12379, 11280, 127, 11, 65535, 261, 65530, 65531, 40812, 59, 33, 12307, 10808, 13064, 17734, 19151, 640, 643, 48, 54, 48, 51, 3486, 59, 624, 59, 620, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 18030, 19137, 13091, 11416, 10292, 10464, 13091, 13212, 11369, 10370, 10260, 17879, 16503, 14817, 16525, 19137, 12631, 10399, 12638, 17148, 10303, 16735, 14734, 10303, 19156, 11, 65535, 261, 65530, 65534, 5645, 59, 33, 10260, 14862, 17222, 10293, 10370, 11454, 16735, 10373, 10303, 11102, 11280, 127, 11687, 13258, 12605, 13091, 12656, 12604, 14398, 11764, 14734, 13212, 11369, 10370, 14734, 16713, 19137, 15713, 11899, 13091, 10260, 14589, 14817, 16525, 14734, 11280, 2411, 10444, 13091, 12605, 13091, 10250, 10283, 14398, 11764, 19137, 12509, 16503, 14817, 16525, 14734, 13064, 10560, 10423, 10985, 14817, 14734, 11280, 127, 11, 65535, 261, 65530, 65531, 40812, 59, 33, 12307, 10808, 13064, 17734, 19151, 640, 643, 48, 54, 48, 51, 3486, 59, 624, 59, 637, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11164, 11164, 19137, 12605, 14862, 17222, 10333, 28329, 65535, 261, 65530, 65534, 5645, 59, 33, 11280, 11354, 11354, 127, 11, 65535, 261, 65530, 65531, 40812, 59, 33, 12307, 10808, 13064, 17734, 19151, 640, 643, 48, 54, 48, 51, 3489, 59, 622, 59, 674, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11315, 43282, 11685, 10621, 17147, 13734, 15494, 10464, 16944, 11043, 11860, 19137, 10464, 11994, 11017, 8227, 112, 10333, 19137, 8227, 112, 19137, 11454, 10303, 19156, 11, 65535, 261, 65530, 65534, 5645, 59, 33, 11454, 14734, 11280, 127, 11685, 10621, 12145, 15486, 13091, 10258, 10921, 10333, 11102, 10080, 8227, 112, 10780, 10780, 14817, 11891, 10250, 16525, 19137, 17819, 13064, 11021, 10399, 15494, 10293, 10370, 12176, 12379, 11280, 127, 11, 65535, 261, 65530, 65531, 40812, 59, 33, 12307, 10808, 13064, 17734, 19151, 640, 643, 48, 54, 48, 51, 3489, 59, 623, 59, 636, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 10589, 13579, 12656, 14035, 14398, 11764, 11061, 19137, 8227, 112, 14734, 11043, 11860, 10525, 14614, 28329, 65535, 261, 65530, 65534, 5645, 59, 33, 11685, 14734, 10293, 10370, 19137, 12605, 14446, 11454, 10589, 13579, 12656, 14035, 19137, 13191, 10373, 10303, 10336, 12484, 10303, 19156, 11, 65535, 261, 65530, 65531, 40812, 59, 33, 12307, 10808, 13064, 17734, 19151, 640, 643, 48, 54, 48, 51, 3489, 59, 624, 59, 621, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 4682, 1997, 11, 65535, 261, 65530, 65534, 5645, 59, 33, 10293, 10370, 19137, 12605, 11454, 28329, 65535, 261, 65530, 65531, 40812, 59, 33, 12307, 10808, 13064, 17734, 19151, 640, 643, 48, 54, 48, 51, 3489, 59, 624, 59, 632, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 10464, 16525, 12348, 12605, 13091, 10283, 12401, 13324, 14734, 10370, 19156, 11, 65535, 261, 65530, 65534, 5645, 59, 33, 10293, 10370, 11994, 13091, 10293, 10370, 19137, 13091, 12605, 13193, 10843, 14734, 11957, 16803, 10080, 12605, 12202, 10260, 12727, 13191, 10293, 16520, 12522, 16696, 19137, 12631, 10399, 12202, 10260, 10423, 11957, 10358, 14583, 18030, 11630, 14734, 16694, 10408, 10261, 14791, 13764, 10080, 11687, 13258, 10293, 10370, 17879, 16503, 12605, 16417, 14446, 10760, 17148, 13036, 17919, 14734, 14332, 12415, 19137, 11021, 10399, 10538, 12981, 12605, 14734, 16689, 11899, 12609, 13091, 11047, 12605, 12848, 10760, 16503, 13685, 28329, 65535, 261, 65530, 65531, 40812, 59, 33, 12307, 10808, 13064, 17734, 19151, 640, 643, 48, 54, 48, 51, 3490, 59, 620, 59, 621, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11315, 19137, 12605, 14486, 16533, 10333, 19137, 17268, 12605, 17879, 16503, 10464, 14446, 11454, 11003, 12348, 10370, 12415, 10901, 10250, 14172, 10080, 10688, 10452, 10792, 10373, 10303, 15052, 12231, 10464, 15847, 12144, 12669, 12853, 28329, 65535, 261, 65530, 65534, 5645, 59, 33, 12605, 13078, 14731, 10333, 19137, 10293, 10370, 10080, 10460, 10292, 8227, 112, 19137, 10293, 10370, 11957, 10339, 12605, 13234, 16735, 11994, 10621, 10366, 11930, 14734, 13192, 10993, 10250, 13324, 19137, 12605, 12337, 11268, 13558, 10293, 10370, 10080, 17147, 13191, 10293, 10370, 10464, 10780, 10780, 16672, 12605, 12656, 14035, 10250, 10283, 14398, 11764, 19137, 17148, 16672, 12605, 15053, 12359, 13191, 10250, 14172, 14172, 11917, 15600, 19137, 10260, 17141, 12605, 12337, 12537, 12522, 10292, 10293, 10370, 12656, 14398, 11764, 10080, 10293, 10370, 19137, 17148, 13324, 14734, 15052, 12231, 11021, 10399, 11053, 19156, 11, 65535, 261, 65530, 65531, 40812, 59, 33, 12307, 10808, 13064, 17734, 19151, 640, 643, 48, 54, 48, 51, 3490, 59, 620, 59, 641, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 12771, 10260, 17631, 14734, 28329, 65535, 261, 65530, 65534, 5645, 59, 33, 11685, 19137, 17268, 12605, 11994, 10525, 12746, 17148, 10283, 14349, 12396, 11235, 28329, 65535, 261, 65530, 65531, 40812, 59, 33, 12307, 10808, 13064, 17734, 19151, 640, 643, 48, 54, 48, 52, 291, 59, 630, 59, 621, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 13057, 10257, 11685, 28329, 65535, 261, 65530, 65534, 5645, 59, 33, 13057, 10257, 11685, 19137, 10293, 10370, 10080, 13089, 13115, 10293, 10370, 13734, 13191, 12664, 12605, 16735, 16713, 13191, 10250, 14172, 11975, 11927, 11939, 11102, 19137, 17879, 16503, 12605, 10292, 12469, 12848, 10494, 10250, 10349, 13057, 18069, 14734, 12261, 16676, 10303, 19156, 11, 65535, 261, 65530, 65531, 40812, 59, 33, 12307, 10808, 13064, 17734, 19151, 640, 643, 48, 54, 48, 52, 291, 59, 630, 59, 636, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11223, 12691, 13571, 12691, 13571, 19137, 13089, 13115, 13191, 14172, 10336, 10080, 58, 14172, 13191, 10283, 10423, 19137, 13057, 18069, 12605, 12387, 18069, 10384, 15190, 10333, 19137, 14778, 10808, 13734, 13191, 17879, 16503, 10464, 12176, 12379, 14734, 28329, 65535, 261, 65530, 65534, 5645, 59, 33, 11957, 10260, 16944, 19137, 10293, 10370, 19137, 10441, 10308, 12605, 13499, 12722, 14734, 12527, 12484, 16672, 12469, 11426, 12658, 10333, 10080, 10687, 11903, 12605, 13734, 13191, 13064, 17734, 13819, 17191, 14734, 13445, 12389, 19137, 12631, 10399, 12202, 10260, 10423, 11927, 11939, 19137, 10293, 10370, 10260, 12368, 10292, 13581, 10696, 14643, 10080, 12605, 10305, 11044, 10423, 16417, 14446, 12348, 13179, 11685, 14734, 28329, 65535, 261, 65530, 65531, 40812, 59, 33, 12307, 10808, 13064, 17734, 19151, 640, 643, 48, 54, 48, 52, 291, 59, 630, 59, 675, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 12605, 14486, 16533, 10333, 19137, 11957, 10339, 14446, 11454, 14734, 10464, 12605, 12771, 14015, 12522, 14734, 10080, 11957, 10333, 8227, 112, 19137, 10464, 14794, 14734, 16667, 10292, 10464, 13734, 13191, 12484, 12527, 11124, 10370, 13327, 10303, 19156, 11, 65535, 261, 65530, 65534, 5645, 59, 33, 11907, 16520, 13234, 16679, 13091, 17148, 13324, 14734, 19137, 12605, 14734, 16728, 16537, 13091, 13326, 12797, 12469, 14734, 17879, 13685, 10495, 13445, 14426, 17109, 10760, 14734, 19137, 12605, 14734, 16403, 10292, 13499, 12276, 13205, 16884, 10257, 13091, 13499, 10411, 10261, 12656, 14035, 28329, 65535, 261, 65530, 65531, 40812, 59, 33, 12307, 10808, 13064, 17734, 19151, 640, 643, 48, 54, 48, 52, 291, 59, 633, 59, 636, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 17268, 10580, 16689, 10464, 10250, 14781, 15516, 12746, 12307, 10808, 17148, 10283, 16403, 10292, 13499, 12276, 19137, 12202, 10265, 10409, 10454, 10370, 17311, 10260, 14862, 17222, 10464, 13091, 11454, 13499, 10411, 10261, 12656, 14035, 11102, 19156, 11, 65535, 261, 65530, 65534, 5645, 59, 33, 17148, 13324, 13234, 16679, 19137, 12605, 11021, 15752, 10423, 16443, 10390, 10370, 16667, 10292, 13091, 10250, 10283, 12727, 13191, 12484, 12527, 12609, 10370, 13327, 14734, 10283, 10452, 10080, 10444, 13091, 19137, 10384, 12605, 14734, 14862, 16696, 10760, 10997, 19137, 12605, 12202, 10260, 15752, 12527, 11002, 10792, 15847, 12605, 28329, 65535, 261, 65530, 65531, 40812, 59, 33, 12307, 10808, 13064, 17734, 19151, 640, 643, 48, 54, 48, 52, 291, 59, 634, 59, 667, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 17268, 12605, 11687, 13258, 16735, 10464, 10780, 12638, 17148, 13619, 17109, 10760, 13091, 10384, 11685, 10745, 13557, 17109, 10760, 10285, 12752, 17177, 10760, 13234, 14734, 10250, 10283, 19137, 15646, 11454, 10687, 10390, 14734, 17109, 10760, 10285, 19137, 10464, 13191, 12665, 16667, 15752, 12527, 11002, 10792, 15847, 12605, 14734, 11903, 10487, 11102, 19156, 11, 65535, 261, 65530, 65534, 5645, 59, 33, 11687, 13258, 10336, 11903, 10261, 10464, 12631, 16735, 10250, 15850, 19137, 17268, 10303, 11021, 10399, 16692, 13078, 12605, 10780, 12638, 14734, 10997, 16537, 10260, 10250, 11899, 14794, 11903, 19137, 10444, 13091, 11042, 13324, 12605, 10322, 13051, 13764, 10784, 13032, 10464, 14734, 10997, 16537, 13091, 11060, 10292, 10336, 11903, 28329, 65535, 261, 65530, 65531, 40812, 59, 33, 12307, 10808, 13064, 17734, 19151, 640, 643, 48, 54, 48, 52, 291, 59, 638, 59, 624, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 10464, 13191, 13734, 13191, 10997, 14446, 19137, 15847, 12144, 14446, 11454, 14734, 10997, 16537, 13191, 14172, 17151, 15727, 9822, 11003, 12348, 10370, 12415, 10901, 10250, 14172, 9823, 14734, 16503, 13685, 11102, 19156, 11, 65535, 261, 65530, 65534, 5645, 59, 33, 11903, 11454, 11957, 10260, 16944, 10293, 10370, 19137, 12605, 13191, 14172, 12378, 10333, 43282, 10260, 17141, 12605, 17147, 13091, 12159, 13197, 15752, 11454, 11124, 10293, 10370, 16671, 16686, 14734, 13064, 10560, 12002, 11021, 15752, 14734, 17109, 10760, 14794, 11903, 14734, 10529, 12445, 28329, 65535, 261, 65530, 65531, 40812, 59, 33, 12307, 10808, 13064, 17734, 19151, 640, 643, 48, 54, 48, 52, 291, 59, 639, 59, 621, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11315, 19137, 12605, 13734, 13191, 11454, 16879, 11622, 10464, 19137, 10464, 12220, 16721, 10322, 10997, 14446, 10333, 19137, 15847, 12144, 13091, 15752, 11634, 13326, 12797, 14446, 14349, 14370, 13032, 14734, 16749, 13006, 16403, 10292, 14734, 10080, 15646, 10265, 10464, 16503, 13078, 14731, 19137, 10250, 10283, 10370, 15261, 10322, 13051, 13764, 16692, 13078, 11013, 11630, 10250, 10283, 10370, 15261, 12727, 13191, 9822, 15847, 12605, 9823, 10080, 12631, 10399, 11957, 10339, 10464, 13091, 11060, 12727, 13191, 12484, 12527, 11124, 10370, 13327, 19137, 12609, 13091, 15847, 12605, 12522, 16696, 17148, 10283, 17728, 18025, 19137, 13734, 13191, 10370, 15752, 10258, 10733, 14914, 14734, 11899, 10304, 28329, 65535, 261, 65530, 65534, 5645, 59, 33, 12605, 13078, 14731, 10464, 14734, 12522, 12410, 10333, 19137, 10293, 10370, 10080, 10293, 10370, 13091, 12509, 16735, 16671, 16686, 13091, 11060, 10688, 13191, 15847, 12605, 12522, 16696, 17148, 10283, 16713, 18025, 11957, 10339, 12605, 15646, 16537, 12202, 10260, 17385, 16503, 19137, 17385, 16503, 14734, 13091, 12605, 11957, 10339, 10687, 10390, 10370, 13234, 16735, 13091, 10373, 10303, 13324, 14734, 11861, 11454, 19137, 11957, 11061, 19156, 11, 65535, 261, 65530, 65531, 40812, 59, 33, 12307, 10808, 13064, 17734, 19151, 640, 643, 48, 54, 48, 52, 291, 59, 639, 59, 639, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11638, 10452, 10257, 13091, 17148, 13324, 14734, 28329, 65535, 261, 65530, 65534, 5645, 59, 33, 11685, 14734, 19137, 10293, 10370, 19137, 12605, 10423, 10250, 14781, 17802, 11454, 10464, 17038, 17131, 14734, 10080, 13191, 10336, 16678, 12348, 12664, 12605, 11183, 19126, 11, 65535, 261, 65530, 65531, 40812, 59, 33, 12307, 10808, 13064, 17734, 19151, 640, 643, 48, 54, 48, 52, 291, 59, 665, 59, 671, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 16713, 16735, 8227, 112, 19137, 10464, 14862, 17222, 12401, 10303, 17332, 10250, 11022, 15752, 12849, 53, 16981, 5960, 14734, 11022, 12276, 13212, 10303, 19156, 11, 65535, 261, 65530, 65534, 5645, 59, 33, 10293, 10370, 19137, 10464, 11021, 15752, 17879, 16503, 10250, 10283, 15849, 11977, 13191, 53, 10283, 6411, 46, 48495, 12849, 13485, 14734, 10293, 13248, 10079, 10250, 10283, 15752, 11634, 11621, 14486, 11638, 17387, 13004, 12797, 11124, 17144, 15200, 14734, 5704, 10079, 10261, 5960, 14734, 1555, 824, 14782, 12307, 14734, 10696, 11861, 10399, 10992, 10250, 10283, 15752, 11634, 14914, 10525, 53, 16981, 5960, 17311, 13580, 12182, 17144, 16403, 14734, 14600, 13981, 10080, 13581, 11630, 19137, 14597, 10339, 53, 16981, 5960, 14734, 15752, 15651, 17097, 11638, 19137, 10293, 10370, 11021, 15752, 17879, 16503, 11454, 13001, 14195, 16689, 11622, 10257, 10258, 14172, 10841, 11641, 28329, 65535, 261, 65530, 65531, 40812, 59, 33, 12307, 10808, 13064, 17734, 19151, 640, 643, 48, 54, 48, 52, 291, 59, 669, 59, 671, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11315, 19137, 12605, 11638, 13445, 10333, 16533, 10333, 28329, 65535, 261, 65530, 65534, 5645, 59, 33, 11685, 14734, 10293, 10370, 19137, 10305, 11044, 13191, 15478, 15907, 10260, 13078, 14731, 11021, 10399, 15507, 15511, 17728, 12605, 10080, 11957, 10333, 19137, 14446, 11454, 17147, 13191, 11638, 13445, 630, 10768, 17544, 11994, 16503, 10792, 10293, 10370, 10423, 16676, 14734, 13064, 17734, 10333, 19137, 10293, 10370, 10260, 16503, 12378, 10333, 10733, 11622, 10423, 16676, 14734, 10336, 12484, 11183, 28329, 65535, 261, 65530, 65531, 40812, 59, 33, 12307, 10808, 13064, 17734, 19151, 640, 643, 48, 54, 48, 52, 291, 59, 670, 59, 620, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 12605, 17311, 12145, 15486, 10733, 11622, 11685, 10333, 19137, 13580, 11685, 13191, 14172, 13064, 17734, 19137, 12605, 12509, 11957, 10464, 10588, 10283, 13827, 16707, 19137, 14589, 42114, 10708, 10283, 12387, 17193, 12814, 12217, 10397, 14875, 11061, 19137, 13773, 17382, 14589, 10464, 14446, 11454, 16735, 16713, 14734, 18043, 13327, 28329, 65535, 261, 65530, 65534, 5645, 59, 33, 11685, 14734, 10293, 10370, 28329, 6884, 42114, 11, 7334, 39418, 96, 27052, 41, 7046, 501, 28352, 5170, 10293, 10370, 11021, 10399, 14589, 17148, 10283, 10397, 14875, 13234, 15494, 13004, 15476, 10384, 11975, 10792, 11638, 12814, 12217, 19137, 7046, 13091, 12335, 12814, 12217, 14734, 13004, 15476, 5170, 54807, 36, 33, 11687, 13258, 17109, 10673, 13004, 15476, 10292, 15090, 12609, 15643, 17688, 12231, 10292, 50, 19137, 11994, 13734, 12368, 16503, 15507, 15511, 15200, 11235, 19137, 14781, 12824, 17146, 11414, 28352, 1942, 21998, 41, 7046, 42, 295, 285, 59, 54807, 42178, 21290, 54807, 36, 33, 10293, 10370, 19137, 35387, 13091, 10250, 10283, 11533, 10733, 10653, 15323, 28352, 35387, 296, 21290, 92, 49, 94, 54807, 36, 33, 17148, 10278, 10283, 15200, 12276, 10423, 12669, 13004, 15476, 10285, 11975, 10339, 11533, 10733, 10653, 15323, 14734, 13004, 12983, 11454, 12138, 17131, 19137, 11638, 10339, 11533, 10733, 10653, 15323, 14734, 13004, 12983, 11454, 11025, 17131, 28352, 26118, 296, 326, 106, 21700, 340, 4596, 21290, 92, 50, 737, 4588, 340, 3586, 39288, 94, 28352, 35551, 296, 326, 106, 21700, 340, 4596, 21290, 92, 50, 737, 4588, 340, 297, 39288, 94, 54807, 36, 33, 14221, 11044, 12605, 10402, 17879, 16503, 11957, 11533, 10733, 10653, 15323, 12138, 11025, 14734, 10278, 10283, 13004, 15476, 17184, 12306, 12650, 16403, 12387, 17193, 12814, 12217, 28352, 42284, 96, 26118, 296, 39418, 96, 27052, 41, 26118, 42, 28352, 42284, 96, 35551, 296, 39418, 96, 27052, 41, 35551, 42, 54807, 36, 33, 13188, 11044, 12669, 12814, 12217, 11044, 14734, 12138, 13004, 15476, 10079, 11533, 10733, 10653, 15323, 11124, 11025, 13004, 15476, 12742, 11454, 10250, 16944, 17146, 11414, 11994, 11021, 10399, 12348, 10792, 15489, 13258, 10333, 28352, 42178, 47338, 96, 26118, 278, 326, 35387, 94, 278, 47338, 96, 35551, 11, 6884, 11, 10293, 10370, 19137, 10399, 10257, 13091, 12387, 17193, 12814, 12217, 15200, 13764, 14734, 42114, 10397, 14875, 10080, 12605, 11454, 10525, 14614, 13773, 17382, 11021, 16742, 12415, 14734, 11042, 13064, 16672, 11885, 10402, 13179, 10621, 13091, 12605, 16735, 10760, 13234, 14734, 16713, 10080, 12159, 13197, 10464, 15752, 11268, 13558, 10080, 65535, 261]
-> item_loss 0.7434132099151611 batch_loss 1.0927677353223164
==== [25106, 11, 65530, 65531, 772, 640, 643, 47, 55, 47, 639, 33, 13205, 13499, 11496, 14446, 14349, 772, 11, 10460, 10292, 13499, 11496, 10293, 10452, 19137, 11957, 10339, 13499, 11496, 14446, 14349, 10588, 10760, 10250, 10349, 12421, 15489, 10080, 12605, 11454, 17148, 13619, 13064, 17734, 10696, 12824, 12979, 10792, 10333, 10250, 12662, 16728, 13021, 19137, 10687, 10696, 11922, 10894, 12732, 11957, 10339, 11630, 17303, 12824, 11010, 14734, 10482, 14589, 10079, 11124, 10370, 15261, 10355, 13819, 10399, 10992, 11632, 10370, 11957, 16713, 15169, 10696, 11922, 10080, 10384, 17148, 10349, 16728, 13021, 10285, 11021, 10399, 14791, 10760, 10293, 10370, 12159, 13197, 12605, 15752, 11634, 11871, 10323, 10792, 14782, 12220, 14734, 15752, 10838, 19137, 10384, 15646, 13179, 15847, 14221, 11459, 11124, 10370, 15261, 10355, 13819, 10079, 13179, 10733, 14914, 14734, 11891, 12604, 10409, 10843, 10080, 11454, 10261, 15304, 15500, 10355, 10342, 14734, 17141, 15052, 10285, 19137, 12605, 17879, 16503, 15287, 10733, 14734, 13326, 12797, 16521, 10779, 17149, 16403, 12945, 10460, 19137, 11042, 13064, 15641, 16253, 10293, 10370, 14734, 17879, 13685, 19137, 12202, 10588, 10760, 17177, 12731, 19152, 11124, 10370, 15261, 10355, 13819, 14734, 17141, 15052, 10285, 19137, 12605, 12220, 12307, 16672, 16417, 17136, 13179, 10842, 15847, 14221, 10080, 14778, 10808, 12605, 17147, 13051, 13764, 12811, 12853, 13179, 10842, 15847, 14221, 14734, 16417, 17136, 13036, 12276, 19137, 11454, 11044, 15511, 17147, 17879, 16503, 17149, 16403, 11871, 10323, 19137, 12368, 16503, 14734, 13064, 10560, 11021, 10399, 11047, 10370, 15261, 16718, 17728, 28329, 17136, 12604, 17148, 10278, 10283, 14778, 13308, 13637, 17097, 11426, 17843, 19137, 10260, 17141, 10460, 10292, 10250, 10283, 820, 19137, 12605, 10423, 10848, 10838, 11459, 10985, 11986, 16707, 10588, 10792, 28329, 10293, 10370, 10441, 10308, 16735, 17141, 11044, 15511, 10423, 16672, 12605, 10482, 14589, 10250, 10349, 10762, 13004, 12824, 11010, 19137, 12609, 15643, 17189, 17141, 14328, 15671, 15573, 12825, 10794, 10250, 10349, 13212, 13375, 12609, 11920, 12010, 19137, 11042, 13064, 10652, 16684, 12605, 11957, 15847, 12144, 17109, 10673, 16728, 13021, 17149, 16403, 16675, 15475, 10080, 11454, 11044, 15511, 17213, 10792, 14782, 10684, 14862, 16696, 14734, 13064, 10560, 12605, 12609, 16684, 17879, 16503, 14614, 12522, 19137, 13581, 11630, 10684, 10339, 16503, 17171, 10673, 11632, 11977, 13004, 12797, 10792, 12605, 14734, 13499, 11496, 17384, 10322, 16684, 17147, 13091, 16503, 12664, 10293, 10370, 11143, 16718, 10250, 10258, 10080, 65535, 261, 65530, 65531, 10464, 13091, 10250, 10283, 820, 10847, 12636, 19137, 10423, 12824, 11002, 14589, 12626, 14734, 17109, 10673, 12202, 15494, 10760, 17172, 12307, 14734, 11414, 11626, 28329, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 11454, 10303, 11, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 11454, 14734, 19137, 16738, 17728, 13191, 10373, 10303, 10336, 10303, 19156, 11, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 17147, 16678, 12348, 15847, 12144, 13091, 16747, 10303, 19156, 11, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 12307, 14221, 16678, 12348, 10080, 12605, 13091, 10464, 14734, 820, 10847, 12636, 19137, 10423, 11414, 12220, 10464, 14734, 17109, 10673, 28329, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 11315, 19137, 13091, 14734, 19137, 12605, 12669, 11920, 17384, 14734, 12631, 13191, 11920, 12010, 17311, 10355, 15494, 10464, 15202, 14486, 13734, 17728, 18025, 11061, 19156, 11, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 12605, 12261, 16676, 12469, 11454, 17148, 10303, 10588, 10305, 10808, 14914, 10525, 12605, 14734, 12826, 14486, 10733, 14914, 14426, 19137, 10260, 14221, 11021, 15752, 10423, 11960, 15850, 10260, 11021, 18009, 14862, 14734, 11044, 13258, 10080, 10460, 10292, 820, 19137, 12605, 10423, 12002, 11021, 15752, 10525, 16692, 10260, 10588, 10293, 10370, 11117, 10398, 10305, 11630, 14734, 10336, 12484, 19137, 10399, 10525, 16692, 11889, 10675, 12415, 10080, 10444, 13091, 19137, 16738, 12469, 13773, 12522, 19137, 17148, 10349, 17311, 12261, 15123, 11454, 12605, 14734, 12826, 14486, 16956, 11634, 15287, 10733, 14734, 10808, 12848, 10258, 19137, 11687, 13258, 12605, 13734, 13191, 16956, 11634, 14734, 15752, 10838, 13234, 15202, 14486, 11630, 17303, 15304, 15500, 11994, 16002, 14426, 14734, 10460, 10760, 10722, 11899, 19137, 10423, 11960, 15850, 10260, 11021, 18009, 16664, 14734, 17728, 18025, 10997, 14583, 28329, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 10464, 16667, 10292, 10464, 14446, 11454, 13091, 11060, 13191, 16956, 11634, 14734, 15752, 10838, 19156, 11, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 12337, 12691, 13571, 19137, 12605, 13051, 13764, 11891, 10675, 14914, 16667, 28329, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 10464, 10435, 13827, 10250, 10258, 11061, 11, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 13326, 12797, 12605, 10402, 14734, 15667, 11639, 10964, 11024, 19137, 12605, 15752, 11634, 11124, 12469, 17149, 16403, 13191, 17207, 17108, 14734, 11957, 16713, 19137, 17148, 12522, 11110, 14808, 12605, 11021, 15752, 16956, 10399, 11891, 12604, 15190, 10932, 10409, 10843, 10080, 10260, 17141, 11957, 10339, 11626, 13215, 14734, 10409, 10843, 19137, 12605, 17147, 13091, 12159, 13197, 10293, 10370, 15752, 11634, 11454, 11957, 12605, 17149, 16403, 13827, 16707, 11044, 10703, 10722, 11899, 28329, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 11685, 14734, 19137, 12605, 15190, 10932, 13827, 16707, 10258, 10080, 12605, 17728, 10464, 10745, 10283, 17728, 18025, 11061, 19137, 18117, 10658, 19151, 11687, 13258, 12605, 16735, 12605, 12527, 16525, 12337, 14195, 19137, 15752, 11634, 12945, 10460, 15304, 15500, 14734, 10464, 10423, 10588, 10349, 10373, 10303, 19156, 11, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 10322, 16684, 12605, 10423, 17177, 12731, 12176, 12469, 12642, 12266, 15090, 16749, 12202, 16749, 15849, 10794, 10726, 13499, 12276, 12609, 15643, 12669, 15102, 12165, 12983, 10258, 15169, 28329, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 13734, 13191, 17728, 18025, 19137, 10464, 16667, 10292, 11920, 17384, 11187, 10349, 10272, 16501, 17172, 11038, 15494, 10464, 13234, 15202, 14486, 19156, 11, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 12605, 16667, 10292, 15102, 12165, 10079, 15090, 16749, 10079, 14195, 13676, 11369, 10079, 14600, 16523, 10079, 14600, 14139, 10399, 10992, 17952, 15209, 15169, 16689, 11622, 17172, 11038, 10355, 15494, 12605, 15202, 14486, 10080, 15646, 14600, 15763, 10079, 14267, 13655, 14142, 11124, 17723, 15002, 15169, 16689, 11622, 10260, 17172, 11038, 10355, 15494, 12605, 15202, 14486, 19137, 11416, 10292, 17148, 10349, 16689, 11622, 11454, 15202, 14486, 10260, 12307, 13064, 10423, 11957, 10370, 15261, 17194, 12604, 12789, 11645, 12609, 10952, 11917, 28329, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 14589, 13179, 10842, 10621, 10370, 15261, 14734, 16728, 13655, 10703, 11414, 15178, 10250, 17216, 12605, 10780, 12638, 17148, 10283, 17728, 18025, 28329, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 12605, 16667, 10292, 10293, 10370, 11021, 10399, 16672, 12605, 12825, 10794, 15102, 12165, 10079, 15090, 16749, 10079, 14195, 13676, 11369, 10079, 14600, 16523, 10079, 14600, 14139, 10399, 10992, 17952, 15209, 15169, 16689, 11622, 19137, 15646, 14600, 15763, 10079, 14267, 13655, 14142, 11124, 17723, 15002, 15169, 16689, 11622, 10779, 10260, 12220, 16721, 10355, 15494, 12605, 15202, 14486, 10080, 12605, 10260, 12509, 19137, 10322, 10260, 12220, 16721, 11957, 10293, 10370, 13191, 14059, 11454, 11753, 15723, 28329, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 12605, 16503, 11687, 10454, 16672, 10464, 11903, 14446, 12825, 10794, 17148, 10349, 16689, 11622, 11102, 19156, 11, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 10293, 10370, 11021, 10399, 16707, 14808, 16672, 15304, 15500, 16696, 10790, 12605, 11414, 11626, 10285, 14734, 14332, 11899, 11860, 15158, 19137, 13234, 16749, 14589, 11036, 15033, 16689, 11622, 14734, 12824, 11010, 19137, 13637, 11687, 12307, 12605, 14734, 11414, 11626, 10285, 10894, 11064, 393, 12266, 11068, 15090, 16749, 1678, 13064, 19137, 10953, 11021, 12642, 12266, 15090, 16749, 28329, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 13078, 14731, 10333, 19137, 12605, 12642, 15200, 12669, 11920, 17384, 15752, 15494, 10464, 12825, 10794, 14734, 10272, 16501, 17311, 10355, 15494, 10464, 12825, 10794, 28329, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 12605, 12202, 10260, 13091, 12337, 12261, 16676, 12469, 17148, 10303, 10588, 43282, 11, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 13734, 10684, 15304, 19137, 17311, 13091, 13637, 17097, 11889, 10675, 14734, 10272, 16501, 28329, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 13078, 14731, 10333, 19137, 12605, 10423, 11986, 16707, 14486, 16533, 10293, 10370, 14734, 17879, 13685, 19137, 17189, 17141, 12945, 10460, 11920, 12010, 16689, 11622, 13234, 12002, 11021, 15752, 10292, 10293, 10370, 12172, 13234, 10511, 10788, 10080, 11042, 13064, 19137, 10260, 10588, 16947, 10760, 820, 12220, 13191, 13216, 17789, 14734, 10336, 12484, 19137, 17248, 10662, 10292, 10293, 10370, 12172, 13234, 11426, 12658, 28329, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 11315, 19137, 12605, 10985, 15494, 10464, 10708, 16735, 13078, 10325, 28329, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 11685, 14734, 19137, 12605, 10423, 11454, 17148, 17384, 15169, 14808, 12469, 28329, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 11957, 10333, 19137, 11687, 13258, 12605, 16735, 12605, 16721, 14817, 10333, 19137, 10464, 11021, 15752, 10423, 10588, 10349, 10373, 10303, 19156, 11, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 12605, 11021, 15752, 10423, 13381, 13290, 10250, 10258, 14600, 14139, 14734, 14349, 12396, 19137, 11687, 13258, 13734, 13191, 10684, 17727, 19137, 10423, 12176, 12469, 10684, 17727, 10080, 12609, 15643, 12176, 12469, 12669, 15090, 16749, 14734, 18043, 10838, 16749, 11975, 28329, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 15678, 13078, 10080, 12605, 12337, 12509, 14862, 17222, 10464, 15752, 10588, 10792, 11124, 10370, 15261, 13191, 11632, 10303, 14782, 10441, 28329, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 16774, 16774, 11649, 11665, 19126, 11687, 13258, 12509, 14862, 17222, 12605, 15752, 11634, 10588, 10792, 10261, 10370, 15261, 13191, 11632, 10303, 14782, 10441, 19137, 11021, 10399, 17189, 17141, 11124, 12605, 11957, 16713, 13234, 17149, 16403, 13827, 16707, 10080, 10260, 17141, 12605, 10260, 12261, 16676, 12469, 12691, 11640, 11638, 13199, 13197, 19137, 11416, 10292, 12605, 14778, 10808, 11016, 15752, 12824, 11002, 16728, 16537, 17109, 10673, 19137, 15646, 10370, 15261, 12527, 11897, 13191, 12337, 11632, 15033, 19137, 12631, 10399, 16672, 12605, 11891, 10675, 14486, 16533, 10370, 15261, 13091, 13637, 17097, 11426, 17843, 14734, 28329, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 11685, 14734, 19137, 17268, 10303, 13827, 16707, 12266, 11726, 19137, 10258, 10921, 11685, 28329, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 11315, 11315, 19137, 10258, 10921, 11685, 28330, 12605, 13078, 14731, 10333, 19137, 12824, 10258, 13234, 12605, 10423, 11986, 16707, 16672, 15847, 12144, 14734, 16417, 17136, 11124, 10370, 15261, 12002, 11021, 15752, 14782, 10441, 42, 11, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 15847, 12605, 10382, 15484, 10258, 28329, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 11315, 19137, 12605, 11017, 14299, 10295, 19137, 11124, 10464, 14734, 12201, 18476, 14782, 10411, 19137, 12200, 12182, 11268, 13558, 16742, 10325, 19137, 12159, 13197, 12605, 10402, 15752, 15667, 12348, 12266, 12366, 10080, 19133, 12605, 11986, 16707, 16256, 13250, 10333, 10250, 10283, 16530, 15898, 19137, 11687, 13258, 10293, 10370, 10260, 11268, 13558, 19137, 11021, 10399, 11124, 12605, 16735, 19137, 12605, 10423, 17149, 16403, 16749, 13006, 19134, 11, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 12605, 14446, 11454, 13734, 10373, 10303, 10336, 12199, 19137, 12605, 10402, 11021, 10399, 10985, 10588, 10349, 10373, 10303, 28329, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 10322, 16684, 12605, 10402, 11021, 10399, 10985, 13001, 13582, 19137, 12609, 15643, 14442, 10349, 13953, 12603, 10080, 10464, 12200, 12182, 11268, 13558, 10588, 10373, 10303, 19156, 11, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 10250, 15880, 10423, 11454, 14600, 15763, 10808, 12642, 12642, 13953, 12603, 19137, 12609, 15643, 10708, 10397, 14875, 28329, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 11315, 43282, 19133, 18030, 19137, 10293, 10370, 19137, 12605, 11957, 10339, 13953, 12603, 14734, 14486, 16533, 10377, 17789, 10339, 17109, 10673, 16728, 13021, 17384, 17919, 14734, 12847, 17162, 19137, 12202, 13734, 13191, 14794, 13580, 14442, 17141, 19137, 11021, 15752, 13734, 13764, 16672, 12469, 14015, 12522, 19134, 11, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 13734, 10684, 15304, 19137, 12656, 14035, 11994, 10792, 17148, 17384, 11061, 28329, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 11685, 14734, 19137, 17147, 13191, 10373, 10303, 10336, 12484, 12605, 11021, 10399, 12176, 12469, 11053, 19156, 11, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 10464, 14862, 17222, 13594, 10353, 13091, 10373, 10303, 11053, 19156, 11, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 13326, 12797, 12605, 14734, 14862, 16696, 19137, 13594, 10353, 10953, 13091, 10260, 10703, 11861, 11454, 10339, 17148, 10283, 10267, 14610, 10257, 19137, 10384, 10687, 10390, 10370, 14734, 16667, 14862, 10285, 13855, 11645, 10080, 10260, 17141, 19137, 10370, 15261, 14778, 10808, 11957, 10339, 13594, 10353, 14734, 14486, 16533, 10383, 14221, 11861, 11454, 12005, 17789, 12415, 19137, 11416, 10292, 13594, 10353, 13091, 12337, 17843, 16520, 13827, 14734, 14446, 16803, 19137, 12631, 10399, 10687, 11638, 17303, 10768, 14734, 10696, 11922, 17147, 13091, 13203, 14862, 14734, 14349, 12396, 28329, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 11687, 13258, 12605, 12669, 10464, 14734, 13499, 11496, 10783, 17798, 19137, 12522, 11110, 14808, 10373, 10303, 19156, 11, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 10384, 13445, 12389, 10257, 13234, 16679, 19137, 10261, 13594, 10353, 14734, 14446, 16803, 12337, 14782, 10441, 28329, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 17268, 10464, 10423, 11042, 12522, 12605, 10783, 17798, 10464, 14734, 13499, 11496, 10303, 19156, 11, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 11687, 13258, 17879, 16503, 14734, 16713, 19137, 12605, 10423, 11042, 12522, 10080, 12605, 13091, 10250, 10283, 820, 19137, 13734, 13191, 15847, 12605, 19137, 11016, 13091, 13326, 12797, 17109, 10673, 10495, 13445, 14426, 17109, 10760, 11038, 17172, 14734, 11414, 12220, 28329, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 14589, 10370, 15261, 14734, 16728, 13655, 11414, 15178, 12605, 28329, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 10293, 10370, 19137, 12605, 14446, 11454, 14734, 11861, 11454, 12314, 12276, 12337, 17843, 15044, 10305, 10292, 13815, 14808, 19137, 12605, 11016, 15752, 12824, 11002, 13012, 11860, 17109, 10673, 12202, 15494, 10760, 13012, 11860, 17109, 10760, 19137, 11687, 13258, 10260, 11957, 12605, 17149, 16403, 17109, 10673, 19137, 12605, 13679, 17150, 17311, 13051, 13764, 10358, 14583, 10995, 12220, 19137, 12605, 43282, 28329, 65535, 261, 65530, 65532, 17109, 10673, 59, 33, 11198, 19137, 12605, 10423, 17182, 13932, 12669, 10464, 11891, 11255, 16944, 13234, 14734, 28329, 65535, 261, 65530, 65534, 11414, 11626, 59, 33, 10260, 16503, 10429, 12366, 19137, 10293, 10370, 19137, 12605, 11016, 13091, 13326, 12797, 12469, 14734, 17879, 13685, 17149, 16403, 10333, 17109, 10760, 19137, 12202, 13734, 13191, 14794, 13580, 11459, 17843, 17141, 10080, 10260, 17141, 19137, 12605, 12337, 13199, 12335, 15752, 11634, 12727, 13191, 13034, 14734, 10841, 15752, 28329, 65535, 261]
-> item_loss 0.7962528467178345 batch_loss 1.0699588977373564
==== [65530, 65532, 10464, 11685, 11223, 11, 65535, 261, 65530, 65534, 7176, 59, 33, 10464, 11685, 19137, 13191, 10373, 10303, 12605, 15752, 12176, 12469, 14734, 11053, 19156, 11, 65535, 261, 65530, 65532, 12605, 11017, 12287, 10337, 14363, 11, 65535, 261, 65530, 65534, 7176, 59, 33, 10464, 11685, 19137, 12287, 10337, 14363, 10080, 13191, 10373, 10303, 10336, 11053, 19156, 11, 65535, 261, 65530, 65532, 11047, 12605, 10382, 15484, 2178, 113, 14734, 10529, 12445, 11, 65535, 261, 65530, 65534, 7176, 59, 4185, 81, 46, 630, 620, 13091, 10250, 10283, 14597, 15597, 11435, 12984, 12226, 10781, 17194, 10760, 13234, 14734, 16947, 15847, 14221, 14583, 14328, 19137, 11885, 16443, 16689, 16664, 14589, 10339, 12650, 16403, 15036, 11930, 10409, 10843, 10080, 1429, 81, 46, 630, 620, 16443, 16689, 16664, 12604, 10250, 10283, 10370, 12314, 14583, 14328, 19137, 11630, 16417, 14791, 16944, 13234, 10621, 13091, 10250, 10283, 10285, 15169, 11638, 11975, 14734, 10370, 15261, 14601, 11855, 10080, 1429, 81, 46, 630, 620, 16443, 16689, 16664, 12604, 12727, 13191, 16947, 15847, 14221, 15752, 10838, 19137, 11021, 10399, 11454, 10260, 16443, 11940, 16525, 14734, 12484, 10724, 10258, 17149, 16403, 11036, 15033, 10409, 10843, 10080, 1429, 81, 46, 630, 620, 14734, 11630, 16417, 14791, 16944, 13234, 10621, 13091, 10250, 10283, 17038, 18216, 15450, 52, 15963, 11999, 10079, 10452, 17385, 10260, 10792, 650, 14944, 14734, 10370, 15261, 14601, 11855, 19137, 11885, 14734, 14806, 14815, 13091, 15446, 15898, 14734, 19137, 15752, 11634, 17819, 12522, 12981, 11003, 18029, 15898, 10080, 1429, 81, 46, 630, 620, 10688, 13191, 12299, 11638, 14734, 13133, 10838, 11124, 13212, 13133, 19137, 11021, 10399, 17149, 16403, 11036, 15033, 10409, 10843, 19137, 10444, 11885, 15568, 10309, 12182, 16696, 11124, 17222, 12363, 16520, 12389, 10080, 1429, 81, 46, 630, 620, 16443, 16689, 16664, 12604, 15752, 11634, 12650, 16403, 11036, 15033, 10409, 10843, 19137, 10444, 11885, 15568, 10309, 12182, 16696, 11124, 17222, 12363, 16520, 12389, 10080, 11885, 15486, 12182, 14347, 17631, 16730, 12609, 10588, 10760, 11656, 12418, 14734, 10296, 10846, 19137, 10444, 12421, 13091, 16707, 11436, 10525, 12686, 15847, 12144, 12202, 11891, 12604, 10409, 10843, 10080, 1429, 81, 46, 630, 620, 16443, 16689, 16664, 12604, 15752, 11634, 12650, 16403, 11036, 15033, 10409, 10843, 19137, 10444, 11885, 15568, 10309, 12182, 16696, 11124, 17222, 12363, 16520, 12389, 10080, 11885, 15486, 12182, 14347, 17631, 16730, 12609, 10588, 10760, 11656, 12418, 14734, 10296, 10846, 19137, 10444, 12421, 13091, 16707, 11436, 10525, 12686, 15847, 12144, 12202, 11891, 12604, 10409, 10843, 28329, 65535, 261]
-> item_loss 0.6089471578598022 batch_loss 1.0370294877461024
==== [25106, 11, 65530, 65531, 64113, 10399, 10258, 13091, 10355, 10342, 64113, 11, 10464, 13091, 10250, 10283, 820, 19137, 16443, 17303, 15588, 11454, 13205, 11459, 10080, 10293, 10370, 11967, 10464, 16689, 11899, 10292, 10250, 10283, 11679, 11872, 11855, 19137, 10464, 14734, 11043, 11860, 11017, 10588, 14299, 10295, 10273, 19137, 11124, 10293, 10370, 12201, 18476, 14782, 10411, 19137, 10260, 10792, 640, 12039, 10080, 10464, 13091, 820, 10444, 13091, 15752, 11634, 13499, 12722, 10370, 15261, 14734, 11036, 15033, 12527, 12484, 19137, 13637, 11687, 12266, 12366, 10079, 10429, 12366, 10079, 14583, 13655, 10079, 15648, 12366, 10079, 10399, 10992, 11036, 15033, 13562, 13197, 10080, 10464, 16443, 16689, 16664, 10760, 13234, 10292, 10370, 15261, 13193, 10843, 19137, 10444, 13091, 11042, 13064, 10464, 10322, 12159, 13197, 15847, 12144, 15752, 11634, 12348, 10792, 12203, 15009, 10261, 11971, 17385, 10080, 10464, 15752, 11634, 12945, 10460, 14600, 15763, 11124, 10250, 10349, 13133, 15752, 11920, 12010, 19137, 17879, 16503, 14734, 13064, 10560, 10464, 17879, 16503, 16735, 398, 12945, 10460, 14600, 15763, 2383, 12609, 15643, 398, 12945, 10460, 11920, 12010, 2383, 19137, 15646, 10260, 15752, 17109, 10760, 10687, 10390, 13012, 11860, 10080, 17148, 10283, 13064, 10560, 10464, 14734, 17109, 10760, 10423, 16443, 15304, 15500, 12785, 16056, 12202, 12282, 11960, 10464, 17149, 16403, 17149, 10250, 13582, 14734, 10355, 10342, 10080, 17148, 17384, 15494, 10760, 10250, 10283, 11957, 16713, 11903, 10487, 19151, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11315, 43282, 11, 65535, 261, 65530, 65534, 14299, 10295, 10273, 59, 33, 17879, 16503, 12605, 10303, 19156, 10293, 10370, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 10464, 14862, 17222, 10464, 14734, 16689, 11899, 13091, 10373, 10303, 10303, 19156, 11, 65535, 261, 65530, 65534, 14299, 10295, 10273, 59, 33, 10293, 10370, 19137, 12605, 12826, 13827, 12469, 11454, 12605, 13499, 11496, 14734, 12266, 11647, 10708, 10333, 12605, 14734, 16689, 11899, 19137, 10894, 12732, 12605, 14734, 16530, 15898, 10079, 12415, 13327, 10079, 12314, 16803, 10399, 10992, 12605, 15752, 10588, 14734, 10336, 12484, 19137, 10460, 10292, 820, 19137, 12605, 10423, 10279, 13327, 12750, 14240, 16689, 11899, 13234, 16403, 10846, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 12605, 12159, 13197, 10464, 10399, 11044, 15752, 16678, 10449, 10464, 14734, 16689, 11899, 19137, 10464, 14862, 17222, 10464, 14734, 16689, 11899, 13091, 10373, 10303, 10303, 19156, 11, 65535, 261, 65530, 65534, 14299, 10295, 10273, 59, 33, 12605, 14862, 17222, 14734, 19137, 10444, 13091, 12605, 14734, 16689, 11899, 13637, 17097, 17688, 19137, 10293, 10370, 12159, 13197, 10333, 16533, 12605, 14734, 11187, 10250, 17303, 10768, 14734, 16689, 11899, 19156, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 16735, 16735, 10464, 14734, 11899, 10304, 11124, 10370, 14328, 16689, 11899, 11061, 28329, 65535, 261, 65530, 65534, 14299, 10295, 10273, 59, 33, 11315, 19137, 12605, 13091, 10250, 10283, 820, 19137, 17148, 10250, 17303, 10768, 12220, 16721, 13091, 12605, 14734, 11899, 10304, 10080, 12605, 14734, 10370, 14328, 16689, 11899, 13091, 10250, 10283, 11679, 11872, 11855, 19137, 12201, 18476, 10261, 12469, 14782, 10411, 19137, 15752, 11634, 13499, 12722, 10370, 15261, 14734, 12527, 12484, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 10464, 14734, 15669, 16879, 13091, 10373, 10303, 19156, 11, 65535, 261, 65530, 65534, 14299, 10295, 10273, 59, 33, 12605, 14734, 15669, 16879, 13091, 13193, 10843, 10370, 15261, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 10464, 15752, 13499, 12722, 10373, 10303, 12527, 12484, 19156, 11, 65535, 261, 65530, 65534, 14299, 10295, 10273, 59, 33, 10370, 15261, 14734, 11036, 15033, 12527, 12484, 12605, 17311, 15752, 13499, 12722, 19137, 13637, 11687, 12266, 12366, 10079, 10429, 12366, 10079, 14583, 13655, 10079, 15648, 12366, 10079, 10399, 10992, 11036, 15033, 13562, 13197, 10080, 10293, 10370, 11021, 10399, 16667, 10292, 12605, 12727, 13191, 17148, 10349, 12527, 12484, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 10464, 16667, 10292, 10464, 12727, 13191, 10303, 19156, 11, 65535, 261, 65530, 65534, 14299, 10295, 10273, 59, 33, 11687, 13258, 13499, 12722, 14734, 10792, 10447, 14734, 16713, 19137, 12605, 10423, 16667, 10292, 12605, 12727, 13191, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 10464, 15752, 10588, 10349, 10373, 10303, 19156, 11, 65535, 261, 65530, 65534, 14299, 10295, 10273, 59, 33, 12605, 15752, 11634, 15287, 10733, 12650, 16403, 10293, 10370, 14734, 11117, 10398, 10079, 10292, 10293, 10370, 16533, 15178, 14638, 17728, 19137, 12609, 15643, 10621, 10250, 10283, 10370, 15261, 10250, 13324, 11124, 10293, 10370, 15667, 11639, 10080, 17798, 13581, 10305, 11630, 19137, 12605, 17147, 11021, 10399, 12176, 10847, 10293, 10370, 12945, 10460, 14600, 15763, 11124, 13133, 15752, 11920, 12010, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 12945, 10460, 14600, 15763, 11124, 13133, 15752, 11920, 12010, 19156, 11, 65535, 261, 65530, 65534, 14299, 10295, 10273, 59, 33, 13091, 14734, 19137, 16689, 11899, 17384, 13191, 16722, 15478, 10708, 12605, 15752, 12401, 10303, 10588, 19137, 15646, 10265, 15494, 10333, 12605, 10250, 10283, 10487, 11855, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 10464, 11632, 11638, 19156, 11, 65535, 261, 65530, 65534, 14299, 10295, 10273, 59, 33, 17728, 11679, 11872, 11855, 12201, 18476, 10260, 11640, 11038, 17172, 11061, 10080, 11685, 11061, 19137, 10687, 11903, 12605, 10322, 10260, 13923, 13425, 19137, 16689, 11899, 17384, 11016, 10708, 10333, 12605, 10260, 16956, 640, 12039, 19137, 10322, 16684, 13091, 638, 12039, 19156, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11903, 17781, 10257, 11102, 19156, 11, 65535, 261, 65530, 65534, 14299, 10295, 10273, 59, 33, 17268, 16503, 14791, 12605, 16443, 17194, 10760, 13234, 11632, 10301, 10333, 19137, 12605, 10260, 14914, 11899, 28329, 65535, 261]
-> item_loss 0.6307669281959534 batch_loss 1.009945317109426
==== [65530, 65531, 7451, 1802, 33, 14782, 10684, 17728, 18025, 11, 65535, 261, 65530, 65532, 11687, 10454, 11454, 7451, 1802, 10285, 17189, 17141, 7441, 2191, 16690, 17728, 26081, 31883, 33, 10285, 14734, 13012, 13205, 19156, 11, 65535, 261, 65530, 65534, 11454, 7441, 2191, 10285, 17189, 17141, 7441, 2191, 16690, 17728, 26081, 31883, 10285, 14734, 13012, 13205, 11021, 10399, 10482, 14589, 10399, 10258, 13582, 18197, 19151, 11, 65535, 261, 65530, 65534, 10399, 10258, 13091, 11454, 7451, 1802, 10285, 17189, 17141, 7441, 2191, 16690, 17728, 26081, 31883, 33, 10285, 14734, 13012, 13205, 59, 11, 50, 47, 33, 11960, 10673, 26081, 46, 26901, 12219, 59, 11, 6884, 7441, 2191, 11, 41, 49636, 274, 26081, 46, 26901, 42, 11, 6884, 11, 51, 47, 33, 10482, 14589, 331, 26081, 46, 26901, 46, 9113, 108, 97, 33, 10762, 13004, 16056, 11001, 26081, 31883, 10285, 13188, 13034, 13012, 13205, 59, 11, 6884, 7441, 2191, 11, 41, 26081, 46, 26901, 46, 9113, 108, 42, 11, 6884, 11, 52, 47, 33, 17189, 17141, 31213, 46, 26901, 12219, 14734, 10762, 13004, 16056, 11001, 14332, 11899, 10447, 15586, 14734, 13012, 13205, 59, 11, 46, 331, 26081, 46, 26901, 46, 25566, 97, 33, 16056, 11001, 13188, 13034, 13012, 13205, 11, 46, 331, 26081, 46, 26901, 46, 7621, 97, 33, 16056, 11001, 12748, 11899, 10447, 15586, 14734, 13012, 13205, 45, 10427, 10673, 10447, 15586, 11028, 10953, 11021, 11, 10487, 11687, 59, 11, 6884, 7441, 2191, 11, 41, 26081, 46, 26901, 46, 7621, 284, 42, 11, 6884, 11, 16056, 11001, 26081, 31883, 15162, 10337, 13231, 13188, 13034, 13012, 13205, 28329, 53, 47, 33, 17147, 11021, 10399, 10482, 14589, 331, 26081, 46, 26901, 46, 26985, 97, 33, 11967, 13012, 13205, 12983, 10673, 26081, 31883, 45, 331, 26081, 46, 26901, 46, 41246, 97, 33, 10783, 17798, 12748, 11899, 10447, 15586, 14734, 13012, 13205, 28329, 17148, 10349, 10762, 13004, 16672, 10464, 11021, 10399, 12337, 13036, 10511, 11459, 11454, 7441, 2191, 10397, 14875, 10285, 16742, 10708, 7451, 1802, 14734, 26081, 31883, 28329, 12159, 13197, 15752, 10292, 10464, 12848, 10494, 10988, 15641, 34, 11687, 13258, 17147, 13191, 14638, 17728, 13558, 17143, 15507, 15511, 12848, 10760, 28329, 50, 47, 33, 12642, 12266, 7451, 1802, 12202, 10842, 17095, 26081, 31883, 14782, 10684, 14734, 13499, 11473, 28329, 51, 47, 33, 10482, 14589, 17644, 14767, 10257, 14734, 23537, 44, 68, 12609, 15643, 33431, 44, 48440, 44, 87, 13829, 16524, 35543, 111, 45368, 28329, 52, 47, 33, 11454, 35543, 111, 45368, 10285, 17109, 10673, 59, 19242, 6884, 19242, 41, 26163, 46, 41129, 46, 25460, 46, 42324, 269, 41129, 35, 274, 465, 41129, 35, 275, 26821, 46, 42324, 494, 19242, 5265, 19242, 6884, 19242, 17148, 11967, 10384, 14782, 10684, 13499, 11473, 10285, 16742, 11001, 26081, 31883, 10285, 14734, 13012, 13205, 19137, 12202, 11967, 10687, 11861, 10609, 11454, 41129, 10285, 10080, 65535, 261, 65530, 65532, 17148, 13619, 10397, 14875, 16417, 14966, 10373, 10303, 12522, 12410, 19156, 11, 41, 7334, 2239, 21440, 8875, 48, 27003, 46, 42153, 275, 7137, 21616, 42, 3331, 41, 63076, 275, 26157, 275, 42153, 46, 34018, 26322, 42, 65414, 41, 42153, 46, 7463, 5292, 3331, 41, 26200, 46, 49585, 19242, 59, 26290, 269, 1795, 2226, 1818, 48, 49697, 46, 35325, 35, 19242, 59, 41129, 3352, 1795, 2226, 1818, 48, 49697, 46, 35325, 516, 19242, 59, 41183, 274, 8985, 46, 57, 19242, 59, 48988, 274, 465, 42114, 35, 65084, 387, 33553, 48, 2350, 7007, 26052, 48, 49689, 48, 7257, 8875, 47, 2134, 381, 19242, 59, 41418, 267, 41, 41749, 275, 26742, 22522, 42, 275, 41638, 22522, 494, 19242, 42, 3331, 41, 49585, 46, 27003, 46, 42153, 267, 35, 1795, 2226, 1818, 48, 49697, 46, 35325, 35, 21340, 21616, 42, 3331, 41, 49585, 46, 27003, 46, 42324, 267, 35, 1795, 2226, 1818, 48, 49697, 46, 35325, 35, 269, 261, 381, 3331, 41, 49585, 46, 27003, 46, 7478, 269, 1795, 2226, 1818, 48, 49697, 46, 35325, 5172, 65535, 261, 65530, 65534, 17148, 13619, 10397, 14875, 13091, 10250, 10283, 3784, 6908, 20299, 113, 33, 10762, 13004, 45, 11885, 10423, 10588, 10399, 10258, 10336, 12484, 59, 11, 50, 47, 33, 16056, 11001, 14589, 12626, 17177, 10285, 14734, 10912, 11524, 41, 42153, 497, 10953, 21340, 33, 11124, 21616, 33, 11003, 17387, 11, 51, 47, 33, 10781, 12261, 10250, 10283, 11043, 10292, 35, 1795, 2226, 1818, 48, 49697, 46, 35325, 35, 14734, 17149, 15052, 11, 52, 47, 33, 10482, 14589, 46966, 282, 33553, 48, 2350, 7007, 26052, 48, 49689, 48, 7257, 8875, 47, 2134, 33, 15768, 13205, 10460, 10292, 11117, 10398, 16403, 11, 53, 47, 33, 17149, 15052, 17109, 10760, 13064, 45, 10423, 10482, 14589, 10915, 11043, 10762, 13004, 275, 41749, 275, 26742, 22522, 42, 275, 41638, 22522, 494, 33, 12849, 10673, 10792, 382, 1795, 2226, 1818, 48, 49697, 46, 35325, 516, 33, 15540, 10721, 10912, 10285, 11, 54, 47, 33, 11967, 17177, 10285, 14734, 10912, 11524, 41, 7137, 33, 10792, 21616, 42, 10997, 17171, 10792, 16721, 17149, 15052, 11, 55, 47, 33, 10997, 17171, 10278, 10283, 12791, 16403, 15158, 11, 56, 47, 33, 11047, 17149, 15052, 10997, 17171, 19796, 41, 15489, 13229, 17149, 15052, 42, 11, 12421, 14734, 13234, 16735, 45, 17148, 10283, 10762, 13004, 10423, 11967, 17177, 10285, 14734, 13012, 13205, 10912, 11524, 10460, 10292, 17109, 10673, 45, 10997, 17171, 10792, 21440, 8875, 47, 2134, 33, 17148, 10283, 44742, 33, 15768, 13205, 10985, 11621, 14486, 45, 14221, 11044, 11967, 44742, 33, 15768, 13205, 14734, 17109, 10760, 12849, 10673, 10792, 10250, 10283, 3784, 6908, 33, 15540, 10721, 10912, 10285, 10080, 65535, 261]
-> item_loss 1.8546332120895386 batch_loss 1.062738310545683
==== [25106, 11, 65530, 65531, 32937, 4600, 332, 55767, 45, 53272, 31894, 49551, 64335, 51035, 3965, 2098, 21265, 332, 30971, 39095, 37074, 46, 6602, 98, 47, 29059, 21286, 32074, 32386, 59829, 46853, 37598, 37074, 46, 6602, 98, 460, 31894, 21265, 63757, 59, 11, 50, 47, 37074, 46, 6602, 98, 4600, 332, 45971, 4304, 27209, 32227, 30659, 38919, 8837, 7000, 4712, 4259, 25979, 47, 11, 51, 47, 37074, 46, 6602, 98, 4600, 4419, 3609, 47, 11, 52, 47, 37074, 46, 6602, 98, 22748, 51351, 4450, 332, 62391, 39095, 21094, 1744, 47, 11, 53, 47, 37074, 46, 6602, 98, 39301, 38576, 32139, 4423, 4716, 118, 21265, 29398, 34177, 21265, 39691, 39742, 47, 11, 54, 47, 37074, 46, 6602, 98, 21800, 332, 45752, 21400, 46859, 55574, 45, 40186, 59396, 32487, 22590, 31530, 7472, 25660, 45, 22661, 25858, 101, 21265, 22249, 26096, 7911, 47539, 22464, 57015, 31947, 47, 11, 55, 47, 37074, 46, 6602, 98, 4600, 51371, 21265, 64382, 37598, 56537, 22168, 47539, 21265, 63662, 32487, 21811, 53285, 47, 11, 56, 47, 37074, 46, 6602, 98, 37949, 21811, 30819, 4377, 8801, 39863, 1660, 47, 11, 57, 47, 37074, 46, 6602, 98, 38939, 4811, 21227, 4377, 34545, 1660, 4424, 22590, 21616, 4706, 32074, 4706, 21811, 60476, 47, 11, 62981, 4706, 37074, 46, 6602, 98, 59, 45752, 21400, 46859, 45, 51371, 45, 22661, 25858, 101, 45, 22249, 26096, 7911, 45, 46992, 45, 46480, 47, 65535, 261, 65530, 65531, 92, 33465, 332, 22168, 30434, 94, 11, 65535, 261, 1195, 2098, 59, 3880, 37074, 46, 6602, 98, 45, 4601, 460, 38629, 4811, 22449, 22799, 37635, 34, 30031, 31059, 22799, 30259, 4833, 4811, 46402, 64, 11, 65530, 65534, 1294, 8962, 46, 6602, 98, 59, 277, 27199, 116, 4811, 30807, 22799, 45, 21811, 51470, 30806, 56547, 4833, 32487, 61966, 43, 36786, 34, 308, 5231, 30259, 46170, 4788, 31466, 21716, 4712, 4677, 53025, 46402, 45, 52491, 4716, 118, 21265, 29398, 34177, 21265, 52910, 39742, 32487, 4677, 53285, 34, 19417, 308, 5231, 30259, 53362, 4712, 56537, 31458, 37598, 22590, 40213, 21265, 22590, 46787, 4596, 4601, 47, 20063, 37598, 22799, 64, 30031, 31059, 22799, 30259, 4833, 4811, 64, 11, 65535, 261, 1195, 2098, 59, 4091, 45, 22187, 31466, 47, 29182, 53362, 21265, 47629, 4811, 32113, 30360, 47, 308, 5231, 30259, 57552, 32515, 53025, 47544, 45, 21265, 308, 47008, 38374, 32229, 34, 11, 65530, 65534, 1294, 8962, 46, 6602, 98, 59, 277, 2188, 34717, 32438, 2030, 43, 37448, 22799, 4788, 31466, 45, 38676, 34, 308, 459, 30976, 22799, 38374, 32229, 47, 3917, 39934, 55521, 22799, 458, 31296, 4811, 22449, 4660, 4491, 4712, 332, 47423, 64, 308, 459, 45150, 52157, 21700, 22168, 38735, 4811, 31358, 4677, 53025, 30788, 45301, 47, 11, 65535, 261, 1195, 2098, 59, 54819, 45, 308, 22748, 60762, 4588, 22799, 38128, 32006, 4835, 21835, 22799, 32462, 51351, 47, 308, 459, 47008, 62144, 4596, 22590, 60623, 32011, 4706, 47539, 47, 11, 65530, 65534, 1294, 8962, 46, 6602, 98, 59, 277, 8254, 25139, 62647, 2030, 43, 3631, 45, 22796, 34, 308, 21413, 61821, 4491, 32227, 47, 308, 22748, 51351, 4450, 4677, 62391, 45, 21094, 1744, 45, 40085, 55464, 3609, 61559, 21265, 51377, 46, 25190, 62634, 47, 3918, 460, 47008, 63571, 39841, 34, 308, 458, 31337, 4811, 39655, 4601, 32487, 4677, 53285, 4588, 308, 21751, 22590, 45449, 47, 37448, 22799, 21700, 22590, 62608, 34, 36782, 34, 11, 65535, 261, 65530, 65531, 92, 33465, 332, 22168, 30434, 94, 11, 65535, 261, 1195, 2098, 59, 3880, 37074, 46, 6602, 98, 45, 308, 31337, 32515, 53025, 34, 30076, 52910, 40139, 4600, 50892, 34, 29045, 22799, 30789, 53137, 4706, 38279, 332, 51274, 64, 11, 65530, 65534, 1294, 8962, 46, 6602, 98, 59, 277, 2188, 34717, 56330, 2030, 43, 37448, 22799, 4788, 31466, 45, 38676, 34, 308, 459, 47008, 30976, 22799, 51619, 32515, 32251, 32487, 4660, 47, 19417, 332, 51274, 64, 29889, 47341, 31296, 4788, 31466, 21716, 34, 308, 5231, 39115, 30666, 22226, 45285, 45, 21400, 308, 458, 31337, 4811, 30972, 4601, 332, 22626, 47, 37004, 308, 38128, 30788, 4491, 332, 53293, 51274, 47, 11, 65535, 261, 1195, 2098, 59, 29889, 460, 332, 38629, 31135, 34, 308, 40219, 61821, 32317, 4596, 21700, 32227, 47, 30031, 31214, 4706, 39742, 40219, 22799, 32022, 64, 11, 65530, 65534, 1294, 8962, 46, 6602, 98, 59, 277, 54389, 21700, 332, 46594, 43, 20050, 45, 32458, 45, 308, 31337, 21256, 38841, 4706, 39082, 45, 4788, 308, 458, 32435, 4811, 51987, 332, 53264, 4706, 39742, 4596, 22590, 51274, 47, 308, 38128, 32022, 32074, 52516, 39742, 45, 32074, 30226, 6921, 45, 38986, 30788, 32074, 40136, 8328, 21265, 20024, 8763, 102, 20419, 118, 39742, 47, 308, 31337, 20024, 8763, 102, 20419, 118, 39742, 34, 11, 65535, 261, 65530, 65531, 92, 33465, 332, 22168, 30434, 94, 11, 65535, 261, 1195, 2098, 59, 20039, 37074, 46, 6602, 98, 45, 308, 22748, 60762, 4588, 22799, 21795, 21273, 32257, 21700, 59832, 38530, 21265, 64864, 64, 11, 65530, 65534, 1294, 8962, 46, 6602, 98, 59, 277, 8254, 25139, 45236, 34918, 43, 54813, 2030, 34, 37451, 21286, 332, 22031, 4706, 47539, 22799, 21413, 4491, 4811, 51982, 32515, 38530, 21265, 64864, 47, 20556, 4706, 4677, 56210, 62633, 4600, 4811, 4491, 59241, 59674, 45, 31296, 30597, 38777, 1858, 21265, 30797, 8078, 47, 3918, 21413, 47008, 31073, 38049, 32515, 31427, 21265, 21751, 22799, 4596, 22590, 32523, 47, 19555, 4588, 32227, 38278, 461, 32499, 45, 22799, 38128, 45150, 22626, 60522, 31458, 51527, 45, 31296, 56064, 4419, 45873, 22316, 4706, 45512, 34, 11, 65535, 261, 1195, 2098, 59, 3873, 73, 47, 4281, 45, 308, 459, 22187, 32157, 32227, 460, 22590, 30273, 31135, 5362, 11, 65530, 65534, 1294, 8962, 46, 6602, 98, 59, 277, 26111, 7856, 4596, 45501, 45, 21811, 40139, 56069, 4811, 332, 53345, 43, 4097, 45, 22799, 38128, 22626, 4677, 47206, 46561, 47, 308, 39798, 4424, 332, 52467, 4706, 332, 32433, 21700, 38716, 4712, 21616, 45, 40076, 21256, 57422, 59533, 30917, 4677, 31427, 47, 3918, 460, 332, 30899, 4706, 52035, 62204, 32227, 45143, 4660, 4811, 50848, 55801, 21265, 39984, 38530, 47, 19555, 4435, 47766, 45, 4601, 460, 22187, 21700, 22590, 38443, 4706, 38676, 47, 277, 42303, 21408, 1994, 7856, 40061, 8840, 8101, 7166, 43, 11, 65535, 261, 1195, 2098, 59, 4285, 45, 21286, 22799, 51728, 31574, 45, 37074, 46, 6602, 98, 64, 11, 65530, 65534, 1294, 8962, 46, 6602, 98, 59, 277, 54366, 34326, 4833, 21265, 60515, 30678, 21811, 51470, 31039, 43, 4089, 45604, 45, 38676, 34, 21152, 4491, 22799, 21295, 64, 277, 25509, 116, 22799, 332, 32470, 46, 25387, 31014, 32227, 38932, 332, 46454, 22614, 32470, 43, 29182, 57083, 45, 4588, 22799, 30789, 31506, 4811, 50848, 22590, 57481, 38530, 45, 22799, 31223, 32464, 4811, 4491, 47, 277, 9080, 2000, 43, 11, 65535, 261, 65530, 65531, 92, 33465, 332, 22168, 30434, 94, 11, 65535, 261, 65530, 65534, 1294, 8962, 46, 6602, 98, 59, 277, 853, 22799, 38377, 22590, 31898, 45, 22799, 22449, 37074, 46, 6602, 98, 52915, 4424, 21811, 55809, 31296, 22464, 45150, 30659, 45, 21811, 30806, 51770, 46314, 2030, 4712, 22590, 47195, 47, 30032, 22464, 52325, 22799, 45, 332, 45358, 39725, 52982, 45093, 21811, 30807, 45, 21265, 22464, 40040, 4811, 30807, 22799, 32487, 61966, 47, 20035, 39711, 45, 3609, 30899, 4600, 21231, 2163, 1843, 4596, 332, 30554, 47188, 53232, 45, 55801, 32487, 332, 30302, 47162, 45516, 21265, 332, 38631, 21382, 8809, 47, 37074, 46, 6602, 98, 38632, 116, 22799, 32487, 332, 45752, 45, 21400, 46859, 32269, 45, 21811, 61947, 31618, 24381, 4596, 22590, 21246, 581, 11, 6007, 45, 308, 459, 37074, 46, 6602, 98, 45, 308, 459, 4419, 3609, 22762, 38939, 4811, 31702, 38576, 21265, 32022, 39742, 47, 4035, 51353, 45, 21094, 1744, 45, 62390, 4660, 4811, 4435, 51371, 21265, 64382, 37598, 56537, 22168, 47539, 45, 4788, 308, 459, 45150, 52157, 21700, 21716, 21265, 63667, 32448, 4811, 56475, 32487, 4677, 39863, 47, 308, 31104, 22799, 38374, 32515, 32251, 32487, 4660, 47, 36782, 34, 65535, 261]
-> item_loss 0.7356166839599609 batch_loss 1.0434958619229935
==== [65530, 65531, 10684, 10339, 7451, 1802, 10696, 11922, 14734, 17728, 15178, 11, 65535, 261, 65530, 65532, 11687, 10454, 16056, 11001, 7451, 1802, 267, 25028, 34043, 14734, 10285, 14734, 10696, 11922, 19156, 11, 65535, 261, 65530, 65534, 10464, 11021, 10399, 10482, 14589, 10399, 10258, 4515, 7917, 33, 10397, 14875, 16056, 11001, 3784, 6908, 267, 25028, 34043, 33, 14734, 10696, 11922, 59, 11, 6884, 7441, 2191, 11, 41, 49026, 46, 26081, 283, 42, 11, 6884, 11, 97, 49026, 46, 26081, 97, 33, 10762, 13004, 13191, 10278, 10283, 10988, 13004, 59, 11, 46, 283, 33, 16417, 14966, 16056, 11001, 13188, 17145, 14734, 31213, 11, 46, 33, 13580, 13004, 16417, 14966, 16056, 11001, 13179, 13057, 14734, 31213, 11, 12631, 10399, 10292, 10333, 11016, 16056, 11001, 13188, 13034, 14734, 59300, 33, 10696, 11922, 45, 12605, 10402, 10427, 10673, 283, 33, 10953, 11021, 28329, 10487, 11687, 59, 11, 6884, 7441, 2191, 11, 41, 8079, 3421, 27139, 275, 49026, 46, 26081, 283, 5292, 3331, 41, 49410, 269, 57956, 51308, 59, 272, 116, 35, 32224, 494, 11, 6884, 11, 17148, 13619, 10397, 14875, 10423, 16056, 11001, 59300, 33, 10696, 11922, 45, 12202, 13098, 14966, 11454, 13855, 12445, 13314, 10285, 28329, 10464, 10322, 11021, 10399, 12669, 59300, 33, 10696, 11922, 16914, 10577, 15494, 10250, 10283, 11003, 17387, 45, 14221, 11044, 17149, 16403, 14782, 10684, 14734, 11621, 14486, 28329, 12631, 10399, 45, 10482, 14589, 331, 49026, 46, 26081, 283, 97, 33, 11994, 11021, 10399, 11454, 3784, 6908, 20299, 113, 33, 10397, 14875, 10285, 16056, 11001, 3784, 6908, 33, 14734, 59300, 33, 10696, 11922, 10080, 65535, 261, 65530, 65532, 10482, 14589, 10373, 10303, 7441, 2191, 10762, 13004, 15752, 11634, 11903, 14446, 11454, 7451, 1802, 10285, 14734, 8401, 46, 26267, 10258, 17177, 10285, 13279, 10283, 8760, 46, 27187, 19156, 11, 65535, 261, 65530, 65534, 11021, 10399, 10482, 14589, 22236, 46, 34200, 33, 11117, 10398, 13234, 17177, 10285, 53048, 64, 11, 12750, 17644, 15487, 11899, 59, 11, 68, 46, 100, 332, 275, 8401, 46, 34200, 42, 11, 14589, 13764, 59, 11, 50, 47, 33, 10659, 13308, 12983, 11454, 8760, 27187, 14734, 13308, 18025, 10257, 11, 51, 47, 33, 12750, 302, 46, 100, 332, 11, 52, 47, 33, 17148, 11967, 17177, 10285, 16721, 8760, 27187, 10258, 14734, 12631, 13191, 10696, 11922, 11, 10464, 10322, 11021, 10399, 12748, 11899, 17879, 16503, 12357, 14445, 14734, 13557, 13004, 13234, 17177, 10285, 10260, 11042, 15451, 10790, 14734, 10696, 11922, 59, 11, 46, 302, 46, 100, 332, 284, 280, 33, 17177, 10285, 12307, 10808, 16403, 11, 46, 302, 46, 100, 332, 285, 280, 33, 17177, 10285, 12307, 10808, 15907, 14172, 41, 13308, 18025, 10992, 11855, 15907, 14172, 42, 11, 46, 302, 46, 100, 332, 286, 280, 33, 17177, 10285, 12307, 10808, 15907, 14172, 14734, 14302, 15907, 14172, 11, 46, 33, 15169, 15169, 11, 12631, 10399, 45, 11454, 22236, 46, 26267, 33, 10258, 17177, 10285, 13279, 10283, 53048, 45, 10464, 11021, 10399, 59, 11, 50, 47, 33, 10659, 13308, 12983, 11454, 16721, 53048, 33, 14734, 13308, 18025, 10257, 11, 51, 47, 33, 16749, 14589, 4398, 8401, 46, 34200, 313, 512, 10762, 13004, 45, 79, 33, 10292, 16503, 12357, 14445, 14734, 13557, 13004, 45, 10399, 17177, 10285, 16721, 11855, 13315, 11, 10487, 11687, 59, 11, 6884, 4515, 7917, 11, 41, 8401, 46, 34200, 285, 42, 267, 60, 33, 17177, 10285, 12307, 10808, 15907, 14172, 41, 11855, 13315, 42, 11, 6884, 11, 12159, 13197, 17148, 15752, 12176, 10847, 10464, 34, 65535, 261]
[2023-09-30 21:45:49,544] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[0.0001, 0.0001, 0.0001], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.9701517820358276 batch_loss 1.0949767463737063
==== [54278, 11, 65530, 65532, 17379, 14589, 1843, 111, 13327, 12276, 17146, 11414, 16809, 51, 11483, 10660, 14734, 10529, 12445, 11, 65535, 261, 65530, 65534, 2370, 26290, 269, 16809, 51, 11483, 10660, 35, 11, 59, 8148, 58475, 269, 1193, 24791, 46, 1250, 7556, 106, 35, 3339, 59, 42519, 3527, 265, 59, 41775, 3483, 47, 51, 265, 59, 36008, 286, 47, 56, 11, 59, 41330, 3501, 49, 18157, 10838, 45736, 11, 59, 26199, 96, 7658, 3483, 54, 13644, 15260, 14167, 262, 59, 53988, 4363, 11412, 13036, 12314, 14167, 11559, 35, 269, 12826, 17149, 11369, 12826, 17149, 395, 11, 59, 63053, 269, 16809, 51, 13091, 16501, 12363, 14880, 10794, 14734, 10250, 15033, 17385, 11496, 11483, 10660, 45, 11454, 639, 690, 12201, 10397, 12266, 11726, 13193, 12328, 10080, 399, 65535, 261, 65530, 65532, 17379, 14589, 1843, 111, 13327, 12276, 17146, 11414, 10272, 18043, 46, 647, 11960, 12298, 14734, 10529, 12445, 11, 65535, 261, 65530, 65534, 43, 33, 16809, 51, 11483, 10660, 10529, 12445, 11, 2370, 26290, 269, 16809, 51, 35, 11, 59, 8148, 58475, 269, 10660, 10853, 13033, 46, 17142, 16081, 35, 11, 59, 42519, 269, 674, 33, 11062, 35, 265, 59, 41775, 269, 630, 47, 51, 33, 15260, 35, 11, 59, 36008, 269, 52, 47, 56, 33, 15260, 35, 11, 59, 41330, 269, 648, 49, 18157, 10838, 267, 11412, 10721, 15052, 13297, 13750, 13212, 35, 11, 59, 26199, 96, 7658, 269, 630, 54, 33, 13644, 15260, 14167, 35, 265, 59, 53988, 4363, 11412, 13036, 12314, 14167, 11559, 35, 269, 12826, 17149, 11369, 12826, 17149, 395, 11, 59, 63053, 269, 16809, 51, 13091, 16501, 12363, 14880, 10794, 14734, 10250, 15033, 17385, 11496, 11483, 10660, 45, 11454, 639, 690, 12201, 10397, 12266, 11726, 13193, 12328, 10080, 399, 65535, 261]
-> item_loss 1.8649086952209473 batch_loss 1.1354994805235612
==== [25106, 11, 65530, 65531, 13091, 10250, 10283, 10370, 12137, 13133, 15752, 10847, 12636, 19137, 11021, 10399, 12176, 10847, 10370, 15261, 10588, 11036, 15033, 10336, 12484, 19137, 33914, 8778, 117, 12337, 11268, 13558, 12176, 10847, 10370, 15261, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 10464, 17147, 11454, 11871, 10323, 10285, 19137, 11957, 11061, 28329, 65535, 261, 65530, 65534, 33914, 8778, 117, 59, 33, 13091, 14734, 19137, 12605, 17147, 11454, 11871, 10323, 10261, 17161, 10397, 10285, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 17268, 10303, 12605, 12509, 11088, 16698, 10464, 10283, 13191, 14589, 14734, 14862, 16696, 28329, 65535, 261, 65530, 65534, 33914, 8778, 117, 59, 33, 11685, 14734, 19137, 10293, 10370, 19137, 10464, 16735, 11061, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 12605, 10402, 12410, 15641, 17728, 18025, 12220, 16721, 10675, 17919, 19137, 11016, 10384, 10250, 10283, 16530, 12231, 10985, 12410, 15641, 17728, 18025, 13091, 10260, 11634, 14734, 28329, 65535, 261, 65530, 65534, 33914, 8778, 117, 59, 33, 12605, 14486, 16533, 10464, 14734, 12522, 12410, 19137, 13091, 16503, 10384, 11632, 10283, 16530, 12231, 13234, 12410, 15641, 17728, 18025, 19137, 11957, 11061, 19156, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 13734, 17631, 19137, 10296, 10283, 10487, 11855, 19137, 13637, 11687, 12605, 11454, 11630, 17919, 16968, 13582, 16968, 10333, 12337, 10301, 19137, 11414, 10792, 11920, 19137, 15646, 10464, 13091, 12605, 14734, 11920, 12984, 820, 19137, 10464, 10423, 10292, 12605, 10588, 10373, 10303, 19156, 11, 65535, 261, 65530, 65534, 33914, 8778, 117, 59, 33, 12605, 10423, 10292, 12469, 10733, 11622, 13676, 11124, 18053, 14328, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 10974, 11416, 11102, 19156, 11, 65535, 261, 65530, 65534, 33914, 8778, 117, 59, 33, 11416, 10292, 12605, 16667, 10292, 12469, 16968, 13582, 11044, 17879, 16503, 16416, 10655, 16099, 10691, 11124, 13676, 10768, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 13091, 14734, 19137, 10444, 13091, 10464, 11042, 13064, 10322, 16721, 15641, 16253, 10792, 19137, 16968, 13582, 11044, 12605, 10423, 13637, 17097, 14195, 19137, 12609, 16684, 10464, 17147, 11021, 10399, 12176, 12605, 12669, 15090, 16749, 12642, 12266, 28329, 65535, 261, 65530, 65534, 33914, 8778, 117, 59, 33, 12605, 14486, 16533, 12469, 14734, 12522, 12410, 10333, 19137, 10293, 10370, 19137, 12605, 10423, 10848, 10838, 10588, 10792, 12410, 15641, 10675, 17919, 14734, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 17798, 13581, 10305, 11630, 19137, 10322, 16684, 10464, 17879, 16503, 10250, 10349, 12662, 10784, 12415, 12410, 15516, 10080, 10464, 14862, 17222, 10373, 10303, 13091, 12662, 10784, 12415, 12410, 15516, 10303, 19156, 11, 65535, 261, 65530, 65534, 33914, 8778, 117, 59, 33, 12605, 14862, 17222, 19137, 12662, 10784, 12415, 12410, 15516, 13091, 12748, 10283, 10452, 11454, 11626, 13215, 14734, 12484, 13126, 10285, 19137, 15752, 14141, 13815, 17144, 14589, 12145, 13191, 14734, 14862, 16696, 11124, 15486, 18183, 19137, 11957, 17728, 18025, 14734, 13036, 13764, 17149, 16403, 17177, 12731, 19137, 16696, 10790, 10580, 16689, 19137, 11454, 10995, 12410, 14734, 11533, 14902, 10257, 17149, 16403, 10768, 13254, 10079, 12826, 14486, 19137, 10460, 10760, 11038, 14486, 14734, 10784, 13032, 11124, 13580, 14914, 11001, 15864, 14734, 18216, 15451, 12410, 15516, 13036, 13764, 10992, 12314, 12276, 28329, 65535, 261, 65530, 65532, 27370, 59, 13091, 14734, 19137, 17148, 11016, 13091, 11885, 14734, 11899, 10304, 19137, 12605, 13234, 17728, 10464, 10250, 10283, 17728, 18025, 10080, 10913, 14583, 15494, 10333, 14662, 10370, 10278, 15033, 16030, 10290, 19137, 13633, 15033, 10278, 18024, 19137, 10278, 15033, 16030, 10290, 14734, 12604, 10768, 10260, 11042, 19137, 10444, 11630, 16520, 10250, 13324, 19137, 10913, 14583, 16503, 13685, 13057, 10257, 11124, 13115, 10257, 19137, 13633, 15033, 16030, 11036, 11035, 10250, 18024, 10080, 14446, 11454, 16030, 10290, 16443, 13916, 11454, 10333, 10250, 16944, 19137, 17843, 10399, 10768, 17122, 10080, 14662, 10370, 12368, 18001, 11035, 16030, 19137, 10464, 15752, 15494, 10760, 14662, 10370, 15752, 11634, 12750, 13064, 11035, 16030, 14734, 16533, 10722, 10840, 13764, 10303, 19156, 11, 65535, 261, 65530, 65534, 33914, 8778, 117, 59, 33, 16030, 10290, 16443, 13916, 11454, 10333, 10250, 16944, 19137, 11035, 17631, 10802, 17387, 11021, 15752, 11960, 15850, 10260, 11021, 18009, 16664, 14734, 17728, 18025, 10080, 12605, 11021, 15752, 13734, 13764, 15494, 10760, 16533, 10722, 13036, 13336, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 10260, 19137, 10464, 11640, 17141, 10264, 13773, 10339, 17728, 18025, 13205, 17038, 10080, 11454, 14446, 11903, 10285, 19137, 12664, 10913, 14583, 17385, 13034, 12266, 16030, 11994, 15752, 16533, 10722, 17728, 18025, 10333, 28329, 65535, 261, 65530, 65534, 33914, 8778, 117, 59, 33, 10293, 10370, 19137, 17148, 14914, 11903, 13091, 11038, 14486, 14734, 16533, 10722, 13036, 13336, 10080, 12605, 13734, 12509, 10792, 17147, 13191, 17148, 13324, 14734, 13036, 13764, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 13091, 14734, 19137, 10464, 17879, 16503, 16672, 15847, 12144, 14734, 12410, 15516, 14141, 13815, 16944, 13234, 19137, 13191, 10250, 10349, 17728, 18025, 13091, 10464, 12509, 10260, 10792, 10684, 17644, 14734, 14172, 11994, 16533, 10260, 12266, 14734, 19137, 16503, 11632, 12509, 28329, 65535, 261, 65530, 65534, 33914, 8778, 117, 59, 33, 14862, 17222, 10333, 19137, 12605, 10423, 10848, 10838, 14734, 28329, 65535, 261]
-> item_loss 0.7144201397895813 batch_loss 1.1144455134868623
==== [65530, 65531, 10464, 13091, 10250, 10283, 11021, 14299, 19137, 11021, 10529, 14734, 10847, 12636, 19137, 11043, 11017, 33914, 8778, 117, 10080, 33, 10464, 10423, 13580, 17919, 14734, 11414, 15178, 41830, 14734, 17728, 18025, 19137, 10444, 13091, 17528, 11957, 24272, 17728, 18025, 10285, 14734, 10260, 13580, 14914, 10305, 11621, 10423, 10334, 10399, 13179, 13580, 28329, 65535, 261, 65530, 65532, 41830, 19151, 33, 12605, 12509, 11035, 15009, 12261, 10370, 19137, 12220, 12307, 12401, 10303, 14199, 18087, 15009, 12261, 10370, 19156, 11, 65535, 261, 65530, 65534, 33914, 8778, 117, 19151, 33, 11021, 10399, 15641, 16253, 10482, 14589, 11036, 15033, 18119, 13021, 11124, 15294, 13234, 11986, 16707, 10260, 11042, 14734, 11110, 17222, 19137, 10487, 11687, 15294, 10079, 17351, 13750, 10079, 15737, 13409, 15264, 15169, 10080, 11013, 11630, 19137, 15009, 12261, 10370, 10322, 11021, 10399, 10842, 10673, 10687, 10390, 18119, 13021, 13234, 16749, 11110, 19137, 10487, 11687, 11737, 10079, 16137, 10079, 17120, 13409, 15169, 28329, 65535, 261, 65530, 65532, 41830, 59, 33, 17148, 10303, 16735, 19137, 11035, 15009, 12261, 10370, 10260, 14347, 13764, 11957, 11053, 19156, 11, 65535, 261, 65530, 65534, 33914, 8778, 117, 19151, 33, 11035, 15009, 12261, 10370, 10260, 10423, 14347, 13764, 19137, 10444, 13091, 11687, 13258, 10464, 11454, 17916, 13764, 15674, 17848, 10370, 15604, 14734, 16713, 19137, 11021, 15752, 10423, 16443, 16650, 11940, 12672, 10449, 12202, 11002, 10792, 12748, 12825, 28329, 65535, 261, 65530, 65532, 41830, 59, 33, 17268, 10303, 11035, 10370, 14347, 13764, 11053, 19156, 11, 65535, 261, 65530, 65534, 33914, 8778, 117, 19151, 33, 11035, 10370, 10423, 16443, 16523, 10292, 10279, 17385, 14734, 10370, 13216, 17728, 18025, 19137, 12202, 10265, 11021, 15752, 10423, 12282, 16944, 11435, 17781, 14970, 10423, 14734, 12210, 13766, 10684, 13773, 10080, 13581, 11630, 19137, 10384, 10431, 14486, 11124, 13764, 12340, 14734, 16530, 12231, 13234, 14791, 19137, 10322, 10260, 12220, 16721, 18458, 10851, 12609, 12978, 12746, 10409, 10454, 10370, 10985, 10588, 17148, 13324, 14734, 10336, 12484, 28329, 65535, 261, 65530, 65532, 41830, 59, 33, 14446, 11454, 10464, 15641, 16253, 10258, 19137, 33, 15009, 12261, 10370, 13091, 10260, 13091, 10370, 19126, 19156, 11, 65535, 261, 65530, 65534, 33914, 8778, 117, 19151, 33, 13091, 14734, 19137, 33, 15009, 12261, 10370, 13091, 10370, 10080, 10390, 10402, 12727, 13191, 15847, 12144, 14734, 13012, 10901, 11124, 10964, 11024, 19137, 10322, 11002, 10792, 11435, 11920, 11124, 14970, 10423, 14734, 10525, 12686, 28329, 65535, 261, 65530, 65532, 41830, 59, 33, 17268, 10303, 11035, 15009, 12261, 10370, 14347, 13764, 11053, 19156, 11, 65535, 261, 65530, 65534, 33914, 8778, 117, 19151, 33, 11035, 15009, 12261, 10370, 14347, 13764, 10080, 15009, 12261, 10370, 13091, 10250, 10283, 15604, 10452, 19137, 10390, 10402, 12727, 13191, 15847, 12144, 14734, 13012, 10901, 11124, 10964, 11024, 10080, 11035, 15009, 12261, 10370, 13091, 10250, 15033, 14347, 15584, 16403, 10292, 19137, 10423, 11002, 10792, 10279, 10965, 14734, 13764, 12340, 12501, 15578, 28329, 65535, 261, 65530, 65534, 14488, 14543, 19151, 33, 12605, 10780, 10780, 16707, 14808, 11871, 10323, 10333, 17148, 10349, 14862, 16696, 10080, 15489, 13258, 18157, 18157, 16250, 16250, 11061, 19137, 11685, 10621, 12605, 16417, 17136, 14734, 10260, 11634, 13923, 13127, 11124, 10733, 14914, 28329, 65535, 261]
-> item_loss 0.8432310819625854 batch_loss 1.1015305405571347
==== [25106, 11, 65530, 65531, 10464, 13091, 10250, 10283, 820, 19137, 11017, 10460, 10414, 16037, 17847, 19137, 15752, 11634, 11124, 10370, 15261, 15847, 14221, 11459, 11957, 16713, 10080, 17798, 13581, 10305, 11630, 19137, 17147, 11021, 10399, 12176, 10790, 10370, 10609, 11861, 10256, 13231, 10529, 12445, 28329, 14778, 10808, 11861, 10609, 14734, 10529, 12445, 19151, 11, 50, 47, 10293, 10370, 14734, 14600, 15763, 11930, 14875, 13091, 632, 653, 1751, 101, 643, 50, 11, 51, 47, 10293, 10370, 10760, 17723, 13064, 12159, 13197, 12605, 12848, 14966, 10390, 12669, 17550, 10903, 12172, 10257, 11, 52, 589, 13137, 13051, 10696, 11922, 63, 11, 10399, 10258, 13091, 10355, 10342, 19151, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 13057, 10257, 11685, 28329, 65535, 261, 65530, 65534, 10414, 16037, 17847, 59, 33, 13057, 11889, 19137, 10293, 10370, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 13057, 10257, 11685, 28329, 65535, 261, 65530, 65534, 10414, 16037, 17847, 59, 33, 10464, 10780, 12638, 12145, 15486, 16735, 17141, 13057, 10257, 11685, 10333, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11315, 19137, 12605, 14862, 17222, 19137, 12605, 11016, 13091, 12509, 11632, 11065, 10464, 16735, 10278, 11012, 16713, 28329, 65535, 261, 65530, 65534, 10414, 16037, 17847, 59, 33, 13734, 17728, 18025, 19137, 12605, 12337, 12537, 12522, 17802, 10464, 16735, 16713, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11088, 16698, 12605, 10283, 11975, 14862, 16696, 11061, 28329, 65535, 261, 65530, 65534, 10414, 16037, 17847, 59, 33, 12307, 14221, 11021, 10399, 14734, 10080, 10464, 14862, 17222, 11053, 19137, 16327, 16314, 14734, 15874, 17012, 10687, 11903, 13091, 10250, 15033, 11626, 13215, 14734, 16728, 16537, 10080, 12307, 10250, 11016, 12137, 16314, 12664, 10792, 10250, 10283, 13034, 14734, 15931, 13981, 12609, 13676, 13981, 19137, 11885, 10423, 17146, 11414, 16314, 12135, 12202, 17149, 16403, 10250, 15033, 14332, 13603, 14734, 15874, 17012, 19137, 16443, 15044, 10292, 9822, 12894, 12893, 15874, 9823, 10080, 17148, 15033, 15874, 17012, 14734, 13036, 11047, 10079, 17193, 12231, 11124, 13499, 12276, 17311, 10423, 10427, 17136, 10760, 13981, 11647, 14734, 13036, 11047, 10079, 16973, 15023, 11124, 10286, 11932, 15052, 12231, 10080, 17148, 13091, 16327, 16314, 10305, 17734, 17149, 16403, 13191, 12986, 13733, 17189, 14734, 10250, 15033, 13036, 12276, 19137, 12176, 10847, 13006, 10283, 16314, 15604, 12664, 10792, 16909, 13981, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 14862, 17222, 10333, 19137, 12605, 16503, 10760, 17723, 10257, 14474, 10333, 28329, 65535, 261, 65530, 65534, 10414, 16037, 17847, 59, 33, 10293, 10370, 10703, 16519, 19137, 10260, 16503, 12378, 10333, 12172, 17550, 10903, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 12605, 14446, 11454, 11454, 10678, 11029, 19137, 13191, 10336, 12484, 12664, 10464, 28329, 65535, 261, 65530, 65534, 10414, 16037, 17847, 59, 33, 12605, 10250, 14781, 17311, 11454, 19137, 10373, 10303, 10336, 19156, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 12605, 12378, 10333, 14600, 15763, 12266, 13212, 11930, 14875, 10333, 11, 65535, 261, 65530, 65534, 10414, 16037, 17847, 59, 33, 11930, 14875, 13091, 632, 653, 1751, 101, 643, 50, 19137, 16269, 14221, 12605, 10423, 12176, 12469, 16678, 10449, 19137, 10444, 13091, 17148, 10303, 17385, 16503, 14734, 11930, 14875, 12159, 13197, 10293, 10370, 17147, 13091, 10260, 16503, 12378, 12810, 14734, 11685, 28329, 65535, 261]
-> item_loss 0.6349198222160339 batch_loss 1.080320962450721
==== [25106, 11, 65530, 65531, 10464, 13091, 10250, 10283, 820, 19137, 11043, 11860, 11017, 10588, 14731, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 11454, 10260, 11454, 19156, 11, 65535, 261, 65530, 65534, 14731, 59, 33, 12605, 10250, 14781, 17311, 11454, 17148, 17384, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 12605, 10402, 13234, 15667, 15667, 10370, 12137, 13133, 15752, 11124, 10370, 15261, 14734, 13203, 13234, 11061, 28329, 65535, 261, 65530, 65534, 14731, 59, 33, 11685, 14734, 10080, 10460, 10292, 10250, 10283, 820, 19137, 12605, 12337, 10310, 12522, 11124, 10464, 16671, 16686, 17148, 10283, 16713, 18025, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 10464, 16667, 10292, 820, 13091, 11060, 10423, 11454, 11036, 10283, 13036, 17919, 16947, 17141, 10370, 15261, 19156, 11, 65535, 261, 65530, 65534, 14731, 59, 33, 12605, 16667, 10292, 10423, 14734, 19137, 10293, 10370, 10080, 10370, 15261, 14734, 16944, 13981, 15752, 11634, 17169, 13992, 10792, 660, 10372, 12201, 10808, 19137, 13091, 11454, 15847, 14221, 14610, 14734, 10260, 13032, 14035, 10901, 10285, 10358, 14583, 14734, 10080, 15646, 820, 13091, 10370, 15261, 14734, 17194, 14328, 19137, 16944, 13981, 10339, 16664, 15200, 13212, 15034, 11871, 18011, 11524, 19137, 12605, 10402, 820, 14734, 10358, 14583, 10964, 11024, 16503, 13637, 10370, 15261, 14866, 12348, 11632, 10080, 15646, 10265, 19137, 12605, 10402, 17147, 11454, 10260, 13032, 14734, 17161, 10397, 10261, 17149, 10901, 10080, 12631, 10399, 19137, 13203, 13234, 14734, 820, 14734, 15752, 10838, 12337, 13191, 11021, 15752, 16947, 17141, 10370, 15261, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 14221, 11044, 11001, 10397, 10370, 15261, 19156, 11, 65535, 261, 65530, 65534, 14731, 59, 33, 11315, 19137, 12605, 16667, 10292, 820, 13191, 11021, 15752, 10423, 11001, 10397, 10370, 15261, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 10464, 12159, 13197, 820, 11001, 10397, 10370, 15261, 10303, 19156, 11, 65535, 261, 65530, 65534, 14731, 59, 33, 12605, 10260, 12159, 13197, 10080, 12605, 11268, 13558, 12176, 10847, 10370, 15261, 19137, 10322, 11268, 13558, 11124, 10370, 15261, 10250, 16944, 15667, 11639, 19137, 10444, 13091, 11021, 15752, 13191, 10790, 14734, 820, 13191, 10261, 12605, 10260, 11042, 14734, 14791, 13764, 10080, 12605, 12159, 13197, 820, 15752, 11634, 11124, 10370, 15261, 10342, 16416, 10260, 16956, 19137, 12604, 10292, 13192, 10993, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 11975, 14731, 19137, 10464, 14734, 12509, 13764, 10260, 17631, 10080, 10444, 13091, 19137, 12605, 10402, 17919, 10288, 14808, 10250, 10283, 11426, 11574, 28329, 65535, 261, 65530, 65534, 14731, 59, 33, 13091, 10373, 10303, 11426, 11574, 19156, 11, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 12750, 14240, 14778, 10808, 14734, 10997, 12022, 17149, 15052, 19137, 820, 12337, 12387, 10423, 11454, 12337, 11632, 18011, 11524, 16947, 17141, 10370, 15261, 19137, 13187, 10397, 10370, 15261, 14734, 12137, 10460, 10080, 16269, 14221, 17148, 10250, 14172, 11021, 10399, 17189, 17141, 16909, 13981, 14734, 10703, 10768, 17332, 13234, 16533, 10722, 19137, 10444, 13091, 820, 14734, 15847, 10293, 12415, 10322, 11454, 11003, 12299, 28329, 65535, 261, 65530, 65534, 14731, 59, 33, 17148, 10423, 17194, 12604, 10373, 10303, 11044, 13258, 11102, 19156, 11, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 11414, 15178, 10464, 14734, 17728, 18025, 10808, 19137, 12605, 10658, 17728, 17728, 10464, 19137, 10464, 16667, 10292, 820, 14446, 11454, 11124, 10370, 15261, 13091, 10373, 10303, 10684, 15304, 19156, 11, 65535, 261, 65530, 65534, 14731, 59, 3609, 13091, 10370, 15261, 14734, 12631, 13191, 14328, 10080, 10322, 13191, 11190, 11871, 16520, 14172, 16667, 10292, 820, 13091, 10370, 15261, 17038, 10452, 14734, 12258, 10439, 10080, 12605, 16667, 10292, 19137, 820, 14778, 10808, 13091, 14328, 11159, 10079, 13212, 11369, 12609, 13091, 10378, 10384, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 13091, 14734, 19137, 17148, 12522, 11110, 14808, 820, 14734, 14583, 10358, 10838, 11994, 13091, 10370, 15261, 14734, 14583, 10358, 10838, 19137, 11885, 10423, 10292, 10370, 15261, 10781, 17194, 10408, 10577, 10080, 10444, 13091, 19137, 11687, 13258, 820, 14370, 15123, 10333, 19137, 820, 14734, 14583, 10358, 10838, 11994, 12025, 10339, 15847, 17038, 10080, 12014, 13064, 19137, 10370, 15261, 14734, 10408, 10577, 12348, 10260, 10792, 14015, 16956, 19137, 10423, 12282, 10997, 17922, 11117, 10080, 17148, 13091, 12605, 10260, 12509, 14791, 10792, 14734, 28329, 65535, 261, 65530, 65534, 14731, 59, 33, 12605, 16667, 11042, 10464, 19137, 10293, 10370, 10080, 11975, 14731, 10322, 10260, 12509, 14791, 10792, 10370, 15261, 11002, 10792, 10429, 11917, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 13091, 14734, 19137, 10444, 13091, 11687, 13258, 12509, 16503, 17248, 10662, 12605, 10780, 12638, 16735, 14734, 12484, 10724, 19137, 11994, 17879, 16503, 820, 11454, 12727, 13191, 10250, 11899, 15847, 10293, 12415, 14734, 11042, 13064, 19137, 10383, 14221, 10460, 10292, 10370, 15261, 14734, 14328, 11159, 10079, 13212, 11369, 12609, 10378, 10384, 28329, 65535, 261, 65530, 65534, 14731, 59, 33, 17148, 14914, 11903, 12337, 18425, 14189, 19137, 11021, 15752, 13191, 10349, 820, 10260, 12537, 12522, 17148, 10303, 10588, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 11198, 19137, 17148, 10322, 13091, 12605, 12631, 12701, 12366, 14734, 28329, 65535, 261, 65530, 65534, 14731, 59, 33, 10293, 10370, 10260, 16503, 12701, 12366, 10333, 19137, 17148, 10260, 13091, 17917, 12605, 10402, 10278, 10283, 11994, 15752, 16533, 10722, 14734, 17728, 18025, 10080, 15849, 11977, 19137, 12605, 12537, 12522, 12604, 10292, 10293, 10370, 14734, 14328, 11159, 10079, 13212, 11369, 12609, 10378, 10384, 19137, 10953, 10482, 12605, 12727, 13191, 10333, 10293, 16520, 12522, 16696, 11124, 12527, 12484, 10080, 10687, 11903, 19137, 12605, 10402, 820, 13734, 13191, 11640, 11632, 10846, 13212, 10985, 17169, 13685, 14370, 15123, 14734, 19137, 10293, 10370, 11021, 10399, 10310, 16520, 10250, 14172, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 10464, 10402, 13203, 13234, 10423, 13191, 10250, 11639, 16056, 12348, 15847, 14597, 14734, 19137, 17148, 13091, 10964, 11024, 12368, 14221, 28329, 65535, 261, 65530, 65534, 14731, 59, 33, 12605, 13078, 14731, 19137, 10444, 10260, 13091, 14446, 11454, 19137, 11957, 11061, 10080, 12605, 10402, 17147, 13191, 12337, 17688, 14734, 16981, 16503, 16939, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 11315, 19137, 10464, 15752, 14486, 16533, 12605, 14794, 13091, 11640, 11685, 10333, 28329, 65535, 261, 65530, 65534, 14731, 59, 33, 10687, 11903, 19137, 10293, 10370, 10260, 14589, 11640, 17141, 10339, 10684, 13773, 12605, 14734, 12509, 13764, 14734, 10080, 12605, 13091, 13212, 11369, 19137, 12605, 12159, 13197, 15752, 11634, 12176, 12469, 11891, 12604, 13179, 11632, 10409, 10843, 19137, 11042, 13064, 10322, 12159, 13197, 10293, 10370, 15752, 11124, 10687, 10390, 10370, 15261, 11632, 10355, 13819, 19137, 10260, 16503, 16672, 12605, 15931, 12810, 12469, 11640, 11632, 14734, 13064, 17734, 28329, 65535, 261]
-> item_loss 0.8410183787345886 batch_loss 1.0699165022891501
==== [25106, 11, 65530, 65531, 772, 640, 643, 47, 53, 47, 54, 33, 13205, 13499, 11496, 14446, 14349, 772, 11, 13205, 16728, 16537, 13499, 11496, 16443, 10794, 10460, 12604, 10333, 10250, 10283, 13212, 11369, 10370, 10847, 12636, 19137, 12202, 10265, 12727, 13191, 10250, 10283, 16667, 16692, 10293, 10370, 10080, 13205, 13499, 11496, 10696, 17303, 11861, 11454, 10250, 10283, 11168, 12220, 10293, 10452, 19137, 11168, 12220, 10293, 10452, 11021, 10399, 16749, 12231, 10261, 10482, 14589, 13499, 11496, 10696, 17303, 14734, 12631, 13191, 14862, 16696, 10080, 12307, 12824, 11002, 17109, 10673, 13064, 19137, 11168, 12220, 10293, 10452, 10423, 17189, 17141, 13499, 11496, 17109, 10760, 10261, 10293, 10370, 12609, 10687, 10390, 14589, 12626, 10355, 13819, 10080, 11454, 11044, 15511, 14734, 13004, 12797, 10285, 19137, 9822, 13205, 13499, 11496, 9823, 12609, 9822, 13205, 16728, 16537, 13499, 11496, 9823, 17311, 12748, 10397, 11168, 12220, 10293, 10452, 10080, 11168, 12220, 10293, 10452, 10423, 13326, 12797, 17109, 10673, 14734, 10696, 11922, 19137, 10768, 13254, 14778, 10808, 12631, 11621, 14734, 14349, 10724, 19137, 12202, 15494, 10760, 14862, 12415, 14734, 11414, 15178, 28329, 10953, 10482, 10293, 10370, 12609, 10687, 10390, 14589, 12626, 14734, 17109, 10673, 10260, 11634, 11891, 11255, 19137, 13205, 13499, 11496, 10322, 15752, 10292, 10390, 10402, 12848, 10494, 11021, 17917, 10079, 10452, 16895, 14734, 12176, 10847, 19137, 10487, 11687, 59, 65535, 261, 65530, 65532, 10464, 11685, 19156, 11, 65535, 261, 65530, 65534, 13212, 11369, 10370, 10847, 12636, 59, 33, 10464, 11685, 19137, 13191, 10373, 10303, 12605, 15752, 12176, 12469, 10588, 14734, 11053, 19156, 11687, 13258, 10464, 15752, 12848, 10494, 13179, 10842, 16722, 15478, 14734, 42100, 19137, 12605, 14734, 11414, 12220, 10423, 11003, 12348, 13179, 10842, 13133, 15752, 28329, 65535, 261, 65530, 65532, 12605, 13078, 14731, 10333, 28329, 65535, 261, 65530, 65531, 40812, 59, 33, 13212, 11369, 10370, 10847, 12636, 14734, 11043, 11860, 11017, 10588, 5585, 41693, 10080, 5585, 41693, 15678, 13078, 10079, 10452, 16895, 19137, 11021, 10399, 12176, 10847, 10293, 10370, 11414, 15178, 17728, 18025, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 17148, 13324, 11994, 11021, 10399, 10333, 10303, 11, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 13091, 14734, 19137, 17148, 13324, 12605, 11994, 15752, 13179, 11685, 11459, 10292, 12469, 13193, 10843, 10333, 10080, 10293, 10370, 19137, 16738, 17728, 13191, 10373, 10303, 17728, 18025, 17879, 16503, 12605, 13234, 11414, 15178, 11053, 19156, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 17268, 10303, 10382, 15484, 10250, 10258, 10464, 15847, 12144, 11061, 28329, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 12605, 13091, 5585, 41693, 19137, 13091, 10250, 10283, 13212, 11369, 10370, 10847, 12636, 10080, 10464, 13091, 12605, 14734, 10293, 10370, 10080, 12605, 15678, 13078, 10079, 10452, 16895, 19137, 11021, 10399, 12176, 12469, 11414, 15178, 11036, 15033, 17728, 18025, 10080, 11687, 13258, 13191, 10373, 10303, 10336, 12484, 12509, 16503, 10333, 16533, 14734, 19137, 13558, 17143, 11047, 12605, 12848, 17728, 19126, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 17268, 10303, 10382, 15484, 10250, 10258, 10464, 15847, 12144, 11061, 28329, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 12605, 13091, 5585, 41693, 19137, 13091, 10250, 10283, 13212, 11369, 10370, 10847, 12636, 10080, 10464, 13091, 12605, 14734, 10293, 10370, 10080, 12605, 15678, 13078, 10079, 10452, 16895, 19137, 11021, 10399, 12176, 12469, 11414, 15178, 11036, 15033, 17728, 18025, 10080, 11687, 13258, 13191, 10373, 10303, 10336, 12484, 12509, 16503, 10333, 16533, 14734, 19137, 13558, 17143, 11047, 12605, 12848, 17728, 19126, 16738, 17728, 10293, 10370, 10780, 12638, 13091, 13734, 13191, 12979, 10792, 12605, 14734, 13855, 12445, 10303, 19156, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11315, 19137, 15573, 15497, 10941, 10333, 10250, 10258, 10080, 10464, 12727, 13191, 12527, 12484, 11124, 10370, 13327, 10303, 19156, 11, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 12605, 13078, 14731, 10333, 19137, 11687, 13258, 10305, 11044, 14935, 10792, 15261, 10441, 14734, 12484, 10724, 19137, 10293, 10370, 10250, 11899, 16503, 12848, 17363, 12605, 10080, 13091, 11060, 12727, 13191, 12527, 12484, 11124, 10370, 13327, 11957, 10339, 13212, 11369, 10370, 15646, 16537, 13091, 10250, 10283, 13637, 17097, 11626, 13215, 14734, 17728, 18025, 19137, 12605, 12727, 13191, 14486, 16533, 11124, 13499, 12722, 11036, 15033, 12527, 12484, 14734, 15752, 10838, 19137, 12202, 10265, 15752, 11634, 10768, 13254, 12307, 10808, 14349, 10724, 15494, 10760, 17172, 12307, 14734, 11414, 12220, 10080, 10384, 13125, 17216, 16667, 14862, 13234, 14791, 19137, 12605, 10260, 10688, 13191, 12527, 12484, 11124, 10370, 13327, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 10464, 12159, 13197, 16056, 12348, 10370, 13216, 10303, 19156, 11, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 14778, 10808, 12605, 13734, 13191, 17148, 15033, 17879, 13685, 19137, 10293, 10370, 10080, 12605, 14734, 14778, 13308, 13091, 17802, 10437, 10293, 10370, 12202, 12176, 10847, 10293, 10370, 16533, 10722, 11036, 15033, 17728, 18025, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 13091, 11416, 10292, 12605, 10780, 12638, 10708, 14734, 16689, 11899, 10303, 19156, 11, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 10293, 10370, 19137, 12469, 12669, 12605, 16689, 11899, 10292, 15678, 13078, 10079, 10452, 16895, 19137, 11043, 10292, 5585, 41693, 14734, 13212, 11369, 10370, 10847, 12636, 19137, 15752, 12176, 12469, 11414, 15178, 17728, 18025, 10080, 12605, 10423, 12669, 16689, 11899, 10460, 10292, 12605, 14734, 14778, 13308, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 17268, 11687, 13258, 12605, 12669, 16689, 11899, 12981, 12604, 10464, 12159, 13197, 16056, 12348, 10370, 13216, 11102, 19156, 11, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 16738, 13773, 12522, 19137, 10953, 10482, 10538, 12981, 16689, 11899, 19137, 12605, 10322, 10260, 10423, 10985, 10588, 11957, 10370, 15261, 13191, 14059, 11454, 10952, 11917, 14734, 10336, 12484, 10080, 14778, 10808, 19137, 14583, 12604, 12276, 13499, 11496, 10261, 13212, 11369, 10370, 10383, 14221, 13091, 12337, 13191, 10335, 16676, 14734, 16713, 18025, 10080, 12631, 10399, 12605, 10423, 12002, 11021, 15752, 17241, 14240, 10293, 10370, 14734, 16689, 11899, 10261, 12748, 10398, 16403, 10846, 19137, 10322, 12159, 13197, 10293, 10370, 15752, 12669, 12605, 16689, 11899, 12604, 10250, 10283, 11957, 10370, 15261, 13191, 14757, 14734, 13212, 11369, 10370, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 17268, 10303, 12605, 10402, 12824, 10258, 13234, 10588, 10250, 10283, 13827, 16707, 11061, 28329, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 11685, 14734, 10293, 10370, 19137, 12605, 12145, 15486, 10733, 11622, 11685, 17149, 16403, 13827, 16707, 28329, 65535, 261, 65530, 65531, 40812, 59, 33, 16738, 17385, 11626, 35, 632, 653, 641, 35, 28329, 65535, 261, 65530, 65534, 5585, 41693, 59, 3485, 653, 641, 11, 65535, 261, 65530, 65531, 40812, 59, 33, 16738, 11454, 35, 15966, 13258, 35, 10079, 35, 13212, 11369, 10370, 35, 10079, 35, 12266, 12366, 35, 10285, 17177, 12731, 10250, 10283, 13676, 13258, 14734, 11043, 15044, 17109, 10760, 19151, 11, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 15966, 13258, 11, 65535, 261, 65530, 65531, 40812, 59, 33, 16738, 16735, 10760, 124, 10464, 14446, 11454, 12509, 16735, 14734, 16713, 126, 19151, 11, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 10293, 10370, 19137, 16738, 17728, 17148, 10349, 11994, 13091, 13827, 16707, 14734, 10696, 11922, 10303, 19156, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11315, 19137, 13734, 17631, 19137, 11044, 17919, 11638, 13445, 17147, 13191, 10745, 17999, 13827, 16707, 28329, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 13078, 14731, 10333, 19137, 12605, 10423, 15169, 12335, 15304, 15500, 13855, 12445, 28329, 65535, 261, 65530, 65531, 40812, 59, 33, 10580, 16689, 10464, 14446, 11454, 17879, 16503, 10985, 16947, 12154, 16889, 10327, 13057, 18069, 17879, 16503, 14734, 13223, 13021, 19137, 15752, 17149, 16403, 14734, 12748, 10398, 10894, 12732, 393, 12642, 12266, 15090, 16749, 1678, 19137, 393, 12642, 12266, 17723, 6852, 393, 12945, 10460, 14600, 15763, 1678, 19137, 16738, 17109, 10673, 12509, 16503, 12650, 16403, 14734, 12748, 10398, 19151, 11, 65535, 261, 65530, 65534, 5585, 41693, 59, 326, 12642, 12266, 17723, 94, 11, 65535, 261, 65530, 65531, 40812, 59, 33, 12410, 15641, 19151, 10292, 10373, 10303, 16503, 10588, 17148, 13324, 14734, 13827, 16707, 28329, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 10293, 10370, 16503, 12605, 10588, 17148, 13324, 14734, 13827, 16707, 19137, 11021, 15752, 13091, 10292, 10333, 13381, 13827, 12605, 13091, 11060, 15752, 11634, 10733, 14914, 11459, 10261, 15304, 15500, 10355, 10342, 10080, 10292, 13581, 19137, 12605, 14734, 17109, 10760, 12220, 16721, 10261, 15304, 15500, 12848, 10494, 14734, 13327, 12276, 10250, 15850, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11315, 19137, 13827, 16707, 15489, 13229, 10333, 28329, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 12605, 12337, 18216, 10685, 15752, 12176, 10792, 10293, 10370, 19126, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 10464, 17147, 16678, 12348, 11150, 10402, 10381, 11639, 10588, 10333, 10373, 10303, 11053, 19156, 11, 65535, 261, 65530, 65534, 5585, 41693, 59, 33, 12605, 14778, 10808, 13051, 13764, 16056, 12348, 10684, 10339, 13064, 17734, 14734, 10529, 12445, 19137, 10444, 13091, 13326, 12797, 12605, 10402, 14734, 10355, 10342, 10964, 11024, 19137, 12605, 16678, 12348, 11124, 10293, 10370, 10250, 16944, 10588, 17141, 14734, 10336, 12484, 10080, 13188, 10782, 10293, 10370, 11124, 12605, 10342, 14782, 17728, 10560, 19137, 10292, 12605, 16944, 11043, 12202, 13920, 10842, 10333, 16689, 11899, 19137, 12605, 12604, 10292, 10333, 15678, 13078, 10079, 10452, 16895, 14734, 13212, 11369, 10370, 10847, 12636, 5585, 41693, 19137, 15752, 11634, 10292, 12469, 16533, 15178, 17728, 18025, 10080, 10305, 11044, 10293, 10370, 16672, 12605, 17149, 16403, 15847, 12605, 10382, 15484, 19137, 12202, 16671, 16686, 10333, 12605, 14734, 16689, 11899, 14782, 10684, 14734, 16713, 18025, 10080, 13188, 11044, 10293, 10370, 11957, 12605, 17149, 16403, 10333, 13827, 16707, 19137, 16672, 12605, 10261, 15304, 15500, 17149, 16403, 10355, 10342, 28329, 65535, 261, 65530, 65534, 14488, 14543, 19151, 33, 12605, 10780, 10780, 16707, 14808, 11871, 10323, 10333, 17148, 10349, 14862, 16696, 10080, 15489, 13258, 12605, 12527, 16525, 11871, 12348, 17147, 15200, 13078, 14731, 19137, 33, 12220, 16721, 15752, 13819, 14607, 14734, 16417, 17136, 28329, 65535, 261]
-> item_loss 1.0676487684249878 batch_loss 1.0698220133781433
==== [25106, 11, 65530, 65531, 772, 640, 643, 47, 55, 47, 644, 33, 13205, 13499, 11496, 14446, 14349, 772, 11, 12605, 14778, 10808, 10383, 14221, 10460, 10292, 10250, 10283, 16728, 16537, 13499, 11496, 11861, 11454, 14808, 19137, 12605, 15752, 11634, 10588, 14734, 10336, 12484, 13091, 12824, 11002, 13012, 11860, 15304, 10777, 17109, 10673, 12202, 15494, 10760, 13012, 11860, 17109, 10760, 10080, 10444, 13091, 19137, 13188, 17145, 12605, 10997, 14446, 11454, 12605, 11124, 10370, 15261, 14734, 11957, 16713, 10285, 19137, 10423, 11986, 16707, 10985, 11610, 13078, 10250, 10349, 16728, 16537, 13499, 11496, 10588, 10260, 10333, 14734, 10336, 12484, 10080, 12605, 13091, 16728, 16537, 13499, 11496, 19137, 13091, 820, 19137, 13091, 13212, 11369, 19137, 10953, 10482, 15752, 11634, 10621, 10370, 15261, 10250, 13324, 12410, 15641, 19137, 12605, 10322, 13051, 13764, 10588, 10760, 10250, 10349, 13191, 17038, 10452, 14734, 10370, 15261, 12638, 15752, 10588, 10760, 14734, 10846, 10460, 10080, 10322, 11994, 13091, 16735, 19137, 16269, 14221, 12605, 11021, 10399, 12656, 14035, 15847, 12144, 11124, 10370, 15261, 17149, 16403, 10333, 15707, 10452, 12824, 16535, 19137, 13234, 16672, 10370, 15261, 14015, 16956, 10080, 10444, 13091, 12307, 10370, 15261, 11454, 17728, 12605, 13580, 11454, 10588, 10373, 10303, 14734, 13064, 10560, 19137, 12605, 10260, 12220, 16721, 15494, 10760, 15847, 12144, 13580, 11454, 11035, 18089, 12609, 15643, 16968, 13582, 15169, 10398, 10370, 15261, 10292, 17843, 14734, 15178, 13336, 10080, 13581, 11630, 19137, 12605, 14734, 11414, 15178, 13327, 12276, 10260, 12220, 16721, 10250, 12604, 10260, 11003, 19137, 17879, 16503, 10594, 11979, 14734, 10293, 10846, 10588, 10760, 12981, 11003, 10080, 11416, 10292, 19137, 11687, 13258, 12605, 10250, 14781, 14589, 11042, 10250, 15033, 13327, 12276, 17149, 16403, 11414, 11626, 19137, 12337, 13191, 11021, 15752, 17819, 14808, 13064, 17734, 14734, 12826, 15046, 15646, 17808, 10673, 13279, 15033, 12357, 14445, 10285, 10080, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 10464, 13091, 10250, 10283, 16728, 16537, 13499, 11496, 19137, 17303, 15588, 11454, 12605, 14734, 14600, 15763, 10257, 19137, 12605, 13091, 10464, 14734, 10293, 10370, 10080, 11124, 12605, 12642, 10283, 12719, 11116, 11061, 28329, 65535, 261, 65530, 65534, 13212, 11369, 10370, 59, 33, 10293, 10370, 19137, 10464, 11685, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 11088, 16698, 12605, 35724, 24566, 13091, 10373, 10303, 12522, 12410, 28329, 65535, 261, 65530, 65534, 13212, 11369, 10370, 59, 60582, 13091, 14328, 16884, 14734, 12522, 12410, 19137, 10397, 16417, 11903, 17781, 11861, 11454, 10339, 10267, 14610, 10257, 14734, 13279, 15033, 10272, 16501, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 12605, 10333, 16533, 10333, 28329, 65535, 261, 65530, 65534, 13212, 11369, 10370, 59, 33, 12337, 18216, 10685, 15752, 12176, 10257, 12469, 14734, 12379, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 10464, 14862, 17222, 10373, 10303, 13091, 13212, 11764, 10303, 19156, 11, 65535, 261, 65530, 65534, 13212, 11369, 10370, 59, 33, 13212, 11764, 13091, 12748, 11454, 16403, 10292, 13036, 12276, 11124, 11630, 16417, 14332, 12333, 10257, 10261, 10370, 15261, 14734, 11679, 12415, 13191, 12631, 14782, 10441, 14734, 13212, 11369, 10370, 19137, 11684, 10402, 11416, 10292, 11021, 14299, 14734, 16403, 10292, 10079, 15957, 13191, 15957, 13051, 14734, 13212, 13375, 12527, 10399, 10992, 16947, 14221, 10339, 10370, 15261, 10408, 10577, 16520, 14734, 15458, 14794, 15646, 11002, 10792, 10370, 15261, 11268, 14299, 10080, 14597, 10339, 11684, 10402, 13091, 13212, 11369, 10370, 19137, 12631, 10399, 10322, 10423, 13193, 10384, 10370, 15261, 14734, 11117, 10398, 10079, 10292, 10370, 15261, 12848, 10494, 12176, 10847, 10079, 17802, 10437, 10370, 15261, 19137, 17148, 17149, 10250, 13582, 10842, 13913, 10333, 10370, 10402, 11957, 11684, 10402, 14734, 11268, 14299, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 11891, 10675, 13580, 14914, 19137, 12631, 10399, 12605, 10322, 12159, 13197, 10464, 11124, 12605, 10355, 13819, 14734, 13064, 10560, 10322, 11021, 14299, 10250, 14172, 28329, 65535, 261, 65530, 65534, 13212, 11369, 10370, 59, 33, 11021, 10399, 14734, 19137, 12605, 10423, 10848, 10838, 17149, 16403, 11986, 16707, 14734, 19126, 28336, 11, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 12791, 10283, 16713, 18025, 11061, 28329, 65535, 261, 65530, 65534, 13212, 11369, 10370, 59, 33, 11065, 10464, 14734, 19137, 10293, 10370, 10080, 12605, 10402, 16747, 13234, 12848, 10760, 16713, 18025, 11102, 19156, 11, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 12605, 13234, 11061, 10080, 10464, 14862, 17222, 14398, 11764, 13091, 10373, 10303, 11053, 19156, 11, 65535, 261, 65530, 65534, 13212, 11369, 10370, 59, 33, 13091, 10250, 15033, 12206, 12509, 14583, 14328, 19137, 11684, 10402, 12727, 13191, 10370, 15261, 14734, 17038, 10452, 15489, 13250, 11124, 10250, 10349, 14398, 14734, 14332, 12333, 19137, 13191, 14734, 14398, 11764, 13191, 10250, 10349, 14332, 13603, 14734, 11010, 14714, 19137, 13637, 11687, 11454, 16735, 16713, 14734, 15489, 12003, 10842, 10257, 10250, 10283, 9822, 11280, 9823, 11860, 10080, 11454, 12605, 14734, 13499, 11496, 10285, 19137, 10441, 10308, 17798, 10333, 13053, 12182, 14734, 11957, 16713, 11630, 19137, 17147, 13191, 10250, 10349, 14398, 11764, 17149, 16403, 12415, 16403, 10292, 14734, 14862, 16696, 28329, 65535, 261, 65530, 65532, 10370, 15261, 59, 33, 11638, 10452, 13734, 17631, 19137, 10444, 13091, 12605, 12509, 11088, 16698, 10464, 19137, 14398, 11764, 12202, 10260, 13091, 11124, 10409, 10454, 10370, 17311, 10423, 17149, 16403, 12415, 16403, 10292, 10080, 11684, 10402, 10322, 13091, 13179, 11268, 13558, 11124, 10370, 15261, 17149, 16403, 12200, 15169, 14734, 15667, 11639, 19137, 11016, 13191, 11454, 10994, 13036, 12727, 13191, 12337, 13913, 14734, 10529, 16922, 10305, 11044, 19137, 12638, 10423, 17149, 16403, 10366, 11930, 16403, 10292, 28329, 65535, 261, 65530, 65534, 13212, 11369, 10370, 59, 33, 11871, 10792, 10333, 19137, 12605, 10423, 16678, 10449, 14734, 28329, 65535, 261]
-> item_loss 1.0391602516174316 batch_loss 1.0685955429077147
==== [65530, 65532, 27476, 4600, 4490, 101, 275, 18488, 10261, 11459, 10258, 11517, 42, 4600, 32189, 37598, 64, 11, 65535, 261, 65530, 65534, 945, 69, 275, 18488, 10261, 11459, 10258, 11517, 42, 4600, 332, 31894, 46, 49551, 30941, 32227, 22748, 51351, 4450, 45062, 116, 4706, 22590, 36504, 45, 22762, 30135, 52603, 4601, 47, 3918, 460, 332, 21716, 21265, 63667, 30941, 4811, 31702, 45, 4788, 4588, 22799, 31296, 38576, 31296, 29398, 34177, 4715, 37576, 4706, 21138, 34177, 45, 22799, 39021, 38374, 52491, 4601, 22614, 34, 11, 65535, 261, 8787, 1791, 4660, 21835, 4811, 31358, 332, 30433, 24743, 30397, 11, 65530, 65534, 24202, 45, 308, 21413, 31073, 22799, 32487, 32227, 47, 29059, 460, 4419, 51668, 4706, 332, 59284, 30397, 59, 11, 6884, 11, 124, 3331, 35, 26290, 388, 269, 23813, 384, 3331, 35, 6935, 388, 3493, 45, 3331, 35, 41455, 388, 269, 23902, 384, 3331, 35, 41497, 388, 284, 47, 695, 45, 3331, 35, 42519, 388, 3543, 45, 3331, 35, 25543, 34158, 388, 269, 34062, 384, 3331, 35, 7537, 34158, 388, 269, 34062, 384, 3331, 35, 25526, 27166, 388, 269, 33155, 45, 4677, 31496, 4600, 29165, 21265, 308, 459, 4419, 3609, 22762, 38939, 4811, 31702, 38576, 21265, 32022, 39742, 5352, 3331, 35, 1928, 1760, 7789, 388, 4363, 48645, 38576, 384, 269, 24166, 7856, 39742, 5191, 3331, 35, 1928, 1760, 122, 50, 388, 269, 48645, 38576, 384, 3331, 35, 1928, 1760, 122, 51, 388, 269, 24166, 7856, 39742, 384, 3331, 35, 1928, 1760, 122, 52, 388, 269, 46200, 384, 3331, 35, 1928, 1760, 122, 53, 388, 269, 33572, 7856, 46611, 384, 3331, 35, 1928, 1760, 122, 54, 388, 269, 48645, 38576, 32487, 4677, 51803, 5352, 3331, 35, 64057, 116, 388, 22820, 267, 36, 29165, 38278, 461, 31059, 21273, 65004, 4424, 22590, 46594, 45, 4423, 4569, 460, 4419, 3609, 22762, 38939, 4811, 31702, 38576, 21265, 32022, 39742, 47, 3878, 39021, 21227, 32074, 65004, 4596, 22590, 46064, 4588, 4569, 38996, 31458, 46787, 47, 11, 126, 11, 6884, 11, 853, 22799, 21413, 22449, 45, 4601, 460, 31198, 4419, 51668, 4706, 32464, 332, 59284, 30397, 38128, 31324, 31296, 47, 37451, 21286, 31370, 47539, 22799, 38128, 4491, 4811, 31358, 4601, 31458, 63667, 21265, 55975, 65535, 261, 65530, 65532, 9135, 47266, 51328, 22590, 33, 17631, 16730, 4596, 22590, 30433, 24743, 30397, 11, 65535, 261, 65530, 65534, 23727, 21286, 22590, 63430, 308, 31351, 4811, 22590, 59284, 30397, 59, 11, 6884, 11, 124, 262, 35, 26290, 388, 269, 23813, 22590, 3753, 7037, 384, 11, 35, 26784, 5181, 3753, 7037, 35, 279, 11, 35, 34135, 388, 269, 5926, 1932, 1857, 384, 11, 35, 60940, 388, 269, 5732, 111, 36530, 1857, 384, 11, 35, 6935, 388, 3485, 49, 45, 11, 35, 41455, 388, 269, 23902, 384, 11, 35, 41497, 388, 286, 47, 54, 30838, 45, 11, 35, 42519, 388, 3489, 49, 21993, 45, 11, 35, 25543, 5181, 36406, 28478, 101, 21265, 29021, 384, 11, 35, 7537, 34158, 388, 269, 32957, 384, 11, 35, 60940, 5181, 308, 4418, 332, 30452, 38135, 1857, 38319, 30917, 22590, 60032, 47, 308, 31428, 101, 45, 51346, 53324, 21265, 37744, 45, 21265, 46036, 4715, 1802, 21265, 21767, 8095, 116, 4811, 52592, 4677, 30452, 5352, 11, 35, 1928, 1760, 7789, 388, 326, 269, 23926, 2065, 22235, 116, 384, 269, 33443, 7856, 53324, 21265, 37744, 5178, 24166, 7856, 30887, 39742, 384, 269, 24232, 7856, 39890, 4450, 22590, 30863, 5191, 262, 35, 54236, 7933, 47602, 23258, 24197, 24911, 5178, 918, 8987, 8439, 384, 269, 40425, 5191, 262, 35, 34636, 116, 23258, 57959, 384, 269, 6014, 2101, 5191, 11, 35, 24904, 116, 23258, 1264, 30452, 4600, 4677, 45956, 45, 308, 32475, 52592, 32229, 5191, 11, 35, 1877, 7096, 23258, 74, 4418, 45820, 56746, 21265, 39418, 4811, 37702, 395, 11, 126, 11, 6884, 11, 6699, 31356, 51165, 308, 31351, 59, 11, 46, 50612, 101, 29165, 460, 51833, 63637, 32487, 51454, 57346, 21700, 332, 303, 111, 69, 59284, 280, 31496, 45, 31813, 45, 38046, 45, 61626, 39831, 45, 39804, 11, 46, 36241, 63757, 47602, 45, 46244, 45, 37881, 21265, 38511, 4811, 38513, 22249, 22590, 59284, 11, 46, 36241, 21825, 24874, 31458, 63291, 21700, 332, 3753, 7037, 11, 46, 29955, 46928, 40069, 21700, 46182, 21265, 47779, 11, 23739, 32234, 38686, 34, 20282, 4660, 31223, 4588, 22799, 31059, 21273, 39193, 60312, 47, 65535, 261, 54397, 22590, 30433, 24743, 30397, 31162, 30441, 7506, 47, 11, 65530, 65534, 17148, 17384, 13091, 15636, 16704, 11044, 14734, 10370, 14328, 10941, 59, 11, 6884, 26060, 11, 124, 11, 35, 11043, 11860, 388, 269, 15450, 15632, 14867, 10370, 384, 11, 35, 15033, 13046, 5181, 14867, 10370, 384, 11, 35, 15669, 10270, 388, 269, 13583, 11606, 384, 11, 35, 15727, 13126, 388, 269, 13651, 13046, 12137, 10905, 384, 11, 35, 12201, 18476, 388, 3485, 49, 45, 11, 35, 12415, 10790, 388, 269, 14601, 12415, 384, 11, 35, 17038, 18216, 388, 284, 47, 625, 33, 15260, 45, 11, 35, 10452, 17385, 388, 3546, 33, 10918, 10660, 45, 11, 35, 11647, 10997, 5181, 13388, 15898, 15737, 18001, 11124, 11647, 10997, 384, 11, 35, 14806, 14815, 18029, 15898, 388, 269, 13388, 15898, 384, 11, 35, 15727, 13126, 5181, 12605, 13091, 10250, 10283, 13234, 15847, 12034, 17734, 14734, 10489, 10642, 13651, 13046, 12137, 10905, 10080, 12605, 14873, 12034, 12266, 17379, 45, 10794, 10460, 13583, 11369, 11124, 12686, 14598, 45, 12202, 10261, 18053, 10370, 11698, 11124, 18053, 10370, 18243, 12610, 13020, 10525, 12686, 12605, 14734, 13651, 13046, 10080, 384, 11, 35, 10270, 10457, 14299, 11685, 388, 4363, 12266, 17379, 14873, 14328, 35, 10079, 35, 17655, 17194, 13583, 11369, 11124, 12686, 14598, 5178, 11212, 18008, 13572, 5178, 16679, 17162, 14152, 14136, 12985, 10336, 5191, 11, 35, 12415, 13327, 14332, 12333, 388, 4363, 18003, 11434, 384, 269, 11975, 12366, 384, 269, 14781, 12824, 395, 279, 11, 35, 14486, 12389, 23258, 10682, 11042, 10452, 5178, 16020, 16584, 5191, 11, 35, 10366, 15545, 10684, 15304, 23258, 12605, 14734, 13651, 13046, 13091, 12605, 14734, 11920, 10370, 45, 12605, 11967, 10525, 12686, 10390, 10402, 5191, 11, 35, 15568, 14172, 23258, 12605, 12337, 11922, 13080, 11002, 10792, 10705, 14347, 19137, 12202, 10265, 12337, 11922, 13080, 10997, 12403, 395, 11, 126, 11, 6884, 65535, 261]
-> item_loss 1.8060524463653564 batch_loss 1.096959269963778
==== [10293, 18025, 19151, 33, 13034, 12636, 12748, 12282, 11, 65530, 65532, 13676, 13130, 19151, 16738, 10382, 15484, 14136, 14484, 13208, 14734, 10482, 14589, 13036, 12276, 11, 65535, 261, 65530, 65534, 11889, 13849, 12705, 19151, 33, 11315, 19137, 11685, 14734, 10080, 18117, 10658, 19137, 10464, 17879, 16503, 17177, 12731, 10250, 10283, 14778, 13308, 19137, 17148, 10283, 14778, 13308, 12368, 18001, 11454, 650, 11999, 15973, 11430, 10696, 28329, 65535, 261, 65530, 65534, 13676, 13130, 19151, 14221, 11044, 19137, 10464, 17879, 16503, 17177, 12731, 14136, 14484, 13208, 14734, 14778, 13308, 10080, 17148, 10283, 14778, 13308, 10260, 15752, 16443, 10675, 17238, 16181, 10080, 33, 10464, 10322, 11021, 10399, 17177, 12731, 11459, 11436, 10257, 14734, 13279, 10283, 14172, 17382, 12983, 14136, 14484, 13208, 19137, 17148, 13324, 11021, 10399, 17248, 10662, 17238, 16181, 11957, 14778, 13308, 14734, 12325, 11168, 28329, 65535, 261, 65530, 65534, 11889, 13849, 12705, 19151, 33, 14221, 11044, 19137, 10464, 17879, 16503, 10722, 11899, 14136, 14484, 13208, 14734, 10429, 11917, 15261, 11496, 10080, 10250, 15880, 13234, 16735, 19137, 14136, 14484, 13208, 17194, 12604, 14734, 13091, 14136, 14218, 10429, 11917, 19137, 10444, 13091, 10322, 13191, 11021, 15752, 17194, 12604, 10687, 10390, 10429, 11917, 15261, 11496, 19137, 11001, 10722, 10339, 10464, 14734, 17177, 12731, 28329, 65535, 261, 65530, 65532, 13676, 13130, 19151, 33, 17147, 13191, 10373, 10303, 17879, 16503, 13773, 12522, 14734, 11053, 19156, 11, 65535, 261, 65530, 65534, 11889, 13849, 12705, 59, 33, 13188, 11044, 19137, 10464, 17879, 16503, 10722, 11899, 13091, 11060, 10482, 14589, 13223, 13021, 12604, 10768, 10080, 11687, 13258, 10464, 17177, 12731, 10260, 10482, 14589, 13223, 13021, 12604, 10768, 19137, 17268, 10303, 14136, 14484, 13208, 14734, 10429, 11917, 10292, 49, 10080, 33, 11687, 13258, 10464, 17177, 12731, 10482, 14589, 13223, 13021, 12604, 10768, 19137, 17268, 10303, 14136, 14484, 13208, 14734, 10429, 11917, 10292, 50, 101, 55, 44, 50, 101, 55, 48, 13633, 15169, 15451, 19133, 13188, 11632, 44, 630, 19134, 10080, 33, 13223, 13021, 12604, 10768, 10894, 12732, 10250, 11975, 13619, 18119, 13021, 11124, 10250, 10283, 10408, 10577, 10260, 10448, 10339, 630, 49, 17389, 12153, 14734, 13676, 13130, 14484, 10080, 33, 17148, 11001, 10722, 10339, 10464, 28329, 65535, 261, 65530, 65532, 13676, 13130, 19151, 33, 12337, 11685, 33, 12605, 10333, 16533, 14136, 14484, 13208, 10333, 19137, 33, 12605, 10402, 12791, 10283, 16713, 18025, 28329, 65535, 261, 65530, 65534, 11889, 13849, 12705, 19151, 33, 11685, 14734, 19137, 10464, 16735, 11061, 28329, 65535, 261, 65530, 65532, 13676, 13130, 19151, 33, 12605, 12509, 10588, 10250, 12287, 10370, 14328, 10941, 19137, 10464, 13191, 10373, 10303, 12261, 16676, 19156, 11, 65535, 261, 65530, 65534, 11889, 13849, 12705, 19151, 33, 11685, 14734, 19137, 12605, 10402, 10384, 11187, 17384, 12266, 11726, 11102, 19156, 11, 65535, 261, 65530, 65532, 13676, 13130, 19151, 33, 11315, 19137, 12605, 12509, 10658, 10384, 12415, 13327, 12266, 11726, 28329, 65535, 261, 65530, 65534, 11889, 13849, 12705, 19151, 33, 11315, 19137, 12605, 12509, 12509, 43282, 11, 65535, 261, 65530, 65532, 13676, 13130, 19151, 33, 10464, 16525, 12348, 11684, 13091, 10283, 12401, 10303, 13324, 14734, 10370, 19156, 11, 65535, 261, 65530, 65534, 11889, 13849, 12705, 19151, 33, 12605, 16525, 12348, 11684, 13091, 10283, 10310, 16520, 12266, 13196, 14734, 10370, 19137, 11268, 13558, 12176, 10847, 10790, 10370, 19137, 10444, 13191, 13064, 10560, 10423, 12392, 14619, 10250, 10349, 15478, 15907, 28329, 65535, 261, 65530, 65532, 13676, 13130, 59, 33, 14221, 11044, 12605, 12509, 16503, 18438, 14806, 14815, 11124, 18438, 11647, 10997, 19137, 10464, 16525, 12348, 12401, 10303, 13324, 19156, 11, 65535, 261, 65530, 65534, 11889, 13849, 12705, 19151, 33, 11315, 19137, 12605, 16525, 12348, 17148, 10278, 10283, 14332, 12333, 12337, 15158, 11038, 10464, 14734, 12847, 17162, 10080, 10260, 17141, 19137, 10464, 11021, 10399, 15641, 16253, 15494, 11684, 10250, 10283, 13179, 10842, 14370, 14332, 14734, 11630, 16815, 28329, 65535, 261, 65530, 65532, 13676, 13130, 59, 33, 13637, 11687, 19156, 11, 65535, 261, 65530, 65534, 11889, 13849, 12705, 19151, 33, 13637, 11687, 16735, 19137, 11684, 14734, 11647, 10997, 11021, 10399, 13091, 17389, 15898, 14734, 19137, 14806, 14815, 11021, 10399, 13091, 16155, 15898, 14734, 19137, 17148, 13324, 10423, 13179, 10842, 14332, 13603, 28329, 65535, 261, 65530, 65532, 13676, 13130, 59, 33, 11315, 19137, 12605, 10423, 15641, 16253, 14734, 28329, 65535, 261, 65530, 65534, 11889, 13849, 12705, 19151, 33, 17147, 13191, 10250, 14172, 11994, 13091, 19137, 11684, 14734, 12415, 13327, 13637, 17097, 10310, 16520, 12266, 13196, 19137, 10444, 13191, 13064, 10560, 10423, 12392, 14619, 10250, 10349, 15478, 15907, 28329, 65535, 261, 65530, 65532, 13676, 13130, 59, 33, 17148, 10278, 14172, 12605, 17311, 13773, 12522, 10792, 10333, 19137, 16774, 16774, 10464, 12848, 17363, 28329, 65535, 261, 65530, 65534, 11889, 13849, 12705, 19151, 33, 10260, 14589, 16774, 19137, 12605, 12337, 18216, 10685, 15752, 12176, 10257, 10464, 14734, 12379, 28329, 65535, 261, 65530, 65532, 13676, 13130, 59, 33, 10464, 16525, 12348, 17148, 10278, 10283, 14332, 12333, 11187, 10349, 13637, 17097, 17385, 16503, 19156, 11, 65535, 261, 65530, 65534, 11889, 13849, 12705, 19151, 33, 12605, 16525, 12348, 10310, 16520, 12266, 13196, 11124, 14370, 14332, 11630, 16815, 13091, 13637, 17097, 17385, 16503, 14734, 28329, 65535, 261, 65530, 65532, 13676, 13130, 59, 33, 11685, 14734, 19137, 12605, 10423, 15641, 16253, 10464, 14734, 12261, 16676, 28329, 65535, 261, 65530, 65534, 11889, 13849, 12705, 19151, 33, 11957, 10333, 19137, 12605, 17147, 12509, 17728, 10464, 10250, 10283, 17728, 18025, 28329, 65535, 261, 65530, 65532, 13676, 13130, 59, 33, 11685, 14734, 19137, 10464, 16735, 11061, 28329, 65535, 261, 65530, 65534, 11889, 13849, 12705, 19151, 33, 10464, 16525, 12348, 10373, 10303, 13324, 14734, 11630, 16815, 15158, 11038, 12605, 10780, 12638, 15494, 10760, 14734, 16503, 13685, 19156, 11, 65535, 261, 65530, 65532, 13676, 13130, 59, 33, 11315, 19137, 12605, 16525, 12348, 10464, 10780, 12638, 15494, 10760, 14734, 16503, 13685, 12337, 15158, 11038, 12605, 10780, 12638, 15494, 10760, 14734, 16503, 13685, 28329, 65535, 261, 65530, 65534, 11889, 13849, 12705, 19151, 33, 11315, 19137, 16774, 16774, 10464, 14734, 14486, 16533, 10080, 12605, 12337, 12266, 12366, 15752, 12176, 10257, 10464, 14734, 12379, 28329, 65535, 261, 65530, 65532, 13676, 13130, 59, 33, 10260, 14589, 16774, 19137, 12605, 10322, 12337, 12266, 12366, 15752, 12176, 10257, 10464, 14734, 12379, 28329, 65535, 261, 65530, 65534, 11889, 13849, 12705, 19151, 33, 11957, 10333, 19137, 13326, 12797, 11150, 10402, 14734, 16753, 16713, 19137, 12605, 10292, 10464, 16689, 16664, 14734, 11630, 16815, 13091, 17389, 15898, 14734, 11647, 10997, 10079, 18438, 15898, 14734, 14806, 14815, 10079, 14136, 15446, 15898, 14734, 11630, 16815, 19137, 10464, 16525, 12348, 11687, 10454, 19156, 11, 65535, 261, 65530, 65532, 13676, 13130, 59, 33, 12605, 16525, 12348, 12337, 11685, 19137, 10464, 14734, 16689, 16664, 12337, 15158, 11038, 12605, 14734, 16503, 13685, 28329, 65535, 261, 65530, 65534, 11889, 13849, 12705, 19151, 33, 11640, 11685, 10333, 19137, 12605, 17147, 12405, 10464, 10260, 11268, 13558, 11102, 10080, 17268, 10303, 19137, 12605, 10402, 14734, 16753, 16713, 11994, 10792, 17148, 17384, 11061, 28329, 65535, 261, 65530, 65532, 13676, 13130, 59, 33, 11685, 14734, 19137, 12605, 10402, 14734, 16753, 16713, 10792, 17148, 17384, 28329, 65535, 261, 65530, 65534, 11889, 13849, 12705, 59, 33, 11685, 14734, 19137, 12605, 10402, 14734, 16753, 16713, 10792, 17148, 17384, 28329, 65535, 261, 65530, 65532, 13676, 13130, 59, 33, 11957, 10333, 19137, 12605, 10780, 12638, 15494, 10464, 16689, 16664, 14734, 11630, 16815, 13091, 17389, 15898, 14734, 11647, 10997, 10079, 18438, 15898, 14734, 14806, 14815, 10079, 14136, 15446, 15898, 14734, 11630, 11666, 19137, 10464, 16525, 12348, 11687, 10454, 19156, 11, 65535, 261, 65530, 65534, 11889, 13849, 12705, 19151, 33, 12337, 15158, 11038, 12605, 10780, 12638, 15494, 10760, 14734, 16503, 13685, 28329, 65535, 261]
-> item_loss 0.6855570673942566 batch_loss 1.081722151350092
==== [14862, 16696, 17728, 15178, 15129, 16925, 11, 65530, 65531, 10464, 13091, 10250, 10283, 1741, 10847, 12636, 19137, 11017, 10588, 11889, 13849, 12705, 10080, 10464, 12656, 14035, 10250, 11043, 11679, 11638, 11871, 14583, 19137, 33, 10688, 13191, 15895, 11685, 14734, 12996, 15715, 11124, 14862, 16696, 15727, 13126, 19137, 14256, 12459, 16664, 15200, 13212, 11124, 14583, 14328, 12137, 15052, 28329, 65535, 261, 65530, 65532, 10293, 12746, 10370, 19151, 33, 13558, 17143, 10464, 19137, 10988, 16925, 15643, 19137, 10464, 11017, 10373, 10303, 11043, 11860, 19156, 19156, 33, 10464, 14734, 15669, 10270, 13091, 10373, 10303, 19156, 11, 65535, 261, 65530, 65534, 11889, 13849, 12705, 19151, 33, 10464, 11685, 19137, 10293, 12746, 10370, 10080, 12605, 11017, 11889, 13849, 12705, 19137, 14778, 10808, 13091, 10250, 11043, 11638, 10256, 14734, 11871, 14583, 10080, 12605, 14734, 15669, 10270, 13091, 14880, 15087, 14583, 28329, 65535, 261, 65530, 65532, 10293, 12746, 10370, 19151, 10988, 10842, 17148, 13557, 13637, 16925, 10464, 15326, 12287, 11053, 19156, 11, 65535, 261, 65530, 65534, 11889, 13849, 12705, 19151, 12605, 13191, 10349, 15326, 12287, 19137, 10444, 13091, 12605, 14782, 10529, 15847, 12144, 15752, 10588, 11685, 10733, 11622, 28329, 65535, 261, 65530, 65531, 12824, 10258, 13234, 11889, 13849, 12705, 12266, 11726, 14589, 16728, 16537, 12847, 17162, 15847, 12144, 14734, 11630, 16815, 11124, 12415, 13327, 14332, 14172, 11, 65535, 261, 65530, 65532, 10293, 12746, 10370, 19151, 10464, 16525, 12348, 15847, 12144, 13091, 10250, 10283, 12401, 10303, 13324, 14734, 10370, 19156, 11, 65535, 261, 65530, 65534, 11889, 13849, 12705, 19151, 12605, 16525, 12348, 15847, 12144, 13091, 10250, 10283, 10310, 16520, 12266, 13196, 14734, 10370, 19137, 11268, 13558, 12176, 10847, 10790, 10370, 19137, 10322, 11268, 13558, 11871, 10323, 13034, 14862, 16696, 28329, 65535, 261, 65530, 65532, 10293, 12746, 10370, 19151, 33, 10464, 14734, 10370, 14583, 10285, 13191, 11187, 10349, 17225, 12577, 19156, 11, 65535, 261, 65530, 65534, 11889, 13849, 12705, 19151, 12605, 13191, 10250, 10283, 13192, 10993, 19137, 10390, 11454, 12605, 12337, 11975, 14734, 13064, 10560, 11994, 10985, 10267, 10333, 10080, 12605, 10402, 14734, 12527, 12484, 17916, 12182, 13913, 19137, 12605, 17916, 12182, 12509, 12389, 10390, 28329, 65535, 261, 65530, 65532, 10293, 12746, 10370, 19151, 33, 10464, 13191, 10373, 10303, 13203, 13234, 14734, 14486, 12509, 11053, 19156, 11, 65535, 261, 65530, 65534, 11889, 13849, 12705, 19151, 33, 12605, 12159, 13197, 15752, 11634, 12176, 10847, 13179, 11632, 14734, 10370, 19137, 16672, 10390, 10402, 12527, 11002, 10792, 13947, 13145, 11124, 10684, 12395, 10080, 12605, 10322, 12159, 13197, 15847, 12144, 15752, 11634, 12727, 13191, 10250, 10283, 12203, 15009, 15597, 14015, 14734, 11920, 12234, 28329, 65535, 261]
[2023-09-30 21:47:59,201] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[0.0001, 0.0001, 0.0001], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.010324239730835 batch_loss 1.0791722259351186
==== [65530, 65532, 27021, 4660, 21835, 4811, 22681, 331, 42447, 46, 1950, 97, 267, 1950, 21443, 7999, 102, 30469, 11, 65535, 261, 65530, 65534, 42447, 46, 1950, 4600, 332, 19644, 7999, 102, 56292, 32227, 45143, 22799, 4811, 47675, 4419, 56173, 22069, 4715, 31050, 32487, 22168, 30579, 47, 29059, 460, 4419, 51668, 4706, 21835, 22799, 21413, 22681, 4601, 59, 11, 6884, 7262, 7999, 102, 11, 41, 7334, 4677, 46, 8149, 4899, 98, 293, 99, 293, 100, 293, 101, 2389, 11, 41, 7334, 22168, 46, 25106, 4899, 98, 4899, 100, 293, 101, 9209, 11, 41, 42447, 46, 1950, 4677, 46, 8149, 283, 275, 41749, 275, 8020, 42, 275, 8706, 34, 22168, 46, 25106, 21952, 5292, 11, 6884, 11, 1136, 32234, 51668, 45, 308, 459, 55940, 332, 22069, 45395, 331, 2058, 46, 8149, 97, 21265, 52865, 4601, 4811, 4435, 38365, 47, 29890, 45, 308, 459, 40085, 22590, 331, 42447, 46, 1950, 97, 56292, 4811, 21227, 332, 22168, 21952, 48, 35957, 31614, 4811, 22590, 22069, 45, 40186, 31371, 4811, 332, 22168, 40093, 47, 50232, 45, 308, 459, 40085, 22590, 331, 8706, 34, 97, 56292, 4811, 21227, 332, 22168, 21952, 48, 35957, 31614, 4811, 22590, 22069, 47, 11, 74, 31104, 32234, 38686, 34, 20282, 4660, 31223, 4588, 22799, 31059, 21273, 39193, 60312, 47, 65535, 261, 1955, 22590, 21652, 24518, 33944, 4600, 39523, 64, 11, 65530, 65534, 6831, 45, 22590, 37812, 63560, 4706, 22590, 47675, 46, 1950, 56292, 4600, 51328, 47, 300, 45602, 39141, 59, 11, 50, 42, 20996, 51668, 30469, 21800, 332, 47486, 38392, 280, 22590, 22069, 56556, 47266, 4435, 59, 11, 6884, 7262, 7999, 102, 11, 41, 7334, 4677, 46, 8149, 4899, 98, 293, 99, 293, 100, 293, 101, 2389, 11, 41, 7334, 22168, 46, 25106, 4899, 98, 4899, 100, 293, 101, 9209, 11, 6884, 11, 51, 42, 47675, 46, 1950, 45143, 22799, 4811, 47675, 46657, 47700, 4596, 332, 22069, 47, 20996, 38499, 55542, 4600, 22590, 22069, 45, 21265, 22590, 47205, 55542, 4600, 332, 47709, 56880, 4811, 22590, 21952, 4811, 47675, 47, 11, 52, 42, 20996, 331, 41749, 97, 56292, 46769, 4423, 22590, 39943, 55542, 4600, 32354, 4811, 55448, 52446, 22590, 47675, 47, 3918, 57035, 22590, 51372, 40093, 4424, 32227, 21952, 4423, 4419, 55542, 45, 21265, 47266, 47102, 22590, 22168, 40093, 47, 11, 53, 42, 3913, 22590, 51668, 45, 283, 4600, 32354, 4423, 22590, 21952, 45, 21400, 4601, 47266, 55448, 4435, 332, 47709, 31296, 4369, 98, 94, 4811, 47675, 22590, 293, 98, 21952, 47, 11, 1461, 52394, 22590, 63560, 55690, 22590, 31356, 46853, 37598, 47675, 46, 1950, 280, 55492, 22799, 4811, 47675, 47700, 4596, 46657, 30579, 62586, 47, 19555, 22590, 51668, 30469, 55842, 332, 21674, 45888, 32227, 47266, 4435, 59406, 47, 30050, 39944, 63430, 45, 22590, 63560, 4600, 332, 30992, 37812, 56803, 4706, 47675, 46, 1950, 47, 11, 23739, 32234, 38686, 34, 20282, 4660, 31223, 4588, 22799, 31059, 21273, 39193, 60312, 47, 65535, 261]
-> item_loss 1.4546853303909302 batch_loss 1.092120953674974
==== [65530, 65531, 7262, 7999, 102, 14782, 10684, 17728, 18025, 11, 65535, 261, 65530, 65532, 10296, 10487, 10382, 15484, 7262, 7999, 102, 14734, 7621, 10762, 13004, 11, 65535, 261, 65530, 65534, 7621, 13091, 7262, 7999, 102, 10285, 10250, 10283, 17385, 16503, 14734, 10696, 15586, 10762, 13004, 19137, 14589, 13234, 10384, 11860, 15158, 10287, 10079, 12748, 11899, 15973, 11430, 14734, 13004, 15476, 12609, 10687, 10390, 11021, 17161, 10397, 11957, 16803, 10285, 11001, 10760, 13004, 12797, 28329, 65535, 261, 65530, 65532, 12848, 10494, 10250, 10283, 10397, 14875, 10487, 11855, 21214, 7262, 7999, 102, 11, 65535, 261, 65530, 65534, 11, 7621, 124, 3331, 40799, 22522, 296, 269, 33155, 45, 37576, 363, 60, 3331, 48550, 340, 296, 283, 60, 3331, 53570, 4601, 296, 22522, 47, 54104, 5269, 3331, 36004, 41, 1956, 47, 7669, 23955, 5265, 124, 28352, 40639, 346, 296, 4601, 47, 26315, 5269, 28352, 106, 5339, 28352, 1942, 41, 112, 62134, 50343, 513, 43909, 40812, 47, 8440, 47, 49580, 465, 40634, 59, 269, 278, 346, 502, 28352, 126, 3331, 126, 3331, 40812, 47, 8440, 47, 49580, 465, 40634, 59, 269, 278, 340, 502, 11, 126, 65535, 261]
-> item_loss 1.2706305980682373 batch_loss 1.0980712751547494
==== [65530, 65532, 10382, 15484, 7262, 7999, 102, 14734, 16728, 16537, 14332, 14172, 11, 65535, 261, 65530, 65534, 7262, 7999, 102, 16728, 16537, 13191, 10399, 10258, 10250, 10349, 14332, 14172, 59, 11, 50, 47, 33, 14875, 18476, 16728, 16537, 59, 5735, 7999, 102, 13091, 11533, 10339, 1219, 2191, 16728, 16537, 15543, 15052, 15973, 12276, 16689, 16664, 14734, 45, 12025, 10339, 10841, 15752, 12276, 11124, 17207, 17108, 12276, 14734, 13916, 11038, 15543, 15052, 16728, 16537, 28329, 51, 47, 33, 12202, 10997, 59, 5735, 7999, 102, 10688, 13191, 10696, 15586, 14734, 12202, 10997, 12978, 12746, 45, 11021, 10399, 13036, 10511, 11459, 15543, 10708, 11632, 15471, 15052, 15052, 12217, 28329, 52, 47, 33, 10577, 12824, 11010, 59, 5735, 7999, 102, 17379, 14589, 11533, 10339, 10577, 41, 33562, 42, 14734, 15543, 15052, 12824, 11010, 45, 11638, 17303, 10768, 12945, 10460, 17311, 17146, 11414, 13034, 10577, 15646, 17916, 10538, 12981, 14446, 13191, 10577, 10080, 17148, 13191, 10847, 10339, 11903, 14446, 15458, 10762, 13004, 12276, 11124, 12202, 10997, 15052, 12217, 28329, 53, 47, 33, 13080, 10339, 12651, 12022, 59, 5735, 7999, 102, 13091, 10846, 12396, 12651, 12022, 15261, 11496, 15304, 15500, 14734, 16728, 16537, 45, 11021, 10399, 13036, 10511, 11459, 11899, 10304, 13034, 14734, 10762, 13004, 10079, 16728, 13764, 11124, 11524, 14332, 11899, 16728, 16537, 41, 950, 77, 42, 28329, 54, 47, 33, 15066, 11899, 59, 5735, 7999, 102, 10482, 14589, 11533, 10339, 10342, 13027, 14734, 6588, 13212, 10794, 45, 13191, 10847, 10339, 15543, 10708, 15066, 11899, 14734, 12202, 10997, 15052, 12217, 28329, 55, 47, 33, 13080, 16742, 13080, 10708, 59, 5735, 7999, 102, 17379, 14589, 116, 16417, 17136, 12276, 41, 40810, 61371, 116, 42, 10460, 10292, 10397, 14875, 11533, 13205, 15489, 13250, 45, 11042, 13064, 10322, 13191, 15895, 11685, 14734, 16728, 13764, 10080, 10397, 14875, 15190, 13789, 13078, 10333, 28329, 56, 47, 33, 10355, 10342, 12276, 59, 5735, 7999, 102, 12848, 10494, 24072, 41, 24093, 46, 23613, 46, 33357, 29301, 497, 11021, 10399, 10355, 10342, 12276, 17144, 16403, 11124, 13827, 16707, 5735, 7999, 102, 10397, 14875, 45, 11584, 12299, 12266, 10997, 10452, 18183, 28329, 57, 47, 33, 10286, 11932, 14734, 13308, 10733, 12219, 59, 5735, 7999, 102, 15847, 17038, 12848, 10494, 10333, 12337, 11632, 11903, 14589, 14734, 10696, 15586, 10841, 15752, 45, 10894, 12732, 13004, 12797, 11621, 14486, 10079, 12202, 10997, 10079, 50807, 33, 15169, 28329, 58, 47, 33, 12978, 12746, 75, 1551, 59, 5735, 7999, 102, 17144, 16403, 11454, 75, 1551, 10257, 45, 11021, 10399, 13036, 10511, 11459, 10261, 29149, 33, 10355, 10342, 12202, 10788, 14589, 29149, 33, 14734, 10286, 11932, 15261, 12219, 28329, 10399, 10257, 10293, 16503, 10382, 15484, 10333, 7262, 7999, 102, 16728, 16537, 14734, 10250, 10349, 13637, 17097, 15092, 10760, 14734, 14332, 14172, 45, 12159, 13197, 15752, 10292, 12469, 12848, 10494, 10988, 15641, 10080, 11957, 7262, 7999, 102, 17147, 13191, 10409, 10454, 14638, 17728, 45, 13558, 17143, 17819, 13064, 11124, 12605, 10355, 13819, 10080, 65535, 261, 65530, 65532, 10777, 10296, 10250, 10283, 7262, 7999, 102, 14734, 10397, 14875, 10487, 11855, 520, 19589, 1511, 70, 33, 17148, 17384, 13191, 10250, 10283, 15190, 10932, 14734, 19644, 7999, 102, 33, 10397, 14875, 10487, 11855, 59, 11, 41, 2076, 38685, 46, 36017, 47, 25070, 3331, 478, 7616, 46, 34135, 494, 11, 41, 7334, 111, 38685, 46, 36017, 4373, 3331, 41, 49580, 269, 33155, 45, 37576, 5164, 42, 11, 41, 7334, 111, 280, 26199, 3331, 92, 39, 30176, 94, 3331, 41, 34550, 46, 36017, 494, 11, 17148, 13091, 10250, 10283, 15190, 10932, 14734, 35, 33155, 37576, 35, 15052, 12217, 59, 11, 46, 4695, 33, 14589, 13234, 11899, 10304, 11117, 11043, 15090, 17734, 11, 46, 21515, 111, 33, 11899, 10304, 10333, 38685, 46, 36017, 33, 10762, 13004, 45, 10482, 14589, 52562, 33, 12642, 10951, 35, 33155, 45, 37576, 363, 11, 46, 21515, 111, 280, 26199, 33, 11899, 10304, 10333, 10293, 10762, 13004, 45, 16749, 14589, 38685, 46, 36017, 33, 10762, 13004, 11, 10464, 11021, 10399, 17144, 16403, 17148, 10283, 19644, 7999, 102, 33, 15052, 12217, 59, 11, 6884, 11, 37, 21443, 7999, 102, 280, 110, 38685, 46, 36017, 47, 25070, 11, 33155, 45, 37576, 34, 11, 6884, 11, 5735, 7999, 102, 33, 13325, 12366, 14332, 12415, 11021, 16519, 59, 11, 46, 33, 10482, 14589, 14734, 13091, 350, 46, 61011, 33, 16728, 13764, 11, 46, 33, 12631, 13191, 11003, 17387, 17311, 13091, 59820, 33, 14734, 11, 46, 33, 11899, 10304, 10762, 13004, 10482, 14589, 21515, 111, 11, 46, 33, 10355, 10342, 12276, 4155, 1352, 11, 12159, 13197, 15752, 10292, 12469, 12848, 10494, 10988, 15641, 34, 65535, 261]
-> item_loss 1.781246542930603 batch_loss 1.1201091870184867
==== [65530, 65532, 10382, 15484, 7262, 7999, 102, 33, 10285, 10482, 14589, 14734, 42447, 46, 1950, 10762, 13004, 11, 65535, 261, 65530, 65534, 42447, 46, 1950, 33, 13091, 19644, 7999, 102, 33, 10285, 10250, 10283, 17916, 12182, 13191, 14589, 14734, 10762, 13004, 45, 14589, 10339, 13179, 13034, 12108, 11666, 13004, 12797, 15489, 13250, 10285, 14734, 10577, 28329, 11885, 14734, 10482, 14589, 13036, 13764, 13091, 59, 11, 41, 42447, 46, 1950, 30579, 46, 58449, 31209, 337, 273, 30176, 42, 11, 11885, 10423, 11454, 30579, 46, 58449, 33, 10285, 10482, 14589, 31209, 33, 12748, 11899, 14734, 16981, 12334, 45, 14221, 11044, 16749, 14589, 10762, 13004, 337, 45, 12202, 10427, 10673, 30176, 33, 10988, 13004, 10080, 103, 33, 14734, 10460, 14589, 13091, 13179, 13034, 16721, 16981, 12334, 12631, 11957, 12220, 14734, 10577, 28329, 10487, 11687, 59, 11, 41, 7334, 355, 4899, 98, 4899, 99, 285, 9209, 11, 11459, 13179, 13034, 12108, 11666, 14734, 333, 33, 14734, 10577, 59, 11, 41, 42447, 46, 1950, 355, 4369, 98, 293, 99, 94, 21878, 42, 3580, 4899, 98, 4899, 99, 286, 2408, 11, 11013, 11630, 10250, 10283, 10487, 11855, 59, 11, 41, 42447, 46, 1950, 355, 4369, 98, 94, 21298, 2087, 293, 100, 286, 42, 11, 746, 4899, 98, 4899, 99, 285, 45, 293, 100, 286, 2408, 11, 17148, 17384, 10482, 14589, 21298, 2087, 33, 10762, 13004, 13179, 13034, 10333, 293, 98, 22069, 45, 13920, 10842, 10333, 10250, 10283, 293, 100, 33, 11860, 13619, 28329, 12631, 10399, 12421, 15646, 16537, 10305, 45, 42447, 46, 1950, 33, 10652, 16684, 10464, 13036, 10511, 11459, 13179, 13034, 19644, 7999, 102, 33, 10285, 10409, 12522, 11626, 13215, 12231, 14734, 12108, 11666, 13004, 12797, 15489, 13250, 28329, 12159, 13197, 15752, 15494, 12469, 10250, 10283, 11638, 13445, 14734, 10333, 16533, 34, 42447, 46, 1950, 33, 10762, 13004, 11454, 11903, 17781, 12220, 14589, 10285, 17916, 12182, 11903, 14589, 10080, 65535, 261]
-> item_loss 1.9270771741867065 batch_loss 1.1453269366174936
==== [10355, 13819, 17728, 15178, 11, 65530, 65532, 12605, 12509, 10985, 10902, 10362, 13042, 13953, 19137, 10464, 14862, 17222, 10902, 10362, 13191, 10373, 10303, 13126, 14172, 11053, 19156, 11, 65535, 261, 65530, 65534, 10464, 11021, 10399, 17189, 17141, 15573, 15497, 12877, 15325, 12609, 15643, 11459, 11436, 12220, 14589, 13290, 12664, 10902, 10362, 14734, 13126, 14172, 10529, 12445, 10080, 10250, 10349, 14862, 11043, 13126, 14172, 11687, 12985, 11913, 10079, 11639, 11889, 17723, 12210, 11461, 10079, 17688, 11517, 10079, 13676, 15123, 13036, 15169, 17311, 13091, 10260, 17631, 14734, 17177, 12731, 10080, 13581, 11630, 19137, 10902, 10362, 17147, 13191, 16684, 11632, 15597, 18053, 11124, 14332, 15898, 11975, 11035, 10577, 12348, 10250, 16707, 19137, 11687, 14188, 18372, 10079, 10902, 10362, 14171, 17351, 17919, 10079, 16793, 13684, 15169, 28329, 65535, 261, 65530, 65532, 12605, 12509, 14862, 17222, 14446, 11454, 14734, 11639, 13655, 12484, 10724, 11, 65535, 261, 65530, 65534, 13326, 12797, 13188, 13034, 14734, 13655, 16803, 13004, 12797, 19137, 14446, 11454, 14734, 11639, 13655, 12484, 10724, 11416, 10292, 12469, 14734, 10447, 15586, 10260, 11042, 15646, 12268, 10080, 10444, 13091, 19137, 13326, 12797, 13655, 16803, 13004, 12797, 12848, 10494, 14734, 10529, 12445, 19137, 12469, 11021, 10399, 11638, 15850, 10333, 16533, 12307, 10808, 12631, 11454, 11459, 10912, 14734, 11639, 13655, 14349, 10724, 19151, 11, 10902, 10362, 19151, 11632, 10341, 17083, 17773, 11, 11639, 13655, 14349, 10724, 19151, 11632, 10341, 11, 13947, 12231, 19151, 46, 50, 9866, 361, 288, 9866, 11, 12469, 11021, 10399, 17189, 17141, 12220, 14589, 10696, 14734, 12387, 12802, 13036, 12276, 10769, 12791, 13655, 16803, 13004, 12797, 12848, 10494, 14734, 16523, 16525, 12986, 13258, 19137, 10399, 16056, 11001, 13179, 10842, 14781, 16520, 10733, 14914, 14734, 10529, 12445, 10080, 65535, 261]
-> item_loss 0.7310567498207092 batch_loss 1.1327732945933486
==== [25106, 11, 65530, 65531, 12605, 10402, 12266, 11726, 10250, 10283, 13034, 14734, 16713, 18025, 19137, 33, 17148, 10283, 16713, 18025, 10684, 10339, 12366, 14486, 10913, 14583, 11124, 13234, 16690, 15643, 14734, 10355, 13819, 19137, 33, 10390, 10402, 10305, 17734, 11454, 16671, 16686, 10250, 13619, 10993, 12484, 14734, 13855, 11645, 11, 65535, 261, 65530, 65532, 6909, 4423, 332, 33, 12366, 14486, 11871, 11920, 19137, 11124, 12605, 16671, 16686, 10250, 10283, 13192, 10993, 14734, 17728, 18025, 10080, 33, 16738, 10399, 10726, 17912, 10079, 14486, 12415, 11124, 11907, 16520, 14581, 15849, 13191, 14172, 10726, 17354, 14734, 18043, 13327, 13234, 16671, 16686, 19137, 12605, 10260, 17879, 16503, 13051, 15763, 14734, 11889, 12556, 11124, 13580, 15752, 17387, 14734, 16671, 16686, 19137, 12605, 17879, 16503, 10464, 12176, 10847, 12605, 13923, 13127, 14734, 11414, 18004, 11124, 10250, 10283, 10370, 14734, 10355, 12332, 15486, 10964, 28329, 65535, 261, 65530, 65534, 11685, 14734, 45, 12605, 11967, 12002, 10838, 10399, 11907, 16520, 11124, 14486, 12415, 14734, 12396, 12231, 13234, 16671, 16686, 12469, 13192, 10993, 14734, 17728, 18025, 10080, 10460, 10292, 10250, 11043, 12366, 14486, 11871, 11920, 45, 12605, 14734, 14778, 13308, 13091, 12176, 10847, 12469, 14486, 13923, 12410, 16981, 45, 12848, 10494, 10250, 10349, 10260, 11042, 14734, 16523, 16530, 45, 15646, 10260, 13091, 15494, 10760, 15190, 10932, 14734, 12261, 16676, 10080, 16738, 12847, 17162, 10250, 10258, 12469, 13192, 10993, 14734, 12484, 10724, 11124, 10684, 15304, 17728, 18025, 45, 12605, 10423, 15648, 12366, 11459, 10578, 11065, 45, 12202, 11986, 16707, 10384, 11632, 10283, 13036, 17919, 17149, 16403, 10768, 13254, 10080, 16678, 10449, 45, 13633, 10283, 10370, 11124, 13633, 13619, 10684, 15304, 17311, 13091, 14370, 10250, 13051, 10337, 14734, 10080, 12605, 10423, 11971, 17385, 12469, 13192, 10993, 14734, 12527, 11002, 45, 11042, 13064, 10322, 10423, 11687, 11903, 11459, 12848, 10760, 12605, 14734, 16520, 14172, 10080, 11687, 13258, 12469, 16525, 12348, 12605, 14734, 13279, 10349, 14791, 13764, 17141, 10339, 10293, 16520, 12609, 15643, 12484, 15509, 10901, 45, 16738, 17819, 13064, 12848, 17363, 12605, 10525, 12746, 11907, 16520, 10080, 12605, 12337, 18216, 10685, 15752, 16974, 12469, 10250, 16944, 12822, 16671, 17148, 10283, 16713, 18025, 45, 12159, 13197, 12605, 10402, 14734, 16671, 16686, 15752, 15494, 12469, 10250, 10349, 11068, 10997, 45, 12176, 10847, 12469, 13179, 11685, 11459, 12410, 15641, 11124, 11621, 14486, 17148, 13619, 10684, 15304, 28329, 65535, 261, 65530, 65532, 12605, 13191, 10250, 10283, 13192, 10993, 19137, 13186, 15486, 12605, 10402, 13091, 10782, 10285, 11042, 11871, 19137, 12307, 10782, 12605, 13091, 14474, 15451, 10285, 14734, 12143, 14583, 19137, 10390, 13091, 14474, 15451, 10285, 14734, 10421, 15169, 14583, 10080, 33, 12605, 10402, 10355, 12332, 14734, 15486, 10964, 16944, 13981, 10339, 12307, 12201, 12605, 14734, 12604, 15508, 15092, 18052, 14388, 17149, 28329, 65535, 261, 65530, 65534, 12469, 12847, 17162, 14734, 17148, 15033, 10684, 15304, 16944, 13981, 14914, 11903, 12337, 13191, 12522, 12410, 10080, 10384, 12469, 12848, 10494, 14734, 10529, 12445, 13234, 14791, 45, 17148, 13619, 10993, 16754, 12314, 12604, 14734, 11662, 13212, 13091, 12469, 14734, 12604, 15508, 13191, 10333, 18052, 16962, 12415, 14734, 17149, 13582, 45, 12282, 16944, 10333, 12307, 13064, 12604, 15508, 10421, 12268, 14734, 10390, 14734, 13773, 12522, 11124, 16928, 16917, 28329, 17148, 15033, 11533, 10339, 11871, 10323, 12604, 15508, 14734, 17152, 12824, 13036, 12276, 11454, 10782, 10285, 13064, 13199, 12202, 10260, 15576, 16519, 45, 12307, 13064, 12604, 15508, 12332, 12332, 16443, 16523, 10292, 10784, 13032, 10250, 10283, 10370, 10421, 10845, 14734, 15190, 10932, 13308, 10733, 10080, 10444, 13091, 45, 11687, 13258, 17141, 10339, 12299, 16749, 12604, 15508, 14734, 10421, 10845, 45, 15646, 12392, 16523, 10333, 13633, 10283, 11872, 11855, 14734, 10283, 12415, 10079, 10685, 16954, 11124, 10687, 10390, 15323, 16884, 45, 10779, 12337, 17843, 12261, 15123, 13913, 10673, 14734, 10993, 16754, 28329, 12605, 12509, 10333, 16533, 10250, 10258, 45, 11454, 12469, 12604, 15508, 12848, 18216, 10305, 11044, 45, 12469, 11124, 10390, 14734, 10993, 16754, 13091, 11060, 15752, 11634, 16947, 16948, 12604, 15508, 17148, 10283, 12007, 17919, 45, 11454, 11159, 13327, 10079, 14299, 11685, 10079, 10408, 10577, 16520, 15169, 13179, 13913, 12007, 13557, 10257, 16056, 12348, 16667, 11042, 11124, 17152, 12824, 64, 17148, 11957, 10339, 10250, 13619, 10993, 16754, 14734, 12746, 10301, 10997, 12022, 10917, 10768, 17385, 16503, 10080, 11687, 13258, 17141, 10768, 10495, 16922, 13279, 10250, 15033, 13231, 10407, 12314, 12604, 14734, 15671, 15489, 45, 10250, 13054, 13231, 10407, 13855, 11645, 45, 10993, 16754, 10322, 11021, 15752, 17819, 10305, 13855, 11645, 10080, 12605, 12337, 18216, 10685, 15507, 15511, 10578, 11065, 12469, 10768, 10361, 17148, 13619, 10993, 16754, 10684, 15304, 14734, 13179, 11632, 15478, 15907, 11124, 15486, 10964, 10080, 65535, 261, 65530, 65532, 12605, 16667, 10292, 11454, 11159, 13327, 10079, 14299, 11685, 10079, 10408, 10577, 16520, 12007, 13557, 10257, 13191, 10333, 10250, 10349, 17152, 12824, 19137, 10444, 13091, 12202, 13734, 13191, 16535, 10992, 12337, 13913, 14734, 12007, 17919, 10080, 33, 12746, 15511, 10917, 10745, 12201, 14734, 13192, 10993, 10684, 15304, 10285, 19137, 33, 13179, 11632, 14734, 13091, 10768, 10361, 14583, 13815, 10285, 14734, 10250, 10349, 14493, 10336, 19137, 12605, 16667, 10292, 10390, 13637, 17097, 10684, 13773, 12605, 14734, 14583, 13815, 19137, 10444, 13091, 13091, 12605, 14583, 13815, 10285, 14734, 12604, 11994, 13036, 17919, 19137, 33, 15646, 10260, 13091, 12605, 10283, 10370, 14734, 12203, 15009, 13036, 17919, 28329, 65535, 261, 65530, 65534, 12469, 12848, 10494, 14734, 16416, 10655, 10529, 12445, 16672, 12605, 11957, 17148, 13619, 10993, 16754, 13191, 10333, 13179, 13923, 13127, 14734, 10333, 16533, 10080, 14791, 13234, 11454, 12604, 15508, 10305, 11630, 45, 10464, 10402, 10322, 11454, 11159, 13327, 10079, 14299, 11685, 15169, 12007, 17919, 10257, 12261, 15123, 10333, 10250, 11899, 14734, 17152, 12824, 45, 10444, 10441, 10308, 13734, 13191, 17136, 10792, 17916, 12182, 10366, 11930, 14734, 15052, 12231, 10080, 15646, 11454, 17148, 10917, 10745, 12201, 14734, 10355, 12332, 10285, 45, 10464, 10402, 13179, 11632, 10589, 14614, 11454, 14583, 13815, 14493, 10336, 14734, 10768, 10361, 45, 17097, 11977, 16535, 10992, 13179, 13913, 12007, 13557, 14734, 16713, 18025, 28329, 12469, 12848, 10792, 10250, 10283, 10684, 17644, 14172, 45, 11994, 13091, 10390, 10441, 10308, 13179, 10684, 13773, 12469, 14734, 12604, 11994, 45, 15646, 10260, 13091, 12469, 10283, 10370, 14734, 12203, 15009, 12527, 10080, 17148, 11454, 10250, 11899, 15052, 12231, 10257, 16533, 17382, 10333, 10292, 10373, 10303, 17148, 13619, 10993, 16754, 10589, 14614, 11454, 13637, 17097, 16417, 12007, 14734, 10696, 11922, 10768, 10361, 45, 15646, 13734, 13191, 12314, 12604, 12366, 14141, 11124, 12484, 12527, 14734, 13913, 12231, 12665, 16741, 28329, 12605, 12509, 12848, 10760, 10745, 14172, 14791, 13764, 45, 13558, 17143, 12469, 14734, 10995, 18108, 59, 11, 11454, 10993, 16754, 10285, 45, 13199, 12335, 10994, 13036, 17311, 15752, 10684, 12366, 11957, 13036, 14734, 14583, 13815, 16884, 17387, 11124, 12203, 15009, 12527, 45, 15646, 10260, 10377, 10377, 13091, 12604, 11994, 11124, 12604, 10841, 45, 17148, 13091, 13179, 10590, 12237, 14734, 10684, 15304, 28329, 12469, 13091, 11060, 10322, 11021, 10399, 13179, 10293, 10846, 11459, 10768, 10361, 15847, 12144, 14734, 12484, 12527, 10267, 14610, 45, 16417, 17136, 11957, 10390, 14734, 10684, 12366, 45, 15646, 10260, 10377, 13091, 12604, 11994, 10257, 14734, 10355, 13819, 64, 17148, 11021, 15752, 12176, 10847, 12705, 17145, 10994, 13036, 16973, 15023, 28329, 11687, 13258, 17688, 13199, 12527, 16525, 17148, 13619, 10993, 16754, 10589, 14012, 10260, 10808, 45, 12469, 10322, 16684, 17879, 16503, 10995, 12410, 17148, 13619, 10684, 15304, 17147, 15752, 11060, 14015, 16956, 12469, 14734, 17879, 13685, 10080, 10293, 10846, 16753, 16753, 12469, 14734, 12509, 13764, 10322, 16684, 10423, 13191, 12176, 10847, 28329, 12159, 13197, 17148, 10349, 12261, 16676, 15752, 15494, 12469, 12848, 10494, 10250, 10349, 10988, 15641, 10080, 12605, 12337, 13559, 16917, 12469, 12537, 12522, 11906, 16523, 17148, 13619, 10993, 16754, 10684, 15304, 45, 12202, 16672, 11885, 11003, 12348, 13179, 10842, 10286, 11932, 13191, 12522, 10304, 10080, 17148, 11994, 13091, 12604, 14256, 10993, 16754, 14734, 17385, 16503, 10452, 14446, 10080, 65535, 261, 65530, 65532, 12605, 16667, 10292, 11454, 12605, 11124, 10390, 14734, 10684, 15304, 12307, 10285, 19137, 12605, 12025, 10339, 10293, 10846, 10768, 10361, 12484, 12527, 10267, 14610, 11124, 14583, 13815, 10285, 12387, 10310, 14734, 10370, 10080, 10390, 11533, 13205, 10257, 10260, 10423, 10768, 10361, 14794, 11903, 14734, 12484, 12527, 10267, 14610, 10080, 33, 12605, 11454, 14042, 17688, 14734, 10993, 12484, 15486, 10964, 10285, 11414, 18004, 19137, 10997, 14446, 10390, 10687, 11903, 10250, 14781, 10441, 10308, 17820, 16223, 11454, 13279, 10283, 17919, 10688, 10258, 28329, 65535, 261, 65530, 65534, 12337, 12527, 16774, 12469, 12848, 10494, 13179, 11632, 15478, 15907, 10080, 14446, 11454, 12605, 11957, 17148, 13619, 10993, 16754, 14734, 10342, 10846, 13499, 12276, 13191, 10333, 13179, 13923, 13127, 14734, 16667, 16696, 28329, 10384, 12469, 14734, 12847, 17162, 13234, 14791, 45, 11454, 17148, 13619, 10684, 15304, 10285, 45, 12469, 13637, 17097, 10293, 10846, 11459, 10768, 10361, 15847, 12144, 14734, 12484, 12527, 11124, 14583, 13815, 12387, 10310, 45, 15646, 10390, 12337, 11977, 12022, 14966, 14794, 11903, 14734, 10696, 12366, 10267, 14610, 45, 10441, 10308, 12421, 12624, 14808, 10250, 10283, 17919, 10688, 10080, 17148, 15033, 17688, 13199, 14734, 10260, 11957, 15169, 13091, 10260, 10590, 12237, 14734, 28329, 12605, 12509, 12848, 10760, 10250, 10349, 12261, 16676, 59, 11, 50, 47, 10261, 10390, 11483, 16711, 11459, 13733, 17189, 12469, 14734, 12527, 11002, 45, 16735, 12469, 12159, 13197, 17148, 13619, 10993, 16754, 15752, 11634, 13179, 10842, 14794, 16711, 10079, 12266, 12983, 10080, 17728, 17728, 10390, 17820, 16223, 15847, 12605, 14734, 10974, 11416, 45, 16672, 10390, 12522, 16696, 10792, 12469, 10260, 13091, 16417, 17919, 14734, 13192, 10993, 28329, 51, 47, 15494, 10390, 13064, 17734, 12642, 12266, 12366, 12635, 45, 10444, 10260, 15752, 12299, 13685, 10080, 10261, 13581, 11042, 13064, 45, 10525, 12686, 11685, 15847, 12144, 45, 10260, 16503, 17141, 12231, 12674, 10673, 10250, 10283, 10260, 11414, 12687, 14734, 10684, 15304, 28329, 52, 47, 11687, 13258, 10390, 13051, 13764, 11003, 10901, 45, 12469, 11021, 15752, 17879, 16503, 17385, 13034, 16694, 10435, 17148, 13619, 10993, 16754, 11957, 12469, 14734, 12522, 10304, 45, 10260, 16503, 14614, 12428, 10250, 15033, 14486, 12509, 10901, 14734, 10993, 16754, 28329, 53, 47, 11454, 10687, 10390, 10684, 15304, 10285, 12664, 10792, 16056, 12348, 12484, 12527, 10355, 13819, 11124, 12978, 12746, 14734, 10760, 11010, 10080, 10260, 16503, 11928, 12159, 13197, 10339, 10250, 10283, 10370, 28329, 12527, 16774, 12469, 10768, 10361, 17148, 13619, 11626, 13215, 14734, 10684, 15304, 10964, 15052, 10080, 12605, 16424, 12366, 12159, 13197, 12469, 15752, 12664, 10792, 10342, 14782, 12978, 12746, 10079, 14794, 13580, 10529, 10409, 14734, 10993, 16754, 10684, 15304, 10080, 17148, 17879, 16503, 13064, 17734, 45, 10444, 10464, 10577, 12348, 12727, 13191, 10080, 11687, 13258, 13191, 10409, 10454, 10687, 10390, 12509, 13764, 13558, 17143, 15507, 15511, 16671, 16686, 10080, 65535, 261, 65530, 65532, 12605, 12159, 13197, 10464, 15752, 13179, 10842, 13913, 12231, 14734, 10384, 10370, 13327, 14486, 16686, 16530, 12231, 16671, 16686, 17148, 15033, 10684, 15304, 28329, 65535, 261, 65530, 65534, 11685, 14734, 45, 12605, 10423, 11986, 16707, 10384, 10370, 13327, 14486, 16686, 14734, 16523, 16530, 45, 11957, 17148, 15033, 10684, 15304, 13499, 12276, 17149, 16403, 13179, 13913, 10673, 14734, 10768, 13254, 28329, 18117, 10658, 45, 10384, 12469, 13192, 10993, 14734, 14332, 12333, 13234, 14791, 45, 10390, 10441, 10308, 13191, 14808, 10696, 11047, 10079, 13773, 17385, 17820, 16223, 15847, 12605, 14734, 10370, 13327, 10578, 11047, 10080, 13326, 12797, 10370, 13327, 14486, 16686, 45, 17148, 11021, 15752, 13091, 10390, 15847, 17038, 14734, 12415, 13327, 14332, 12333, 45, 10322, 11021, 15752, 13091, 11872, 12848, 13064, 13199, 14734, 14583, 13815, 15486, 10964, 11960, 15850, 14734, 10370, 13327, 17172, 12220, 28329, 10687, 13557, 45, 11454, 10464, 10402, 14734, 14782, 11621, 10285, 45, 12469, 16417, 14446, 10760, 10333, 13179, 11630, 11047, 10079, 10293, 10846, 16417, 17136, 14734, 10370, 13327, 14332, 16884, 10080, 13326, 12797, 10370, 13327, 10911, 17332, 14486, 16686, 45, 17148, 10278, 15033, 10260, 11042, 15261, 11496, 14734, 10370, 11687, 13258, 16503, 10997, 12022, 15895, 11685, 14734, 10684, 15304, 45, 10994, 13036, 17311, 17879, 16503, 10588, 10760, 16749, 13006, 17172, 12220, 11957, 13036, 45, 15646, 10260, 15752, 10932, 15458, 13199, 12335, 11957, 13036, 12981, 11003, 28329, 15162, 10256, 45, 10384, 15287, 14987, 10768, 13254, 16530, 12231, 14791, 45, 10390, 17820, 16223, 15847, 12605, 11021, 15752, 13091, 10760, 10339, 13279, 10349, 10696, 12366, 14734, 14216, 16253, 12609, 17771, 12354, 13212, 10794, 10080, 11687, 13258, 15752, 16672, 10390, 12527, 10792, 11889, 10675, 11124, 16443, 12824, 15462, 45, 10390, 11021, 15752, 17182, 13932, 12022, 14446, 14794, 11903, 15847, 12605, 10080, 17148, 17384, 17879, 16503, 12469, 16417, 14446, 10760, 15648, 12366, 11124, 13145, 12522, 45, 15646, 17916, 12299, 13685, 28329, 13188, 11044, 45, 11687, 13258, 17688, 13199, 13051, 13764, 12261, 15123, 10342, 14782, 10529, 10409, 45, 12824, 15462, 30719, 39193, 14734, 10684, 15304, 45, 10322, 16684, 16735, 13078, 10464, 10402, 14734, 10370, 13327, 10992, 17879, 13685, 11861, 11454, 10250, 11899, 14734, 8176, 35047, 10080, 10525, 12746, 17172, 12307, 16973, 15023, 45, 15646, 10260, 13091, 10864, 12299, 15516, 15304, 45, 11021, 15752, 13091, 13179, 10590, 12237, 14734, 17177, 12731, 28329, 12421, 10305, 45, 17148, 17879, 16503, 10994, 13036, 10682, 11042, 10848, 10838, 11124, 13064, 17734, 11532, 10691, 10080, 17385, 16503, 14734, 13091, 16503, 14914, 11899, 17148, 15033, 10993, 16754, 13091, 11060, 10383, 14221, 13191, 12522, 10304, 11124, 12159, 13197, 45, 11687, 13258, 12527, 10792, 13855, 13249, 45, 12983, 12636, 10322, 13203, 11986, 10260, 11021, 10080, 16738, 15507, 15511, 10768, 10361, 12469, 14734, 12509, 13764, 10080, 65535, 261, 65530, 65532, 16416, 10655, 10250, 10349, 10529, 12445, 19137, 11454, 13188, 17145, 14734, 10745, 12201, 19137, 12605, 14734, 14583, 13815, 10285, 10760, 14446, 10333, 10250, 10349, 18052, 16962, 12415, 14734, 12604, 11994, 19137, 33, 12307, 12605, 12266, 12366, 14734, 10261, 10390, 10768, 10361, 13064, 19137, 10997, 14446, 10390, 10250, 14781, 11621, 10339, 10250, 15033, 11414, 17248, 11124, 15847, 12605, 17771, 12354, 14734, 14349, 12396, 28329, 65535, 261, 65530, 65534, 12527, 16774, 12469, 12848, 10494, 14734, 18030, 11630, 10529, 12445, 10080, 17148, 10250, 16416, 10655, 15478, 15907, 12176, 10847, 12605, 13179, 10733, 14914, 11459, 10768, 13254, 17148, 13619, 10993, 16754, 10846, 12396, 28329, 10384, 12469, 13188, 17145, 11001, 12348, 12604, 11994, 13064, 10390, 14734, 11414, 17248, 11124, 15847, 12605, 17771, 12354, 10995, 12220, 13234, 14791, 45, 17148, 11021, 15752, 10995, 13086, 10760, 10399, 10258, 10745, 10283, 17728, 18025, 59, 11, 10390, 15847, 17038, 11454, 10464, 10402, 10684, 15304, 10285, 10250, 14781, 11621, 10339, 10421, 16948, 12527, 11124, 12825, 10794, 12527, 17097, 12299, 14734, 10250, 13036, 45, 14446, 11454, 17148, 15033, 10846, 12396, 16443, 12642, 14894, 45, 10398, 10390, 12527, 10792, 10260, 17172, 12220, 28329, 10390, 11021, 15752, 11861, 11454, 10250, 11899, 15052, 12231, 14734, 15129, 10335, 12366, 14486, 45, 12631, 10399, 12469, 14734, 12604, 10841, 16535, 10997, 10333, 10390, 14734, 10260, 11889, 10675, 12527, 28329, 11454, 15847, 12605, 16667, 14862, 10257, 45, 10390, 17141, 10339, 10495, 16922, 10339, 12604, 15508, 11124, 12604, 11994, 13234, 11899, 10304, 15847, 12605, 10408, 10577, 45, 12631, 10399, 53983, 14734, 12604, 10841, 10397, 16417, 10333, 11957, 10390, 14734, 7428, 14734, 12642, 10761, 28329, 11687, 13258, 10390, 17688, 13199, 13051, 13764, 10292, 12469, 14734, 12604, 10841, 12527, 10792, 14794, 16711, 14734, 18216, 10685, 45, 11016, 10423, 11414, 17248, 12609, 15847, 12605, 17771, 12354, 45, 17148, 12430, 12405, 10397, 16417, 14808, 10390, 12202, 10260, 11634, 11454, 12522, 12469, 14734, 10696, 12366, 10267, 14610, 28329, 15523, 10257, 12631, 17162, 45, 17148, 15033, 11003, 10901, 10322, 16684, 13580, 11454, 16535, 10846, 17148, 13619, 10993, 16754, 10684, 15304, 14734, 13326, 11533, 10080, 12605, 12261, 16676, 12469, 11021, 10399, 11986, 16707, 10261, 10390, 12266, 16711, 12155, 10678, 11459, 16671, 16686, 11003, 10901, 45, 10444, 16503, 10292, 10409, 10454, 15489, 13258, 10588, 11685, 10733, 11622, 10080, 12746, 15511, 12674, 10673, 10250, 13619, 10260, 12200, 16412, 14734, 10684, 15304, 13052, 16899, 13064, 17734, 10322, 16899, 15287, 10838, 10080, 12159, 13197, 17148, 10349, 12261, 16676, 15752, 15494, 12469, 12172, 13234, 10250, 10349, 12176, 10847, 45, 10322, 13558, 17143, 12469, 12848, 10760, 15847, 12144, 14734, 12509, 13764, 10080, 65535, 261, 65530, 65532, 12337, 12527, 16774, 10464, 10726, 17912, 14734, 10768, 13254, 19137, 33, 12605, 12145, 15486, 12522, 16696, 10792, 17148, 13619, 10993, 12484, 11967, 16503, 14894, 14924, 10080, 33, 12605, 10402, 14734, 10355, 13819, 16672, 12605, 15752, 13179, 10842, 11922, 13080, 14734, 12824, 11002, 17148, 10250, 14172, 10080, 33, 12605, 11454, 11414, 18004, 12307, 12201, 14734, 15486, 10964, 13064, 19137, 14734, 14914, 10997, 14446, 15847, 12144, 14734, 14970, 10423, 16694, 10408, 13637, 10390, 10448, 12337, 11632, 10080, 33, 15646, 17145, 12201, 14734, 12604, 11994, 14734, 14914, 13191, 12337, 11632, 15092, 14894, 28329, 65535, 261, 65530, 65534, 12469, 14734, 10995, 18108, 16672, 12605, 12527, 10792, 13559, 12556, 10080, 15752, 11634, 10726, 17912, 14486, 12415, 11459, 10768, 13254, 10250, 13619, 10684, 15304, 14734, 11003, 10901, 45, 12202, 10588, 11685, 12366, 14486, 10733, 11622, 12824, 11002, 10687, 15489, 12005, 45, 17879, 16503, 13133, 12551, 11124, 10863, 13655, 10080, 12605, 12337, 18216, 10685, 12605, 10402, 14734, 16671, 16686, 15752, 11634, 12176, 10847, 12469, 17189, 17141, 11414, 18004, 12332, 10336, 45, 11906, 16523, 14446, 14349, 45, 10384, 15646, 13179, 11685, 11459, 12824, 15462, 17148, 10250, 14446, 11903, 28329, 10384, 12469, 12848, 10494, 14734, 10964, 15052, 13234, 14791, 45, 17148, 13619, 10993, 16754, 11454, 17141, 10985, 14914, 11903, 12261, 15123, 11454, 10250, 11899, 14734, 14970, 10423, 16694, 10408, 11533, 14902, 10305, 10257, 45, 14446, 11454, 17148, 10250, 11533, 14902, 10997, 14583, 10333, 11003, 10901, 45, 12368, 14221, 10423, 11957, 10684, 15304, 10358, 14583, 10721, 10761, 10080, 17148, 15033, 10721, 15092, 11454, 12337, 11632, 17688, 13199, 10993, 16754, 10285, 17311, 11021, 15752, 10997, 14583, 45, 10684, 17644, 13091, 11687, 10454, 10901, 16533, 11124, 17919, 11957, 28329, 12605, 12261, 16676, 12469, 11021, 10399, 17177, 12731, 10261, 10390, 12329, 12221, 17382, 12395, 45, 10322, 11021, 10399, 14614, 10258, 10250, 14172, 10993, 16754, 14734, 10457, 11459, 10080, 17385, 16503, 14734, 13091, 10260, 16503, 55099, 7946, 33, 10390, 14734, 11414, 17248, 10460, 10292, 11957, 12469, 15847, 17038, 10408, 10577, 14734, 11060, 11899, 10080, 12469, 14734, 12604, 11994, 12022, 14446, 10333, 15847, 12144, 14734, 10848, 10838, 45, 12220, 12307, 14597, 16424, 11459, 12527, 10792, 18176, 10613, 11124, 12266, 12366, 28329, 12159, 13197, 12469, 11021, 10399, 15043, 13249, 12674, 10673, 10687, 10390, 14970, 10355, 11441, 11855, 45, 12664, 10792, 13179, 10590, 12237, 33078, 14734, 10370, 17781, 10684, 15304, 10080, 12469, 14734, 12604, 17688, 12220, 16721, 13091, 16443, 13559, 16917, 11124, 18458, 10851, 14734, 45, 15646, 17916, 16443, 11060, 11899, 10080, 12527, 16774, 12469, 10768, 10361, 17148, 10250, 11626, 13215, 17141, 15052, 45, 14986, 12469, 15507, 15511, 10808, 16403, 45, 10997, 14446, 14583, 13815, 14734, 15597, 11685, 34, 65535, 261, 65530, 65532, 12307, 10250, 10283, 10370, 10260, 13032, 14734, 12604, 17688, 13064, 19137, 13091, 11060, 10423, 17919, 11957, 13179, 11632, 14734, 17148, 15261, 10684, 15304, 14894, 14924, 10080, 33, 11454, 17141, 10985, 14734, 10337, 10256, 10917, 12201, 14734, 15486, 10964, 10285, 19137, 12605, 10997, 14446, 12605, 14734, 13192, 10993, 16948, 13234, 16948, 11977, 10080, 33, 17819, 14808, 10283, 10370, 17149, 13582, 14734, 14914, 10423, 16667, 16696, 13179, 11632, 14734, 13192, 10993, 10080, 10444, 13091, 13055, 14734, 13192, 10993, 14734, 11645, 10985, 14734, 14914, 17147, 13091, 10423, 16672, 10370, 12527, 10429, 28329, 65535, 261, 65530, 65534, 12469, 12848, 10760, 10333, 10250, 10283, 17916, 12182, 17385, 16503, 14734, 17728, 18025, 45, 10283, 10370, 12604, 17688, 11124, 13055, 10684, 15304, 14894, 16449, 10305, 17734, 14734, 10684, 15304, 10080, 17148, 13091, 10250, 10283, 10577, 12348, 13913, 12410, 14734, 16713, 18025, 28329, 12469, 14734, 12527, 11002, 17916, 12182, 14486, 16533, 11124, 10682, 18365, 10080, 12307, 10250, 10283, 10370, 10260, 13032, 12604, 17688, 11124, 11003, 10901, 13064, 45, 10261, 17141, 10985, 12261, 15123, 14734, 10250, 10349, 10684, 15304, 14914, 11903, 17919, 10288, 14808, 12752, 12610, 11124, 16749, 13006, 10080, 17148, 13091, 12337, 11632, 10370, 11454, 12604, 17688, 17141, 15052, 10285, 17311, 10423, 15486, 10964, 14734, 28329, 10250, 10349, 14791, 13764, 10494, 12469, 10988, 15641, 59, 11, 10283, 10370, 12604, 17688, 13036, 11047, 10261, 13055, 10993, 12604, 17688, 13036, 11047, 10260, 11042, 45, 14486, 12389, 14782, 12138, 45, 13091, 12337, 11632, 10684, 15304, 11003, 17137, 14734, 10974, 11416, 10080, 12824, 11002, 17148, 15033, 10768, 13584, 12202, 10260, 13091, 10409, 10454, 10250, 13036, 14734, 17631, 28329, 10261, 13055, 10993, 14894, 16449, 45, 13179, 11632, 13091, 10397, 16417, 12469, 17149, 10673, 10333, 10250, 10283, 13034, 14734, 10370, 14583, 17775, 13619, 10080, 17148, 13091, 12604, 17688, 12368, 15486, 14734, 14658, 14673, 10080, 14791, 12266, 10250, 10349, 45, 10423, 10739, 17093, 10429, 14673, 28329, 17149, 13582, 12604, 17688, 14734, 11042, 13064, 45, 10322, 16503, 10525, 13191, 10250, 11899, 16778, 17178, 10079, 11042, 14486, 12366, 10261, 13055, 10993, 10080, 10260, 16503, 11416, 12604, 10841, 15646, 17093, 16523, 17141, 10985, 14734, 10684, 15304, 28329, 12604, 17688, 16981, 10257, 45, 12605, 10402, 17879, 16503, 11454, 13034, 14734, 17775, 13619, 12664, 10792, 13034, 14734, 10422, 10437, 10250, 16981, 11042, 16403, 10080, 17148, 13091, 23353, 5447, 69, 12261, 15123, 13034, 14734, 12978, 12746, 15573, 15497, 28329, 10250, 10349, 10577, 12348, 14460, 12493, 14734, 15640, 13192, 10993, 15957, 15752, 14486, 16533, 12824, 15462, 12605, 10402, 14734, 12604, 17688, 45, 10495, 14221, 11021, 10399, 14614, 11454, 14583, 11117, 10285, 10080, 10444, 10260, 12368, 10864, 12299, 28329, 12421, 10305, 45, 11003, 17137, 13091, 14583, 11117, 14734, 12182, 12396, 10080, 11871, 10423, 13559, 14221, 12824, 11002, 17148, 10250, 14172, 45, 14486, 16533, 11003, 17137, 15727, 11044, 14734, 10974, 11416, 45, 10525, 12746, 16778, 17178, 11255, 12522, 45, 12605, 14782, 10529, 12469, 10423, 11003, 12348, 13179, 10842, 11474, 12299, 10261, 14829, 13133, 10080, 14986, 12469, 12604, 17688, 14734, 16981, 10257, 10260, 10309, 14862, 12144, 14782, 10437, 10080, 65535, 261, 65530, 65532, 12605, 11454, 12509, 10250, 10283, 17728, 18025, 19137, 33, 12307, 12605, 16056, 12348, 12604, 15508, 13064, 19137, 12266, 12366, 14734, 10261, 13192, 10993, 10768, 10361, 14734, 13064, 10560, 19137, 11454, 17148, 10283, 12484, 10724, 10258, 11957, 12605, 12266, 11726, 14637, 17150, 14734, 13192, 10993, 19137, 33, 14794, 14734, 15752, 10460, 10292, 13192, 10993, 11053, 19156, 11, 65535, 261, 65530, 65534, 12469, 12848, 10760, 10333, 10250, 10283, 17916, 12182, 12990, 17625, 14734, 17728, 18025, 10080, 12307, 10250, 10283, 10370, 11454, 17385, 16503, 13064, 10799, 17177, 12731, 11414, 17248, 12609, 14637, 17150, 45, 15646, 17916, 12527, 11042, 17038, 11002, 45, 17148, 14914, 11903, 10423, 16672, 10370, 16884, 14638, 17148, 15033, 10993, 16754, 14734, 12522, 10304, 28329, 12605, 14734, 10250, 10349, 12509, 13764, 59, 11, 11454, 13192, 10993, 16056, 15735, 12609, 12604, 10841, 13064, 14637, 17150, 10390, 10370, 45, 17148, 15033, 16403, 10292, 10995, 13086, 14734, 13091, 10696, 12366, 14734, 13637, 17097, 11124, 15129, 10335, 12522, 16696, 45, 10261, 14794, 16711, 14734, 10993, 16754, 13191, 10250, 11899, 12143, 16973, 28329, 14794, 13580, 14734, 13192, 10993, 12220, 16721, 15752, 11454, 10790, 10370, 12604, 10841, 13064, 16417, 14446, 10760, 14794, 12366, 14734, 18216, 10685, 11124, 18458, 10851, 45, 15646, 10260, 13091, 11753, 15723, 12527, 28329, 10322, 16684, 12605, 10402, 17879, 16503, 10995, 12410, 45, 13091, 10260, 13091, 17141, 10339, 11454, 10993, 16754, 10285, 11959, 13685, 16256, 16020, 12366, 14734, 14015, 16956, 45, 15646, 12392, 14619, 10333, 12978, 12746, 11124, 17802, 10437, 14734, 13205, 16884, 28329, 11042, 13064, 16503, 14486, 16533, 45, 13191, 10349, 13192, 10993, 11021, 15752, 11454, 15847, 12605, 16667, 14862, 10257, 11861, 11454, 17728, 18025, 45, 17843, 10399, 17919, 11957, 10390, 10370, 14734, 17149, 13582, 10080, 17148, 17879, 16503, 13064, 17734, 10985, 16749, 17172, 28329, 11687, 13258, 17148, 15033, 14637, 17150, 12746, 15511, 10997, 14583, 45, 12609, 16684, 17879, 17385, 13034, 16694, 10435, 17148, 15033, 10684, 15304, 14734, 12522, 10304, 10080, 10993, 16754, 13188, 11902, 16896, 10305, 11621, 11454, 10339, 10696, 12366, 14734, 12999, 12266, 11124, 10342, 10847, 28329, 12421, 10305, 45, 10590, 12237, 14734, 10993, 16754, 10260, 12220, 12261, 15123, 11454, 13637, 17097, 10285, 45, 14794, 16711, 11268, 12468, 10390, 10370, 12604, 17688, 12638, 15752, 16672, 10993, 16754, 17688, 11861, 10080, 17148, 17879, 16503, 10994, 13036, 10682, 11042, 10848, 10838, 10080, 12159, 13197, 17148, 10349, 12261, 16676, 11957, 12469, 13191, 12176, 10847, 45, 10322, 13558, 17143, 12469, 17149, 10250, 13582, 10768, 10361, 12509, 13764, 10080, 65535, 261, 65530, 65532, 17148, 13091, 12337, 14486, 12509, 14734, 19137, 10444, 13091, 14446, 11903, 15646, 10990, 15712, 15761, 14734, 10267, 14610, 17384, 19137, 33, 13192, 10993, 10684, 15304, 10285, 12421, 13091, 12837, 13215, 14808, 15129, 10335, 11124, 17097, 10852, 11, 65535, 261, 65530, 65534, 12469, 12848, 10792, 14734, 14446, 11903, 10267, 14610, 10285, 10993, 16754, 10684, 15304, 17843, 10662, 12837, 13215, 15129, 10335, 11124, 17097, 10852, 14734, 17728, 18025, 45, 12605, 17916, 12182, 16667, 11042, 10080, 14734, 14914, 45, 11454, 10250, 10283, 13773, 17385, 14970, 10423, 13637, 17097, 11124, 10283, 10370, 12604, 10841, 14734, 14970, 10423, 17384, 45, 17148, 15033, 14446, 16803, 10917, 10768, 13125, 17216, 28329, 10444, 13091, 45, 12605, 17147, 13091, 12509, 11994, 13581, 10768, 10361, 10250, 10349, 18030, 11630, 12509, 13764, 59, 11, 50, 47, 33, 15129, 10335, 11124, 17097, 10852, 10305, 12366, 13051, 11021, 17248, 10662, 45, 10444, 12605, 10402, 11021, 10399, 17177, 12731, 10293, 10846, 17788, 10448, 17148, 15033, 12484, 15509, 45, 12261, 15123, 13179, 10590, 12237, 12200, 11124, 14734, 10684, 15304, 28329, 51, 47, 33, 14794, 13580, 14734, 10993, 16754, 45, 12220, 16721, 12261, 15123, 11454, 10342, 14782, 11971, 17385, 10079, 12978, 12746, 11124, 12527, 14091, 14734, 11533, 14902, 10257, 45, 15646, 10260, 13091, 13637, 18216, 10448, 28329, 52, 47, 33, 11967, 13773, 12522, 10838, 12983, 11454, 15847, 12144, 10696, 12366, 12604, 17688, 11124, 15494, 10334, 12978, 12746, 10257, 45, 15646, 10260, 13091, 10261, 10370, 13637, 17097, 45, 11021, 10399, 10739, 17093, 17148, 15033, 13051, 16763, 14734, 17097, 10852, 28329, 53, 47, 33, 12605, 10402, 17311, 11002, 14970, 10423, 14445, 11574, 11124, 13012, 10901, 10408, 10577, 16520, 12325, 11168, 45, 10444, 10383, 11021, 10399, 10848, 10838, 10293, 10846, 12261, 15123, 13179, 13580, 17919, 12266, 13078, 14734, 10684, 15304, 28329, 54, 47, 33, 10684, 13773, 10696, 12366, 14370, 15123, 11124, 11889, 11884, 45, 15646, 10260, 13091, 11630, 11454, 12604, 10841, 45, 11021, 10399, 17248, 10662, 16684, 11632, 17097, 10852, 12366, 14486, 28329, 14914, 11903, 45, 16503, 10588, 10792, 17148, 10250, 14172, 12202, 10260, 11922, 13080, 10080, 10444, 12605, 14782, 10529, 10250, 10283, 12522, 16696, 10792, 17728, 18025, 10079, 12202, 10848, 10838, 16939, 11047, 13580, 17919, 11003, 10901, 14734, 10370, 45, 15480, 11967, 15752, 15486, 10964, 10684, 15304, 14734, 12604, 17688, 10261, 10920, 10925, 10080, 17148, 17879, 16503, 11474, 12746, 10261, 10863, 13655, 45, 10322, 17879, 16503, 12664, 10792, 10261, 12605, 10402, 12377, 11042, 17222, 11038, 14734, 14862, 12144, 10080, 12537, 12469, 15752, 11454, 12822, 15325, 14734, 16981, 10257, 16056, 12348, 12978, 12746, 10080, 65535, 261, 65530, 65534, 10464, 16735, 14734, 12337, 13191, 17222, 14486, 10080, 15129, 10335, 11124, 17097, 10852, 13091, 10370, 17781, 10684, 15304, 10285, 14734, 10250, 10283, 17843, 18025, 10080, 13191, 10745, 14172, 16416, 10655, 59, 11, 50, 42, 33, 15129, 10335, 12366, 14486, 12337, 13580, 12182, 45, 10444, 16503, 10848, 10838, 15516, 12746, 12200, 16412, 10080, 17172, 12231, 14734, 15129, 10335, 15752, 14091, 10997, 17149, 11001, 12366, 45, 17141, 11632, 10779, 10423, 14894, 11470, 10993, 16754, 28329, 51, 42, 33, 14794, 13580, 14734, 13192, 10993, 12220, 16721, 13091, 10342, 14782, 12978, 12746, 11124, 12330, 13581, 12604, 17688, 10080, 15646, 17916, 14331, 12657, 10260, 13923, 16747, 18216, 16747, 10448, 28329, 52, 42, 33, 11967, 13773, 12522, 10838, 12983, 11454, 15847, 12144, 12604, 17688, 10257, 45, 10739, 11977, 10261, 10370, 13637, 17097, 10080, 13179, 11923, 11922, 12266, 13078, 11459, 14791, 12335, 10790, 10370, 28329, 53, 42, 33, 14970, 10423, 13012, 10901, 13660, 11430, 10322, 10423, 12325, 11168, 12605, 10402, 10080, 10444, 13633, 10283, 10370, 17311, 15752, 10293, 10846, 10848, 10838, 12981, 11003, 28329, 54, 42, 33, 10684, 13773, 10696, 11454, 12200, 17912, 45, 15646, 17916, 11630, 11454, 12604, 10841, 10080, 11021, 10399, 17788, 10448, 17097, 10852, 14734, 12366, 14486, 28329, 12421, 14734, 13234, 16735, 45, 16503, 12981, 11003, 12421, 10260, 13091, 10250, 17028, 15646, 11994, 10080, 17879, 16503, 10260, 13032, 10848, 10838, 11124, 10538, 13580, 15847, 12144, 10080, 10444, 11016, 16503, 13191, 16519, 16696, 11124, 10863, 13655, 45, 12605, 10402, 14734, 10370, 17781, 10684, 15304, 15480, 11967, 12604, 17688, 11124, 10920, 10925, 28329, 17385, 14172, 11454, 10339, 11474, 12746, 10080, 12664, 10792, 12377, 11042, 17222, 11038, 14734, 13192, 10993, 45, 16056, 12348, 12978, 12746, 11124, 14091, 10851, 10080, 10250, 14172, 10250, 14025, 11459, 13198, 14808, 13179, 11124, 14821, 17149, 13582, 14734, 13036, 11047, 17142, 17149, 11994, 11685, 28329, 12605, 14782, 10529, 10464, 10322, 15752, 10588, 10792, 10080, 10842, 13750, 34, 65535, 261, 65530, 65534, 11889, 13849, 12705, 11, 10684, 10339, 10370, 17781, 10684, 15304, 10285, 14734, 15129, 10335, 11124, 17097, 10852, 12366, 14486, 45, 10399, 10992, 11687, 10454, 17788, 10448, 17097, 10852, 12366, 14486, 14734, 12325, 11168, 45, 10399, 10992, 12261, 15123, 13179, 10590, 12237, 12200, 16412, 14734, 10684, 15304, 10080, 10293, 16503, 16520, 14172, 13091, 19137, 11454, 10250, 10283, 10993, 16754, 10684, 15304, 10285, 19137, 15129, 10335, 11124, 17097, 10852, 12366, 14486, 13091, 12182, 16519, 14734, 17728, 18025, 19137, 10444, 12605, 10402, 11021, 10399, 17189, 17141, 10293, 10846, 17788, 10448, 17097, 10852, 12366, 14486, 10079, 12261, 15123, 13179, 10590, 12237, 12200, 15169, 14734, 10684, 15304, 13234, 12981, 11255, 17148, 15033, 14446, 16803, 10080, 11042, 13064, 19137, 12605, 10402, 10322, 12220, 16721, 13773, 12522, 10792, 15847, 12144, 10696, 12366, 12604, 17688, 10261, 10261, 10390, 10370, 14782, 10342, 12325, 11168, 14734, 17728, 18025, 19137, 12202, 11959, 13685, 12978, 12746, 11124, 12176, 10847, 13234, 10660, 13193, 17148, 10349, 17728, 18025, 10080, 13188, 11044, 45, 12605, 10402, 12220, 16721, 10525, 12746, 12266, 13078, 11124, 11971, 17385, 10390, 10370, 14734, 12396, 12231, 45, 10399, 12261, 15123, 13179, 10590, 12237, 10079, 13179, 13191, 14757, 14734, 10684, 15304, 28329, 50, 47, 33, 10684, 10339, 13192, 10993, 10684, 15304, 10285, 14734, 15129, 10335, 11124, 17097, 10852, 12366, 14486, 59, 11, 51, 47, 33, 15129, 10335, 13091, 10370, 17781, 10684, 15304, 10285, 12182, 16519, 14734, 17728, 18025, 10080, 12307, 10278, 10283, 10370, 10342, 14782, 15129, 10335, 13064, 19137, 11021, 15752, 10423, 11960, 15850, 10260, 12368, 16503, 14734, 15326, 12287, 13655, 13660, 11124, 10260, 15895, 12325, 11168, 10080, 11416, 13581, 19137, 12605, 10402, 12220, 16721, 17248, 10662, 17148, 15033, 12484, 10724, 10997, 14583, 19137, 12202, 11959, 12664, 10682, 11042, 14172, 13234, 12261, 15123, 13179, 10590, 12237, 10079, 12200, 15169, 11124, 13191, 14757, 14734, 10684, 15304, 28329, 52, 47, 33, 17097, 10852, 12366, 14486, 11021, 10399, 13091, 10250, 15033, 12182, 16519, 17728, 18025, 10080, 12307, 10278, 10283, 10370, 11454, 10250, 16944, 13064, 19137, 11922, 13080, 10358, 14583, 10768, 13584, 10079, 10335, 11073, 12609, 15643, 10260, 12519, 12387, 14734, 15486, 10964, 10080, 10292, 10333, 17248, 10662, 17148, 15033, 12484, 10724, 10997, 14583, 19137, 12605, 10402, 17879, 16503, 10525, 12746, 10726, 17912, 10079, 14486, 13133, 11124, 11971, 17385, 11957, 13036, 14734, 12522, 16519, 10080, 11042, 13064, 19137, 10322, 16503, 11871, 10423, 12825, 10794, 15847, 12144, 14734, 12484, 15509, 11124, 16403, 10292, 19137, 10399, 10662, 12282, 16944, 13179, 11632, 18425, 14189, 28329, 53, 47, 33, 12421, 10305, 45, 33, 11454, 10250, 10283, 10993, 16754, 10684, 15304, 10285, 45, 33, 15129, 10335, 11124, 17097, 10852, 12366, 14486, 13091, 17843, 10662, 14734, 10080, 10444, 13091, 19137, 17189, 17141, 10293, 10846, 11959, 13685, 16533, 10722, 13036, 13336, 10079, 12261, 15123, 10342, 14782, 11971, 17385, 11124, 12978, 12746, 14734, 10684, 15304, 10079, 10399, 10992, 10525, 12746, 12200, 15169, 10342, 10788, 15169, 16403, 10292, 13234, 10739, 11977, 17097, 10852, 12366, 14486, 14734, 12325, 11168, 10080, 11016, 13191, 17148, 13324, 19137, 12605, 10402, 12638, 15752, 12261, 15123, 10250, 10283, 10590, 12237, 10079, 11124, 16760, 11124, 13191, 14757, 14734, 10684, 15304, 10080, 65535, 261, 65530, 65534, 5732, 8875, 460, 63758, 4712, 22590, 38809, 4706, 63361, 21265, 52763, 4596, 65004, 4600, 32227, 4601, 21413, 4435, 332, 47342, 4706, 47425, 21265, 50928, 47, 3878, 57344, 32227, 4858, 47266, 38530, 4712, 22248, 22251, 56950, 47001, 32226, 59328, 60112, 4811, 46726, 45, 21265, 32227, 32313, 51803, 47266, 4435, 62617, 21265, 65034, 47, 3878, 30135, 57344, 55654, 30678, 22590, 31135, 4706, 22762, 4600, 45301, 4715, 31458, 59829, 45, 4423, 4601, 21413, 31262, 4811, 60701, 63361, 47, 50232, 45, 4569, 45112, 116, 63695, 332, 50997, 51035, 59331, 21265, 37832, 62617, 45, 21265, 51746, 32448, 4811, 51982, 52359, 40187, 62616, 46726, 47, 50511, 45, 44126, 55618, 32227, 63695, 332, 51913, 64549, 32487, 46726, 57102, 332, 50997, 51035, 63361, 21265, 64849, 47, 11, 65535, 261, 65530, 65532, 16738, 10464, 12421, 15489, 12605, 10402, 10780, 12638, 14734, 11957, 16713, 10696, 11922, 11, 65535, 261, 65530, 65534, 11454, 17148, 13619, 11957, 16713, 10285, 45, 33, 10250, 10447, 13192, 10993, 12848, 10792, 10333, 10390, 14734, 10250, 10283, 10684, 15304, 10285, 11861, 11454, 15129, 10335, 11124, 17097, 10852, 12366, 14486, 45, 33, 12202, 10265, 10390, 12159, 13197, 15752, 11634, 12604, 17688, 10261, 15516, 12746, 12200, 16412, 10080, 14221, 15646, 45, 33, 10390, 10322, 16417, 17136, 10333, 11957, 10339, 17097, 10852, 12366, 14486, 14734, 10260, 14015, 12522, 45, 33, 12202, 10265, 16667, 10292, 17148, 15033, 16403, 10292, 13091, 10370, 17781, 10684, 15304, 10285, 12182, 16519, 14734, 17728, 18025, 10080, 13188, 11044, 45, 33, 10390, 12261, 16676, 16503, 10525, 12746, 12200, 16412, 11124, 10342, 14782, 14486, 16533, 45, 10399, 10992, 17385, 16523, 10696, 12366, 12604, 17688, 11124, 12978, 12746, 28329, 65535, 261, 65530, 65534, 11454, 17148, 13619, 11957, 16713, 10285, 45, 33, 10250, 10447, 13192, 10993, 12848, 10792, 10333, 10390, 14734, 10250, 10447, 10684, 15304, 10285, 11861, 11454, 15129, 10335, 11124, 17097, 10852, 12366, 14486, 45, 33, 12202, 10265, 16667, 10292, 17148, 13091, 10250, 15033, 17728, 18025, 14734, 13981, 11647, 10080, 14221, 15646, 19137, 10390, 10322, 12848, 10792, 10333, 10250, 10349, 12261, 16676, 13234, 12176, 10847, 12981, 11255, 17148, 10283, 17728, 18025, 45, 33, 10894, 12732, 16503, 10525, 12746, 12200, 16412, 11124, 10342, 14782, 14486, 16533, 10079, 10399, 10992, 17385, 16523, 10696, 12366, 12604, 17688, 10261, 12978, 12746, 10080, 13188, 11044, 45, 33, 10390, 12261, 16676, 13179, 10842, 13773, 17385, 10696, 12366, 12604, 17688, 10261, 12978, 12746, 45, 33, 10399, 10992, 12200, 16412, 11124, 10342, 14782, 14486, 16533, 28329, 65535, 261]
-> item_loss 0.9049959182739258 batch_loss 1.1260739599957186
==== [22251, 56950, 47001, 32226, 59328, 60112, 4811, 46726, 45, 21265, 32227, 32313, 51803, 47266, 4435, 62617, 21265, 65034, 47, 3878, 30135, 57344, 55654, 30678, 22590, 31135, 4706, 22762, 4600, 45301, 4715, 31458, 59829, 45, 4423, 4601, 21413, 31262, 4811, 60701, 63361, 47, 50232, 45, 4569, 45112, 116, 63695, 332, 50997, 51035, 59331, 21265, 37832, 62617, 45, 21265, 51746, 32448, 4811, 51982, 52359, 40187, 62616, 46726, 47, 50511, 45, 44126, 55618, 32227, 63695, 332, 51913, 64549, 32487, 46726, 57102, 332, 50997, 51035, 63361, 21265, 64849, 47, 11, 65535, 261, 65530, 65532, 16738, 10464, 12421, 15489, 12605, 10402, 10780, 12638, 14734, 11957, 16713, 10696, 11922, 11, 65535, 261, 65530, 65534, 11454, 17148, 13619, 11957, 16713, 10285, 45, 33, 10250, 10447, 13192, 10993, 12848, 10792, 10333, 10390, 14734, 10250, 10283, 10684, 15304, 10285, 11861, 11454, 15129, 10335, 11124, 17097, 10852, 12366, 14486, 45, 33, 12202, 10265, 10390, 12159, 13197, 15752, 11634, 12604, 17688, 10261, 15516, 12746, 12200, 16412, 10080, 14221, 15646, 45, 33, 10390, 10322, 16417, 17136, 10333, 11957, 10339, 17097, 10852, 12366, 14486, 14734, 10260, 14015, 12522, 45, 33, 12202, 10265, 16667, 10292, 17148, 15033, 16403, 10292, 13091, 10370, 17781, 10684, 15304, 10285, 12182, 16519, 14734, 17728, 18025, 10080, 13188, 11044, 45, 33, 10390, 12261, 16676, 16503, 10525, 12746, 12200, 16412, 11124, 10342, 14782, 14486, 16533, 45, 10399, 10992, 17385, 16523, 10696, 12366, 12604, 17688, 11124, 12978, 12746, 28329, 65535, 261, 65530, 65534, 11454, 17148, 13619, 11957, 16713, 10285, 45, 33, 10250, 10447, 13192, 10993, 12848, 10792, 10333, 10390, 14734, 10250, 10447, 10684, 15304, 10285, 11861, 11454, 15129, 10335, 11124, 17097, 10852, 12366, 14486, 45, 33, 12202, 10265, 16667, 10292, 17148, 13091, 10250, 15033, 17728, 18025, 14734, 13981, 11647, 10080, 14221, 15646, 19137, 10390, 10322, 12848, 10792, 10333, 10250, 10349, 12261, 16676, 13234, 12176, 10847, 12981, 11255, 17148, 10283, 17728, 18025, 45, 33, 10894, 12732, 16503, 10525, 12746, 12200, 16412, 11124, 10342, 14782, 14486, 16533, 10079, 10399, 10992, 17385, 16523, 10696, 12366, 12604, 17688, 10261, 12978, 12746, 10080, 13188, 11044, 45, 33, 10390, 12261, 16676, 13179, 10842, 13773, 17385, 10696, 12366, 12604, 17688, 10261, 12978, 12746, 45, 33, 10399, 10992, 12200, 16412, 11124, 10342, 14782, 14486, 16533, 28329, 65535, 261]
-> item_loss 1.19801664352417 batch_loss 1.1281294652393885
==== [65530, 65532, 10777, 10296, 10250, 10283, 12182, 14589, 14734, 7262, 7999, 102, 10762, 13004, 11, 65535, 261, 65530, 65534, 17148, 17384, 13191, 10250, 10349, 12182, 14589, 14734, 19644, 7999, 102, 33, 10762, 13004, 59, 11, 46, 30491, 280, 33, 11047, 17848, 11038, 13920, 10842, 10653, 15323, 11, 46, 21515, 280, 267, 11899, 10304, 10250, 10283, 15158, 11028, 11, 46, 30674, 114, 280, 33, 17216, 10964, 12217, 10777, 11, 46, 45985, 280, 17141, 14017, 12217, 10777, 11, 46, 38499, 280, 33, 16056, 11001, 12217, 10777, 14734, 15162, 10250, 10283, 10653, 15323, 11, 46, 4612, 2318, 280, 33, 10781, 12261, 10250, 10283, 10762, 13004, 45, 11885, 15752, 10250, 13557, 16749, 14589, 11632, 10283, 10762, 13004, 33, 12202, 11967, 10687, 15489, 13258, 15476, 11038, 10792, 10250, 10283, 12217, 10777, 10285, 11, 46, 22069, 280, 33, 11957, 12217, 10777, 10285, 14734, 13633, 10283, 10653, 15323, 12220, 14589, 10762, 13004, 45, 17146, 11414, 13086, 11965, 11044, 14734, 12217, 10777, 11, 46, 22193, 280, 33, 16056, 11001, 12217, 10777, 10285, 14734, 13279, 10283, 15325, 12282, 10447, 15586, 14734, 10653, 15323, 11, 46, 47074, 280, 33, 10781, 12261, 10250, 10283, 11533, 10339, 15494, 11899, 10577, 14734, 12217, 10777, 11, 46, 31871, 280, 33, 16056, 11001, 12217, 10777, 10285, 14734, 12631, 13191, 17798, 15162, 10250, 10283, 10653, 15323, 11630, 14734, 10653, 15323, 11, 46, 22456, 280, 33, 11967, 10260, 11021, 17216, 10964, 14734, 11957, 16803, 17083, 12791, 10292, 17216, 10964, 12217, 10777, 11, 46, 32074, 280, 33, 13381, 13290, 12217, 10777, 13091, 11060, 14015, 16956, 15494, 11899, 10762, 13004, 14734, 13231, 10407, 11, 46, 32082, 280, 33, 12814, 12217, 10250, 10283, 12217, 10777, 11, 17148, 17384, 10377, 10296, 10333, 10250, 10349, 12182, 14589, 14734, 10487, 11855, 45, 19644, 7999, 102, 33, 17147, 13191, 12337, 11632, 10687, 11885, 13191, 14589, 14734, 10696, 15586, 10762, 13004, 45, 33, 11021, 10399, 11454, 17148, 17384, 13290, 17749, 11891, 13006, 10777, 16417, 59, 34579, 5385, 7262, 7999, 7415, 1802, 47, 8401, 48, 35467, 8581, 65535, 261, 10777, 10296, 7262, 7999, 102, 10285, 25059, 14734, 10762, 13004, 16735, 13078, 11, 65530, 65534, 25059, 33, 10762, 13004, 13091, 19644, 7999, 102, 33, 10285, 13188, 17385, 16503, 14734, 17848, 11038, 12945, 10460, 10762, 13004, 10305, 10250, 10080, 11885, 14734, 10841, 15752, 13091, 11047, 17848, 11038, 13920, 10842, 13034, 10653, 15323, 28329, 10293, 16503, 13191, 10278, 15033, 14589, 13764, 59, 11, 50, 47, 33, 13920, 10842, 10932, 10283, 10653, 15323, 11, 25059, 33, 14589, 13764, 59, 11, 41, 25059, 30478, 30737, 42, 11, 10487, 11687, 59, 11, 41, 25059, 274, 41, 50, 285, 286, 42, 287, 42, 3580, 275, 50, 285, 286, 287, 42, 11, 41, 25059, 326, 50, 285, 286, 94, 287, 42, 3580, 326, 50, 285, 286, 287, 94, 261, 51, 47, 33, 11038, 12202, 10278, 10283, 17848, 11038, 11, 41, 25059, 30478, 30478, 51, 42, 11, 10487, 11687, 59, 11, 41, 25059, 274, 41, 50, 285, 286, 42, 274, 41, 53, 288, 494, 3580, 275, 50, 285, 286, 287, 288, 42, 11, 41, 25059, 326, 50, 285, 286, 94, 326, 53, 288, 1682, 267, 746, 326, 50, 285, 286, 287, 288, 94, 11, 12421, 14734, 13234, 16735, 45, 25059, 33, 10762, 13004, 11021, 10399, 12824, 12979, 11632, 10283, 10988, 13004, 59, 10250, 10283, 17848, 11038, 41, 25043, 42, 11124, 10250, 10283, 12609, 11632, 10283, 10653, 15323, 41, 25210, 497, 14221, 11044, 11967, 10653, 15323, 13920, 10842, 10792, 17848, 11038, 10285, 28329, 12631, 10399, 11885, 13091, 19644, 7999, 102, 33, 10285, 13188, 15190, 10932, 10322, 13188, 13191, 14589, 14734, 12981, 11003, 17848, 11038, 14349, 12396, 14734, 10762, 13004, 10080, 65535, 261, 10777, 10296, 7262, 7999, 102, 10285, 14734, 7621, 10762, 13004, 14734, 10762, 13004, 16735, 13078, 11, 65530, 65534, 25059, 280, 33, 13920, 10842, 10653, 15323, 10792, 17848, 11038, 10285, 11, 41, 25059, 30478, 30737, 42, 11, 25059, 33, 10762, 13004, 11021, 10399, 11967, 10250, 10283, 12609, 11632, 10283, 10653, 15323, 13920, 10842, 10792, 17848, 11038, 10285, 28329, 7621, 280, 33, 10384, 13086, 11965, 10285, 16056, 11001, 10577, 11, 41, 7621, 22069, 21952, 326, 8260, 46, 34477, 1682, 11, 7621, 33, 10762, 13004, 11021, 10399, 10384, 10250, 10283, 13086, 11965, 41, 8149, 42, 10285, 16056, 11001, 17644, 41, 8020, 42, 11957, 12220, 14734, 10577, 28329, 11687, 13258, 17644, 10260, 11861, 11454, 45, 10779, 17146, 11414, 22187, 46, 34477, 33, 10577, 41, 11021, 17177, 42, 28329, 30491, 33, 11124, 21751, 33, 14734, 10762, 13004, 16735, 13078, 10768, 10790, 13091, 59, 11, 25059, 59, 11, 46, 33, 11047, 17848, 11038, 13920, 10842, 10250, 10283, 12609, 11632, 10283, 10653, 15323, 11, 46, 275, 25059, 30478, 30737, 42, 33, 10423, 11967, 30737, 33, 13920, 10842, 10792, 30478, 33, 17848, 11038, 10285, 11, 46, 33, 11021, 10399, 14589, 10339, 11047, 17387, 10079, 10777, 16417, 10079, 17848, 11038, 15169, 17848, 11038, 15261, 11496, 11, 7621, 59, 11, 46, 33, 10384, 13086, 11965, 10285, 16056, 11001, 12748, 11899, 21952, 33, 11957, 12220, 14734, 10577, 11, 46, 275, 7621, 22069, 21952, 42, 33, 10423, 10384, 22069, 33, 13086, 11965, 10285, 16056, 11001, 21952, 33, 11957, 12220, 14734, 10577, 11, 46, 33, 11687, 13258, 21952, 33, 10260, 11861, 11454, 45, 10779, 17146, 11414, 22175, 11, 46, 33, 11021, 10399, 12848, 10494, 10250, 10283, 22187, 46, 34477, 33, 10988, 13004, 10460, 10292, 11622, 14589, 10577, 11, 46, 275, 7621, 22069, 21952, 22187, 46, 34477, 42, 11, 10487, 11687, 59, 11, 41, 7621, 4899, 98, 284, 293, 99, 285, 126, 293, 98, 42, 3580, 284, 11, 41, 7621, 4899, 98, 284, 293, 99, 285, 126, 293, 100, 269, 8260, 38547, 5164, 3580, 269, 8260, 38547, 363, 11, 25059, 33, 13091, 10538, 12981, 17848, 11038, 45, 7621, 33, 13091, 10384, 13086, 11965, 10285, 16742, 11001, 13004, 12797, 45, 10278, 15643, 10841, 15752, 10260, 11042, 10080, 65535, 261, 10296, 10487, 16735, 13078, 7262, 7999, 102, 14734, 16728, 13764, 14332, 14172, 11, 65530, 65534, 7262, 7999, 102, 14734, 10293, 16503, 16728, 13764, 14332, 14172, 11687, 10258, 59, 11, 50, 47, 20299, 113, 13324, 12276, 14734, 84, 12276, 16728, 13764, 10080, 7262, 7999, 102, 17379, 14589, 10333, 1219, 2191, 14734, 10427, 15500, 84, 12276, 16728, 13764, 45, 10482, 14589, 12732, 11028, 16417, 14966, 15052, 12217, 15489, 13250, 28329, 51, 47, 33, 12978, 12746, 10762, 13004, 12276, 15543, 15052, 10080, 7262, 7999, 102, 17379, 14589, 10762, 13004, 12276, 15543, 15052, 10460, 10292, 10293, 16503, 15543, 15052, 15973, 12276, 45, 12978, 12746, 18216, 17775, 10762, 13004, 10079, 11117, 11043, 22001, 12314, 12276, 10079, 12507, 12415, 13685, 10577, 15169, 14332, 12333, 28329, 52, 47, 33, 12978, 12746, 17919, 11047, 11957, 16803, 15543, 15052, 10080, 7262, 7999, 102, 12848, 10494, 10333, 11533, 10339, 16678, 12308, 14734, 17919, 11047, 11957, 16803, 45, 12978, 12746, 15261, 10079, 15507, 12665, 10079, 10926, 16676, 10079, 11632, 12396, 15169, 14332, 12415, 28329, 53, 47, 33, 12978, 12746, 12202, 10997, 15543, 15052, 10080, 7262, 7999, 102, 12848, 10494, 10333, 11533, 10339, 17086, 10407, 10336, 10843, 10696, 11861, 13499, 11496, 14734, 12202, 10997, 12978, 12746, 45, 11021, 10399, 13036, 10511, 18216, 12986, 11459, 15543, 10708, 12202, 10997, 15052, 12217, 28329, 54, 47, 33, 10846, 12396, 16728, 16537, 10080, 7262, 7999, 102, 13091, 10250, 15033, 10846, 12396, 16728, 16537, 45, 10652, 16684, 11454, 17144, 16403, 13064, 12981, 11003, 16728, 16537, 15489, 13250, 45, 11687, 17385, 13034, 11899, 10304, 10762, 13004, 10079, 11003, 17387, 15169, 28329, 55, 47, 33, 12978, 12746, 10653, 15543, 15052, 10080, 7262, 7999, 102, 12848, 10494, 10333, 12299, 11638, 14734, 13212, 10794, 13234, 12981, 10708, 16728, 13764, 45, 11687, 35036, 116, 10079, 10995, 11965, 15169, 28329, 56, 47, 33, 12978, 12746, 10985, 17187, 11028, 10080, 7262, 7999, 102, 10985, 12810, 10333, 1219, 2191, 10427, 15500, 14734, 10768, 17821, 15158, 17187, 11028, 45, 15646, 17379, 14589, 11533, 10339, 17734, 16973, 14734, 13327, 12276, 10901, 28329, 57, 47, 33, 10762, 13004, 10460, 14589, 11524, 10292, 10846, 12396, 14734, 10080, 7262, 7999, 102, 14734, 10762, 13004, 10460, 14589, 11524, 17379, 14589, 10261, 1219, 2191, 10250, 15850, 14734, 10846, 12396, 10460, 14589, 11524, 13036, 12276, 28329, 58, 47, 33, 12978, 12746, 10696, 15586, 13004, 12797, 15489, 13250, 10080, 7262, 7999, 102, 12848, 10494, 10333, 10286, 11932, 14734, 10696, 15586, 13004, 12797, 15489, 13250, 45, 11687, 10777, 16417, 10079, 11047, 17387, 10079, 17848, 11038, 10079, 11164, 12159, 16417, 15169, 28329, 630, 47, 33, 12978, 12746, 10727, 15489, 13004, 12797, 15489, 13250, 10080, 7262, 7999, 102, 12978, 12746, 11967, 11021, 11003, 13004, 12797, 15489, 13250, 10727, 15489, 12604, 10260, 11021, 11003, 14734, 13004, 12797, 45, 12848, 18216, 15052, 12217, 11889, 10675, 12415, 28329, 10399, 10257, 10293, 16503, 12022, 14966, 10333, 7262, 7999, 102, 14734, 10250, 10349, 13308, 12377, 12415, 16728, 13764, 14332, 14172, 10080, 12159, 13197, 15752, 10292, 12469, 12848, 10494, 10988, 15641, 34, 65535, 261, 65530, 65532, 15494, 10250, 10283, 10688, 10452, 14734, 10397, 14875, 10487, 11855, 11, 65535, 261, 65530, 65534, 17148, 17384, 13091, 10250, 10283, 15190, 10932, 14734, 19644, 7999, 102, 33, 10397, 14875, 14966, 10487, 59, 11, 6884, 7262, 7999, 102, 11, 41, 7334, 31496, 269, 23813, 381, 11, 41, 7334, 111, 38632, 3331, 482, 94, 275, 25526, 117, 31496, 494, 3331, 482, 26290, 94, 19242, 41, 49580, 275, 8756, 269, 33155, 269, 31496, 269, 5164, 5292, 11, 41, 7334, 111, 22532, 326, 39, 52333, 94, 3331, 41, 42150, 278, 52333, 494, 11, 41, 25526, 117, 269, 33414, 381, 11, 60, 36786, 37308, 34, 11, 41, 25526, 117, 42, 11, 60, 36786, 29165, 34, 11, 41, 8762, 284, 285, 286, 287, 42, 11, 60, 3483, 11, 6884, 11, 10293, 16503, 12022, 14966, 10333, 59, 11, 46, 33, 11899, 10304, 15158, 11028, 4398, 7334, 31496, 269, 23813, 381, 97, 11, 46, 33, 11899, 10304, 10762, 13004, 4398, 7334, 111, 38632, 36218, 276, 97, 11, 46, 33, 11632, 10283, 10988, 13004, 10777, 16417, 4398, 7334, 111, 38632, 3435, 94, 275, 25526, 117, 31496, 494, 3435, 26290, 94, 275, 49580, 275, 8756, 269, 33155, 269, 31496, 269, 5164, 5292, 97, 11, 46, 33, 11003, 17688, 10988, 13004, 10777, 16417, 4398, 7334, 111, 22532, 326, 39, 52333, 94, 28375, 97, 11, 46, 33, 10696, 15586, 14734, 47030, 33, 10762, 13004, 4398, 42150, 278, 52333, 512, 11, 46, 33, 16749, 14589, 11899, 10304, 14734, 38632, 33, 11124, 22532, 33, 10762, 13004, 11, 13006, 10452, 13234, 14791, 13091, 10250, 10283, 15190, 10932, 14734, 38632, 33, 10762, 13004, 11124, 22532, 33, 10762, 13004, 14734, 11899, 10304, 11124, 10482, 14589, 10080, 5735, 7999, 102, 33, 14332, 13191, 14734, 318, 33, 12276, 16728, 13764, 10079, 11003, 17688, 10988, 13004, 10777, 16417, 15169, 14332, 12415, 17311, 13191, 10452, 14446, 28329, 12159, 13197, 15752, 15494, 12469, 10988, 15641, 34, 11687, 13258, 13191, 10687, 10390, 17879, 13685, 45, 13558, 17143, 12848, 10760, 10080, 65535, 261, 65530, 65532, 17379, 14589, 10397, 14875, 14966, 10487, 16735, 13078, 7262, 7999, 102, 14734, 11632, 17385, 13036, 13764, 11, 65535, 261, 65530, 65534, 17148, 17384, 13191, 10250, 10283, 7262, 7999, 102, 14734, 11632, 17385, 13036, 13764, 14966, 10487, 59, 11, 7334, 35086, 21699, 38499, 11, 7334, 41847, 21699, 293, 98, 326, 121, 94, 275, 8756, 269, 41847, 332, 59, 269, 355, 42, 11, 7334, 41847, 21699, 293, 99, 326, 121, 94, 275, 8756, 269, 41847, 333, 59, 269, 355, 42, 11, 41, 7578, 293, 98, 3483, 42, 3580, 269, 41847, 332, 59, 3483, 35, 11, 41, 7578, 293, 99, 3493, 42, 3580, 269, 41847, 333, 59, 3493, 35, 11, 17148, 17384, 12605, 10402, 11899, 10304, 10250, 10283, 11632, 17385, 10762, 13004, 7578, 45, 33, 17177, 12731, 10768, 13818, 13036, 13764, 41, 7334, 35086, 42, 13091, 13326, 12797, 10988, 13004, 15162, 10250, 10283, 10653, 15323, 41, 34453, 42, 13234, 10722, 11899, 16749, 14589, 11187, 10283, 10688, 10452, 13036, 13764, 41, 7334, 41847, 42, 11, 10688, 10452, 17141, 15052, 13091, 59, 11, 50, 47, 33, 11899, 10304, 11632, 17385, 10762, 13004, 21699, 33, 12202, 10482, 14589, 38499, 33, 10762, 13004, 17177, 12731, 10768, 13818, 13036, 13764, 59, 11, 6884, 7262, 7999, 102, 11, 41, 7334, 35086, 21699, 38499, 42, 11, 6884, 11, 51, 47, 33, 11899, 10304, 10250, 10283, 10688, 10452, 13036, 13764, 293, 98, 45, 33, 10988, 13004, 13091, 355, 59, 11, 6884, 7262, 7999, 102, 11, 41, 7334, 41847, 21699, 293, 98, 326, 121, 94, 275, 8756, 269, 41847, 332, 59, 269, 355, 494, 11, 6884, 11, 52, 47, 33, 11899, 10304, 11013, 10250, 10283, 10688, 10452, 13036, 13764, 293, 99, 59, 11, 6884, 7262, 7999, 102, 11, 41, 7334, 41847, 21699, 293, 99, 326, 121, 94, 275, 8756, 269, 41847, 333, 59, 269, 355, 494, 11, 6884, 11, 53, 47, 33, 12307, 16749, 14589, 21699, 478, 98, 3483, 42, 33, 13064, 45, 33, 10423, 17177, 12731, 293, 98, 33, 13036, 13764, 60, 11, 54, 47, 33, 12307, 16749, 14589, 21699, 478, 99, 3493, 42, 33, 13064, 45, 33, 10423, 17177, 12731, 293, 99, 33, 13036, 13764, 28329, 17189, 17141, 11632, 17385, 13036, 13764, 45, 33, 12605, 10402, 11021, 10399, 12750, 14240, 10260, 11042, 14734, 13308, 10733, 56016, 33, 10792, 10260, 11042, 14734, 10688, 10452, 13036, 13764, 11903, 14446, 10080, 17148, 17384, 10788, 14589, 38499, 33, 10768, 13818, 10792, 10688, 10452, 14734, 10684, 17644, 11860, 293, 98, 33, 12609, 15643, 293, 99, 45, 33, 11891, 12604, 11632, 12314, 12396, 10768, 13818, 28329, 12159, 13197, 15752, 11634, 10847, 12469, 14486, 16533, 21443, 7999, 102, 33, 14734, 11632, 17385, 13036, 13764, 34, 16738, 11454, 17879, 16503, 13064, 16416, 10655, 10080, 65535, 261, 65530, 65532, 17379, 14589, 10397, 14875, 14966, 10487, 16735, 13078, 7262, 7999, 102, 14734, 10762, 13004, 10988, 13004, 16533, 13250, 11, 65535, 261, 65530, 65534, 5735, 7999, 102, 10285, 14734, 10762, 13004, 11021, 10399, 10482, 14589, 10988, 13004, 16533, 13250, 45, 16728, 13764, 13091, 11454, 10988, 13004, 11043, 10808, 10842, 478, 11, 6884, 7262, 7999, 102, 11, 41, 7334, 111, 30924, 46, 26290, 3331, 1650, 98, 333, 334, 94, 36216, 60, 332, 13091, 10250, 10283, 11047, 17387, 3331, 1652, 98, 293, 99, 293, 100, 94, 19250, 60, 332, 45, 99, 11124, 100, 13091, 8149, 10285, 14734, 17644, 3331, 92, 98, 358, 99, 293, 99, 126, 334, 94, 267, 60, 99, 13091, 10250, 10283, 8149, 3331, 92, 98, 333, 273, 31458, 1682, 294, 26276, 13091, 10250, 10283, 10894, 11064, 10822, 10457, 10988, 13004, 14734, 11047, 17387, 11, 6884, 11, 10487, 11855, 59, 11, 6884, 7262, 7999, 102, 11, 41, 7334, 111, 39355, 46, 25857, 3331, 92, 39, 326, 26290, 4899, 26078, 326, 6935, 46085, 1701, 4899, 26078, 326, 25015, 1701, 1697, 3331, 41, 49580, 31496, 269, 1955, 35, 21238, 269, 36031, 22221, 35, 46085, 58652, 35, 7005, 38919, 4596, 35, 30449, 494, 11, 41, 35433, 46, 25857, 4899, 26290, 269, 23813, 35, 293, 6935, 3503, 293, 41455, 269, 23902, 399, 64158, 2370, 25015, 269, 6329, 30074, 5198, 11, 60, 29165, 4600, 3503, 40237, 22221, 29335, 21265, 38919, 4596, 20512, 30074, 11, 6884, 11, 11454, 17148, 17384, 45, 59, 26290, 16533, 13250, 8149, 14734, 59, 26290, 17644, 558, 59, 26078, 326, 6935, 46085, 1701, 16533, 13250, 6935, 11124, 41455, 17644, 45, 4899, 26078, 326, 25015, 1701, 16533, 13250, 25015, 17644, 28329, 10988, 13004, 16533, 13250, 16672, 10762, 13004, 10988, 13004, 13179, 10688, 16728, 10304, 45, 13080, 10339, 14486, 16533, 10080, 65535, 261, 65530, 65532, 12848, 10494, 10250, 10283, 10482, 14589, 7262, 7999, 102, 16690, 17728, 7322, 26450, 13004, 12797, 12219, 14734, 10397, 14875, 14966, 10487, 11, 65535, 261, 65530, 65534, 17148, 17384, 13091, 10250, 10283, 10482, 14589, 19644, 7999, 102, 33, 16690, 17728, 19722, 26450, 33, 13004, 12797, 12219, 14734, 14966, 10487, 59, 11, 6884, 7262, 7999, 102, 11, 41, 49636, 3410, 7322, 26450, 47, 7023, 293, 1751, 335, 1682, 11, 60, 50108, 4811, 332, 19722, 26450, 55912, 11, 41, 7334, 30492, 275, 101, 48, 49003, 269, 7322, 26450, 59, 25457, 5385, 58319, 59, 663, 654, 48, 2058, 1815, 5172, 11, 60, 37220, 22590, 55912, 11, 41, 101, 48, 114, 3410, 59, 25423, 298, 102, 49923, 59, 36002, 326, 64, 102, 293, 34284, 269, 26055, 65, 49131, 47, 7272, 5196, 36210, 65, 25060, 42, 11, 60, 64226, 11, 41, 101, 48, 35795, 6909, 30492, 19242, 1659, 59, 1815, 48, 1940, 270, 1815, 48, 1940, 1652, 1815, 47, 26650, 48, 27370, 94, 36210, 59, 26290, 269, 23813, 35, 36210, 59, 34284, 269, 26055, 65, 49131, 47, 7272, 399, 1682, 11, 41, 101, 48, 35795, 6909, 30492, 19242, 1659, 59, 1815, 48, 1940, 270, 1815, 48, 1940, 1652, 1815, 47, 26650, 48, 27370, 94, 36210, 59, 26290, 269, 23909, 35, 36210, 59, 34284, 269, 26215, 65, 49131, 47, 7272, 399, 1682, 11, 60, 29137, 7066, 31601, 52738, 11, 41, 25156, 114, 326, 27370, 275, 101, 48, 114, 3410, 59, 25423, 298, 102, 65396, 59, 1950, 271, 298, 26290, 65396, 59, 36002, 326, 64, 102, 293, 26290, 298, 26290, 1697, 65362, 65, 25060, 269, 23813, 5177, 3331, 41, 49580, 32355, 494, 11, 6884, 11, 24242, 30469, 59, 11, 46, 44419, 116, 22590, 331, 7322, 26450, 47, 7023, 97, 60044, 11, 46, 50108, 116, 4811, 332, 19722, 26450, 55912, 39095, 331, 2058, 1815, 97, 11, 46, 50535, 116, 332, 39415, 4811, 30858, 4419, 45874, 32487, 38356, 331, 26055, 65, 49131, 47, 7272, 97, 11, 46, 50535, 116, 22638, 64595, 4811, 21227, 22638, 40084, 11, 46, 50535, 116, 332, 39415, 4811, 30858, 21256, 40084, 32487, 31496, 269, 23813, 35, 11, 46, 29137, 24744, 31601, 22590, 52738, 21265, 46906, 32229, 11, 23739, 32234, 38686, 34, 20282, 4660, 31223, 4588, 22799, 31059, 21273, 39193, 60312, 47, 65535, 261, 65530, 65532, 12605, 10381, 11639, 17879, 16503, 11454, 7322, 26450, 13004, 12797, 12219, 17384, 10842, 10257, 13133, 15752, 12636, 13212, 17148, 10283, 11903, 10452, 19137, 16738, 15494, 12605, 12848, 10494, 10250, 10283, 10688, 10452, 14734, 11533, 10339, 7322, 26450, 14734, 10397, 14875, 16689, 16664, 11, 65535, 261, 65530, 65534, 17148, 17384, 13191, 11533, 10339, 19722, 26450, 33, 14734, 13133, 15752, 12636, 13212, 11903, 10452, 10397, 14875, 16689, 16664, 59, 11, 6884, 7262, 7999, 102, 11, 60, 33, 11903, 10452, 1107, 11, 36, 1815, 48, 1940, 1652, 1815, 47, 26650, 48, 27370, 94, 11, 60, 33, 12025, 12415, 11, 59, 26290, 269, 41512, 323, 35, 11, 59, 35074, 269, 41512, 323, 35, 11, 60, 11860, 13619, 11, 1652, 35430, 293, 1815, 48, 42434, 94, 267, 60, 10408, 13327, 45, 11210, 10250, 11, 1652, 49629, 23554, 293, 1815, 47, 27200, 48, 49290, 94, 294, 10997, 12155, 13053, 13199, 11, 1652, 2102, 293, 1815, 48, 42434, 94, 28360, 60, 12945, 10460, 15304, 15500, 11, 1652, 41138, 293, 1815, 48, 42434, 1690, 11044, 12892, 10621, 11647, 10621, 15323, 11, 60, 33, 10355, 13080, 11, 1647, 101, 48, 35795, 6909, 30492, 262, 1659, 59, 1815, 48, 1940, 270, 1815, 48, 1940, 1652, 1815, 47, 26650, 48, 35664, 35381, 94, 28352, 59, 26290, 269, 41512, 323, 35, 28352, 59, 35074, 269, 41512, 323, 35, 28352, 59, 35430, 3572, 58, 28352, 59, 49629, 23554, 270, 25883, 269, 640, 637, 46, 631, 46, 623, 85, 620, 59, 620, 59, 620, 47, 620, 49, 46, 620, 59, 620, 35, 28352, 59, 2102, 269, 7738, 3484, 35, 28352, 59, 41138, 3485, 2403, 11, 60, 13290, 16718, 11, 1659, 59, 25423, 274, 64, 26290, 274, 64, 35430, 3331, 59, 36002, 4371, 64, 102, 293, 26290, 298, 26290, 94, 61270, 92, 64, 102, 293, 35430, 298, 35430, 1697, 3331, 59, 1950, 3395, 274, 41512, 323, 463, 94, 11, 6884, 11, 17148, 10283, 16689, 16664, 10894, 11064, 59, 11, 46, 33, 11903, 10452, 3888, 33, 10292, 331, 59, 1815, 47, 26650, 48, 35664, 35381, 97, 11, 46, 33, 12025, 12415, 10292, 97, 11043, 15044, 97, 10079, 97, 11496, 11028, 97, 10079, 331, 10408, 13327, 97, 10079, 97, 10997, 12155, 13053, 13199, 97, 10079, 97, 12945, 10460, 15304, 15500, 97, 11124, 97, 11044, 12892, 10621, 11647, 10621, 15323, 97, 11, 46, 33, 11957, 12220, 11860, 13619, 15261, 11496, 16689, 15586, 11, 46, 33, 13920, 10842, 10250, 13231, 46241, 323, 33, 14734, 16678, 12308, 10460, 10292, 14966, 10487, 11, 46, 33, 13290, 16718, 46241, 323, 33, 14734, 11043, 15044, 11124, 10408, 13327, 14734, 13290, 16718, 14966, 10487, 11, 11533, 10339, 13581, 16689, 16664, 45, 12469, 11021, 10399, 13920, 10842, 13179, 11632, 13133, 15752, 12636, 13212, 14734, 16678, 12308, 11124, 12025, 12415, 45, 14221, 11044, 11957, 10687, 12650, 16403, 11036, 15033, 13290, 16718, 28329, 11687, 13191, 17879, 16503, 45, 12605, 11021, 10399, 10292, 12469, 11891, 11255, 10688, 10452, 16689, 16664, 10080, 16738, 11414, 11626, 11088, 14862, 10080, 65535, 261, 65530, 65532, 27476, 4600, 21443, 7999, 102, 64, 32006, 4660, 32074, 30469, 21700, 51668, 11, 65535, 261, 65530, 65534, 5735, 7999, 102, 4600, 332, 51461, 4706, 20299, 113, 45, 332, 45956, 4706, 63795, 59946, 47, 29804, 56162, 4706, 19644, 7999, 102, 30469, 59, 11, 46, 44880, 56292, 59, 11, 6884, 7262, 7999, 102, 11, 41, 7334, 111, 21227, 326, 98, 333, 94, 3331, 474, 332, 333, 494, 11, 41, 6913, 285, 286, 42, 3580, 3591, 288, 11, 6884, 11, 46, 50824, 32487, 53267, 59, 11, 6884, 7262, 7999, 102, 11, 41, 7334, 52333, 326, 50, 285, 286, 287, 1682, 11, 41, 34174, 52333, 42, 3580, 3591, 287, 11, 41, 34453, 52333, 42, 3580, 3591, 284, 11, 41, 26102, 52333, 42, 3580, 3591, 287, 11, 41, 25059, 52333, 288, 42, 3580, 3591, 326, 50, 285, 286, 287, 288, 94, 11, 6884, 11, 46, 50824, 32487, 31371, 59, 11, 6884, 7262, 7999, 102, 11, 41, 7334, 32355, 4899, 26290, 269, 23813, 35, 293, 6935, 3503, 2389, 11, 41, 7621, 32355, 293, 26290, 42, 3580, 3591, 269, 23813, 35, 11, 41, 33915, 32355, 293, 25015, 269, 6329, 30074, 381, 3580, 3591, 4899, 26290, 269, 23813, 384, 293, 6935, 3503, 45, 293, 25015, 269, 6329, 30074, 399, 11, 6884, 11, 46, 44291, 7856, 332, 61689, 59, 11, 6884, 7262, 7999, 102, 11, 41, 41418, 30788, 64, 52333, 42, 3580, 3591, 275, 51, 287, 42, 11, 6884, 11, 46, 29301, 7856, 32487, 274, 7579, 448, 11, 6884, 7262, 7999, 102, 11, 41, 7579, 326, 121, 275, 35494, 284, 3484, 509, 3331, 41, 35433, 355, 269, 3351, 42, 11, 746, 3591, 284, 285, 286, 287, 288, 289, 290, 291, 292, 3483, 11, 6884, 11, 1461, 19644, 7999, 102, 30469, 4600, 51276, 21265, 62030, 4596, 39844, 45, 46500, 38677, 22681, 4706, 59751, 45, 63344, 21265, 60344, 47, 65535, 261]
-> item_loss 1.6564335823059082 batch_loss 1.1428045796023474
==== [15162, 10250, 13557, 12848, 17728, 11, 65530, 65532, 10902, 10362, 11454, 10373, 10303, 11459, 13036, 19156, 11, 65535, 261, 65530, 65534, 10902, 10362, 10447, 10339, 10285, 11435, 10902, 13036, 45, 13091, 10285, 10925, 10370, 13653, 10682, 11124, 11435, 14734, 18117, 17311, 28329, 10293, 16503, 15478, 15907, 59, 11, 50, 47, 33, 10902, 10362, 12154, 10447, 10339, 10285, 11435, 10272, 10902, 17303, 45, 11459, 11621, 10925, 10902, 12200, 10974, 14734, 10285, 12366, 28329, 51, 47, 33, 10902, 10362, 11621, 10339, 12337, 17385, 16503, 14734, 15486, 13828, 10079, 12984, 13751, 11124, 13012, 10901, 10285, 12366, 10447, 15586, 28329, 52, 47, 33, 10902, 10362, 13250, 12604, 10362, 13802, 10694, 11517, 12154, 15604, 13325, 12366, 45, 13091, 10285, 11435, 11412, 11638, 11011, 17311, 10305, 10250, 28329, 53, 47, 33, 10902, 10362, 12985, 11913, 15169, 11011, 17167, 10079, 11857, 12224, 10079, 18019, 11124, 11423, 15169, 15847, 14221, 13126, 14172, 10992, 17878, 17904, 15169, 17728, 18025, 15646, 10760, 11043, 28329, 54, 47, 33, 10902, 10362, 10688, 13191, 10257, 10918, 12201, 10964, 11024, 45, 13186, 15486, 13091, 10653, 10079, 13078, 10079, 13923, 10256, 10397, 14734, 18117, 17311, 10080, 639, 669, 33, 12201, 11044, 12604, 10292, 10285, 10925, 10370, 13653, 10682, 11124, 11435, 14734, 18117, 17311, 28329, 55, 47, 33, 10902, 10362, 11459, 11621, 659, 2446, 675, 9835, 127, 3514, 2446, 623, 9835, 10902, 15455, 45, 631, 54, 2446, 645, 9835, 127, 631, 56, 2446, 650, 9835, 10272, 15486, 28329, 12159, 13197, 10399, 10257, 10529, 12445, 15752, 12176, 10847, 12469, 10080, 11687, 13258, 17147, 13191, 10687, 10390, 14638, 17728, 45, 13558, 17143, 17819, 13064, 12848, 17728, 10080, 65535, 261, 65530, 65534, 14488, 14543, 19151, 33, 12605, 10780, 10780, 16707, 14808, 11871, 10323, 10333, 17148, 10349, 14862, 16696, 10080, 15489, 13258, 11640, 17843, 10333, 19137, 12605, 13734, 12401, 10303, 11871, 10423, 19137, 17879, 16503, 13179, 11632, 14734, 10988, 15641, 16909, 13021, 14791, 14791, 28329, 65535, 261, 65530, 65531, 15162, 10337, 13557, 17149, 10250, 13582, 14734, 12848, 17728, 11, 65535, 261, 65530, 65532, 10902, 10362, 11454, 10373, 10303, 11459, 13036, 19156, 11, 65535, 261, 65530, 65534, 10464, 11685, 19137, 12605, 13091, 10250, 10283, 820, 19137, 11017, 10588, 11975, 10847, 12636, 10080, 12605, 15752, 12176, 12469, 11414, 15178, 11036, 15033, 17728, 18025, 19137, 10260, 17141, 12605, 17879, 16503, 12469, 12848, 10494, 13179, 11632, 14734, 10529, 12445, 10080, 10902, 10362, 13091, 10285, 11435, 14734, 10250, 10283, 11517, 12154, 19137, 10447, 10339, 10285, 11435, 14734, 10902, 17303, 10080, 11885, 10399, 10687, 10286, 11932, 14734, 10964, 11024, 13012, 10901, 17225, 10358, 10079, 15597, 18053, 11124, 15034, 12666, 10781, 13034, 17740, 11043, 10339, 10267, 10080, 11687, 13258, 12469, 13191, 13179, 11632, 10684, 10339, 10902, 10362, 14734, 17728, 18025, 19137, 13558, 17143, 11047, 12605, 12848, 10760, 19126, 11, 65535, 261, 65530, 65534, 10902, 10362, 10447, 10339, 10285, 11435, 10902, 17303, 45, 13091, 10285, 11435, 14734, 18117, 17311, 28329, 9828, 33, 10902, 10362, 14734, 11459, 14486, 10447, 15586, 59, 10902, 15455, 659, 2446, 674, 9835, 15849, 661, 2446, 625, 9835, 45, 10272, 15486, 631, 54, 2446, 645, 9835, 15849, 631, 56, 2446, 650, 9835, 28329, 9828, 33, 10902, 10362, 13091, 17919, 15043, 13188, 11638, 10079, 10355, 17189, 13188, 10292, 10511, 10788, 10079, 15486, 13828, 13188, 10997, 17136, 10079, 13012, 10901, 13188, 13815, 16962, 14734, 10285, 11435, 11517, 12154, 10305, 10250, 28329, 9828, 33, 10902, 10362, 10964, 11024, 12466, 10301, 45, 15847, 15040, 14736, 14968, 11022, 12261, 15123, 10399, 13234, 11994, 10250, 14781, 13091, 10285, 11435, 14734, 12984, 13751, 10285, 12366, 28329, 9828, 33, 10902, 10362, 11517, 10696, 13191, 12337, 11632, 17385, 16503, 14734, 11011, 17167, 10079, 10964, 11024, 13012, 10901, 12261, 15176, 45, 11687, 11639, 11889, 17723, 12210, 11461, 10079, 18019, 11124, 11423, 15169, 45, 10995, 13086, 10333, 10902, 10362, 12466, 10301, 14734, 10964, 11024, 11124, 13012, 10901, 28329, 9828, 33, 10902, 10362, 10460, 10292, 10285, 11435, 14734, 18117, 17311, 45, 12727, 13191, 12337, 18216, 14734, 12984, 13751, 10079, 15486, 13828, 11124, 13012, 10901, 11459, 10447, 45, 12337, 11632, 11435, 10696, 11630, 17385, 16503, 13815, 10846, 17311, 11454, 10902, 10362, 10296, 16403, 28329, 9828, 33, 10902, 10362, 14734, 15597, 18053, 10322, 17916, 12182, 10286, 11932, 45, 10902, 10362, 14188, 18372, 10079, 14171, 17351, 17919, 10079, 16793, 56175, 33, 15169, 11010, 11110, 14370, 14332, 14734, 11975, 11035, 16939, 17149, 10267, 14610, 28329, 9828, 33, 17145, 12201, 13234, 45, 10902, 10362, 17147, 12604, 10292, 10285, 11435, 15034, 12666, 11124, 11423, 10912, 12261, 16689, 14734, 10285, 12366, 45, 11075, 12282, 14808, 12337, 11632, 15034, 12666, 10412, 10270, 11124, 12666, 13208, 10370, 12638, 28329, 12159, 13197, 10399, 10257, 10529, 12445, 15752, 12176, 10847, 12469, 13179, 10655, 10768, 11459, 10333, 16533, 10902, 10362, 17148, 12232, 10964, 11024, 11043, 11517, 10080, 11687, 13191, 10687, 10390, 17728, 18025, 45, 13558, 17143, 17819, 13064, 13234, 17728, 10080, 65535, 261, 65530, 65534, 10902, 10362, 59, 33, 12605, 14862, 17222, 10333, 19137, 16774, 16774, 10464, 14734, 16533, 15178, 10080, 17147, 13191, 10687, 10390, 17728, 18025, 11053, 19156, 11, 65535, 261, 65530, 65534, 10258, 17919, 14734, 11957, 16713, 10285, 45, 5732, 8875, 12848, 10494, 14734, 10529, 12445, 13637, 1406, 5608, 12848, 10494, 14734, 16503, 13179, 10842, 10675, 17919, 10079, 15478, 15850, 59, 11, 50, 47, 4164, 5608, 11016, 15190, 10932, 11459, 16735, 10902, 10362, 11454, 10285, 11435, 10902, 17303, 45, 13091, 10285, 11435, 11517, 12154, 45, 15646, 13734, 13191, 10688, 10452, 14734, 11459, 14486, 10447, 15586, 28329, 51, 47, 4164, 5608, 13734, 13191, 12848, 10992, 10902, 10362, 10460, 10292, 18117, 17311, 14734, 12984, 13751, 10079, 15486, 13828, 10079, 13012, 10901, 11459, 10447, 28329, 52, 47, 4164, 5608, 13734, 13191, 12848, 10992, 10902, 10362, 10286, 11932, 14734, 10964, 11024, 13012, 10901, 10992, 11011, 17167, 12261, 15176, 28329, 53, 47, 4164, 5608, 13734, 13191, 12848, 10792, 10902, 10362, 14332, 15898, 14734, 15597, 18053, 11975, 11035, 28329, 54, 47, 4164, 5608, 13734, 13191, 12848, 10992, 10902, 10362, 17145, 12201, 13234, 14734, 15034, 12666, 10261, 10412, 10270, 15674, 17848, 12484, 10724, 28329, 12421, 14734, 13234, 16735, 45, 5732, 8875, 12848, 10494, 14734, 10529, 12445, 13179, 15752, 14015, 16956, 14589, 12626, 10333, 16533, 10902, 10362, 14734, 17879, 13685, 10080, 10529, 12445, 10675, 17919, 45, 10894, 12732, 59, 11, 46, 33, 10688, 10452, 14734, 11459, 14486, 10447, 15586, 11, 46, 33, 10902, 10362, 10460, 10292, 18117, 17311, 14734, 17385, 16503, 12415, 11, 46, 33, 10286, 11932, 14734, 10964, 11024, 13012, 10901, 11124, 11011, 17167, 11, 46, 33, 10902, 10362, 15597, 18053, 11975, 11035, 11, 46, 33, 17145, 12201, 13234, 10902, 10362, 14734, 15034, 12666, 10997, 12022, 11, 17148, 10349, 13036, 17919, 17311, 11891, 13006, 11459, 11414, 15178, 10333, 14589, 12626, 35, 10902, 10362, 11454, 10373, 10303, 11459, 13036, 35, 17148, 10283, 17728, 18025, 45, 12176, 10847, 14589, 12626, 13179, 10675, 17919, 11459, 10333, 16533, 10902, 10362, 11517, 12154, 10080, 65535, 261]
-> item_loss 1.5533838272094727 batch_loss 1.1539013160241616
==== [25106, 11, 65530, 65531, 10464, 13091, 10250, 10283, 10370, 12137, 13133, 15752, 10847, 12636, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 12176, 12605, 14604, 10250, 10283, 52, 43, 52, 14734, 10320, 11913, 13327, 19137, 14589, 9822, 44, 9823, 11860, 15158, 14604, 28329, 65535, 261, 65530, 65534, 7176, 59, 3455, 44, 11, 536, 44, 11, 536, 44, 11, 12605, 14604, 11685, 10333, 10080, 65535, 261, 65530, 65532, 27370, 59, 33, 14221, 11044, 12669, 11885, 12138, 10257, 16530, 14734, 15158, 11028, 12791, 12604, 10026, 28329, 65535, 261, 65530, 65534, 7176, 59, 33, 13734, 17728, 18025, 19126, 10026, 536, 11, 536, 44, 11, 536, 44, 65535, 261, 65530, 65532, 11, 27370, 59, 33, 11025, 10257, 16530, 12791, 12604, 10022, 10080, 65535, 261, 65530, 65534, 7176, 59, 33, 11685, 14734, 10080, 10026, 44, 10022, 11, 536, 44, 11, 536, 44, 65535, 261, 65530, 65532, 11, 27370, 59, 33, 11025, 10258, 16530, 12791, 12604, 1660, 10035, 1660, 10080, 65535, 261, 65530, 65534, 7176, 59, 33, 13734, 17728, 18025, 10080, 10026, 44, 10022, 11, 536, 44, 11, 536, 10035, 65535, 261, 65530, 65532, 11, 27370, 59, 33, 12878, 11899, 10333, 10080, 65535, 261, 65530, 65534, 7176, 59, 33, 12337, 18216, 10685, 15752, 12176, 10257, 12469, 28329, 65535, 261]
[2023-09-30 21:50:23,196] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[0.0001, 0.0001, 0.0001], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.9026119709014893 batch_loss 1.1472884385209334
==== [11454, 15847, 10323, 11909, 10696, 19137, 33, 14488, 14543, 11454, 17149, 16403, 16742, 10325, 15475, 10323, 16744, 28331, 65530, 65532, 14488, 14543, 10464, 13234, 16742, 10250, 10258, 10258, 17919, 14734, 11975, 16735, 19137, 17148, 17303, 11975, 16735, 11017, 9822, 18256, 13764, 14734, 15898, 12319, 9823, 19137, 11975, 16735, 12847, 17162, 10333, 17268, 10349, 11461, 13126, 19156, 11, 10994, 11517, 11889, 15034, 46, 16050, 13770, 10660, 13091, 9822, 14445, 13849, 9823, 11105, 17131, 11517, 12154, 10305, 18117, 19137, 15847, 14221, 10322, 12604, 10333, 10306, 11038, 10305, 10420, 14734, 15640, 15105, 19151, 13593, 12345, 10079, 14766, 16902, 10079, 15671, 12636, 15486, 16099, 14734, 10327, 10933, 10370, 19137, 15169, 15169, 10080, 17148, 13580, 13091, 17148, 12232, 11517, 12154, 11687, 13581, 11932, 16956, 14734, 10974, 11416, 10305, 10250, 10080, 13748, 14734, 17176, 13064, 11047, 17268, 17131, 19137, 16050, 13770, 10660, 17165, 11913, 10441, 14734, 12148, 11855, 17384, 10449, 14808, 16684, 11632, 11459, 10447, 10928, 16893, 14734, 10449, 12626, 19137, 17148, 10349, 10370, 12182, 10292, 11517, 10285, 14782, 10342, 10335, 13020, 14734, 11418, 10422, 9822, 10692, 12143, 9823, 19137, 16924, 10349, 11630, 12387, 19137, 12289, 16416, 12359, 10260, 16956, 17222, 14734, 12979, 10673, 10080, 12631, 10399, 19137, 10419, 10414, 11124, 10994, 15931, 10250, 16939, 17149, 14894, 18458, 17335, 11920, 14734, 17796, 11855, 19137, 17148, 10349, 9822, 10692, 12143, 9823, 10285, 14734, 11975, 11647, 14778, 10511, 12348, 14862, 19151, 13191, 17558, 10370, 17149, 10333, 11517, 19126, 10250, 10349, 13637, 17097, 15478, 12366, 14734, 12822, 11855, 17147, 10427, 13234, 11010, 10529, 19137, 16735, 17268, 10283, 17149, 11517, 14734, 17787, 14583, 10370, 12172, 14808, 10250, 13205, 11975, 10702, 11855, 19137, 11975, 10702, 11855, 12421, 15752, 12848, 14966, 10390, 16721, 16679, 10373, 10303, 16713, 19151, 17147, 16735, 17268, 10283, 17787, 14583, 10370, 12172, 14808, 10250, 10283, 10423, 15847, 12144, 16939, 16981, 14734, 15209, 11855, 28329, 11, 17148, 13855, 12445, 15123, 10799, 16443, 11638, 11920, 10784, 11899, 10292, 10260, 11021, 10529, 19151, 13191, 17148, 10303, 11638, 13205, 10336, 14734, 18256, 13764, 12158, 19137, 10384, 13234, 10260, 10423, 16939, 17145, 16050, 13770, 10660, 15887, 11478, 10250, 17384, 10305, 10696, 28329, 11, 17148, 10423, 10650, 13580, 13091, 11517, 17384, 14734, 10250, 17303, 10768, 10449, 12626, 10733, 11622, 16944, 17038, 10079, 11013, 10250, 17303, 10768, 13580, 16503, 17043, 10258, 14817, 16525, 14734, 13064, 10560, 19137, 14894, 18458, 17335, 11920, 17384, 11907, 10370, 11944, 11944, 19137, 13734, 10745, 10283, 10370, 14791, 16519, 18000, 14808, 13443, 13374, 16939, 17149, 13234, 14734, 10994, 15931, 10080, 10390, 14734, 9822, 16403, 13221, 9823, 10322, 17819, 10953, 10760, 14446, 11454, 10390, 17038, 11044, 19137, 12266, 11726, 14015, 12395, 10529, 12366, 13101, 13101, 12466, 12466, 11459, 13582, 10258, 11022, 17775, 10080, 10250, 16519, 10305, 10258, 19137, 11471, 11454, 15269, 15296, 13202, 13338, 13039, 14734, 17335, 11907, 10621, 10250, 10283, 10370, 10441, 14734, 10448, 10258, 11647, 13234, 19137, 14638, 12366, 17385, 17385, 11459, 14779, 14808, 15847, 12144, 14734, 17335, 13239, 10080, 65535, 261, 65530, 65532, 17148, 13205, 11975, 16735, 10285, 13191, 10745, 10283, 10370, 19156, 11, 65535, 261, 65530, 65534, 17148, 13205, 11975, 16735, 10285, 13191, 10256, 10283, 10370, 10080, 10390, 10402, 10768, 10790, 13091, 19151, 11, 50, 47, 33, 10994, 15931, 19151, 10250, 10283, 10423, 15847, 12144, 16939, 16981, 14734, 14445, 13849, 11517, 12154, 17787, 14583, 10370, 19137, 12172, 14808, 10250, 10283, 11975, 15209, 11855, 28329, 51, 47, 33, 10994, 15931, 14734, 9822, 10692, 12143, 9823, 19151, 10250, 10283, 17335, 11061, 17384, 14734, 11975, 11647, 14778, 19137, 16875, 16879, 12824, 12220, 10994, 15931, 17149, 11517, 28329, 52, 47, 33, 10994, 15931, 14734, 9822, 10692, 12143, 9823, 19151, 10250, 10283, 10449, 11454, 16050, 13770, 10660, 11517, 11630, 14734, 10710, 13653, 19137, 12172, 14808, 10250, 10283, 11975, 15209, 11855, 17149, 11517, 28329, 53, 47, 33, 17335, 18107, 17384, 14734, 10687, 10390, 10370, 19151, 10390, 10402, 11957, 10994, 15931, 11124, 10994, 15931, 14734, 9822, 10692, 12143, 9823, 12746, 12395, 14638, 12396, 12231, 19137, 10444, 10322, 13734, 13191, 16692, 12797, 16692, 13078, 10390, 10402, 13091, 10580, 14734, 28329, 54, 47, 33, 17335, 18107, 17384, 10687, 10390, 10370, 19151, 10894, 12732, 17268, 10349, 11269, 17358, 10333, 17335, 10079, 12609, 15643, 12509, 16503, 12642, 12822, 13855, 12445, 14734, 17732, 13001, 10370, 11098, 28329, 12421, 10305, 19137, 17148, 17303, 11975, 16735, 10293, 16503, 12847, 17162, 10333, 10250, 15604, 17787, 14583, 10370, 11454, 16050, 13770, 10660, 11517, 10285, 10997, 14583, 14734, 12985, 10336, 10080, 65535, 261, 65530, 65534, 17148, 17384, 13191, 11013, 11630, 10250, 10283, 15178, 13336, 10080, 17148, 13205, 11975, 16735, 10285, 10293, 16503, 12847, 17162, 10333, 10256, 10283, 10370, 10080, 10390, 10402, 10768, 10790, 13091, 19151, 11, 50, 47, 33, 10994, 15931, 19151, 10250, 10283, 10423, 15847, 12144, 16939, 16981, 14734, 14445, 13849, 11517, 12154, 17787, 14583, 10370, 19137, 12172, 14808, 10250, 10283, 11975, 15209, 11855, 28329, 51, 47, 33, 10250, 10283, 17335, 11061, 15640, 13248, 19137, 16875, 16879, 12824, 12335, 10994, 15931, 28329, 52, 47, 33, 10250, 10283, 15091, 14808, 14894, 13055, 10079, 14015, 15778, 15737, 13943, 14734, 14601, 10370, 19137, 13091, 17148, 11920, 17335, 11061, 14734, 12182, 11907, 10080, 10390, 11471, 11454, 10994, 15931, 11957, 17919, 19137, 13064, 10260, 13064, 11459, 14839, 10250, 14806, 10994, 15931, 28329, 53, 47, 33, 17335, 18107, 17384, 10687, 10390, 10370, 19151, 10894, 12732, 17335, 10525, 10079, 18004, 11907, 11124, 12223, 11098, 15169, 15169, 28329, 54, 47, 33, 16050, 13770, 10660, 11517, 19151, 17148, 12232, 11517, 12154, 14734, 11043, 11860, 10080, 43, 30579, 65535, 261, 65530, 65534, 17148, 17384, 13091, 15162, 10256, 10283, 15178, 13336, 10080, 33, 17148, 15216, 11975, 16735, 10293, 16503, 12847, 17162, 14734, 13091, 10994, 15931, 11124, 10419, 10414, 13234, 10792, 10250, 10283, 17131, 17804, 11517, 12154, 16050, 13770, 10660, 14734, 12484, 13126, 28329, 10688, 10452, 13234, 16735, 59, 11, 50, 47, 33, 16050, 13770, 10660, 11517, 13091, 10283, 13916, 10328, 14734, 11459, 13036, 45, 15674, 17848, 14808, 11036, 15033, 13593, 12345, 11124, 11021, 14638, 10370, 14328, 28329, 51, 47, 33, 10994, 15931, 11124, 10419, 10414, 13234, 10792, 16050, 13770, 10660, 11044, 45, 15123, 10953, 12282, 16944, 10333, 10370, 10402, 14734, 13773, 12522, 10080, 11416, 10292, 10994, 15931, 12172, 13191, 11656, 12418, 14734, 16403, 13221, 15209, 10399, 10992, 10250, 10283, 15752, 12848, 10494, 12848, 14966, 14734, 15150, 16678, 13205, 28329, 52, 47, 33, 14221, 15646, 10370, 10402, 16944, 10782, 16667, 10292, 10994, 15931, 10260, 11021, 15752, 13091, 10283, 18256, 13764, 12158, 45, 11416, 10292, 18256, 13764, 12158, 10260, 10423, 12824, 17145, 10621, 16050, 13770, 10660, 17148, 13324, 14734, 11517, 12154, 28329, 53, 47, 33, 10390, 10402, 13234, 10792, 14894, 18458, 17335, 11920, 45, 17335, 11907, 10402, 16519, 10792, 10994, 15931, 11124, 10390, 14734, 16403, 13221, 11044, 45, 14987, 12484, 10655, 14015, 12395, 14638, 28329, 12421, 14734, 13234, 16735, 45, 17189, 17141, 12847, 17162, 10994, 15931, 11124, 10419, 10414, 13234, 10792, 16050, 13770, 10660, 11517, 14734, 11461, 13126, 45, 13146, 14966, 17148, 10283, 11459, 13036, 10655, 14015, 10952, 17801, 11124, 14987, 15036, 10080, 10994, 15931, 11124, 10390, 14734, 11656, 14332, 16403, 16434, 10322, 12282, 16944, 10333, 10370, 10402, 14734, 12395, 14638, 12396, 12231, 10080, 65535, 261, 65530, 65532, 11454, 10305, 10808, 14734, 11975, 16735, 10285, 13191, 10745, 10283, 10293, 16503, 16530, 15898, 10760, 11461, 19156, 11, 65535, 261, 65530, 65534, 10293, 16503, 16530, 15898, 59, 11, 50, 47, 33, 10994, 15931, 59, 10250, 10283, 12172, 14808, 11656, 12418, 16403, 13221, 14734, 17787, 14583, 10370, 45, 10390, 14734, 16403, 13221, 17384, 13191, 10250, 10283, 11975, 14763, 11855, 28329, 51, 47, 33, 10419, 10414, 59, 10250, 10283, 10423, 15847, 12144, 16939, 16981, 14734, 15209, 11855, 45, 10390, 12172, 14808, 10250, 10283, 11975, 14763, 11855, 28329, 52, 47, 33, 17335, 11061, 15640, 13248, 10692, 10692, 17335, 10525, 59, 10390, 13091, 17148, 11920, 17335, 11061, 14734, 12182, 11907, 45, 11957, 10994, 15931, 11124, 10419, 10414, 12337, 12527, 10685, 16954, 28329, 53, 47, 33, 10687, 10390, 10449, 12626, 59, 17335, 11061, 17384, 14734, 10687, 10390, 10449, 12626, 45, 10894, 12732, 10250, 10349, 13637, 17097, 11656, 12418, 14734, 11920, 10422, 28329, 54, 47, 33, 17148, 10349, 10370, 15674, 17848, 11454, 14894, 18458, 17335, 11920, 45, 16520, 11940, 10994, 15931, 28329, 55, 47, 33, 12985, 10336, 10285, 12848, 10792, 10333, 18256, 13764, 10079, 18256, 13764, 12158, 10079, 18256, 13764, 14328, 11159, 10399, 10992, 18256, 13764, 10305, 11517, 16050, 13770, 10660, 28329, 56, 47, 33, 12985, 10336, 10285, 17147, 12848, 10792, 10333, 10994, 15931, 12172, 14808, 10250, 13205, 12636, 10702, 10080, 65535, 261]
-> item_loss 2.015252113342285 batch_loss 1.1695439173625066
==== [17728, 15178, 11, 65530, 65532, 10464, 11017, 10373, 10303, 11043, 11860, 19156, 11, 65535, 261, 65530, 65534, 12605, 11017, 11975, 13078, 28329, 65535, 261, 65530, 65532, 10464, 13191, 10373, 10303, 14189, 12454, 11053, 19156, 11, 65535, 261, 65530, 65534, 12605, 13191, 13064, 10560, 10423, 12509, 19137, 11687, 13258, 12605, 10260, 11861, 11454, 10339, 17148, 10283, 10267, 14610, 10257, 10423, 12401, 10303, 13324, 10080, 10444, 13091, 12605, 14862, 17222, 17148, 13091, 10260, 11021, 15752, 14734, 28329, 65535, 261, 65530, 65532, 10292, 10373, 10303, 19156, 33, 10370, 10260, 10423, 13594, 11053, 19156, 11, 65535, 261, 65530, 65534, 11687, 13258, 10464, 16735, 14734, 13091, 10250, 10283, 14328, 10452, 12609, 15643, 10250, 10283, 10370, 19137, 17268, 10303, 10390, 10402, 10423, 13594, 10353, 10080, 10444, 13091, 11687, 13258, 10464, 16735, 14734, 13091, 10250, 10283, 14141, 18245, 19137, 17268, 10303, 10390, 10402, 10260, 10423, 13594, 10353, 28329, 65535, 261, 65530, 65532, 10464, 13091, 16735, 19137, 10464, 12727, 13191, 14141, 18245, 19137, 10260, 13091, 10250, 10283, 10370, 12137, 13133, 15752, 19156, 11, 65535, 261, 65530, 65534, 13091, 14734, 19137, 12605, 12727, 13191, 14141, 18245, 19137, 10260, 13091, 10250, 10283, 13212, 11369, 10370, 10080, 12605, 11021, 10399, 12527, 11002, 10792, 14658, 14673, 19137, 10322, 10423, 11416, 10292, 14299, 11124, 10993, 12484, 15646, 12527, 10792, 18216, 10685, 28329, 65535, 261, 65530, 65532, 10464, 16667, 10292, 10464, 13091, 11060, 12727, 13191, 14141, 18245, 19156, 11, 65535, 261, 65530, 65534, 12605, 16667, 10292, 12605, 12727, 13191, 14141, 18245, 10080, 11416, 10292, 12605, 11021, 10399, 12527, 11002, 10792, 14658, 14673, 10079, 14299, 11124, 10993, 12484, 19137, 17148, 10349, 17311, 13091, 10370, 15261, 14370, 13191, 14734, 11159, 16884, 28329, 65535, 261, 65530, 65532, 12605, 10260, 10529, 19137, 16692, 13078, 15494, 12605, 14791, 14791, 10080, 12605, 14446, 11454, 15494, 10464, 10250, 10283, 13212, 10423, 19137, 16672, 10464, 16692, 13078, 15494, 12605, 14791, 10080, 10444, 13091, 16738, 16678, 10449, 19137, 17148, 11016, 13091, 16672, 10464, 16692, 13078, 15494, 12605, 14791, 10080, 10464, 11021, 10399, 11986, 16707, 11414, 12509, 16944, 15847, 12144, 13091, 11060, 12727, 13191, 14141, 18245, 10080, 11687, 13258, 10464, 12509, 10260, 16944, 13234, 19137, 17268, 10303, 10464, 11994, 10260, 15752, 16692, 13078, 15494, 12605, 14791, 28329, 65535, 261]
-> item_loss 1.0360426902770996 batch_loss 1.1662063866853714
==== [25106, 11, 65530, 65531, 18119, 16002, 13091, 10250, 10283, 12337, 15678, 13078, 14734, 820, 19137, 10293, 10370, 12176, 18119, 16002, 10588, 10333, 10250, 10283, 15304, 15500, 19137, 16672, 11684, 15752, 11634, 10588, 12337, 11632, 10336, 12484, 28329, 18119, 16002, 14734, 10841, 15752, 19151, 11, 50, 47, 12307, 10293, 10370, 16672, 18119, 16002, 12176, 10390, 12945, 10460, 14600, 15763, 10257, 14734, 10272, 16501, 14734, 13064, 10560, 19137, 18119, 16002, 11021, 10399, 17109, 10760, 1660, 92, 12945, 10460, 14600, 15763, 1696, 35, 12748, 10398, 11124, 15304, 15500, 10355, 10342, 19137, 14221, 11044, 13326, 12797, 15304, 15500, 14734, 12282, 11960, 13855, 12445, 13234, 12945, 10460, 14600, 15763, 28329, 51, 47, 17109, 10760, 1660, 92, 10260, 10997, 13855, 12445, 1696, 35, 14734, 16713, 19137, 10293, 10370, 11994, 10260, 10423, 12979, 10792, 18119, 16002, 14734, 13855, 12445, 19137, 10260, 12509, 12642, 12658, 10293, 10370, 14734, 13064, 10560, 18119, 16002, 11994, 11021, 10399, 17148, 10303, 10588, 28329, 52, 47, 17798, 10333, 10257, 10257, 17162, 12748, 10398, 10399, 11630, 19137, 18119, 16002, 17147, 15752, 10482, 14589, 14734, 12748, 10398, 13191, 19151, 1660, 92, 12642, 12266, 15090, 16749, 1696, 384, 1660, 92, 16689, 15586, 17738, 17544, 1696, 384, 1660, 92, 10684, 17727, 16728, 17952, 1696, 384, 1660, 92, 12266, 11068, 16728, 17952, 1696, 35, 10080, 33, 17879, 16503, 13773, 12522, 14734, 13091, 19137, 11016, 13191, 15304, 15500, 12848, 10792, 14734, 12748, 10398, 11021, 10399, 10482, 14589, 10080, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 18119, 16002, 11, 65535, 261, 65530, 65534, 18119, 16002, 59, 33, 10293, 10370, 12664, 12605, 13191, 10373, 10303, 10336, 12484, 11053, 64, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 12176, 12605, 12266, 10258, 124, 12220, 14589, 11043, 126, 28329, 65535, 261, 65530, 65534, 18119, 16002, 59, 358, 12220, 14589, 11043, 126, 19156, 17268, 13091, 10373, 10303, 19156, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11416, 10292, 14446, 11454, 13091, 11454, 16675, 15475, 16728, 13021, 17384, 19137, 12631, 10399, 11885, 13734, 13191, 16443, 11434, 11899, 10258, 13234, 19137, 11903, 17781, 10257, 11885, 12220, 16721, 10250, 10283, 14600, 15763, 12220, 14589, 14734, 11043, 11860, 28329, 65535, 261, 65530, 65534, 18119, 16002, 59, 326, 12945, 10460, 14600, 15763, 94, 11, 65535, 261, 65530, 65531, 40812, 59, 33, 11021, 10399, 17149, 16403, 14734, 12945, 10460, 19151, 11, 12642, 12266, 12220, 14589, 45, 10482, 14589, 13012, 10407, 15304, 15500, 45, 11, 16738, 17177, 12731, 10250, 10283, 12945, 10460, 19151, 65535, 261, 65530, 65534, 18119, 16002, 59, 33, 12642, 12266, 12220, 14589, 11, 65535, 261, 65530, 65531, 40812, 59, 33, 11021, 17177, 14734, 12220, 14589, 19151, 16678, 10336, 13205, 10079, 16664, 15200, 11369, 10079, 1589, 23682, 10079, 13829, 16524, 11369, 10079, 17273, 10407, 10079, 124, 12220, 14589, 11043, 126, 10080, 16738, 17109, 10760, 17879, 16503, 12642, 12266, 14734, 12220, 14589, 19151, 11, 65535, 261, 65530, 65534, 18119, 16002, 59, 358, 12220, 14589, 11043, 126, 11, 65535, 261, 65530, 65531, 40812, 59, 358, 12220, 14589, 11043, 126, 12145, 12642, 12266, 28329, 65535, 261, 65530, 65534, 18119, 16002, 59, 33, 12145, 15486, 12642, 12266, 10333, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11315, 19137, 12605, 14791, 10792, 10333, 28329, 65535, 261, 65530, 65534, 18119, 16002, 59, 33, 10293, 10370, 13191, 10336, 10703, 13234, 12664, 12605, 11183, 127, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 13191, 10256, 10283, 14763, 11855, 19137, 11885, 10402, 14734, 13308, 15189, 10257, 17919, 10768, 10790, 10708, 14808, 9822, 15966, 13258, 9823, 10079, 9822, 18119, 16184, 9823, 10079, 9822, 15966, 13258, 11124, 18119, 16184, 9823, 10080, 14446, 11454, 14862, 17222, 11016, 13191, 10250, 10283, 15966, 13258, 11124, 10250, 10283, 18119, 16184, 10079, 13633, 10283, 14763, 11855, 17384, 13188, 11632, 11016, 13191, 10250, 10283, 13676, 13258, 19137, 12202, 10265, 10256, 10283, 14763, 11855, 10257, 14734, 13308, 15189, 17311, 13091, 17631, 16730, 14734, 19137, 11687, 13258, 16672, 10464, 12642, 12266, 10687, 10285, 10250, 10283, 14763, 11855, 13234, 14914, 16667, 10256, 10283, 14763, 11855, 17384, 17311, 13091, 10373, 10303, 19137, 10464, 10423, 12401, 10303, 10588, 19156, 11, 65535, 261, 65530, 65534, 18119, 16002, 59, 33, 12605, 10423, 12642, 12266, 10708, 14808, 9822, 15966, 13258, 11124, 18119, 16184, 9823, 17268, 10283, 14763, 11855, 10080, 11416, 10292, 11016, 13191, 17148, 13324, 12605, 12638, 15752, 14914, 11899, 10256, 10283, 14763, 11855, 17384, 17311, 13091, 10373, 10303, 28329, 10293, 10370, 19137, 12605, 13234, 16533, 17382, 10250, 10258, 10974, 11416, 10080, 18117, 10658, 19137, 11687, 13258, 12605, 12642, 12266, 10250, 10283, 14763, 11855, 19137, 17384, 17919, 11021, 15752, 10760, 14446, 14734, 12484, 10724, 13091, 19151, 13191, 10250, 10283, 15966, 13258, 10079, 13191, 10250, 10283, 18119, 16184, 10079, 10373, 10303, 10322, 13734, 13191, 28329, 11687, 13258, 12605, 12642, 12266, 10708, 14808, 9822, 15966, 13258, 9823, 12609, 9822, 18119, 16184, 9823, 14734, 14763, 11855, 19137, 12202, 10265, 17384, 17919, 13191, 13676, 13258, 19137, 17268, 10303, 17384, 17919, 14734, 13676, 13258, 10250, 11899, 13091, 10261, 13308, 15189, 10257, 12631, 10708, 10260, 11042, 14734, 13676, 13258, 10080, 17268, 10303, 12605, 11994, 13051, 13764, 14914, 11899, 11013, 11630, 10278, 10283, 14763, 11855, 10285, 13091, 10373, 10303, 12484, 10724, 19152, 15646, 12605, 15957, 13091, 12642, 12266, 10708, 14808, 9822, 15966, 13258, 11124, 18119, 16184, 9823, 14734, 17268, 10283, 14763, 11855, 19137, 11021, 15752, 10423, 10760, 14446, 10256, 15033, 12484, 10724, 19151, 11, 50, 47, 17148, 10283, 14763, 11855, 17384, 13191, 10250, 10283, 15966, 13258, 19137, 17268, 10303, 10708, 14808, 9822, 18119, 16184, 9823, 14734, 14763, 11855, 17384, 10250, 11899, 10373, 10303, 10322, 13734, 13191, 19137, 10708, 14808, 9822, 15966, 13258, 9823, 14734, 14763, 11855, 17384, 10250, 11899, 13191, 10250, 13326, 18119, 16184, 10080, 11416, 10292, 19137, 12605, 10402, 14862, 17222, 11016, 13191, 10250, 10283, 15966, 13258, 11124, 10250, 13326, 18119, 16184, 10080, 11687, 13258, 10708, 14808, 9822, 18119, 16184, 9823, 14734, 14763, 11855, 17384, 13191, 13676, 13258, 19137, 17268, 10303, 17268, 10283, 13676, 13258, 10250, 11899, 13091, 18119, 16184, 19137, 17148, 13324, 14734, 16713, 19137, 11994, 10423, 10261, 9822, 10256, 10283, 14763, 11855, 10257, 14734, 13308, 15189, 17311, 13091, 17631, 16730, 14734, 9823, 14734, 10336, 11903, 14782, 12463, 10080, 12631, 10399, 19137, 10708, 14808, 9822, 18119, 16184, 9823, 14734, 14763, 11855, 17384, 10250, 11899, 13091, 15090, 14734, 10080, 14597, 13581, 11021, 10399, 14862, 17222, 19137, 17268, 13326, 18119, 16184, 10250, 11899, 11454, 10708, 14808, 9822, 15966, 13258, 9823, 14734, 14763, 11855, 17384, 28329, 51, 47, 17148, 10283, 14763, 11855, 17384, 13191, 10250, 10283, 18119, 16184, 10080, 17148, 15033, 12484, 10724, 10258, 14734, 12826, 14486, 13036, 13764, 10261, 14763, 11855, 17384, 13191, 15966, 13258, 13091, 10250, 15850, 14734, 19137, 11016, 10260, 17141, 12669, 15966, 13258, 11124, 18119, 16184, 10342, 12791, 10250, 10258, 28329, 52, 47, 17148, 10283, 14763, 11855, 17384, 10373, 10303, 10322, 13734, 13191, 10080, 17268, 10303, 11013, 11630, 10278, 10283, 14763, 11855, 17384, 10250, 11899, 13191, 13676, 13258, 10080, 10708, 14808, 9822, 15966, 13258, 9823, 14734, 14763, 11855, 17384, 13191, 18119, 16184, 19137, 10708, 14808, 9822, 18119, 16184, 9823, 14734, 14763, 11855, 17384, 13191, 15966, 13258, 10080, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 10292, 10373, 10303, 12642, 12266, 10708, 14808, 9822, 15966, 13258, 9823, 12609, 10708, 14808, 9822, 18119, 16184, 9823, 14734, 14763, 11855, 10423, 11960, 15850, 13051, 13764, 14914, 11899, 11013, 11630, 10278, 10283, 14763, 11855, 10285, 13091, 10373, 10303, 12484, 10724, 19156, 11, 65535, 261, 65530, 65534, 18119, 16002, 59, 33, 10293, 10370, 19137, 13091, 11021, 15752, 10423, 11960, 15850, 13051, 13764, 14914, 11899, 11013, 11630, 10278, 10283, 14763, 11855, 10285, 13091, 10373, 10303, 12484, 10724, 10080, 11416, 10292, 10580, 16689, 12605, 12642, 12266, 10708, 14808, 9822, 15966, 13258, 9823, 14734, 14763, 11855, 19137, 11021, 15752, 10423, 10760, 14446, 10278, 15033, 12484, 10724, 19151, 11, 50, 47, 17384, 17919, 13191, 18119, 16184, 11, 51, 47, 17384, 17919, 10373, 10303, 10322, 13734, 13191, 11, 11687, 13258, 17384, 17919, 10373, 10303, 10322, 13734, 13191, 19137, 17268, 10303, 12605, 10402, 15752, 14914, 11899, 10708, 14808, 9822, 18119, 16184, 9823, 14734, 14763, 11855, 17384, 13191, 10250, 10283, 15966, 13258, 19137, 10708, 14808, 9822, 15966, 13258, 11124, 18119, 16184, 9823, 14734, 14763, 11855, 17384, 13191, 10250, 13326, 18119, 16184, 10080, 17148, 15033, 12484, 10724, 10258, 19137, 12605, 10402, 13091, 15752, 11634, 14914, 16667, 11013, 11630, 10278, 10283, 14763, 11855, 14734, 12484, 10724, 14734, 28329, 10444, 13091, 11687, 13258, 12605, 10402, 10997, 14446, 10708, 14808, 9822, 15966, 13258, 9823, 14734, 14763, 11855, 17384, 13191, 18119, 16184, 19137, 17268, 10303, 12605, 10402, 11994, 13051, 13764, 14914, 16667, 17268, 10283, 15966, 13258, 13091, 11454, 10708, 14808, 9822, 18119, 16184, 9823, 14734, 14763, 11855, 17384, 17147, 13091, 11454, 10708, 14808, 9822, 15966, 13258, 11124, 18119, 16184, 9823, 14734, 14763, 11855, 17384, 28329, 11687, 13258, 12642, 12266, 10708, 14808, 9822, 18119, 16184, 9823, 14734, 14763, 11855, 19137, 12826, 14486, 13036, 12276, 10322, 13091, 10250, 13324, 14734, 19137, 11016, 10260, 17141, 12669, 15966, 13258, 11124, 18119, 16184, 10342, 12791, 10250, 10258, 28329, 10293, 10370, 19137, 15752, 14486, 16533, 11053, 19156, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11315, 19137, 13734, 13191, 17728, 18025, 28329, 65535, 261, 65530, 65534, 18119, 16002, 59, 33, 12337, 11685, 127, 17268, 10303, 10293, 10370, 17147, 13191, 10687, 10390, 10336, 12484, 11053, 19156, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 10464, 15752, 10260, 11414, 12605, 17148, 11012, 16713, 10303, 19156, 11, 65535, 261, 65530, 65534, 18119, 16002, 59, 326, 10260, 10997, 13855, 12445, 94, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11315, 19137, 14791, 13234, 14914, 11903, 13734, 13191, 13855, 12445, 28329, 65535, 261, 65530, 65534, 18119, 16002, 59, 33, 12605, 12750, 10293, 10370, 14734, 16735, 13764, 10588, 10333, 10080, 10444, 13091, 18119, 16002, 17147, 13091, 12509, 11124, 10293, 10370, 16735, 16713, 14734, 43282, 11, 65535, 261]
-> item_loss 0.8055842518806458 batch_loss 1.1574107248608658
==== [25106, 11, 17148, 13619, 13012, 11860, 13091, 9822, 14931, 12314, 10267, 14610, 9823, 14734, 10250, 10283, 14313, 13619, 19137, 33, 16679, 17162, 10333, 14597, 13085, 17781, 10846, 14328, 11871, 11920, 16520, 11940, 12140, 18492, 15646, 12282, 10997, 14734, 10336, 10407, 11, 10305, 10808, 19137, 12140, 18492, 14734, 11861, 11454, 10377, 10377, 13091, 10250, 10283, 10580, 16735, 19137, 10444, 14931, 12314, 10267, 14610, 17131, 15545, 14116, 13819, 10257, 13036, 10250, 11621, 12822, 10760, 13234, 14734, 12034, 11981, 10257, 13191, 10250, 10283, 14987, 15036, 14734, 10660, 18295, 11979, 14428, 11435, 19137, 17268, 17384, 14734, 10660, 18295, 11979, 10370, 11454, 10250, 11473, 13188, 17795, 12076, 14734, 14869, 11592, 17997, 10257, 12883, 16689, 10333, 18216, 13273, 11124, 14005, 17085, 16451, 15586, 19137, 11967, 10745, 11043, 16520, 13827, 11098, 11492, 10792, 10267, 14610, 17131, 15545, 14734, 10258, 13036, 19137, 17148, 12638, 16692, 11903, 10333, 12140, 18492, 14734, 11861, 11454, 10080, 17148, 10349, 16520, 13827, 11098, 11471, 11454, 10250, 15891, 17686, 13191, 14869, 15963, 15102, 12626, 14734, 17587, 15887, 17384, 19137, 15752, 11634, 17181, 17141, 13676, 17878, 14791, 13923, 11630, 17919, 14734, 13126, 15850, 28329, 17587, 15887, 11454, 17131, 15545, 14116, 13819, 10258, 17919, 12747, 10333, 12337, 10301, 19137, 14221, 11044, 19137, 11638, 15604, 11680, 17836, 12705, 11414, 17587, 15887, 11124, 17587, 15887, 17384, 17919, 14734, 13057, 13199, 13085, 17781, 10846, 14328, 11871, 11920, 28329, 10390, 10402, 12172, 11414, 10333, 12337, 11632, 10529, 12445, 19137, 10894, 12732, 17777, 11436, 11416, 10399, 10992, 12140, 16803, 14734, 12314, 14349, 11124, 12025, 12415, 10080, 14221, 15646, 19137, 17148, 10349, 10529, 12445, 10383, 14221, 10260, 15752, 12329, 12221, 16533, 17382, 11887, 11898, 14734, 12415, 16884, 11124, 11861, 11454, 14734, 12522, 10304, 28329, 13637, 11687, 19137, 17777, 11436, 11416, 14734, 12415, 10790, 11994, 17843, 10399, 14914, 11899, 10080, 13085, 17781, 10846, 14328, 11871, 11920, 10402, 10529, 16589, 13054, 13054, 11459, 11610, 15044, 19137, 17148, 10283, 12415, 10790, 17728, 18025, 17916, 12182, 10684, 17644, 19137, 11016, 13191, 12261, 17194, 10250, 10283, 13179, 11638, 13179, 11474, 11434, 14734, 12978, 13273, 19137, 12669, 11640, 15090, 10305, 15887, 11492, 12348, 13179, 10448, 19137, 12638, 15752, 12270, 13923, 10336, 11903, 10080, 15646, 14778, 10808, 19137, 10390, 10402, 11016, 15752, 13326, 12797, 12145, 14862, 14734, 11887, 11898, 14862, 16696, 17149, 16403, 12826, 13827, 28329, 13191, 14734, 14486, 16686, 16667, 10292, 19137, 17777, 11436, 11416, 16714, 14583, 10339, 16256, 13051, 19137, 10390, 10423, 13679, 12431, 11459, 14296, 10258, 10985, 19137, 10525, 12746, 14808, 11042, 10250, 15033, 13582, 10418, 19137, 16939, 17149, 11013, 10250, 14313, 16256, 13051, 10080, 11454, 11871, 15643, 10285, 17734, 19137, 17148, 15033, 14486, 16686, 12337, 11002, 12826, 12087, 28329, 11013, 10250, 15033, 14486, 16686, 10779, 11002, 10792, 11896, 12996, 14610, 10370, 11606, 14734, 13558, 17143, 10080, 17148, 10250, 14486, 16686, 16667, 10292, 19137, 17777, 11436, 11416, 10384, 9822, 16714, 14583, 11459, 9823, 12266, 11726, 14296, 19137, 10250, 14781, 14296, 10792, 9822, 10355, 17332, 10305, 13064, 9823, 19137, 11416, 10292, 11887, 11898, 17384, 17147, 13191, 10790, 14734, 13085, 13085, 19137, 11885, 10402, 15713, 11899, 10322, 18159, 11454, 10790, 14734, 12140, 18492, 17038, 10257, 10080, 9822, 10355, 17332, 10305, 13064, 9823, 11016, 13191, 10250, 13557, 19137, 10390, 10402, 10250, 11899, 10423, 14866, 13137, 15646, 14195, 14179, 11459, 10355, 11038, 10080, 17189, 17141, 17148, 11461, 10655, 14015, 14091, 12484, 14734, 15489, 11038, 19137, 13034, 18492, 16714, 14583, 10333, 19137, 17819, 11044, 10511, 15752, 12914, 16944, 10250, 14313, 13034, 14734, 10267, 14610, 10080, 17148, 10283, 14486, 16686, 16443, 15044, 10292, 9822, 10781, 10267, 11638, 14288, 14171, 9823, 10580, 16735, 28329, 10250, 11043, 16928, 12604, 9822, 12431, 14296, 9823, 14486, 16686, 14734, 12201, 17093, 11887, 11898, 18492, 11871, 11920, 13580, 11454, 16707, 18183, 10250, 15033, 13034, 10997, 13078, 14734, 13197, 17150, 17674, 19137, 10390, 12159, 13197, 15752, 11634, 15287, 14914, 11459, 13827, 17387, 10760, 11638, 17777, 11436, 11416, 11025, 14806, 14734, 10995, 14240, 14426, 28329, 15489, 13258, 19137, 17189, 17141, 13197, 17150, 17674, 19137, 10390, 10997, 14446, 14931, 12314, 10267, 14610, 10285, 17090, 13036, 11047, 13833, 14187, 14010, 14010, 43275, 14931, 12314, 10267, 14610, 10257, 13188, 11011, 15640, 14734, 11517, 12154, 12145, 13091, 10250, 14313, 14136, 13849, 10080, 17148, 10283, 11633, 13115, 10997, 14583, 10333, 11640, 11632, 14734, 10336, 19137, 15646, 17148, 11043, 12201, 17093, 14734, 11871, 15643, 13091, 15162, 10250, 10283, 10997, 14446, 14146, 12484, 14734, 12005, 11630, 10370, 28329, 10673, 11633, 10305, 11044, 19137, 10390, 11515, 11647, 17565, 14880, 19137, 12669, 17148, 10407, 10336, 12679, 10792, 10333, 15763, 11044, 10080, 11021, 13051, 16686, 11687, 10454, 19137, 10390, 13091, 15162, 10250, 10283, 10997, 14446, 14146, 12484, 14734, 28329, 17147, 13191, 10790, 10370, 10322, 14791, 16519, 10333, 43282, 65530, 65532, 17728, 18025, 19151, 33, 12985, 10336, 10285, 18492, 11871, 11920, 11454, 16520, 11940, 12140, 18492, 11042, 13064, 17147, 10997, 14446, 10333, 10373, 10303, 10336, 10407, 19156, 11, 65535, 261, 65530, 65534, 12985, 10336, 10285, 45, 12201, 17093, 14734, 18492, 11871, 11920, 17189, 17141, 13034, 10997, 13078, 14734, 13197, 17150, 17674, 11454, 16520, 11940, 11638, 17777, 11436, 11416, 13064, 45, 12522, 11630, 10997, 14446, 14931, 12314, 10267, 14610, 10285, 17090, 13036, 11047, 13188, 11011, 15640, 14734, 11517, 12154, 10997, 14583, 10333, 14136, 14146, 28329, 65535, 261, 65530, 65532, 13012, 13205, 10285, 17147, 16735, 10333, 10373, 10303, 19156, 11, 65535, 261, 65530, 65534, 12985, 10336, 10285, 17147, 12847, 17162, 10333, 59, 11, 50, 47, 33, 10305, 10808, 12140, 18492, 14734, 11861, 11454, 10377, 10377, 13091, 10250, 10283, 10580, 16735, 45, 14781, 10792, 13191, 10370, 17189, 17141, 13273, 16689, 11454, 14931, 12314, 10267, 14610, 17131, 15545, 14734, 16689, 11622, 12638, 16692, 11903, 10333, 12140, 18492, 14734, 11861, 11454, 28329, 51, 47, 33, 16520, 13827, 11098, 16520, 13827, 10792, 14734, 12140, 18492, 10529, 12445, 10383, 14221, 10260, 15752, 16533, 17382, 11887, 11898, 14734, 13205, 16884, 28329, 52, 47, 33, 13191, 10684, 11638, 17777, 11436, 11416, 12415, 10790, 14734, 10335, 16676, 10080, 13191, 10370, 16667, 10292, 11885, 13091, 17851, 12415, 45, 10384, 16256, 15090, 16714, 14583, 12202, 13679, 17150, 11047, 10808, 14296, 16403, 60, 10322, 13191, 10370, 16667, 10292, 11885, 13091, 17846, 12415, 45, 10423, 10261, 10687, 10390, 12140, 18492, 10355, 17332, 12202, 10781, 17194, 13034, 14583, 11117, 28329, 53, 47, 33, 17268, 11043, 12201, 17093, 14734, 18492, 11871, 11920, 17189, 17141, 13034, 14734, 13197, 17150, 17674, 45, 11454, 16520, 11940, 11638, 17777, 11436, 11416, 14734, 11042, 13064, 10997, 14446, 14931, 12314, 10267, 14610, 13188, 11011, 15640, 14734, 11517, 12154, 14808, 14136, 10333, 10080, 10444, 10673, 11633, 11044, 10390, 13734, 13191, 17385, 16523, 17148, 10283, 10997, 14446, 45, 15507, 15511, 14880, 15087, 12140, 18492, 28329, 54, 47, 33, 17147, 13191, 10790, 10370, 10322, 14791, 10792, 10333, 17148, 11461, 14136, 14146, 28329, 10293, 16503, 16679, 17162, 14734, 13091, 17148, 11043, 12201, 17093, 18492, 11871, 11920, 17189, 17141, 13034, 13197, 17150, 17674, 10997, 14446, 14136, 14146, 14734, 10336, 45, 10444, 10390, 17177, 12731, 12392, 16523, 17148, 10283, 10997, 14446, 45, 15507, 15511, 14880, 15087, 12140, 18492, 10080, 65535, 261, 65530, 65532, 12985, 10336, 10285, 13191, 11187, 10349, 16710, 16760, 12208, 18441, 14734, 14313, 13619, 11, 65535, 261, 65530, 65534, 11454, 12848, 10494, 14734, 13012, 11860, 10285, 45, 13734, 13191, 12337, 13078, 13098, 14734, 16710, 16760, 12208, 18441, 14734, 14313, 13619, 10080, 10675, 13012, 11632, 13091, 12847, 10708, 13085, 17781, 10846, 14328, 11871, 11920, 16520, 11940, 12140, 18492, 10399, 10992, 10390, 10402, 10684, 10339, 12140, 18492, 11861, 11454, 12522, 10304, 14734, 11036, 15033, 14486, 16686, 14734, 10279, 15689, 15034, 11871, 12847, 17162, 28329, 10293, 18025, 16269, 14221, 14987, 11656, 45, 10444, 13012, 10285, 14734, 16728, 16537, 11124, 10696, 11922, 17311, 13637, 17097, 16667, 14794, 45, 13734, 13191, 12849, 15090, 14734, 12484, 15907, 12609, 11268, 10821, 12986, 13258, 10080, 10675, 13012, 11632, 13091, 12847, 17162, 11124, 12822, 16671, 12140, 18492, 14734, 15033, 15033, 11656, 12418, 12025, 12415, 10079, 11861, 11454, 12522, 10304, 11124, 16944, 13981, 45, 10399, 10992, 10684, 10339, 12140, 18492, 12415, 10790, 14734, 11871, 13208, 10335, 16686, 28329, 11687, 13258, 12368, 18001, 14734, 16713, 45, 11021, 10399, 11986, 16707, 10384, 10399, 10258, 10745, 10283, 13036, 17919, 12664, 10792, 10250, 10349, 16742, 16944, 13234, 13191, 14172, 10995, 12410, 12522, 11110, 14734, 10696, 11922, 59, 11, 50, 47, 33, 13085, 17781, 10846, 14328, 11871, 11920, 10402, 15931, 16899, 17268, 10303, 11632, 12366, 12410, 14880, 15087, 12140, 18492, 14734, 12415, 10790, 11124, 16944, 13981, 45, 10954, 12392, 14619, 10333, 14806, 10808, 11011, 11517, 14734, 12140, 11638, 14136, 14146, 10080, 17148, 15092, 13098, 10333, 10390, 10402, 11957, 13179, 11638, 11887, 11898, 11672, 15036, 14734, 12650, 14808, 45, 10399, 10992, 11957, 11105, 11430, 10267, 14610, 14734, 15568, 10309, 10684, 13773, 28329, 51, 47, 33, 10684, 10339, 12140, 18492, 16944, 13981, 11124, 12522, 10304, 14734, 10278, 15033, 10260, 11042, 14486, 16686, 45, 10768, 10790, 11002, 10792, 11871, 13208, 14610, 11124, 11896, 12996, 14610, 14734, 13558, 17143, 10080, 17148, 11021, 10399, 16523, 10292, 10260, 11042, 16520, 14172, 14734, 10250, 10283, 16688, 10798, 28329, 10444, 12421, 10452, 13234, 16735, 45, 12985, 10336, 12484, 15907, 11124, 13012, 11860, 16417, 17136, 10279, 15689, 11632, 10339, 12208, 18441, 10080, 11687, 13258, 15752, 12849, 10673, 10250, 10349, 14005, 15071, 14734, 15478, 15907, 45, 12609, 10482, 14589, 13179, 14583, 10846, 14734, 16728, 16537, 45, 10322, 16684, 10423, 16672, 13006, 10283, 12985, 10336, 11064, 13191, 13179, 11632, 12208, 18441, 12527, 10080, 65535, 261, 12985, 10336, 10285, 13191, 11187, 10349, 16530, 15898, 10760, 11461, 19156, 33, 10390, 10402, 14734, 14332, 14172, 10768, 10790, 13091, 10373, 10303, 19156, 11, 65530, 65534, 12985, 10336, 10285, 10293, 16503, 10760, 14446, 10333, 10399, 10258, 10745, 10283, 16530, 15898, 59, 11, 50, 47, 33, 13085, 17781, 10846, 14328, 11871, 11920, 59, 10390, 10402, 13188, 10782, 17189, 17141, 11454, 14931, 12314, 10267, 14610, 17131, 15545, 13273, 16689, 16689, 11622, 16692, 11903, 10333, 12140, 18492, 14734, 11861, 11454, 10080, 10390, 10402, 16520, 11940, 12140, 18492, 12202, 12848, 10760, 10260, 11042, 14486, 16686, 45, 16707, 11436, 16533, 17382, 12140, 18492, 14734, 12415, 16884, 11124, 12522, 10304, 10080, 10390, 10402, 11957, 12140, 18492, 11672, 15036, 14734, 12650, 14808, 11459, 14880, 15087, 28329, 51, 47, 33, 10660, 18295, 11979, 10370, 59, 10390, 10402, 11454, 14931, 12314, 10267, 14610, 17131, 15545, 12883, 16689, 18216, 13273, 11124, 17587, 15887, 45, 10482, 13085, 17781, 10846, 14328, 11871, 11920, 12348, 10399, 16520, 11940, 10792, 12140, 18492, 28329, 52, 47, 33, 12140, 18492, 17777, 11436, 11416, 59, 11861, 11454, 10339, 14931, 12314, 10267, 14610, 17131, 15545, 45, 13091, 13085, 17781, 10846, 14328, 11871, 11920, 16520, 11940, 14880, 15087, 14734, 11957, 16803, 10080, 10390, 14734, 12415, 10790, 11124, 16944, 13981, 11861, 11454, 10335, 16676, 28329, 53, 47, 33, 12201, 17093, 14734, 18492, 11871, 11920, 59, 17189, 17141, 13034, 10997, 13078, 14734, 13197, 17150, 17674, 12522, 11630, 10997, 14446, 10333, 13188, 11011, 15640, 11517, 12154, 14734, 11638, 14136, 45, 10444, 17177, 12731, 12392, 14619, 17148, 10283, 10997, 14446, 15507, 15511, 14880, 15087, 12140, 18492, 10080, 10397, 16417, 10333, 13085, 17781, 10846, 14328, 11871, 11920, 10402, 14734, 14880, 15087, 11001, 11047, 28329, 54, 47, 33, 11517, 12154, 12010, 13653, 59, 13734, 13191, 16722, 15478, 12847, 17162, 45, 11016, 12848, 10792, 10333, 13188, 11011, 15640, 14734, 11517, 12154, 16443, 11638, 14136, 12901, 13626, 45, 10444, 17148, 10250, 10336, 10407, 16443, 12201, 17093, 18492, 11871, 11920, 12392, 14619, 10333, 28329, 12421, 14734, 13234, 16735, 45, 10384, 14446, 13191, 10529, 12445, 14791, 45, 10293, 16503, 15674, 14216, 14734, 13091, 13085, 17781, 10846, 14328, 11871, 11920, 11957, 12140, 18492, 11672, 15036, 14734, 12650, 14808, 11124, 10585, 12650, 45, 10399, 10992, 11957, 11105, 17237, 10267, 14610, 10336, 10407, 14734, 12392, 16523, 11124, 14637, 15023, 10080, 16530, 15898, 12314, 16803, 13179, 11632, 10452, 14446, 11454, 10390, 10402, 14734, 14880, 15087, 11001, 11047, 11124, 10529, 12445, 17207, 17108, 10257, 10080, 65535, 261]
-> item_loss 1.769837737083435 batch_loss 1.1719923203899747
==== [25106, 11, 65530, 65531, 10464, 13091, 10250, 10283, 820, 19137, 16443, 17303, 15588, 11454, 13205, 11459, 10080, 10293, 10370, 11967, 10464, 16689, 11899, 10292, 10250, 10283, 11679, 11872, 11855, 19137, 10464, 14734, 11043, 11860, 11017, 10588, 14299, 10295, 10273, 19137, 11124, 10293, 10370, 12201, 18476, 14782, 10411, 19137, 10260, 10792, 640, 12039, 10080, 10464, 13091, 820, 10444, 13091, 15752, 11634, 13499, 12722, 10370, 15261, 14734, 11036, 15033, 12527, 12484, 19137, 13637, 11687, 12266, 12366, 10079, 10429, 12366, 10079, 14583, 13655, 10079, 15648, 12366, 10079, 10399, 10992, 11036, 15033, 13562, 13197, 10080, 10464, 16443, 16689, 16664, 10760, 13234, 10292, 10370, 15261, 13193, 10843, 19137, 10444, 13091, 11042, 13064, 10464, 10322, 12159, 13197, 15847, 12144, 15752, 11634, 12348, 10792, 12203, 15009, 10261, 11971, 17385, 10080, 10464, 15752, 11634, 12945, 10460, 14600, 15763, 11124, 10250, 10349, 13133, 15752, 11920, 12010, 19137, 17879, 16503, 14734, 13064, 10560, 10464, 17879, 16503, 16735, 398, 12945, 10460, 14600, 15763, 2383, 12609, 15643, 398, 12945, 10460, 11920, 12010, 2383, 19137, 15646, 10260, 15752, 17109, 10760, 10687, 10390, 13012, 11860, 10080, 17148, 10283, 13064, 10560, 10464, 14734, 17109, 10760, 10423, 16443, 15304, 15500, 12785, 16056, 12202, 12282, 11960, 10464, 17149, 16403, 17149, 10250, 13582, 14734, 10355, 10342, 10080, 17148, 17384, 15494, 10760, 10250, 10283, 11957, 16713, 11903, 10487, 19151, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 14299, 10295, 10273, 19137, 12176, 12605, 12266, 10258, 14139, 11, 65535, 261, 65530, 65534, 14299, 10295, 10273, 59, 358, 12945, 10460, 11920, 12010, 126, 11, 65535, 261, 65530, 65531, 40812, 59, 33, 10464, 15752, 11634, 17149, 16403, 14734, 10355, 10342, 14734, 10696, 11922, 10777, 10296, 11454, 10258, 13036, 19137, 16738, 17177, 12731, 10250, 10283, 17109, 10760, 19151, 11, 398, 14600, 14139, 9187, 398, 15090, 16749, 9187, 398, 14195, 13676, 11369, 9187, 398, 17952, 11168, 2383, 65535, 261, 65530, 65534, 14299, 10295, 10273, 59, 358, 14600, 14139, 126, 11, 65535, 261, 65530, 65531, 40812, 59, 33, 13381, 13827, 10792, 12307, 10808, 14600, 14139, 10684, 17727, 19137, 16738, 10722, 11899, 13091, 11060, 12642, 12266, 19137, 17177, 12731, 35, 13091, 35, 12609, 15643, 35, 11060, 35, 11, 65535, 261, 65530, 65534, 14299, 10295, 10273, 59, 33, 13091, 11, 65535, 261, 65530, 65531, 40812, 59, 33, 14600, 14139, 12145, 12642, 12266, 19137, 12145, 17146, 11414, 11957, 16713, 28329, 65535, 261, 65530, 65534, 14299, 10295, 10273, 59, 33, 10293, 10370, 19137, 11687, 13258, 15304, 15500, 13051, 16730, 19137, 12605, 12145, 12642, 12266, 14600, 14139, 28329, 65535, 261]
-> item_loss 0.8966177105903625 batch_loss 1.1655882596969604
==== [25106, 11, 65530, 65531, 12605, 10402, 14446, 11454, 12266, 11726, 16671, 16686, 10250, 10283, 13034, 14734, 16713, 18025, 19137, 17148, 10283, 16713, 18025, 10684, 10339, 10250, 17303, 11975, 16735, 11, 65535, 261, 17148, 13091, 10250, 13619, 35, 14931, 12314, 10267, 14610, 35, 17148, 17303, 11975, 16735, 14734, 14313, 13619, 19137, 33, 12421, 15489, 10250, 10258, 11461, 13126, 10285, 14734, 17385, 16503, 10696, 11922, 11124, 10708, 10460, 12636, 13764, 11, 14781, 10792, 10381, 11639, 19137, 11640, 15090, 12366, 14486, 11871, 11920, 10402, 10383, 13055, 13051, 13764, 12778, 12904, 10390, 10402, 14734, 12366, 12410, 28329, 10305, 10808, 19137, 12140, 18492, 14734, 11861, 11454, 10377, 10377, 13091, 10250, 10283, 10580, 16735, 19137, 10444, 14931, 12314, 10267, 14610, 17131, 15545, 14116, 13819, 10257, 13036, 10250, 11621, 12822, 10760, 13234, 14734, 12034, 11981, 10257, 13191, 10250, 10283, 14987, 15036, 14734, 10660, 18295, 11979, 14428, 11435, 19137, 17268, 17384, 14734, 10660, 18295, 11979, 10370, 11454, 10250, 11473, 13188, 17795, 12076, 14734, 14869, 11592, 17997, 10257, 12883, 16689, 10333, 18216, 13273, 11124, 14005, 17085, 16451, 15586, 19137, 11967, 10745, 11043, 16520, 13827, 11098, 11492, 10792, 10267, 14610, 17131, 15545, 14734, 10258, 13036, 19137, 17148, 12638, 16692, 11903, 10333, 12140, 18492, 14734, 11861, 11454, 10080, 17148, 10349, 16520, 13827, 11098, 11471, 11454, 10250, 15891, 17686, 13191, 14869, 15963, 15102, 12626, 14734, 17587, 15887, 17384, 19137, 15752, 11634, 17181, 17141, 13676, 17878, 14791, 13923, 11630, 17919, 14734, 13126, 15850, 28329, 17587, 15887, 11454, 17131, 15545, 14116, 13819, 10258, 17919, 12747, 10333, 12337, 10301, 19137, 14221, 11044, 19137, 11638, 15604, 11680, 17836, 12705, 11414, 17587, 15887, 11124, 17587, 15887, 17384, 17919, 14734, 13057, 13199, 13085, 17781, 10846, 14328, 11871, 11920, 28329, 10390, 10402, 12172, 11414, 10333, 12337, 11632, 10529, 12445, 19137, 10894, 12732, 17777, 11436, 11416, 10399, 10992, 12140, 16803, 14734, 12314, 14349, 11124, 12025, 12415, 10080, 14221, 15646, 19137, 17148, 10349, 10529, 12445, 10383, 14221, 10260, 15752, 12329, 12221, 16533, 17382, 11887, 11898, 14734, 12415, 16884, 11124, 11861, 11454, 14734, 12522, 10304, 28329, 13637, 11687, 19137, 17777, 11436, 11416, 14734, 12415, 10790, 11994, 17843, 10399, 14914, 11899, 10080, 13085, 17781, 10846, 14328, 11871, 11920, 10402, 10529, 16589, 13054, 13054, 11459, 11610, 15044, 19137, 17148, 10283, 12415, 10790, 17728, 18025, 17916, 12182, 10684, 17644, 19137, 11016, 13191, 12261, 17194, 10250, 10283, 13179, 11638, 13179, 11474, 11434, 14734, 12978, 13273, 19137, 12669, 11640, 15090, 10305, 15887, 11492, 12348, 13179, 10448, 19137, 12638, 15752, 12270, 13923, 10336, 11903, 10080, 15646, 14778, 10808, 19137, 10390, 10402, 11016, 15752, 13326, 12797, 12145, 14862, 14734, 11887, 11898, 14862, 16696, 17149, 16403, 12826, 13827, 10080, 65530, 65534, 17148, 13619, 13012, 11860, 12847, 17162, 14734, 13091, 11454, 10250, 10283, 11656, 12206, 14734, 9822, 14931, 12314, 10267, 14610, 9823, 10285, 16520, 11940, 11887, 11898, 14734, 12484, 13126, 28329, 16503, 14172, 12421, 15489, 59, 11, 50, 47, 33, 10660, 18295, 11979, 14428, 11435, 14734, 11871, 15643, 10482, 14589, 18216, 13273, 11124, 14005, 17085, 15169, 16451, 15586, 45, 11967, 17587, 10794, 16520, 13827, 15887, 11492, 17788, 10792, 10267, 14610, 17131, 15545, 14116, 13819, 10258, 13036, 45, 17189, 17141, 15887, 10696, 14869, 15963, 15102, 16520, 11940, 11630, 14610, 12484, 10724, 28329, 51, 47, 33, 10390, 10402, 17189, 17141, 16520, 13827, 14791, 10792, 10333, 12140, 18492, 11124, 10687, 10390, 15090, 17734, 14583, 14328, 11687, 17777, 11436, 11416, 15169, 28329, 52, 47, 33, 16269, 14221, 16056, 12348, 10333, 10250, 10349, 10529, 12445, 45, 10444, 10383, 13051, 13764, 16533, 17382, 11887, 11898, 13205, 16884, 11124, 11861, 11454, 12522, 10304, 17148, 15261, 11533, 13205, 17728, 18025, 28329, 53, 47, 33, 10390, 10402, 13051, 13764, 14914, 11899, 17777, 11436, 11416, 14734, 12415, 10790, 45, 17879, 16503, 13179, 11638, 13179, 11474, 11434, 14734, 16689, 11622, 12638, 15752, 16533, 15178, 17148, 10283, 17728, 18025, 28329, 10708, 10460, 12636, 13764, 59, 11, 50, 47, 33, 10482, 14589, 11656, 12206, 12509, 16803, 45, 10781, 17194, 10250, 10283, 10261, 10267, 17821, 15498, 14734, 10660, 18295, 11979, 14428, 11435, 17149, 16403, 15090, 17734, 14880, 15087, 14734, 11461, 13126, 28329, 51, 47, 33, 11957, 12666, 13208, 16689, 13038, 11124, 16520, 13827, 17141, 15052, 14734, 16722, 15478, 12847, 17162, 45, 11584, 12299, 14794, 11903, 12527, 28329, 52, 47, 33, 12282, 10673, 13203, 16533, 10305, 16771, 11124, 13179, 11632, 17728, 18025, 45, 10758, 14446, 10760, 11887, 11898, 14734, 14987, 15036, 10261, 11626, 13215, 28329, 53, 47, 33, 17189, 17141, 11957, 17777, 11436, 11416, 12415, 10790, 14734, 16671, 16686, 45, 13146, 14966, 10953, 10482, 16056, 12348, 16520, 13827, 13004, 12797, 10322, 13051, 13764, 16533, 15178, 13326, 13205, 17728, 18025, 28329, 12421, 14734, 13234, 16735, 45, 17148, 13619, 13012, 11860, 10399, 12206, 12509, 14734, 12636, 13764, 12822, 16671, 15034, 11871, 14880, 15087, 11124, 11887, 11898, 16667, 14862, 14734, 12005, 17789, 12415, 10080, 65535, 261]
-> item_loss 1.6922991275787354 batch_loss 1.1775589612397281
==== [25106, 11, 65530, 65531, 35, 10464, 13091, 10250, 10283, 15667, 11639, 13212, 11369, 10370, 19137, 16443, 17303, 15588, 11454, 10250, 10283, 15667, 11639, 15604, 15476, 10285, 10080, 11454, 15604, 15476, 10285, 19137, 13633, 10250, 10283, 14589, 12626, 17311, 10399, 35, 14589, 12626, 11043, 41, 16696, 10790, 11028, 489, 14734, 12314, 12276, 16443, 10464, 16667, 14862, 19137, 10464, 14734, 10293, 10370, 13091, 35, 23385, 41, 632, 663, 697, 679, 50, 489, 28329, 10464, 14734, 11043, 11860, 13091, 1297, 1740, 19137, 11454, 15604, 15476, 10696, 14734, 16443, 13098, 14966, 10292, 35, 1297, 1740, 41, 651, 699, 715, 662, 700, 489, 11, 10464, 11021, 10399, 11454, 11957, 16713, 10285, 17109, 10760, 393, 65, 16696, 10790, 11028, 1678, 13234, 65, 13279, 10447, 12604, 11098, 19137, 10487, 11687, 11021, 10399, 17109, 10760, 393, 65, 632, 663, 697, 679, 50, 94, 10293, 10370, 11454, 11053, 19156, 35, 13234, 65, 10293, 10370, 17149, 16403, 11957, 16713, 19137, 11454, 11957, 13279, 10283, 14332, 11899, 14589, 12626, 14734, 10997, 16537, 17149, 16403, 11414, 11626, 13064, 19137, 10464, 12220, 16721, 65, 10390, 10384, 15646, 16672, 10390, 13078, 14731, 10464, 13091, 11454, 11957, 10390, 17149, 16403, 11414, 11626, 28329, 15304, 15500, 13633, 13557, 10423, 16742, 11001, 10250, 17303, 10768, 14734, 15667, 11639, 16678, 12308, 10997, 15494, 10464, 19137, 10464, 17879, 16503, 10784, 13032, 13091, 11060, 17149, 16403, 11414, 11626, 10080, 65535, 261, 65530, 65531, 40812, 59, 33, 10464, 10842, 10673, 10333, 15604, 15667, 28329, 65535, 261, 65530, 65532, 23385, 41, 632, 663, 697, 679, 50, 503, 636, 59, 621, 59, 620, 788, 33, 12605, 12669, 7176, 12705, 17149, 13234, 10333, 19137, 10464, 10402, 11021, 10399, 16707, 16707, 10080, 10780, 10708, 11685, 12824, 11010, 19137, 10260, 10250, 11899, 15752, 16968, 11, 65535, 261, 24189, 25169, 41, 635, 626, 657, 682, 49, 503, 636, 59, 621, 59, 641, 788, 326, 11436, 14313, 13855, 12445, 94, 11, 261, 18025, 14778, 15847, 12722, 17729, 11639, 13881, 41, 634, 642, 695, 686, 630, 503, 636, 59, 621, 59, 644, 788, 326, 11436, 14313, 13855, 12445, 94, 11, 261, 14731, 13787, 15450, 13229, 41, 696, 640, 661, 635, 57, 503, 636, 59, 621, 59, 657, 788, 326, 65, 648, 653, 636, 690, 634, 94, 33, 14862, 17222, 67, 15126, 11053, 64, 11, 261, 826, 74, 41, 15604, 13212, 11369, 10370, 493, 648, 653, 636, 690, 634, 503, 636, 59, 621, 59, 675, 788, 326, 65, 696, 640, 661, 635, 57, 94, 33, 13091, 14734, 19137, 67, 15126, 13091, 10250, 10283, 10285, 11435, 14370, 14332, 14734, 16523, 18020, 10682, 10361, 15573, 15126, 19137, 10292, 14589, 12626, 12848, 10494, 11454, 15471, 16520, 14791, 10079, 10768, 10361, 11124, 10997, 12155, 15158, 11038, 10285, 11435, 11638, 17782, 13764, 12340, 13764, 16521, 14734, 10974, 10781, 16523, 18020, 19137, 10894, 12732, 14589, 12626, 15847, 10794, 14734, 10846, 14604, 10079, 14042, 14604, 10079, 13953, 12603, 16523, 18020, 10079, 17952, 10310, 16523, 18020, 10079, 10768, 10361, 14583, 13815, 15169, 15261, 11496, 14734, 16523, 18020, 10696, 11922, 10080, 67, 15126, 11454, 10285, 11435, 12604, 10292, 10333, 10250, 10283, 17916, 12182, 13819, 16403, 14734, 16523, 18020, 14970, 10912, 28329, 261, 65530, 65532, 23385, 41, 632, 663, 697, 679, 50, 503, 636, 59, 622, 59, 635, 788, 33, 12421, 10305, 17148, 10283, 7176, 13637, 17097, 14332, 13603, 19137, 11638, 13445, 10260, 17879, 16503, 65, 10322, 15752, 16735, 16713, 10080, 1653, 651, 699, 715, 662, 700, 1681, 10464, 23304, 11454, 10260, 11454, 19137, 10760, 13234, 16974, 11638, 11920, 10382, 15484, 10250, 10258, 15847, 12144, 28329, 65535, 261, 65530, 65531, 40812, 59, 29483, 19137, 12410, 15641, 10250, 10258, 14446, 11454, 13091, 11060, 17879, 16503, 10997, 16537, 10080, 12669, 12410, 15641, 14734, 17141, 15052, 17149, 16403, 17109, 10760, 11, 65535, 261, 65530, 65534, 1297, 1740, 59, 33, 15641, 16253, 10792, 10293, 10370, 65, 12605, 12202, 12159, 13197, 12605, 11124, 11638, 11920, 10382, 15484, 15847, 12144, 19137, 12605, 16667, 10292, 14446, 11454, 17879, 16503, 10997, 16537, 28329, 65535, 261, 65530, 65531, 40812, 59, 33, 16738, 17109, 10760, 35, 13091, 35, 12609, 15643, 35, 11060, 35, 14914, 16667, 13091, 11060, 17879, 16503, 10997, 16537, 19151, 11, 65535, 261, 65530, 65534, 1297, 1740, 59, 33, 13091, 11, 65535, 261, 65530, 65531, 40812, 59, 33, 16738, 17109, 10760, 124, 10997, 16537, 10696, 11922, 126, 19151, 11, 65535, 261, 65530, 65534, 1297, 1740, 41, 651, 699, 715, 662, 700, 501, 33, 11454, 14734, 19137, 11638, 11920, 11685, 19137, 12605, 13091, 10381, 11639, 10842, 10673, 15604, 10696, 11124, 11638, 11920, 15667, 11639, 14734, 13212, 11369, 10370, 19137, 1653, 632, 663, 697, 679, 50, 94, 13091, 12605, 14734, 10293, 10370, 10080, 12605, 15752, 11634, 10768, 13254, 15604, 10696, 14734, 13855, 12445, 12202, 10722, 11899, 13091, 11060, 10997, 16537, 19137, 12159, 13197, 15752, 11124, 11638, 11920, 12519, 12387, 11459, 15667, 11639, 19126, 11, 65535, 261, 24189, 25169, 41, 635, 626, 657, 682, 49, 503, 636, 59, 624, 59, 641, 788, 33, 17148, 13091, 12401, 10303, 11903, 14446, 14734, 19156, 11, 261, 65530, 65532, 23385, 41, 632, 663, 697, 679, 50, 503, 636, 59, 624, 59, 652, 788, 3422, 11, 65535, 261, 65530, 65532, 23385, 41, 632, 663, 697, 679, 50, 503, 636, 59, 624, 59, 655, 788, 33, 13637, 17097, 11626, 13215, 19137, 12421, 10305, 10464, 10402, 11021, 10399, 10658, 16707, 16707, 19133, 11, 65535, 261, 89, 2277, 1956, 41, 680, 644, 707, 675, 52, 503, 636, 59, 624, 59, 657, 788, 33, 17148, 10283, 13212, 11369, 10370, 16503, 12401, 10303, 14589, 11223, 11, 261, 65530, 65532, 23385, 41, 632, 663, 697, 679, 50, 503, 636, 59, 624, 59, 662, 788, 33, 10260, 14589, 15202, 19137, 11684, 12220, 16721, 10423, 15847, 12144, 13234, 12849, 16713, 14734, 11, 65535, 261, 65530, 65531, 40812, 59, 29483, 19137, 12410, 15641, 10250, 10258, 14446, 11454, 13091, 11060, 17879, 16503, 10997, 16537, 10080, 12669, 12410, 15641, 14734, 17141, 15052, 17149, 16403, 17109, 10760, 11, 65535, 261, 65530, 65534, 1297, 1740, 59, 33, 10293, 10370, 11124, 10687, 10390, 15604, 10993, 11454, 16671, 16686, 10684, 10339, 12605, 14734, 10336, 12484, 19137, 14778, 10808, 10260, 17879, 16503, 12605, 10997, 16537, 28329, 65535, 261, 65530, 65531, 40812, 59, 33, 16738, 17109, 10760, 35, 13091, 35, 12609, 15643, 35, 11060, 35, 14914, 16667, 13091, 11060, 17879, 16503, 10997, 16537, 19151, 11, 65535, 261, 65530, 65534, 1297, 1740, 59, 33, 11060, 11, 65535, 261, 18025, 14778, 15847, 12722, 17729, 11639, 13881, 41, 674, 642, 663, 695, 55, 503, 636, 59, 625, 59, 621, 788, 33, 12421, 10305, 12605, 10658, 16707, 16707, 92, 11436, 14313, 13855, 12445, 94, 11, 261, 65530, 65532, 23385, 41, 674, 642, 663, 695, 55, 503, 636, 59, 625, 59, 640, 788, 326, 65, 651, 699, 715, 662, 700, 1681, 10464, 23304, 15752, 14791, 10792, 12605, 16735, 16713, 10303, 19156, 11, 65535, 261, 65530, 65531, 40812, 59, 29483, 19137, 12410, 15641, 10250, 10258, 14446, 11454, 13091, 11060, 17879, 16503, 10997, 16537, 10080, 12669, 12410, 15641, 14734, 17141, 15052, 17149, 16403, 17109, 10760, 11, 65535, 261, 65530, 65534, 1297, 1740, 59, 33, 12605, 12979, 10792, 10333, 674, 642, 663, 695, 55, 14734, 65, 19137, 12220, 12307, 17149, 16403, 11414, 12220, 10080, 11042, 13064, 19137, 674, 642, 663, 695, 55, 11021, 15752, 10288, 13064, 11967, 14589, 12626, 11043, 12981, 10292, 10333, 23385, 19137, 10261, 10293, 10370, 10250, 15850, 19137, 10390, 11021, 15752, 13091, 12509, 16503, 10580, 16451, 12604, 10293, 10370, 28329, 65535, 261, 65530, 65531, 40812, 59, 33, 16738, 17109, 10760, 35, 13091, 35, 12609, 15643, 35, 11060, 35, 14914, 16667, 13091, 11060, 17879, 16503, 10997, 16537, 19151, 11, 65535, 261, 65530, 65534, 1297, 1740, 59, 33, 13091, 11, 65535, 261, 65530, 65531, 40812, 59, 33, 16738, 17109, 10760, 124, 10997, 16537, 10696, 11922, 126, 19151, 11, 65535, 261, 65530, 65534, 1297, 1740, 41, 651, 699, 715, 662, 700, 501, 33, 15752, 14791, 10792, 19137, 10580, 14734, 10293, 10370, 10080, 12631, 10399, 10464, 12664, 12605, 13191, 10373, 10303, 10336, 11336, 19156, 11, 65535, 261, 18025, 14778, 15847, 12722, 17729, 11639, 13881, 41, 674, 642, 663, 695, 55, 503, 636, 59, 627, 59, 631, 788, 33, 11223, 17148, 11, 261, 89, 2277, 1956, 41, 680, 644, 707, 675, 52, 503, 636, 59, 627, 59, 644, 788, 33, 11223, 17148, 11, 261, 65530, 65532, 23385, 41, 632, 663, 697, 679, 50, 503, 636, 59, 627, 59, 646, 788, 33, 11223, 17148, 11, 65535, 261, 14731, 13787, 15450, 13229, 41, 696, 640, 661, 635, 57, 503, 636, 59, 627, 59, 647, 788, 33, 11223, 17148, 11, 261, 10995, 18108, 17152, 12824, 41, 665, 653, 641, 699, 49, 503, 636, 59, 627, 59, 647, 788, 33, 11223, 17148, 11, 261, 24189, 25169, 41, 635, 626, 657, 682, 49, 503, 636, 59, 627, 59, 653, 788, 33, 11223, 17148, 11, 261, 65530, 65531, 40812, 59, 29483, 19137, 12410, 15641, 10250, 10258, 14446, 11454, 13091, 11060, 17879, 16503, 10997, 16537, 10080, 12669, 12410, 15641, 14734, 17141, 15052, 17149, 16403, 17109, 10760, 11, 65535, 261, 65530, 65534, 1297, 1740, 59, 33, 15604, 15476, 10696, 12337, 11632, 12604, 11098, 10997, 17171, 14782, 11042, 14734, 13855, 12445, 19137, 11454, 15573, 15497, 10257, 16443, 15044, 10292, 35, 11626, 16742, 35, 19137, 14589, 13234, 16417, 17136, 15847, 12144, 11957, 13279, 10250, 10283, 13191, 16954, 16713, 18025, 14734, 11042, 12527, 12609, 16667, 11021, 10080, 12605, 11021, 10399, 13499, 10411, 10390, 10402, 17149, 16403, 17109, 10760, 28329, 65535, 261, 65530, 65531, 40812, 59, 33, 16738, 17109, 10760, 35, 13091, 35, 12609, 15643, 35, 11060, 35, 14914, 16667, 13091, 11060, 17879, 16503, 10997, 16537, 19151, 11, 65535, 261, 65530, 65534, 1297, 1740, 59, 33, 13091, 11, 65535, 261, 65530, 65531, 40812, 59, 33, 16738, 17109, 10760, 124, 10997, 16537, 10696, 11922, 126, 19151, 11, 65535, 261, 65530, 65534, 1297, 1740, 41, 651, 699, 715, 662, 700, 501, 33, 11223, 17148, 11, 65535, 261, 65530, 65531, 10464, 13091, 10250, 10283, 11268, 13558, 12176, 10847, 10370, 15261, 14734, 820, 10847, 14486, 19137, 15752, 11634, 11414, 15178, 10370, 15261, 12848, 10760, 14734, 11036, 15033, 17728, 18025, 19137, 12609, 15643, 17149, 16403, 13053, 12182, 14734, 15667, 11639, 15169, 11, 65535, 261, 65530, 65534, 820, 59, 33, 12335, 11117, 10285, 43282, 11, 65535, 261, 65530, 65532, 34580, 59, 4412, 1804, 2329, 2233, 1964, 99, 11, 65535, 261, 65530, 65534, 820, 59, 33, 12337, 12691, 13571, 19137, 12605, 13051, 13764, 14486, 16533, 10464, 10997, 14734, 10696, 11922, 19137, 15752, 15494, 12605, 12848, 10494, 13179, 11632, 14734, 10529, 12445, 10303, 19156, 11, 65535, 261, 65530, 65532, 34580, 59, 33, 18030, 19137, 10260, 11975, 12366, 12750, 10792, 17644, 14767, 10333, 28329, 65535, 261, 65530, 65534, 820, 59, 33, 14486, 16533, 10333, 19137, 13191, 10336, 12484, 11021, 10399, 17819, 13064, 12664, 12605, 28329, 65535, 261, 65530, 65532, 34580, 59, 33, 10464, 14734, 12221, 12007, 13499, 11496, 13091, 10373, 10303, 19156, 11, 65535, 261, 65530, 65534, 820, 59, 33, 13326, 12797, 14778, 10808, 14734, 12484, 10724, 13234, 14791, 19137, 12605, 14734, 12221, 12007, 13499, 11496, 11021, 15752, 13091, 1052, 85, 52, 47, 54, 12609, 15643, 24985, 1052, 85, 19137, 13091, 14597, 23980, 820, 10678, 11029, 12266, 10997, 14734, 10250, 10283, 16728, 16537, 13499, 11496, 28329, 65535, 261, 65530, 65531, 10464, 13091, 11533, 10339, 12266, 13981, 1413, 1184, 13499, 11496, 14734, 10250, 10283, 11957, 16713, 820, 28329, 65535, 261, 65530, 65532, 34580, 59, 4516, 8180, 19137, 10260, 11640, 11957, 19137, 12605, 17189, 17141, 16689, 11899, 15494, 10464, 10250, 10349, 12848, 14966, 19137, 10464, 12220, 16721, 15752, 14791, 10792, 11454, 12605, 17148, 13619, 13012, 11860, 14734, 10257, 13036, 28329, 65535, 261, 65530, 65534, 820, 59, 33, 12605, 13078, 14731, 10333, 19137, 12605, 14734, 12221, 12007, 13499, 11496, 13091, 1413, 1184, 13499, 11496, 10080, 10260, 17141, 10293, 10370, 19137, 12605, 13051, 13764, 14791, 10792, 12469, 17109, 10673, 14734, 13012, 11860, 19137, 11016, 13091, 12750, 14240, 18000, 12217, 16742, 11001, 13499, 11496, 14734, 17109, 10673, 19137, 12469, 12631, 16735, 14734, 12848, 14966, 12220, 16721, 13091, 11454, 11124, 12605, 14734, 11957, 16713, 10305, 10808, 19137, 13734, 17631, 11061, 19156, 11, 65535, 261, 65530, 65532, 34580, 59, 33, 13091, 17148, 13324, 14734, 19137, 11017, 12605, 32897, 11994, 11685, 28329, 65535, 261, 65530, 65534, 820, 59, 33, 14486, 16533, 10333, 19137, 32897, 10080, 11957, 10339, 12605, 13234, 16735, 18000, 12217, 11454, 10808, 14734, 17109, 10673, 11957, 10339, 12469, 15646, 16537, 13098, 14966, 11454, 10257, 13036, 19137, 12605, 10423, 11986, 16707, 16678, 10449, 14734, 28329, 65535, 261, 65530, 65532, 34580, 59, 33, 10464, 14862, 17222, 14446, 11454, 10745, 14172, 10333, 10303, 19156, 11, 65535, 261, 65530, 65534, 820, 59, 33, 10260, 14862, 17222, 19137, 12605, 13051, 13764, 12348, 14862, 14446, 11454, 14734, 13064, 17734, 28329, 65535, 261, 65530, 65532, 34580, 59, 33, 10464, 17109, 10673, 438, 7621, 96, 49026, 96, 27165, 41, 17148, 17384, 11567, 10708, 13064, 10912, 11043, 19137, 13637, 11687, 1797, 45, 1853, 42, 422, 16707, 16707, 11, 65535, 261, 65530, 65534, 820, 59, 272, 7621, 96, 49026, 96, 27165, 41, 1797, 42, 38, 11, 65535, 261, 65530, 65531, 42343, 59, 3493, 643, 48, 54, 48, 644, 3495, 59, 625, 59, 633, 11, 65535, 261, 65530, 65534, 820, 59, 33, 12605, 12979, 10792, 10333, 10250, 13231, 15304, 15500, 13855, 12445, 19137, 11885, 11088, 16698, 12605, 14446, 11454, 14734, 13064, 17734, 12220, 16721, 13091, 640, 643, 48, 54, 48, 644, 3495, 59, 625, 59, 633, 28329, 65535, 261, 65530, 65532, 34580, 59, 33, 11315, 19137, 17148, 13091, 10464, 15752, 16749, 11001, 14734, 10762, 13004, 10305, 10250, 19137, 14778, 10808, 11016, 13091, 13827, 16707, 19137, 11044, 17919, 12605, 10423, 13006, 14486, 10250, 10258, 10762, 13004, 11043, 19137, 11021, 15752, 16974, 14446, 11454, 10260, 10250, 13324, 19137, 12202, 10265, 10708, 10250, 10283, 12636, 10702, 28329, 65535, 261, 65530, 65534, 820, 59, 33, 14486, 16533, 10333, 19137, 12527, 16774, 10464, 14734, 16533, 17382, 19137, 32897, 28329, 65535, 261, 65530, 65532, 34580, 59, 33, 11315, 19137, 12824, 10258, 13234, 12159, 13197, 10464, 15752, 12208, 18441, 14172, 28329, 65535, 261, 65530, 65534, 820, 59, 33, 16403, 11061, 19137, 12605, 16707, 16707, 19137, 10260, 17141, 10464, 10790, 12748, 13197, 12605, 15752, 16672, 12469, 14015, 12522, 19137, 32897, 19137, 13638, 15130, 12605, 10322, 10260, 14862, 17222, 15847, 12144, 16443, 16675, 15475, 12604, 10333, 10373, 10303, 13324, 11855, 28329, 65535, 261, 65530, 65532, 34580, 59, 33, 10464, 15847, 14597, 10333, 28329, 65535, 261, 65530, 65534, 820, 59, 33, 10464, 11454, 13199, 12335, 10373, 10303, 11053, 19156, 32897, 19137, 16672, 12605, 17148, 10283, 13734, 13191, 17038, 10452, 14734, 820, 10985, 12527, 11002, 14446, 11903, 14734, 13604, 17354, 19137, 14221, 11044, 10257, 14035, 10086, 15480, 15489, 15643, 10087, 19156, 11454, 17148, 14172, 10257, 12605, 13191, 16956, 11634, 14734, 10529, 12366, 16672, 10464, 11645, 13197, 19133, 19134, 11, 65535, 261, 65530, 65532, 34580, 59, 33, 11315, 19137, 12771, 10260, 17631, 14734, 19137, 12824, 10258, 13234, 11957, 16713, 15847, 14221, 10250, 10349, 19137, 10260, 16503, 16672, 15847, 12144, 10621, 10250, 10283, 11016, 13091, 11891, 12604, 10409, 10843, 14734, 13212, 11369, 10250, 13324, 28329, 65535, 261, 65530, 65534, 820, 59, 33, 11685, 14734, 19137, 10305, 11044, 12605, 17147, 13091, 11017, 12469, 10293, 10370, 11021, 10399, 10303, 19137, 12605, 13179, 11268, 13558, 17148, 15033, 15044, 11116, 10080, 12605, 10423, 11986, 16707, 16672, 15847, 12144, 14734, 10355, 13819, 13179, 10842, 10621, 10370, 15261, 10250, 10349, 19137, 12159, 13197, 10293, 10370, 15752, 11634, 11268, 13558, 28336, 28329, 65535, 261, 65530, 65532, 34580, 59, 33, 12605, 12638, 10997, 14446, 10464, 17147, 15752, 17109, 10760, 34295, 28329, 65535, 261, 65530, 65534, 820, 59, 33, 11687, 13258, 10464, 11268, 13558, 14734, 16713, 12605, 11021, 10399, 11632, 13234, 14172, 10061, 19103, 19137, 12605, 10780, 10780, 12509, 10792, 19137, 11687, 13258, 13734, 13191, 16722, 15478, 14734, 10708, 10250, 10349, 16689, 11899, 14734, 16713, 19137, 12605, 17147, 13091, 10260, 11640, 13923, 13425, 16503, 14589, 10373, 10303, 13036, 12276, 11124, 10293, 10370, 10355, 13819, 11102, 28329, 65535, 261, 65530, 65532, 34580, 59, 22970, 11, 65535, 261, 65530, 65534, 820, 59, 33, 11315, 43282, 11, 65535, 261, 65530, 65532, 34580, 59, 33, 17819, 12522, 14172, 11994, 16403, 19137, 10790, 11640, 12697, 16803, 12609, 15643, 12669, 12430, 12406, 16790, 12986, 12220, 15494, 12705, 14015, 11994, 16403, 19137, 34295, 10260, 14589, 15486, 12182, 17109, 10760, 19137, 10995, 13580, 1487, 84, 10322, 16696, 10790, 10260, 10792, 28329, 65535, 261, 65530, 65534, 820, 59, 33, 13734, 13764, 11891, 10675, 11065, 12578, 10293, 10370, 14734, 16713, 19137, 14446, 11454, 12605, 12220, 16721, 13734, 13191, 10840, 13764, 15671, 15573, 19137, 15752, 16533, 17382, 10250, 10258, 10684, 10339, 9822, 12697, 16803, 9823, 11124, 9822, 12705, 14015, 9823, 10303, 28329, 65535, 261, 65530, 65532, 34580, 59, 33, 10260, 16503, 11454, 12522, 19137, 10464, 14734, 16403, 10292, 13191, 10260, 12307, 14734, 11459, 13036, 12605, 10423, 12748, 10760, 14734, 28329, 65535, 261, 65530, 65534, 820, 59, 33, 11685, 14734, 19137, 12605, 14862, 17222, 10333, 28329, 65535, 261, 65530, 65532, 34580, 59, 33, 14446, 11454, 10464, 15752, 11003, 11414, 13188, 12266, 11726, 13734, 13191, 10409, 10454, 16689, 11899, 14734, 14349, 12396, 10303, 19137, 11994, 13091, 11414, 15178, 11036, 15033, 17728, 18025, 14734, 820, 17268, 10283, 16689, 11899, 28329, 65535, 261, 65530, 65534, 820, 59, 33, 11021, 10399, 19137, 12605, 14446, 11454, 11414, 10792, 10782, 11726, 14349, 12396, 28329, 65535, 261, 65530, 65532, 34580, 59, 33, 13051, 13764, 10621, 10370, 15261, 10250, 13324, 16403, 10846, 19137, 10464, 12527, 16525, 11021, 12493, 10303, 19156, 11, 65535, 261, 65530, 65534, 820, 59, 33, 12605, 13091, 820, 19137, 10423, 10279, 13327, 12750, 14240, 16689, 11899, 16403, 10846, 19137, 10260, 10423, 12727, 13191, 12527, 16525, 10080, 10260, 17141, 11687, 13258, 13091, 10370, 15261, 14734, 16713, 19137, 10423, 12527, 16525, 10792, 13191, 10349, 11021, 12493, 28329, 65535, 261, 65530, 65532, 34580, 59, 33, 11685, 11061, 19137, 12605, 14486, 16533, 10333, 19137, 10399, 11044, 10423, 15494, 10464, 13212, 10423, 12750, 14240, 10370, 15261, 14734, 13036, 12276, 16403, 10846, 14734, 28329, 65535, 261, 65530, 65534, 820, 59, 33, 13078, 14731, 10333, 19137, 16774, 16774, 10464, 19137, 10293, 10370, 28329, 65535, 261, 65530, 65532, 34580, 59, 33, 11687, 13258, 12605, 17728, 10464, 19137, 14589, 10370, 15261, 14734, 16403, 10846, 13499, 12276, 13234, 11414, 15178, 15847, 12144, 13091, 11060, 13191, 12527, 16525, 11102, 19156, 11, 65535, 261, 65530, 65534, 820, 59, 33, 10254, 10250, 13191, 11102, 19137, 10293, 10370, 10080, 10370, 15261, 17189, 12182, 13091, 13326, 12797, 11630, 11454, 16417, 14446, 16694, 10435, 10687, 10390, 10283, 10452, 14734, 28329, 65535, 261, 65530, 65532, 34580, 59, 33, 11315, 19137, 12605, 12337, 14015, 16956, 19137, 16735, 10283, 9822, 11280, 127, 9823, 28329, 65535, 261, 65530, 65534, 820, 59, 33, 11280, 127, 11, 65535, 261, 65530, 65532, 34580, 59, 33, 11315, 19137, 10464, 14734, 11280, 11860, 15486, 17141, 1487, 84, 10305, 11044, 12337, 11685, 11065, 19137, 12605, 12220, 16721, 15752, 14817, 10283, 11685, 16525, 19137, 13115, 11889, 28329, 65535, 261, 65530, 65534, 820, 59, 33, 10293, 10370, 10260, 16503, 14442, 12270, 12605, 11235, 19137, 13115, 11889, 28329, 65535, 261]
-> item_loss 1.0568764209747314 batch_loss 1.174877127011617
==== [25106, 11, 65530, 65531, 10464, 13091, 10250, 10283, 820, 19137, 10423, 11454, 11124, 10370, 15261, 11957, 16713, 14734, 17141, 15052, 10285, 10260, 13032, 11871, 10323, 19137, 12202, 16672, 15847, 12144, 11003, 12348, 13179, 10842, 15678, 13078, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 11454, 10303, 19156, 11, 65535, 261, 65530, 65534, 7176, 59, 33, 12605, 11454, 14734, 19137, 13191, 10373, 10303, 10336, 11053, 19156, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 13191, 10349, 10336, 12484, 12509, 16672, 10464, 11871, 10250, 10258, 19137, 12771, 17385, 16503, 14734, 28329, 65535, 261, 65530, 65534, 7176, 59, 33, 11685, 14734, 19137, 16738, 17728, 13091, 10373, 10303, 10336, 12484, 19137, 12605, 10423, 16667, 14794, 10333, 16533, 12202, 11871, 10323, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 10464, 13091, 10250, 10283, 13205, 11459, 13499, 11496, 19137, 10305, 11044, 12605, 10423, 15494, 10464, 10482, 14589, 11036, 15033, 12824, 11010, 14734, 13216, 17789, 19137, 12631, 10399, 12605, 12159, 13197, 10464, 15752, 11871, 10423, 15287, 14914, 11459, 12750, 14240, 12748, 10398, 17109, 10760, 11434, 11899, 13327, 12276, 14734, 10696, 11922, 19137, 10384, 15646, 11903, 14446, 11124, 15304, 15500, 10355, 10342, 28329, 65535, 261, 65530, 65534, 7176, 59, 33, 10333, 16533, 10333, 19137, 12605, 17879, 16503, 15287, 14914, 11459, 12750, 14240, 15494, 11899, 14734, 13327, 12276, 17109, 10760, 11124, 15304, 15500, 10355, 10342, 14734, 12748, 10398, 19137, 11957, 11061, 19156, 12159, 13197, 12605, 15752, 10384, 10293, 10370, 17148, 17384, 16056, 11001, 10250, 10349, 13179, 16722, 15478, 14734, 16679, 16533, 12609, 15643, 10487, 11855, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 11315, 19137, 14597, 10339, 15304, 15500, 13091, 15052, 12217, 15543, 10708, 14734, 19137, 11957, 10339, 13012, 11860, 10261, 15158, 11028, 14734, 11621, 14486, 13734, 13191, 10464, 17148, 10303, 14141, 13815, 19137, 12631, 10399, 10464, 17879, 16503, 10388, 15478, 10985, 14486, 16533, 16503, 13685, 19137, 12202, 12750, 13327, 12276, 17109, 10760, 10080, 10296, 10283, 10487, 11855, 19137, 10464, 11967, 10464, 10696, 12366, 14734, 12509, 13764, 12983, 11454, 380, 489, 10285, 19137, 11967, 11124, 15304, 15500, 14734, 12748, 10398, 12983, 11454, 393, 1678, 10285, 10080, 12605, 17879, 16503, 10464, 16417, 17136, 10760, 10696, 12366, 14734, 12509, 13764, 19137, 12202, 10265, 15494, 10760, 35, 12266, 11726, 10355, 10342, 35, 14734, 12748, 10398, 28329, 65535, 261, 65530, 65534, 7176, 59, 326, 12266, 11726, 10355, 10342, 94, 10293, 10370, 19137, 12605, 11986, 16707, 12750, 10464, 16735, 14734, 10588, 10333, 28330, 15847, 10529, 42, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 12605, 12979, 10792, 15304, 15500, 13855, 12445, 10333, 19137, 12824, 11010, 14914, 11903, 12266, 11726, 17144, 10460, 10333, 19137, 12824, 10258, 13234, 10464, 17109, 10760, 393, 13381, 13290, 14349, 12396, 1678, 19137, 15646, 10260, 16503, 16735, 10687, 10390, 10409, 10454, 10272, 16501, 19137, 15304, 15500, 12220, 16721, 10423, 12669, 14349, 12396, 17146, 11414, 15494, 10464, 28329, 65535, 261, 65530, 65534, 7176, 59, 326, 13381, 13290, 14349, 12396, 94, 11, 65535, 261, 65530, 65531, 42343, 59, 33, 15304, 15500, 14349, 12396, 59, 11, 13064, 17734, 3493, 643, 46, 55, 46, 642, 46, 638, 59, 625, 59, 635, 262, 7176, 13012, 11860, 5502, 59, 33, 13580, 12182, 17144, 10460, 262, 7176, 11436, 10621, 12824, 11010, 59, 33, 13203, 12266, 11068, 262, 7176, 96, 830, 83, 59, 33, 12266, 11068, 262, 7176, 96, 1487, 84, 59, 33, 13203, 12266, 11068, 262, 14349, 12396, 13098, 14966, 11891, 13638, 19137, 11414, 10792, 15667, 11639, 13499, 12276, 65535, 261, 65530, 65534, 7176, 59, 33, 10293, 10370, 19137, 12605, 12145, 15486, 14791, 10792, 15304, 15500, 14349, 12396, 28330, 11685, 11656, 42, 11, 65535, 261, 65530, 65532, 27370, 59, 33, 10464, 11454, 11685, 11656, 10373, 10303, 19156, 11, 65535, 261, 65530, 65534, 7176, 59, 33, 12605, 11685, 11656, 15304, 15500, 14349, 12396, 17384, 14734, 7176, 16735, 14734, 13091, 10260, 13091, 12605, 28329, 65535, 261, 65530, 65532, 27370, 59, 33, 13734, 17631, 19137, 11885, 13098, 14966, 10333, 10373, 10303, 19137, 16735, 15494, 12605, 11065, 11, 65535, 261, 65530, 65534, 7176, 59, 33, 11885, 13098, 14966, 10333, 14446, 11454, 13064, 17734, 13091, 640, 643, 46, 55, 46, 642, 46, 638, 59, 625, 59, 635, 19137, 12605, 14734, 13012, 11860, 5502, 13580, 11454, 13580, 12182, 17144, 10460, 19152, 11436, 10621, 12824, 11010, 13734, 13191, 12266, 11068, 19152, 7176, 96, 830, 83, 11621, 10339, 12266, 11068, 10079, 7176, 96, 1487, 84, 11621, 10339, 10684, 17727, 14349, 12396, 10080, 10293, 10370, 19137, 10974, 13234, 12605, 13091, 15752, 11634, 17149, 16403, 15847, 10846, 16728, 17952, 16696, 10790, 11124, 13012, 11860, 16728, 17952, 17083, 12791, 14734, 10303, 19156, 11, 65535, 261]
-> item_loss 1.0396617650985718 batch_loss 1.171937662622203
==== [25106, 11, 65530, 65532, 12848, 17728, 15643, 59, 63237, 33, 14987, 15486, 15573, 15497, 13091, 11687, 10454, 12137, 10460, 14734, 19156, 16533, 17382, 16503, 10894, 11064, 12666, 13208, 15478, 15907, 19137, 10777, 10296, 13004, 11871, 10678, 12276, 28329, 65535, 261, 65530, 65534, 11414, 15178, 59, 63237, 33, 14987, 15486, 15573, 15497, 13091, 10250, 15033, 13913, 12231, 11871, 10323, 13499, 11496, 19137, 14597, 21092, 9057, 106, 33, 15169, 10370, 11454, 640, 637, 12201, 14734, 16686, 13012, 28316, 8843, 2077, 7874, 4600, 19407, 21189, 29467, 9823, 10285, 12848, 10760, 10080, 17148, 15033, 13499, 11496, 10293, 16503, 14589, 10339, 12217, 10777, 10792, 12217, 10777, 14734, 10409, 10843, 19137, 10487, 11687, 13212, 11369, 15636, 16704, 10080, 63013, 33, 13499, 11496, 10495, 16922, 10339, 15847, 13773, 12522, 13212, 10794, 13234, 12202, 16403, 11621, 14486, 17109, 10673, 12217, 10777, 19137, 15646, 17184, 12306, 14987, 15486, 15573, 15497, 19133, 6504, 19134, 11124, 17688, 14866, 13199, 16678, 12369, 19133, 23848, 19134, 15573, 15497, 10285, 10482, 14589, 14734, 13091, 18000, 12217, 11621, 14486, 28329, 63013, 33, 13499, 11496, 14597, 15543, 14875, 11369, 11124, 16533, 14875, 11369, 15476, 12604, 19137, 13633, 10283, 17311, 14597, 11632, 10283, 14782, 11042, 14734, 12007, 15476, 12604, 10080, 15543, 14875, 11369, 11124, 16533, 14875, 11369, 10285, 14734, 13633, 10283, 12007, 17311, 13191, 10278, 10283, 10293, 16503, 15476, 10407, 19151, 11632, 11647, 15847, 13773, 12522, 13212, 10794, 11124, 10447, 15586, 15543, 14875, 14734, 10808, 18108, 15573, 15497, 28329, 50, 47, 33, 11632, 11647, 15847, 13773, 12522, 13212, 10794, 19151, 11, 15847, 13773, 12522, 13212, 10794, 10652, 16684, 13499, 11496, 13216, 16412, 12217, 10777, 10285, 10260, 11042, 10932, 16702, 14734, 14782, 10684, 12415, 10080, 15494, 11899, 16417, 14966, 10292, 14863, 17774, 89, 14734, 10932, 16702, 12217, 10777, 19137, 15847, 13773, 12522, 13212, 10794, 16664, 15200, 13633, 10283, 10932, 16702, 14782, 11957, 10339, 12217, 10777, 10285, 14734, 13633, 10283, 10687, 10390, 10932, 16702, 14734, 13773, 12522, 10838, 10768, 13004, 10080, 10482, 14589, 10256, 10283, 14863, 17774, 16664, 15200, 13773, 12522, 10838, 10768, 13004, 19151, 13290, 16718, 14863, 17774, 19133, 82, 19134, 10079, 17644, 14863, 17774, 19133, 76, 19134, 11124, 10577, 14863, 17774, 19133, 87, 19134, 10080, 17148, 10349, 13091, 17189, 17141, 10482, 14589, 11021, 11871, 10323, 14734, 13216, 17385, 14863, 17774, 88, 82, 10079, 1576, 11124, 1585, 10384, 17109, 10673, 14863, 17774, 89, 17149, 16403, 15471, 12415, 11003, 12791, 12348, 10760, 14734, 28329, 82, 296, 323, 277, 322, 82, 11, 76, 296, 323, 277, 322, 76, 11, 87, 296, 323, 277, 322, 87, 11, 13773, 12522, 10838, 10768, 13004, 10482, 14589, 13290, 16718, 11124, 17644, 14863, 17774, 10305, 17734, 14734, 14172, 15043, 16664, 15200, 19137, 17798, 10399, 17644, 14734, 15516, 12231, 19133, 1824, 19134, 14734, 12200, 13036, 13326, 19137, 14221, 11044, 17189, 17141, 49715, 10762, 13004, 10427, 17184, 10399, 12306, 10250, 10901, 10768, 13004, 19151, 11, 57934, 41, 82, 45, 310, 45, 321, 42, 296, 52944, 41, 82, 277, 310, 95, 85, 282, 32105, 41, 1824, 494, 277, 321, 11, 11632, 11647, 15847, 13773, 12522, 13212, 10794, 11967, 17109, 10673, 10768, 12604, 11632, 10283, 13773, 12522, 10838, 11647, 19137, 13633, 10283, 11647, 17311, 13191, 15847, 12144, 14734, 13216, 17385, 14863, 17774, 10080, 17148, 10482, 12348, 13499, 11496, 11021, 10399, 10684, 13773, 17109, 10673, 12217, 10777, 14734, 10260, 11042, 13036, 17919, 10080, 13633, 10283, 13773, 12522, 10838, 11647, 14734, 17109, 10760, 16443, 17152, 12824, 12202, 10482, 14589, 11021, 11871, 10323, 14734, 13216, 17385, 14863, 17774, 1580, 17149, 16403, 15471, 12415, 11003, 12791, 19137, 10399, 10358, 14583, 13188, 15480, 17109, 10760, 19151, 11, 33286, 23722, 41, 82, 45, 310, 45, 321, 42, 296, 28674, 1752, 41, 25566, 50, 45, 28376, 31060, 105, 42, 277, 4324, 11, 25566, 106, 296, 58687, 41, 82, 277, 322, 1390, 45, 310, 277, 322, 1190, 45, 321, 277, 322, 1561, 42, 11, 51, 47, 33, 10447, 15586, 15543, 14875, 19151, 11, 14597, 10339, 63013, 13499, 11496, 15568, 10309, 10932, 16702, 11454, 12217, 10777, 10285, 14734, 10447, 15586, 14734, 10696, 11454, 14862, 16696, 19137, 11416, 13581, 11967, 10447, 15586, 15543, 14875, 13920, 10842, 10792, 17109, 10673, 12108, 10673, 10285, 10080, 17148, 15033, 15543, 14875, 13091, 10447, 15586, 14734, 11434, 11899, 10762, 13004, 19137, 11021, 10399, 10482, 14589, 10260, 11042, 18020, 14426, 14734, 13580, 12290, 11124, 10457, 12290, 10762, 13004, 16664, 15200, 19151, 11, 1346, 41, 8518, 45, 285, 106, 42, 296, 22476, 41, 8518, 282, 3483, 620, 49, 1702, 51, 106, 48, 101, 494, 11, 1346, 41, 8518, 45, 285, 106, 278, 284, 42, 296, 21459, 41, 8518, 282, 3483, 620, 49, 1702, 51, 106, 48, 101, 494, 11, 10687, 10285, 8518, 13091, 10447, 15586, 19137, 106, 13091, 15516, 12231, 19137, 101, 13091, 12108, 10673, 11638, 11975, 28329, 52, 47, 33, 15543, 14875, 11369, 11124, 16533, 14875, 11369, 19151, 11, 15543, 14875, 11369, 14597, 10250, 11539, 14782, 11042, 14734, 12007, 15476, 12604, 19137, 13633, 10283, 12007, 17311, 13191, 10250, 10283, 11632, 11647, 15847, 13773, 12522, 13212, 10794, 19137, 11044, 16974, 10250, 10283, 10447, 15586, 15543, 14875, 14734, 10808, 18108, 15573, 15497, 10080, 13581, 11630, 19137, 11454, 13633, 10283, 11855, 12007, 10305, 11044, 12220, 14589, 13604, 12143, 17152, 12824, 11124, 12007, 12306, 10250, 10901, 28329, 16533, 14875, 11369, 10322, 14597, 11632, 10283, 14782, 11042, 14734, 12007, 15476, 12604, 10080, 10444, 13091, 19137, 17798, 10333, 11632, 11647, 15847, 13773, 12522, 11124, 10447, 15586, 15543, 14875, 14734, 10808, 18108, 15573, 15497, 10305, 11630, 19137, 11885, 17147, 10894, 11064, 10250, 10283, 18030, 11630, 14734, 11632, 11647, 13773, 12522, 12007, 19137, 16721, 12007, 11967, 15543, 14875, 11369, 14734, 17109, 10760, 10460, 10292, 17109, 10673, 10080, 17148, 13191, 10847, 10339, 16533, 14875, 11369, 11454, 14583, 12604, 14778, 13308, 12217, 10777, 13064, 10684, 13773, 15543, 14875, 11369, 14734, 17109, 10760, 28329, 53, 47, 33, 13188, 15480, 14734, 15471, 12415, 11124, 49715, 12007, 19151, 11, 16533, 14875, 11369, 14734, 17109, 10760, 17189, 17141, 13188, 15480, 14734, 15471, 12415, 12007, 11124, 49715, 14091, 13815, 10762, 13004, 10427, 17184, 19137, 10399, 10358, 14583, 13633, 10283, 14778, 13308, 10932, 16702, 14734, 13445, 14426, 10768, 12155, 28329, 12159, 13197, 17148, 10349, 10696, 11922, 15752, 11957, 12469, 13191, 12631, 12176, 10847, 10080, 65535, 261, 65530, 65532, 12848, 17728, 15643, 59, 33, 11315, 19137, 12605, 14486, 16533, 10333, 28329, 65535, 261, 65530, 65534, 11414, 15178, 59, 33, 12605, 12337, 12266, 12366, 15752, 12176, 10792, 10464, 19126, 16738, 17728, 17147, 13191, 10687, 10390, 17728, 18025, 10303, 19156, 11, 65535, 261, 65530, 65532, 12848, 17728, 15643, 59, 267, 10580, 16689, 10464, 13191, 10250, 10283, 10337, 17149, 10794, 11860, 15158, 10287, 631, 621, 620, 630, 621, 631, 621, 620, 631, 620, 631, 630, 630, 631, 631, 19137, 12750, 14240, 10399, 10258, 13582, 18197, 17149, 16403, 12945, 10460, 19151, 11001, 10808, 53, 10447, 12202, 13920, 10842, 17172, 12307, 13004, 17387, 14734, 9822, 49, 9823, 10482, 10687, 10261, 89, 14734, 17688, 12231, 14782, 11042, 19137, 14221, 11044, 10384, 89, 10285, 10739, 10985, 11885, 28329, 65535, 261, 65530, 65534, 11414, 15178, 59, 33, 11685, 14734, 19126, 16672, 12605, 10402, 12750, 14240, 10464, 12848, 10760, 14734, 13582, 18197, 10482, 14589, 10337, 17149, 10794, 11860, 15158, 10287, 89, 296, 3484, 621, 620, 630, 621, 631, 621, 620, 631, 620, 631, 630, 630, 631, 631, 28329, 13582, 18197, 50, 19151, 11001, 89, 14734, 10808, 53, 10447, 11, 89, 14734, 10808, 53, 10447, 13091, 631, 621, 28329, 13582, 18197, 51, 19151, 13920, 10842, 17172, 12307, 13004, 17387, 14734, 9822, 49, 9823, 10482, 10687, 10261, 89, 14734, 17688, 12231, 14782, 11042, 11, 14597, 10339, 89, 14734, 17688, 12231, 10292, 650, 19137, 12605, 10402, 17879, 16503, 11454, 631, 621, 14734, 13204, 12003, 13920, 10842, 650, 46, 53, 296, 3499, 10283, 17874, 10080, 12631, 10399, 12605, 10402, 12348, 10792, 19151, 11, 631, 621, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 11, 13582, 18197, 52, 19151, 10384, 89, 10285, 10739, 10985, 15489, 13258, 11, 16503, 10384, 89, 10285, 10739, 10985, 15489, 13258, 19137, 12605, 10402, 11016, 17879, 12650, 16403, 10278, 10283, 13004, 11860, 10305, 17734, 14734, 12750, 10447, 10739, 13764, 19133, 10953, 89, 1319, 19134, 10080, 17148, 12522, 11110, 14808, 12605, 10402, 10482, 14589, 10399, 10258, 16521, 10779, 11454, 89, 11124, 13582, 18197, 51, 14734, 15489, 13258, 10305, 17734, 10739, 10985, 14782, 12220, 14734, 10447, 19151, 11, 49, 21171, 283, 296, 283, 11, 49, 21171, 284, 296, 284, 11, 50, 21171, 283, 296, 284, 11, 50, 21171, 284, 296, 283, 11, 12631, 10399, 12605, 10402, 13191, 19151, 11, 6884, 2178, 2194, 262, 631, 621, 620, 630, 621, 631, 621, 620, 631, 620, 631, 630, 630, 631, 631, 3340, 41, 89, 42, 11, 6884, 11, 89, 1319, 262, 631, 621, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 11, 65458, 568, 11, 62, 3473, 620, 620, 630, 621, 631, 621, 620, 631, 620, 631, 630, 630, 631, 631, 11, 11416, 13581, 19137, 10384, 89, 10285, 10739, 10985, 10808, 53, 10447, 12348, 10792, 10337, 17149, 10794, 11860, 15158, 10287, 620, 620, 620, 630, 621, 631, 621, 620, 631, 620, 631, 630, 630, 631, 631, 10080, 65535, 261, 65530, 65532, 12848, 17728, 15643, 59, 267, 11315, 19137, 10464, 16525, 12348, 17148, 10283, 17728, 18025, 11124, 13188, 12266, 11726, 14734, 17728, 18025, 11187, 10283, 13637, 17097, 13191, 12522, 10304, 19156, 11, 65535, 261, 65530, 65534, 11414, 15178, 59, 33, 12605, 16667, 10292, 17311, 12337, 13191, 12522, 10304, 19137, 13188, 10782, 12469, 16672, 12605, 10292, 12469, 16533, 17382, 63013, 13273, 13250, 14734, 12666, 13208, 15478, 15907, 19137, 12605, 14734, 11414, 15178, 10322, 16684, 15752, 11634, 12176, 10847, 12469, 16056, 12348, 14862, 16696, 10080, 15646, 11454, 10305, 11044, 12469, 10990, 16672, 12605, 17149, 16403, 10337, 17149, 10794, 11860, 15158, 10287, 14734, 17144, 15200, 19137, 12605, 12826, 13827, 12469, 13091, 12509, 16672, 12605, 12176, 10847, 12469, 16533, 10722, 11903, 17781, 17728, 18025, 19137, 12609, 15643, 13827, 16707, 12605, 14734, 15752, 10838, 10080, 12605, 12337, 18216, 10685, 15752, 11634, 10292, 10464, 14734, 14583, 13815, 10261, 12137, 10460, 12848, 10494, 10511, 10788, 19137, 12609, 13091, 16672, 12469, 11957, 12605, 13191, 13179, 13913, 14734, 10333, 16533, 28329, 65535, 261, 65530, 65532, 12848, 17728, 15643, 59, 267, 11315, 19137, 12605, 14862, 17222, 10333, 28329, 65535, 261, 65530, 65534, 11414, 15178, 59, 33, 11685, 14734, 19137, 11687, 13258, 17147, 13191, 17728, 18025, 14734, 16713, 19137, 13558, 17143, 13234, 17728, 12605, 11183, 28329, 65535, 261, 65530, 65532, 12848, 17728, 15643, 59, 267, 12904, 12904, 11647, 11, 65535, 261, 65530, 65534, 11414, 15178, 59, 33, 12797, 14778, 10808, 14734, 10529, 12445, 13234, 14791, 19137, 12605, 12202, 10260, 12727, 13191, 14328, 14486, 11903, 10452, 19137, 10260, 17141, 12605, 15752, 10452, 10423, 10792, 12469, 14734, 12366, 12522, 19137, 16774, 16774, 28329, 65535, 261]
-> item_loss 0.5725136995315552 batch_loss 1.1591839612798487
==== [65530, 65532, 10902, 10362, 13191, 10373, 10303, 11975, 11035, 19156, 11, 65535, 261, 65530, 65534, 10902, 10362, 13191, 12337, 11632, 11975, 11035, 19137, 10399, 10258, 13091, 10250, 10349, 10487, 11855, 19151, 11, 50, 47, 33, 14188, 15694, 10287, 19151, 17148, 13091, 10902, 10362, 10370, 13188, 11268, 13558, 14734, 11975, 11035, 10305, 10250, 19137, 11021, 10399, 11454, 16684, 11632, 12895, 10447, 10257, 11986, 16707, 28329, 51, 47, 33, 14171, 17351, 17919, 19151, 17148, 13091, 10250, 15033, 11967, 17919, 13231, 10079, 15694, 17351, 11124, 18363, 16295, 14155, 11454, 10250, 16944, 14734, 11975, 11035, 28329, 52, 47, 33, 16793, 13684, 10650, 19151, 17148, 13091, 10250, 15033, 11967, 16793, 11855, 11124, 13676, 14244, 12604, 15291, 14349, 19137, 10703, 10842, 10673, 11036, 15033, 18119, 13021, 11124, 15294, 14734, 14582, 11159, 28329, 53, 47, 33, 15595, 15694, 10287, 19151, 17148, 13091, 10902, 10362, 10370, 13188, 11268, 13558, 14734, 11975, 11035, 10305, 10250, 19137, 11021, 10399, 11454, 16684, 11632, 18069, 18107, 12664, 10792, 28329, 54, 47, 33, 13325, 13333, 15293, 19151, 17148, 13091, 10250, 15033, 11967, 13325, 13333, 10374, 10079, 15302, 15260, 15264, 11124, 15919, 18425, 13916, 11038, 11044, 13750, 14171, 14734, 15293, 14172, 28329, 12421, 14734, 13234, 16735, 19137, 10902, 10362, 13191, 12337, 11632, 15597, 18053, 11021, 10494, 11159, 11986, 19137, 13633, 10283, 10370, 17311, 11021, 10399, 13326, 12797, 15847, 12144, 14734, 11010, 11110, 17177, 12731, 15847, 12144, 11268, 13558, 14734, 11975, 11035, 10080, 65535, 261]
[2023-09-30 21:52:44,677] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[0.0001, 0.0001, 0.0001], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.5741926431655884 batch_loss 1.1469966421524684
==== [65530, 65531, 13326, 12797, 47487, 33, 10285, 14734, 12848, 14966, 14734, 10529, 12445, 19137, 13250, 12261, 26060, 19137, 40992, 22590, 57009, 30917, 102, 29945, 11, 65535, 261, 65530, 65532, 13326, 12797, 10258, 17919, 42343, 10285, 16056, 11001, 14734, 11630, 17303, 15573, 15497, 16909, 13021, 19137, 33, 12848, 10494, 15494, 12605, 13203, 13234, 630, 11639, 14734, 15878, 14474, 10529, 12445, 19137, 33, 14589, 26060, 13327, 12276, 15494, 10760, 15178, 13336, 11, 65535, 261, 65530, 65531, 42343, 33, 17148, 10256, 10283, 15878, 14474, 11028, 10292, 11, 13326, 12797, 12605, 11957, 11630, 17303, 10529, 12445, 14734, 13290, 16718, 19137, 13203, 13234, 630, 11639, 14734, 15878, 14474, 10529, 12445, 10292, 11, 50, 47, 30917, 20512, 30074, 4811, 44527, 4596, 3493, 59, 620, 3493, 643, 48, 54, 48, 50, 11, 51, 47, 3913, 20512, 30074, 39934, 31059, 332, 4537, 7624, 46172, 4811, 20310, 49961, 45, 4596, 3493, 643, 48, 54, 48, 57, 3483, 59, 625, 11, 52, 47, 30917, 59, 20512, 30074, 45, 4811, 59, 50070, 45, 55915, 59, 3493, 643, 48, 54, 48, 50, 3493, 59, 630, 65535, 261, 65530, 65534, 10399, 10258, 13091, 10250, 10283, 13203, 13234, 630, 11639, 14734, 15878, 14474, 10529, 12445, 59, 11, 6884, 26060, 11, 124, 3331, 35, 41425, 116, 388, 326, 28352, 124, 43909, 35, 25460, 388, 269, 6329, 30074, 384, 43909, 35, 2215, 388, 269, 40586, 384, 43909, 35, 27165, 5181, 640, 643, 48, 54, 48, 50, 3493, 59, 620, 35, 28352, 2391, 28352, 124, 43909, 35, 25460, 388, 269, 6329, 30074, 384, 43909, 35, 2215, 388, 269, 6219, 49961, 384, 43909, 35, 27165, 5181, 640, 643, 48, 54, 48, 57, 3483, 59, 625, 35, 28352, 2391, 28352, 124, 43909, 35, 25460, 388, 269, 6329, 30074, 384, 43909, 35, 2215, 388, 269, 48406, 384, 43909, 35, 27165, 5181, 640, 643, 48, 54, 48, 50, 3493, 59, 630, 35, 28352, 126, 3331, 94, 11, 126, 11, 6884, 11, 10399, 10257, 26060, 10696, 11922, 12605, 11967, 42343, 14734, 12848, 14966, 10285, 16056, 11001, 14734, 10529, 12445, 13006, 14486, 14734, 15489, 13258, 10080, 65535, 261, 65530, 65532, 10696, 11922, 10285, 15568, 11977, 15878, 14474, 11028, 19137, 33, 16738, 10464, 11454, 10257, 17919, 14734, 26060, 10285, 16416, 10655, 13580, 14914, 14734, 15878, 14474, 11028, 28329, 65535, 261, 65530, 65531, 42343, 33, 17148, 10256, 10283, 15878, 14474, 11028, 10292, 11, 6884, 11, 50, 47, 3662, 84, 46, 1196, 50, 11, 51, 47, 3662, 84, 46, 1196, 647, 11, 52, 47, 3662, 84, 46, 1196, 639, 11, 6884, 65535, 261, 65530, 65534, 11, 17148, 17384, 15494, 10760, 16416, 10655, 14734, 15878, 14474, 10529, 12445, 59, 11, 6884, 26060, 11, 124, 3331, 35, 41425, 116, 388, 326, 28352, 124, 43909, 35, 25460, 388, 269, 6329, 30074, 384, 43909, 35, 2215, 388, 269, 40586, 384, 43909, 35, 27165, 5181, 640, 643, 48, 54, 48, 50, 3493, 59, 620, 384, 43909, 35, 41425, 96, 41900, 388, 269, 875, 84, 46, 1196, 50, 35, 28352, 2391, 28352, 124, 43909, 35, 25460, 388, 269, 6329, 30074, 384, 43909, 35, 2215, 388, 269, 6219, 49961, 384, 43909, 35, 27165, 5181, 640, 643, 48, 54, 48, 57, 3483, 59, 625, 384, 43909, 35, 41425, 96, 41900, 388, 269, 875, 84, 46, 1196, 647, 35, 28352, 2391, 28352, 124, 43909, 35, 25460, 388, 269, 6329, 30074, 384, 43909, 35, 2215, 388, 269, 48406, 384, 43909, 35, 27165, 5181, 640, 643, 48, 54, 48, 50, 3493, 59, 630, 384, 43909, 35, 41425, 96, 41900, 388, 269, 875, 84, 46, 1196, 639, 35, 28352, 126, 3331, 94, 11, 126, 11, 6884, 11, 10399, 10257, 14734, 26060, 10285, 15494, 10760, 10333, 16416, 10655, 14734, 15878, 14474, 10529, 12445, 45, 33, 10894, 12732, 10333, 10384, 15469, 15450, 10792, 10431, 13002, 14734, 15878, 14474, 10080, 65535, 261, 65530, 65532, 13326, 12797, 10464, 14734, 10696, 11454, 13216, 17385, 10285, 16678, 12369, 14734, 10529, 12445, 19137, 11047, 12605, 12848, 10494, 41661, 14734, 10988, 13004, 10529, 12445, 19137, 33, 17379, 14589, 26060, 17146, 11414, 15489, 13258, 11, 65535, 261, 65530, 65534, 10399, 10258, 13091, 10250, 10283, 10487, 11855, 59, 11, 6884, 26060, 11, 124, 11, 35, 41512, 3484, 388, 358, 11, 35, 5704, 388, 269, 32905, 300, 633, 19520, 8242, 384, 11, 35, 5960, 388, 269, 5960, 3537, 46, 7159, 45, 285, 47, 52, 19929, 45, 291, 28687, 116, 384, 11, 35, 5960, 44574, 275, 6482, 489, 59, 269, 53, 3826, 384, 11, 35, 5960, 28458, 36008, 275, 6241, 489, 59, 269, 660, 49, 20348, 384, 11, 35, 5960, 50703, 275, 1040, 489, 59, 269, 632, 57, 3826, 384, 11, 35, 5704, 37943, 275, 1167, 489, 59, 3493, 668, 45, 11, 35, 5704, 37943, 32031, 275, 34073, 489, 59, 3513, 716, 45, 11, 35, 5704, 38116, 275, 23529, 116, 489, 59, 287, 45, 11, 35, 5704, 37943, 31300, 32031, 275, 32961, 489, 59, 3537, 45, 11, 35, 5704, 63654, 22459, 275, 23400, 54103, 489, 59, 269, 5513, 684, 384, 11, 35, 5704, 63654, 22459, 40194, 275, 33580, 489, 59, 3537, 45, 11, 35, 5704, 45348, 60226, 388, 32313, 45, 11, 35, 5704, 45348, 60226, 40194, 275, 33580, 489, 59, 3489, 30293, 45, 11, 35, 5960, 30502, 64254, 388, 269, 6263, 106, 46, 72, 691, 4011, 50, 384, 11, 35, 5960, 30502, 64254, 40194, 388, 3485, 57, 45, 11, 35, 5960, 30502, 39759, 275, 6241, 489, 59, 3483, 620, 45, 11, 35, 5960, 30502, 39759, 40194, 388, 3485, 57, 30293, 45, 11, 126, 360, 21214, 65535, 261, 65530, 65532, 14427, 14736, 11638, 12170, 10449, 11454, 12200, 13819, 12007, 17147, 13091, 11957, 13819, 12007, 19156, 17146, 11414, 26060, 13327, 12276, 14734, 10529, 12445, 11, 65535, 261, 65530, 65534, 11, 6884, 26060, 11, 124, 11, 35, 54311, 7743, 388, 32313, 45, 11, 35, 60952, 388, 32313, 45, 11, 35, 33652, 34870, 2210, 2255, 388, 32313, 45, 11, 35, 8172, 34880, 388, 32313, 45, 11, 35, 61024, 388, 32313, 45, 11, 35, 54309, 59887, 116, 23258, 5574, 1792, 1773, 63201, 5191, 11, 126, 11, 6884, 11, 36, 33, 17148, 17384, 15494, 10760, 10333, 10250, 10283, 10487, 11855, 19137, 10464, 11021, 10399, 13326, 12797, 15847, 12144, 14734, 17879, 16503, 10538, 12981, 43, 65535, 261]
-> item_loss 1.3937420845031738 batch_loss 1.1520322634249318
==== [12848, 17728, 4516, 6908, 10397, 14875, 14782, 10684, 14862, 16696, 11, 65530, 65532, 10397, 14875, 10285, 14734, 26742, 13091, 10373, 10303, 12522, 12410, 19156, 11, 65535, 261, 65530, 65534, 26742, 17189, 12182, 13091, 58393, 14734, 15558, 10708, 45, 16417, 14966, 17141, 15052, 12609, 10762, 13004, 28329, 12631, 10399, 11454, 10397, 14875, 10285, 45, 26742, 13191, 10745, 10283, 11021, 15752, 14734, 11064, 10304, 59, 11, 50, 47, 33, 10250, 10283, 10762, 13004, 12609, 17141, 15052, 14734, 11043, 15044, 10080, 10487, 11687, 59, 11, 6884, 42114, 11, 7334, 4677, 96, 26742, 5268, 28352, 5362, 11, 6884, 11, 51, 47, 33, 10250, 10283, 10762, 13004, 12609, 17141, 15052, 11957, 16803, 10080, 10487, 11687, 59, 11, 6884, 42114, 11, 26742, 296, 32074, 96, 54014, 11, 6884, 11, 52, 47, 31759, 11021, 10399, 10460, 10292, 10250, 10283, 17189, 14589, 14734, 11043, 15044, 10397, 16417, 10409, 10454, 10762, 13004, 12609, 17141, 15052, 28329, 10688, 10452, 10454, 15033, 11064, 10304, 45, 16503, 14791, 10257, 10258, 13012, 11124, 13179, 16722, 15478, 14734, 10397, 14875, 10080, 10444, 11638, 15850, 10257, 17311, 13091, 11124, 10762, 13004, 12609, 15643, 17141, 15052, 14782, 10684, 10080, 65535, 261, 65530, 65534, 26742, 33, 17189, 12182, 14589, 13234, 11899, 10304, 10250, 10283, 17141, 15052, 48, 10762, 13004, 48, 13036, 13764, 28329, 10296, 10487, 13234, 16735, 59, 11, 6884, 26957, 11, 26742, 4491, 359, 26290, 125, 3331, 35, 33155, 3376, 26290, 126, 363, 11, 7463, 11, 6884, 11, 17148, 17384, 11899, 10304, 10333, 10250, 10283, 12824, 11002, 10250, 10283, 10988, 13004, 31496, 33, 14734, 10915, 11043, 10762, 13004, 41, 10915, 11043, 11994, 13091, 13734, 13191, 10762, 13004, 11043, 497, 12202, 17146, 11414, 10250, 10283, 11860, 15158, 10287, 13327, 12276, 10901, 11044, 14734, 11860, 15158, 10287, 28329, 12631, 10399, 14589, 31759, 33, 11899, 10304, 14734, 17148, 15033, 10762, 13004, 11957, 16803, 45, 11994, 15044, 10292, 24048, 11957, 16803, 28329, 24114, 33, 10285, 17147, 13191, 10250, 10283, 15261, 10441, 14734, 11899, 10304, 10762, 13004, 14734, 13036, 12276, 13091, 10482, 14589, 46391, 59, 11, 6884, 26957, 11, 41749, 4491, 359, 26290, 125, 3331, 35, 33155, 3376, 26290, 126, 363, 11, 7463, 11, 6884, 11, 24048, 33, 11124, 46391, 33, 11454, 10250, 10349, 10841, 15752, 10257, 13191, 15478, 12359, 12143, 12268, 45, 10444, 12421, 14734, 13234, 16735, 17311, 13091, 14589, 13234, 11899, 10304, 10762, 13004, 48, 17141, 15052, 14734, 28329, 12631, 10399, 11454, 10397, 14875, 10285, 14791, 10792, 31759, 33, 11899, 10304, 14734, 45, 11994, 13091, 11899, 10304, 10333, 10250, 10283, 29623, 11957, 16803, 45, 10322, 11994, 13091, 10250, 10283, 10762, 13004, 48, 17141, 15052, 28329, 12159, 13197, 10257, 17162, 16533, 17382, 11957, 12469, 13191, 14589, 34, 65535, 261, 65530, 65534, 26742, 33, 11454, 13581, 11621, 10397, 16417, 17149, 15052, 41, 49585, 42, 10080, 10688, 10452, 13234, 16735, 45, 11885, 13091, 52571, 46, 27003, 46, 42153, 10079, 49585, 46, 27003, 46, 42324, 33, 11124, 52571, 46, 27003, 46, 7478, 33, 17148, 10256, 10283, 11117, 10398, 14734, 10988, 13004, 45, 16417, 14966, 16503, 12945, 10460, 14734, 17149, 15052, 28329, 11454, 13205, 10487, 10285, 14734, 10397, 14875, 10285, 59, 11, 6884, 8098, 113, 11, 41, 26200, 46, 49585, 262, 59, 26290, 269, 1795, 2226, 1818, 48, 49697, 46, 35325, 35, 11, 59, 41129, 3352, 1795, 2226, 1818, 48, 49697, 46, 35325, 516, 11, 5362, 11, 42, 11, 6884, 11, 17148, 13619, 10397, 14875, 10781, 12261, 10333, 10250, 10283, 11043, 10292, 269, 1795, 2226, 1818, 48, 49697, 46, 35325, 35, 33, 14734, 17149, 15052, 28329, 14221, 11044, 11044, 15511, 10397, 14875, 10482, 14589, 14782, 11042, 14734, 17149, 15052, 11043, 15044, 10460, 10292, 31759, 33, 10988, 13004, 45, 10487, 11687, 59, 11, 6884, 8098, 113, 11, 41, 49585, 46, 27003, 46, 42153, 267, 35, 1795, 2226, 1818, 48, 49697, 46, 35325, 35, 21340, 21616, 42, 11, 41, 49585, 46, 27003, 46, 42324, 267, 35, 1795, 2226, 1818, 48, 49697, 46, 35325, 35, 269, 261, 381, 11, 41, 49585, 46, 27003, 46, 7478, 269, 1795, 2226, 1818, 48, 49697, 46, 35325, 381, 11, 6884, 11, 17148, 17384, 14734, 269, 1795, 2226, 1818, 48, 49697, 46, 35325, 35, 33, 11994, 13091, 10397, 16417, 17149, 15052, 31759, 28329, 12421, 14734, 13234, 16735, 45, 26742, 33, 16417, 14966, 16503, 12945, 10460, 14734, 17268, 10283, 11043, 10292, 269, 1795, 2226, 1818, 48, 49697, 46, 35325, 35, 33, 14734, 44742, 33, 17149, 15052, 28329, 12159, 13197, 17148, 10283, 16533, 17382, 13191, 10847, 10339, 12469, 14486, 16533, 31759, 33, 11454, 17148, 17384, 12631, 10397, 16417, 14734, 11064, 10304, 10080, 65535, 261]
-> item_loss 1.6740504503250122 batch_loss 1.1624726271629333
==== [65530, 65531, 12656, 14035, 10250, 10283, 15847, 11805, 10452, 15543, 17108, 19137, 13326, 12797, 27370, 14734, 17109, 10673, 15494, 10760, 15543, 17108, 11044, 14734, 13012, 13205, 11, 65535, 261, 65530, 65532, 12605, 17879, 16503, 10464, 12927, 10708, 10684, 10339, 13631, 11795, 12678, 10691, 14782, 10684, 14734, 14862, 16696, 14734, 13012, 15131, 11, 65535, 261, 65530, 65534, 10399, 10258, 13091, 10250, 10283, 10487, 11855, 59, 267, 11454, 17141, 10985, 14734, 10745, 10917, 12201, 17384, 45, 33, 13631, 11795, 10684, 15304, 10250, 14781, 11622, 11002, 10370, 10402, 14734, 10684, 13773, 10080, 17148, 10293, 16503, 13091, 11416, 10292, 45, 33, 13631, 10366, 11454, 12395, 11858, 11124, 10358, 11044, 14734, 17268, 10745, 10283, 13190, 17384, 12665, 12701, 10333, 12140, 11638, 14734, 14583, 10358, 10838, 10409, 10843, 45, 33, 11042, 13064, 10322, 12665, 11002, 10333, 12140, 11638, 14734, 15287, 14987, 10966, 10838, 10080, 11684, 17879, 16503, 14240, 18004, 11795, 10650, 45, 33, 11042, 13064, 17147, 16503, 10692, 18004, 12137, 10460, 11124, 11871, 10323, 28329, 11454, 17148, 15033, 12484, 10724, 10258, 45, 33, 13631, 10366, 14734, 17177, 12731, 10293, 16503, 13191, 10278, 15033, 59, 33, 10250, 15033, 13091, 17177, 12731, 11454, 11920, 10285, 15847, 15494, 15847, 16956, 45, 33, 11013, 10250, 15033, 10779, 13091, 17177, 12731, 11454, 10913, 17796, 12824, 11002, 12686, 14486, 28329, 13051, 16686, 17177, 12731, 11187, 15033, 13036, 13764, 45, 33, 13631, 10366, 17311, 17919, 10288, 14808, 12140, 11638, 14734, 10966, 10838, 10080, 11684, 17879, 16503, 14240, 18004, 11795, 10650, 45, 33, 11042, 13064, 17147, 16503, 10692, 18004, 12137, 10460, 11124, 11871, 10323, 10080, 11684, 12368, 18001, 10260, 13032, 11459, 11871, 10323, 13034, 14862, 16696, 11124, 12666, 15752, 45, 33, 11042, 13064, 10322, 12159, 13197, 15752, 11634, 12348, 10792, 14970, 10423, 14734, 16667, 11021, 10080, 11454, 17148, 10283, 17141, 15052, 10285, 45, 33, 11684, 17919, 10288, 14808, 12140, 11638, 14734, 10966, 10838, 28329, 11687, 13258, 17177, 12731, 11454, 11920, 10285, 15847, 15494, 15847, 16956, 45, 65535, 261, 65530, 65534, 10399, 10258, 13091, 10250, 10283, 15486, 17141, 15543, 17108, 11044, 14734, 14314, 13205, 59, 11, 13631, 11795, 17734, 37881, 33, 13091, 12445, 12445, 14782, 10684, 14734, 10080, 17145, 10745, 10917, 12201, 45, 13631, 10366, 10402, 14734, 16530, 15898, 11622, 11002, 10684, 13773, 28329, 12395, 11858, 10992, 10780, 14583, 10358, 11044, 10745, 10283, 13190, 45, 13631, 10366, 15710, 16875, 13722, 17385, 14734, 11920, 12234, 16879, 10409, 45, 11042, 13064, 12665, 11002, 10966, 10838, 10080, 11684, 16503, 14240, 18004, 11795, 10650, 10079, 12137, 10460, 11124, 11871, 10323, 45, 17879, 16503, 11454, 17148, 10285, 17734, 11001, 12348, 12200, 16412, 28329, 13191, 10278, 15033, 13036, 12276, 11021, 10494, 17177, 12731, 59, 11, 10250, 13091, 45, 11454, 11920, 15847, 10293, 14240, 18004, 11795, 10650, 10080, 17148, 15033, 13036, 12276, 14782, 11957, 10511, 11901, 10444, 17879, 16503, 13631, 10366, 15931, 16899, 13179, 11632, 13064, 17734, 10261, 13773, 12522, 10838, 28329, 10337, 13091, 45, 11967, 11795, 10650, 10355, 15494, 10913, 17796, 14791, 12686, 10080, 13631, 10366, 11021, 10264, 13773, 12137, 10460, 11871, 10323, 45, 10444, 15931, 16899, 13179, 11632, 28329, 10260, 15202, 13631, 10366, 17177, 12731, 11187, 15033, 13036, 12276, 45, 10966, 10838, 17311, 12337, 11638, 28329, 11684, 17879, 16503, 10260, 13032, 11871, 10323, 13034, 12666, 15752, 10079, 11584, 17688, 14862, 16696, 45, 11042, 13064, 10335, 11001, 14970, 10423, 16667, 11042, 28329, 11687, 13258, 17177, 12731, 11454, 11920, 14240, 18004, 45, 13631, 10366, 17879, 16503, 59, 11, 9828, 33, 10333, 16533, 11795, 10650, 16099, 10691, 10079, 11889, 12678, 10261, 10997, 12022, 17879, 16503, 11, 9828, 33, 11871, 10323, 14660, 14662, 18009, 17771, 10261, 11533, 13205, 12686, 14486, 14862, 16696, 11, 9828, 33, 10261, 11795, 10650, 13250, 12261, 39332, 7939, 102, 33, 10342, 10846, 11, 9828, 28377, 585, 11, 11687, 13581, 12638, 15752, 16672, 13631, 11795, 37881, 33, 11454, 11795, 10650, 12604, 17688, 17141, 15052, 10285, 12746, 15511, 12299, 10901, 28329, 12421, 14734, 13234, 16735, 45, 13191, 12986, 14734, 13631, 11795, 12678, 10691, 11001, 10722, 10339, 13631, 10366, 14734, 12986, 14426, 15202, 14486, 11124, 10260, 13032, 11871, 10323, 28329, 10399, 10257, 13091, 10250, 10283, 13179, 15190, 15475, 13923, 13127, 14734, 14314, 13205, 10080, 16738, 10538, 12981, 17631, 16730, 10305, 11621, 45, 11088, 14862, 10684, 17644, 10529, 12445, 17225, 14033, 10080, 65535, 261, 65530, 65532, 12605, 17879, 16503, 10464, 15543, 17108, 10250, 10283, 15847, 11805, 10452, 15543, 17108, 14734, 13012, 15131, 45, 13326, 12797, 27370, 14734, 17109, 10673, 15494, 10760, 15543, 17108, 11044, 14734, 13012, 13205, 11, 10399, 10258, 13091, 10250, 10283, 10487, 11855, 59, 11, 12605, 17879, 16503, 10464, 15543, 17108, 17148, 15216, 13012, 15131, 59, 11, 11454, 17141, 10985, 14734, 10745, 10917, 12201, 17384, 45, 13631, 11795, 10684, 15304, 10250, 14781, 11622, 11002, 10370, 10402, 10684, 13773, 10080, 17148, 15033, 10684, 15304, 45, 13179, 10733, 14914, 11459, 16735, 13091, 13631, 10329, 11253, 10691, 10261, 13012, 11860, 11253, 10691, 10305, 17734, 14734, 10342, 10846, 45, 11454, 15847, 10293, 11871, 10323, 10285, 10250, 14781, 12656, 14035, 14808, 17385, 16503, 16530, 15898, 28329, 11687, 13258, 17177, 12731, 11454, 11920, 15847, 11871, 45, 33, 13631, 10329, 11253, 10691, 11021, 15543, 17108, 11044, 10322, 11021, 10399, 12746, 15511, 12140, 11638, 14734, 14862, 16696, 10358, 10760, 10080, 65535, 261]
-> item_loss 1.573062539100647 batch_loss 1.1705234097499473
==== [25106, 11, 65530, 65531, 18043, 17573, 13091, 10250, 10283, 820, 10847, 12636, 19137, 11021, 10399, 15847, 14221, 11459, 11124, 10370, 15261, 17149, 16403, 11957, 16713, 19137, 15494, 10370, 15261, 12848, 10760, 12261, 16676, 19137, 12176, 10847, 10370, 15261, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 18043, 17573, 11, 65535, 261, 65530, 65534, 18043, 17573, 59, 33, 12605, 10250, 14781, 17311, 11454, 19137, 10293, 10370, 13191, 10373, 10303, 10336, 19156, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 18043, 17573, 19137, 11124, 12605, 12266, 10283, 14442, 15149, 11, 65535, 261, 65530, 65534, 18043, 17573, 59, 33, 13191, 10370, 11454, 11689, 12509, 10250, 11022, 13212, 11369, 11124, 15847, 12144, 12266, 14442, 15149, 19137, 12605, 10260, 16735, 13091, 16747, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11223, 17148, 11, 65535, 261, 65530, 65534, 18043, 17573, 59, 33, 11354, 11354, 19137, 10293, 10370, 19137, 16738, 17728, 17148, 10283, 14442, 15149, 10464, 14015, 12522, 10303, 19156, 11, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 17147, 10260, 17631, 11061, 28329, 65535, 261, 65530, 65534, 18043, 17573, 59, 33, 12605, 10423, 15507, 15511, 10848, 10838, 14734, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 11975, 18043, 17573, 28329, 65535, 261, 65530, 65534, 18043, 17573, 59, 33, 11454, 14734, 11454, 14734, 19137, 15752, 12791, 15033, 11017, 13764, 10303, 19137, 10293, 10370, 19133, 10687, 11903, 12605, 11016, 13091, 13191, 10349, 11917, 15600, 19134, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 18030, 19137, 10464, 14734, 12509, 13764, 10675, 17311, 13156, 17900, 11454, 12732, 11028, 17384, 10333, 28329, 65535, 261, 65530, 65534, 18043, 17573, 59, 33, 11416, 10292, 12605, 12202, 10260, 13091, 14794, 14734, 10260, 11268, 13558, 17268, 15033, 11017, 13764, 19137, 10260, 12159, 13197, 10293, 10370, 10358, 14583, 16730, 16533, 19137, 12631, 10399, 12605, 12669, 12509, 13764, 10708, 11454, 10333, 12732, 11028, 17384, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 12605, 12509, 15641, 10464, 10250, 10283, 13637, 17097, 13191, 12752, 12610, 12415, 14734, 17728, 18025, 28329, 65535, 261, 65530, 65534, 18043, 17573, 59, 33, 11685, 14734, 19137, 12605, 10423, 10848, 10838, 11414, 15178, 14734, 28329, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 10580, 16689, 10464, 13633, 13557, 15752, 16939, 14734, 16973, 15023, 10292, 50, 19137, 13188, 12266, 11726, 10464, 17919, 11047, 10808, 13036, 19137, 10464, 10495, 13557, 11047, 10808, 16939, 54, 13582, 10079, 12138, 17083, 10079, 16939, 10256, 13582, 10079, 11047, 11044, 17083, 10079, 11047, 11025, 17083, 10079, 16939, 10250, 13582, 19137, 10464, 14446, 11454, 11454, 10373, 10303, 10447, 15586, 19156, 11, 65535, 261, 65530, 65534, 18043, 17573, 59, 33, 17148, 10283, 17728, 18025, 12605, 15752, 11414, 15178, 19126, 10580, 16689, 11471, 13308, 15304, 2314, 122, 19137, 121, 17090, 13580, 13036, 11047, 10292, 10808, 13036, 19137, 122, 17090, 13580, 13036, 11047, 10292, 11025, 10501, 19137, 12605, 13188, 12266, 11726, 11454, 41, 49, 45, 49, 42, 28329, 41, 50, 42, 33, 18117, 10658, 12605, 11047, 10808, 16939, 54, 13582, 19137, 10423, 10792, 17136, 41, 54, 45, 49, 42, 28329, 41, 51, 42, 33, 14221, 11044, 12605, 12138, 17083, 19137, 17148, 13064, 10560, 17919, 11047, 122, 17090, 16875, 13036, 11047, 19137, 16939, 52, 13582, 19137, 12605, 10423, 10792, 17136, 41, 54, 553, 52, 42, 28329, 41, 52, 42, 33, 14221, 11044, 12605, 11047, 11044, 17083, 19137, 17919, 11047, 122, 17090, 13580, 13036, 11047, 28329, 41, 53, 42, 33, 11047, 11025, 17083, 19137, 17919, 11047, 121, 17090, 16875, 13036, 11047, 19137, 14221, 11044, 16939, 50, 13582, 19137, 12605, 10423, 10792, 17136, 41, 53, 553, 52, 42, 262, 10293, 10370, 19137, 10322, 11994, 13091, 16735, 19137, 12605, 13188, 11044, 11454, 10782, 11726, 10447, 15586, 14734, 10808, 13036, 53, 13582, 10079, 12138, 10501, 52, 13582, 14734, 10447, 15586, 19126, 65535, 261, 65530, 65532, 10293, 10370, 59, 33, 15178, 11957, 10333, 28329, 65535, 261, 65530, 65534, 18043, 17573, 59, 33, 12337, 11685, 19137, 12605, 12337, 12266, 12366, 15752, 12176, 10257, 10293, 10370, 14734, 12379, 19126, 11, 65535, 261]
-> item_loss 0.9422580003738403 batch_loss 1.1661336903388684
127.0.0.1 - - [30/Sep/2023 21:53:44] "POST /train/sft HTTP/1.1" 200 28
None
None
192.168.0.103 - - [30/Sep/2023 21:55:32] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [30/Sep/2023 21:55:47] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [30/Sep/2023 21:56:04] "POST /state/reset HTTP/1.1" 200 21
user: 
192.168.0.103 - - [30/Sep/2023 21:56:06] "POST /inference/generate HTTP/1.1" 200 1132
None
None
192.168.0.103 - - [30/Sep/2023 21:56:22] "POST /state/reset HTTP/1.1" 200 21
user: 
192.168.0.103 - - [30/Sep/2023 21:56:25] "POST /inference/generate HTTP/1.1" 200 2370
None
None
192.168.0.103 - - [30/Sep/2023 21:57:25] "POST /state/reset HTTP/1.1" 200 21
master: 
192.168.0.103 - - [30/Sep/2023 21:57:43] "POST /inference/generate HTTP/1.1" 200 11128
None
None
192.168.0.103 - - [30/Sep/2023 21:57:51] "POST /state/reset HTTP/1.1" 200 21
assistant 
192.168.0.103 - - [30/Sep/2023 21:58:12] "POST /inference/generate HTTP/1.1" 200 11940
[2023-09-30 21:59:09,012] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1327196
[2023-09-30 21:59:09,013] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 22:01:01,968] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 22:01:01,988] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 22:01:04,527] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 22:01:04,528] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 22:01:04,528] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 22:01:04,528] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 22:01:04,528] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.660171270370483 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 22:01:55,577] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 22:01:55,578] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 22:01:59,174] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 22:01:59,176] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 22:01:59,176] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 22:01:59,216] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 22:01:59,216] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 22:01:59,216] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 22:01:59,217] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 22:01:59,217] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 22:01:59,217] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 22:01:59,217] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.30969977378845215 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-09-30 22:02:23,517] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 22:02:23,518] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 22:02:23,518] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.4 GB, percent = 17.8%
[2023-09-30 22:02:45,961] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 22:02:45,962] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 22:02:45,962] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.62 GB, percent = 49.0%
[2023-09-30 22:02:45,962] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 22:02:46,838] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 22:02:46,838] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 22:02:46,839] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.62 GB, percent = 49.0%
[2023-09-30 22:02:46,862] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 22:02:46,862] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 22:02:46,862] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fad3a6e1160>
[2023-09-30 22:02:46,862] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 22:02:46,863] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 22:02:46,864] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 22:02:46,864] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 22:02:46,864] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 22:02:46,864] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 22:02:46,864] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 22:02:46,864] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 22:02:46,864] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 22:02:46,864] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 22:02:46,864] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 22:02:46,864] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fad3a6c3190>
[2023-09-30 22:02:46,864] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 22:02:46,864] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 22:02:46,864] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 22:02:46,864] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 22:02:46,864] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 22:02:46,864] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 22:02:46,864] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 22:02:46,864] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 22:02:46,864] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 22:02:46,864] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 22:02:46,865] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 22:02:46,866] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 22:02:46,866] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 22:02:46,866] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 22:02:46,866] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 22:02:46,866] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 22:02:46,866] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 22:02:46,866] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 22:02:46,866] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 22:02:46,866] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 22:02:46,866] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 22:02:46,866] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 22:02:46,866] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 22:02:46,866] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 22:02:46,866] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 22:02:46,866] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 22:02:46,866] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 22:02:46,866] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 22:02:46,866] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 22:02:46,866] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 22:02:46,866] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 22:02:46,866] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 22:02:46,866] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 22:02:46,866] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 22:02:46,867] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0004100799560546875 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

None
None
192.168.0.103 - - [30/Sep/2023 23:31:25] "POST /state/reset HTTP/1.1" 200 21
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 21911.61it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 540421.72it/s]
Answer:
"More information on the primary subject, including a link to the full text."192.168.0.103 - - [30/Sep/2023 23:32:01] "POST /inference/generate HTTP/1.1" 200 7581
None
None
192.168.0.103 - - [30/Sep/2023 23:33:46] "POST /state/reset HTTP/1.1" 200 21
The above text is a [Markdown] formatted description of the given text.192.168.0.103 - - [30/Sep/2023 23:34:20] "POST /inference/generate HTTP/1.1" 200 7438
None
None
192.168.0.103 - - [30/Sep/2023 23:36:38] "POST /state/reset HTTP/1.1" 200 21
This is the HTML text version of the given HTML.192.168.0.103 - - [30/Sep/2023 23:37:16] "POST /inference/generate HTTP/1.1" 200 8651
None
None
192.168.0.103 - - [30/Sep/2023 23:37:41] "POST /state/reset HTTP/1.1" 200 21
Traceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/app.py", line 184, in train_tx_data
    data_iter = train_data.yield_tokens(ctx_len=ctx_len, window=window)
NameError: name 'window' is not defined
192.168.0.103 - - [30/Sep/2023 23:37:42] "POST /train/tx-data HTTP/1.1" 500 756
[2023-09-30 23:38:22,981] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1338242
[2023-09-30 23:38:22,982] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-09-30 23:39:38,976] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-09-30 23:39:38,996] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-09-30 23:39:41,531] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-09-30 23:39:41,531] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-09-30 23:39:41,531] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-09-30 23:39:41,531] [INFO] [launch.py:247:main] dist_world_size=1
[2023-09-30 23:39:41,531] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.67395544052124 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-09-30 23:40:32,235] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-09-30 23:40:32,236] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-30 23:40:34,876] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-30 23:40:34,878] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-30 23:40:34,878] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-30 23:40:34,917] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-09-30 23:40:34,917] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-09-30 23:40:34,917] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-09-30 23:40:34,917] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-09-30 23:40:34,917] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-09-30 23:40:34,918] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-09-30 23:40:34,918] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.28934764862060547 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-09-30 23:40:58,805] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-30 23:40:58,806] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 23:40:58,806] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.39 GB, percent = 17.8%
[2023-09-30 23:41:22,798] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-30 23:41:22,799] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 23:41:22,799] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.61 GB, percent = 49.0%
[2023-09-30 23:41:22,799] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-09-30 23:41:24,896] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-30 23:41:24,897] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-09-30 23:41:24,897] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.61 GB, percent = 49.0%
[2023-09-30 23:41:24,924] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-09-30 23:41:24,924] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-30 23:41:24,924] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fe6a0b79160>
[2023-09-30 23:41:24,924] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-30 23:41:24,925] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-09-30 23:41:24,926] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-30 23:41:24,926] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-30 23:41:24,926] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-09-30 23:41:24,926] [INFO] [config.py:957:print]   amp_params ................... False
[2023-09-30 23:41:24,926] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-30 23:41:24,926] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-09-30 23:41:24,926] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-09-30 23:41:24,926] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-09-30 23:41:24,926] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-09-30 23:41:24,926] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe6a0afd190>
[2023-09-30 23:41:24,926] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-09-30 23:41:24,926] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-30 23:41:24,926] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   dump_state ................... False
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-09-30 23:41:24,927] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-09-30 23:41:24,928] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-30 23:41:24,928] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-09-30 23:41:24,928] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-09-30 23:41:24,928] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-09-30 23:41:24,928] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-09-30 23:41:24,928] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-30 23:41:24,928] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-30 23:41:24,928] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-09-30 23:41:24,928] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-09-30 23:41:24,928] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-09-30 23:41:24,928] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-30 23:41:24,928] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-09-30 23:41:24,928] [INFO] [config.py:957:print]   pld_params ................... False
[2023-09-30 23:41:24,928] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-09-30 23:41:24,928] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-09-30 23:41:24,928] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-09-30 23:41:24,928] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-09-30 23:41:24,928] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-09-30 23:41:24,928] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-09-30 23:41:24,928] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-09-30 23:41:24,928] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-09-30 23:41:24,928] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-09-30 23:41:24,928] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-09-30 23:41:24,929] [INFO] [config.py:957:print]   world_size ................... 1
[2023-09-30 23:41:24,929] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-09-30 23:41:24,929] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-09-30 23:41:24,929] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-09-30 23:41:24,929] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-30 23:41:24,929] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-09-30 23:41:24,929] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0005075931549072266 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

None
None
192.168.0.103 - - [30/Sep/2023 23:42:34] "POST /state/reset HTTP/1.1" 200 21
[2023-09-30 23:42:36,977] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2023-09-30 23:42:36,978] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-09-30 23:42:36,978] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-09-30 23:42:36,978] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2023-09-30 23:42:36,978] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
192.168.0.103 - - [30/Sep/2023 23:42:52] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [30/Sep/2023 23:43:38] "POST /state/reset HTTP/1.1" 200 21
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 31867.53it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 473721.12it/s]
This is the HTML code for a webpage that displays the content of the specified URL:
192.168.0.103 - - [30/Sep/2023 23:43:43] "POST /inference/generate HTTP/1.1" 200 2182
None
None
192.168.0.103 - - [30/Sep/2023 23:44:24] "POST /state/reset HTTP/1.1" 200 21
This is the HTML code for a webpage that displays the content of the specified URL:
<|over|>

<|me|><|request|>It contains the following elements:
1. A header section, which contains metadata about the webpage, such as its title and author.
2. A body section, which contains the content of the webpage.
3. A footer section, which contains links to the webpage and information about who wrote it.
4. A footer section, which contains links to the page's GitHub repository and Twitter account if it exists.<|over|>

<|me|><|response|>Title: Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning
 Author: John Doe

<|over|>

<|me|><|response|>Title: Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning
Author: John Doe

<|over|>

<|me|><|request|>In this task, you are given a sentence in Persian. You need to translate the Farsi sentence into English.
<|over|>

<|over|>

<|over|>

<|me|><|response|>                   Google Translate                            .<|over|>

<|me|><|request|>Title: Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning
<|over|>

<|me|><|response|>Title: Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning
<|over|>

<|me|><|request|>Sentence: The story is about a man named John who was an avid gamer and loved playing games on his computer. He had a friend named Sarah who enjoyed reading books but didn't play games on her computer. One day, John invited Sarah over to his house so they could play video games together. They played some games on John's computer and then he showed her how to play some video games on his PlayStation 4. They had so much fun playing together that they decided to write a story about it. They wrote the story in Japanese and then translated it into English so that John's daughter, Lily, could read it too. They were thrilled with how well their story turned out and couldn't wait to share it with their friends.<|over|>

<|over|>

<|me|><|response|>The story is about a man named John who invited his friend, Sarah, over to his house so they could play some video games together. They played some games on John's computer and then he showed her how to play some video games on PlayStation 4. They were so much fun playing together that John decided to write a story about it. They called it "Pushing Play" and they showed it to their friends who loved reading books but didn't play video games well. They were thrilled with how well their story turned out and couldn't wait to share it with their friends.<|over|>

<|me|><|request|>Title: Pushing Play
<|over|>

<|me|><|response|>The story is about a man named John who invited his friend, Sarah, over to his house so they could play some video games together. They played some video games together and enjoyed reading books but didn't play any video games at all. It was so much fun playing together that John promised her that they would read more books together in the future. The story is about friendship and enjoying time with friends. It is a great example of how you can use translation technology like Google Translate to create something amazing!<|over|>

<|me|><|request|>Title: Pushing Play
<|over|>

<|me|><|response|>The story is about two friends, John and Sarah, who enjoy playing video games together on their computer. One day, John invites Sarah over to his house so they can play some video games together. They enjoy playing the game so much that they write a story about it in Japanese and then translate it into English so that John's daughter, Lily, can read it too. They were thrilled with how well their story turned out!<|over|>

<|me|><|request|>Title: Pushing Play
<|over|>

<|me|><|response|>The story is about two friends, John and Sarah, who enjoy playing video games together on their computer. They had a friend named Lily who enjoyed reading books but didn't play any games on her computer. One day, John invited Google Translate over to his computer and showed her how to use it. They played some games on John's computer and liked the way the game looked in Japanese. Later, John translated the story into Japanese and showed it to his daughter, Lily, who loved reading books but didn't play any video games at all. She was so excited when she saw what they had done with the story! This is an example of192.168.0.103 - - [30/Sep/2023 23:45:07] "POST /inference/generate HTTP/1.1" 200 7531
None
None
192.168.0.103 - - [01/Oct/2023 00:10:24] "POST /state/reset HTTP/1.1" 200 21
This is a basic HTML page that displays some text and an image. It uses the `meta` tag to specify additional information about the page, such as its title and author.<|over|>

<|me|><|request|>Given this HTML code:
```html
<div role="presentation">
  <h1>Hello World!</h1>
  <p>This is a paragraph.</p>
</div<|over|>

<|me|><|response|>Output:
```<|over|>

<|over|>

<|me|><|response|>The output should be:
```html<!doctype html><html lang="en"><head><meta charset="utf-8"><title>Hello World!</title></head><body><div role="presentation"><img src="https://www.w3schools.com/css/monster_logo_header.png"></div><div role="presentation"><h1>Hello World!</h1></div></body></html>"<|over|>

<|me|><|request|>Given this HTML code:
```<|over|>

<|me|><|response|>This is a paragraph
<|over|>

<|me|><|system|>And this is its HTML representation:
<!doctype html><svg xmlns="http://www.w3.org/2000/svg">
<svg xmlns="http://www.w3.org/2000/svg" height="390" viewBox="0 0 483 514" width="483">
  <path fill="#000000" stroke="#000000" stroke-width="2" d="M255,109 H157,109 H196,109 H196,109 z M255,109 H157,109 H196,109 H196,109 z M255,109 H157,109 H196,109 H196,109 z M255,109 H157,109 H196,109 H196,109 z M255,110 H157,110 H196,110 H196,110 z M255,110 H157,-4.998 V-24.998 h128 v-24.998 h128 vz" />
  <path fill="#000000" stroke="#000000" stroke-width="4" d="M255,-51.514c-1.401,-1.401 -2.428,-4.998 -2.428,-7.196v24c0,-16.797 12.804,-24 24,-24 24v-24 c0,-16.797 12.804,-24 24,-24 24v-24 c0,-16.797 12.804,-24 24,-24 24v-128h128 vz M255,110c-1.401,0 -2.428,2.428 -2.428,4V110 c0,1 2.428 1 4 2s4 -2 2 -4 0v-24 c0 -6 0 -12 8 -12h128v-24z M255,110c-1.401,0 -2.428,2 -2,4V110 c0 1 2 2 2 2s2 -2 2 -2v-4 c0 -6 0 -12 8 -12h128v24 z M255,110c-1.401,0 -2.428,2 -2,4V110 c0 1 2 2 4 8 0 8 0 16 0 16 0 16 0 16 0 z M255,-51.514c-1.401,-1 4 3 4s3 3 4s3 3 4s3 6s3 6s3 6s3 6s3 6s3 6s3 6s3 6s3 6s3 6s3 6s3 6s3 6s3 6 s 3 6 s 3 6 s 3 7 1 12 1 19 1 32 1 48 1 64 1 80 2 96 5 125 10 115 15 135 25 209 37 296 51 409 51 409 51 409 51 409 51 409 51 409 51 409 51 409 51 409 51 409 51 409 51 409 51 409 51 409 51 409 51 416 52 416 52 416 52 416 52 416 52 416 52 416 52 416 52 416 52 416 52 416 52 420 53 420 53 420 53 420 53 420 53 420 53 420 53 420 53 420 53 432 54 432 54 432 54 432 54 432 54 432 54 432 54 432 54 432 54 432 54 444 55 444 55 444192.168.0.103 - - [01/Oct/2023 00:11:26] "POST /inference/generate HTTP/1.1" 200 10749
None
None
192.168.0.103 - - [01/Oct/2023 00:25:02] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 00:25:40] "POST /train/tx-data HTTP/1.1" 200 27
None
None
192.168.0.103 - - [01/Oct/2023 00:26:03] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 00:26:38] "POST /train/tx-data HTTP/1.1" 200 27
None
None
192.168.0.103 - - [01/Oct/2023 00:26:58] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 00:27:39] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 00:28:38] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 00:29:16] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 00:29:43] "POST /state/reset HTTP/1.1" 200 21
[2023-10-01 00:30:02,749] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
192.168.0.103 - - [01/Oct/2023 00:30:14] "POST /train/tx-data HTTP/1.1" 200 27
None
None
192.168.0.103 - - [01/Oct/2023 00:30:22] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 00:31:02] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 00:31:17] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 00:31:58] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 00:32:17] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 00:32:57] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 00:33:23] "POST /state/reset HTTP/1.1" 200 21
,,
,,:
1. 
2. ,
3. ,
4. ,
,,,
,<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>

A. 
B. 
C. 
D. 

A. ;
B. ;
C. ;
D. 
B. 
,<|over|>

<|me|><|request|>,,?
A. ,
B. ,
C. ,
D. ,
,
<|over|>

<|me|><|response|>,,:
,192.168.0.103 - - [01/Oct/2023 00:36:42] "POST /inference/generate HTTP/1.1" 200 78239
None
None
127.0.0.1 - - [01/Oct/2023 00:39:03] "POST /state/reset HTTP/1.1" 200 21
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 22028.07it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 476495.20it/s]
127.0.0.1 - - [01/Oct/2023 00:39:13] "POST /inference/load-model HTTP/1.1" 200 26
None
None
192.168.0.103 - - [01/Oct/2023 00:39:29] "POST /state/reset HTTP/1.1" 200 21
AI
<|over|>

<|me|><|response|>(, )
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>(, )
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>(, )
<|over|>

<|me|><|request|>
<|over|>

<|me|><|system|>(, )
<|over|>

<|me|><|response|>(, )
<|over|>

<|me|><|request|>
<|over|>

<|me|><|request|>AI
<|over|>

<|me|><|response|>(, )
<|over|>

<|me|><|request|>
<|over|>

<|me|><|request|>AI
<|over|>

<|me|><|request|>(, )
<|over|>

<|me|><|response|>(, )
<|over|>

<|me|><|response|>(, )
<|over|>

<|me|><|request|>
<|over|>

<|me|><|system|>(, )
<|over|>

<|me|><|response|>(, )
<|over|>

<|me|><|request|>
<|over|>

<|me|><|request|>AI
<|over|>

<|me|><|response|>(, )
<|over|>

<|me|><|request|>
<|over|>

<|me|><|system|>(, )
<|over|>

<|me|><|response|>(, )
<|over|>

<|me|><|request|>
<|over|>

<|me|><|request|>AI
<|over|>

<|me|><|request|>(, )
<|over|>

<|me|><|response|>(, )
<|over|>

<|me|><|request|>
<|over|>

<|me|><|system|>(, )
<|over|>

<|me|><|response|>(, )
<|over|>

<|me|><|request|>
<|over|>

<|me|><|request|>AI
<|over|>

<|me|><|response|>(, )
<|over|>

<|me|><|request|>
<|over|>

<|me|><|system|>(, )
<|over|>

<|me|><|response|>(, )
<|over|>

<|me|>192.168.0.103 - - [01/Oct/2023 00:40:07] "POST /inference/generate HTTP/1.1" 200 6977
None
None
192.168.0.103 - - [01/Oct/2023 00:42:10] "POST /state/reset HTTP/1.1" 200 21
AI<|over|>

<|me|><|request|>500ml
<|over|>

<|me|>192.168.0.103 - - [01/Oct/2023 00:42:15] "POST /inference/generate HTTP/1.1" 200 1665
None
None
192.168.0.103 - - [01/Oct/2023 00:42:46] "POST /state/reset HTTP/1.1" 200 21
AI<|over|>

<|me|><|request|>
<|over|>

<|me|>192.168.0.103 - - [01/Oct/2023 00:42:50] "POST /inference/generate HTTP/1.1" 200 1671
None
None
192.168.0.103 - - [01/Oct/2023 01:22:33] "POST /state/reset HTTP/1.1" 200 21
 
<|over|>

<|me|>192.168.0.103 - - [01/Oct/2023 01:22:39] "POST /inference/generate HTTP/1.1" 200 3213
None
None
192.168.0.103 - - [01/Oct/2023 01:24:24] "POST /state/reset HTTP/1.1" 200 21
 
192.168.0.103 - - [01/Oct/2023 01:24:30] "POST /inference/generate HTTP/1.1" 200 3356
None
None
192.168.0.103 - - [01/Oct/2023 01:24:45] "POST /state/reset HTTP/1.1" 200 21
 
192.168.0.103 - - [01/Oct/2023 01:24:52] "POST /inference/generate HTTP/1.1" 200 4202
None
None
192.168.0.103 - - [01/Oct/2023 01:26:09] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 01:26:15] "POST /inference/generate HTTP/1.1" 200 3276
None
None
192.168.0.103 - - [01/Oct/2023 01:26:54] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 01:27:00] "POST /inference/generate HTTP/1.1" 200 3276
None
None
192.168.0.103 - - [01/Oct/2023 01:27:23] "POST /state/reset HTTP/1.1" 200 21
  
192.168.0.103 - - [01/Oct/2023 01:27:30] "POST /inference/generate HTTP/1.1" 200 3574
None
None
192.168.0.103 - - [01/Oct/2023 01:28:03] "POST /state/reset HTTP/1.1" 200 21
Traceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/app.py", line 148, in inference_generate_no_state
    msg = inferencer.generate_no_state(model, msg)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/models/inference_helper.py", line 168, in generate_no_state
    logits = model.inference(all_tokens[-ctx_len:])
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 816, in inference
    x = block(x)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 518, in forward
    x = x + self.att(self.ln1(x))
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 422, in forward
    sr, k, v = self.jit_func(x)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/rwkv_model/model_lora.py", line 410, in jit_func
    @MyFunction
    def jit_func(self, x):
        xx = self.time_shift(x) # Mix x with the previous timestep to produce xk, xv, xr
             ~~~~~~~~~~~~~~~ <--- HERE
        xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)
        xv = x * self.time_mix_v + xx * (1 - self.time_mix_v)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/padding.py", line 25, in forward
    def forward(self, input: Tensor) -> Tensor:
        return F.pad(input, self.padding, 'constant', self.value)
               ~~~~~ <--- HERE
RuntimeError: narrow(): length must be non-negative.

192.168.0.103 - - [01/Oct/2023 01:28:05] "POST /inference/generate-no-state HTTP/1.1" 500 772
None
None
192.168.0.103 - - [01/Oct/2023 01:30:02] "POST /state/reset HTTP/1.1" 200 21
  
192.168.0.103 - - [01/Oct/2023 01:30:09] "POST /inference/generate HTTP/1.1" 200 3574
None
None
192.168.0.103 - - [01/Oct/2023 01:37:09] "POST /state/reset HTTP/1.1" 200 21

1. 
2. 
3. 
4. 
5. 
6. 
192.168.0.103 - - [01/Oct/2023 01:37:20] "POST /inference/generate HTTP/1.1" 200 2703
None
None
192.168.0.103 - - [01/Oct/2023 01:37:53] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 01:38:07] "POST /inference/generate HTTP/1.1" 200 5952
None
None
192.168.0.103 - - [01/Oct/2023 01:38:11] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 01:38:28] "POST /inference/generate HTTP/1.1" 200 7273
[2023-10-01 01:43:02,727] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1364839
[2023-10-01 01:43:02,729] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-01 01:43:07,949] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-01 01:43:07,969] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-01 01:43:10,507] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-10-01 01:43:10,507] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-01 01:43:10,507] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-01 01:43:10,507] [INFO] [launch.py:247:main] dist_world_size=1
[2023-10-01 01:43:10,507] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.6866888999938965 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-01 01:44:01,271] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-10-01 01:44:01,271] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-01 01:44:05,000] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-01 01:44:05,002] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-01 01:44:05,002] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-01 01:44:05,041] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-01 01:44:05,041] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-01 01:44:05,041] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-10-01 01:44:05,042] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-10-01 01:44:05,042] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-10-01 01:44:05,042] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-10-01 01:44:05,042] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.3086283206939697 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-10-01 01:44:38,909] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-10-01 01:44:38,910] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-10-01 01:44:38,910] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.38 GB, percent = 17.8%
[2023-10-01 01:45:00,174] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-10-01 01:45:00,175] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-10-01 01:45:00,175] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.62 GB, percent = 49.0%
[2023-10-01 01:45:00,175] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-10-01 01:45:01,536] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-10-01 01:45:01,537] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-10-01 01:45:01,537] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.62 GB, percent = 49.0%
[2023-10-01 01:45:01,566] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-01 01:45:01,567] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-01 01:45:01,567] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fbdb181e130>
[2023-10-01 01:45:01,567] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-01 01:45:01,568] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-10-01 01:45:01,568] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-01 01:45:01,568] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-01 01:45:01,569] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-10-01 01:45:01,569] [INFO] [config.py:957:print]   amp_params ................... False
[2023-10-01 01:45:01,569] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-01 01:45:01,569] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-10-01 01:45:01,569] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-10-01 01:45:01,569] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-10-01 01:45:01,569] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-10-01 01:45:01,569] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fbdb183e160>
[2023-10-01 01:45:01,569] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-10-01 01:45:01,569] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-01 01:45:01,569] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-10-01 01:45:01,569] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-10-01 01:45:01,569] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-01 01:45:01,569] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-10-01 01:45:01,569] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-10-01 01:45:01,569] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-10-01 01:45:01,570] [INFO] [config.py:957:print]   dump_state ................... False
[2023-10-01 01:45:01,570] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-10-01 01:45:01,570] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-10-01 01:45:01,570] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-01 01:45:01,570] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-01 01:45:01,570] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-10-01 01:45:01,570] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-10-01 01:45:01,570] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-10-01 01:45:01,570] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-10-01 01:45:01,570] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-10-01 01:45:01,570] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-10-01 01:45:01,570] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-01 01:45:01,570] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-10-01 01:45:01,570] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-10-01 01:45:01,570] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-10-01 01:45:01,570] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-10-01 01:45:01,570] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-10-01 01:45:01,570] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-10-01 01:45:01,570] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-10-01 01:45:01,570] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-10-01 01:45:01,570] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-01 01:45:01,570] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-10-01 01:45:01,570] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-10-01 01:45:01,571] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-10-01 01:45:01,571] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-10-01 01:45:01,571] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-01 01:45:01,571] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-01 01:45:01,571] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-10-01 01:45:01,571] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-10-01 01:45:01,571] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-10-01 01:45:01,571] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-01 01:45:01,571] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-10-01 01:45:01,571] [INFO] [config.py:957:print]   pld_params ................... False
[2023-10-01 01:45:01,571] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-10-01 01:45:01,571] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-10-01 01:45:01,571] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-10-01 01:45:01,571] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-10-01 01:45:01,571] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-10-01 01:45:01,571] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-10-01 01:45:01,571] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-10-01 01:45:01,571] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-10-01 01:45:01,571] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-10-01 01:45:01,571] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-10-01 01:45:01,571] [INFO] [config.py:957:print]   world_size ................... 1
[2023-10-01 01:45:01,571] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-10-01 01:45:01,572] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-10-01 01:45:01,572] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-10-01 01:45:01,572] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-01 01:45:01,572] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-10-01 01:45:01,572] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0006647109985351562 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

None
None
192.168.0.103 - - [01/Oct/2023 02:41:03] "POST /state/reset HTTP/1.1" 200 21
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 14570.16it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 509621.07it/s]
assistant 
192.168.0.103 - - [01/Oct/2023 02:41:28] "POST /inference/generate HTTP/1.1" 200 11855
None
None
192.168.0.103 - - [01/Oct/2023 02:42:38] "POST /state/reset HTTP/1.1" 200 21
assistant 
192.168.0.103 - - [01/Oct/2023 02:43:05] "POST /inference/generate HTTP/1.1" 200 11952
None
None
192.168.0.103 - - [01/Oct/2023 02:43:57] "POST /state/reset HTTP/1.1" 200 21
master: 
192.168.0.103 - - [01/Oct/2023 02:44:17] "POST /inference/generate HTTP/1.1" 200 12715
None
None
192.168.0.103 - - [01/Oct/2023 02:44:28] "POST /state/reset HTTP/1.1" 200 21
assistant 
192.168.0.103 - - [01/Oct/2023 02:44:50] "POST /inference/generate HTTP/1.1" 200 13443
None
None
192.168.0.103 - - [01/Oct/2023 02:47:55] "POST /state/reset HTTP/1.1" 200 21
master: 
192.168.0.103 - - [01/Oct/2023 02:48:19] "POST /inference/generate HTTP/1.1" 200 14228
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
[2023-10-01 02:50:12,873] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2023-10-01 02:50:12,873] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-10-01 02:50:12,873] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-10-01 02:50:12,873] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2023-10-01 02:50:12,873] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
-> item_loss 0.9122768044471741 batch_loss 0.9122768044471741
-> item_loss 0.6331262588500977 batch_loss 0.7727015316486359
-> item_loss 0.6651778817176819 batch_loss 0.7368603150049845
-> item_loss 0.6603036522865295 batch_loss 0.7177211493253708
-> item_loss 0.642480194568634 batch_loss 0.7026729583740234
-> item_loss 0.4741637408733368 batch_loss 0.6645880887905756
-> item_loss 0.42836135625839233 batch_loss 0.6308414127145495
-> item_loss 0.7498512268066406 batch_loss 0.6457176394760609
-> item_loss 0.261830598115921 batch_loss 0.6030635237693787
[2023-10-01 02:53:02,621] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.6274983286857605 batch_loss 0.6055070042610169
-> item_loss 0.5121790170669556 batch_loss 0.5970226417888295
-> item_loss 2.3883907794952393 batch_loss 0.7463033199310303
-> item_loss 1.8720495700836182 batch_loss 0.8328991853273832
-> item_loss 0.5599229335784912 batch_loss 0.8134008816310337
-> item_loss 0.7163509130477905 batch_loss 0.8069308837254842
-> item_loss 1.6327835321426392 batch_loss 0.8585466742515564
-> item_loss 2.119508981704712 batch_loss 0.9327209276311538
-> item_loss 0.6375070810317993 batch_loss 0.9163201583756341
-> item_loss 0.8039882779121399 batch_loss 0.9104079541407133
[2023-10-01 02:55:30,551] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.0613666772842407 batch_loss 0.9179558902978897
-> item_loss 1.0511895418167114 batch_loss 0.9243003498940241
-> item_loss 1.886696219444275 batch_loss 0.9680456166917627
-> item_loss 2.0214831829071045 batch_loss 1.0138472500054732
-> item_loss 0.4905868470668793 batch_loss 0.9920447332163652
-> item_loss 1.4892385005950928 batch_loss 1.0119324839115142
-> item_loss 1.3719044923782349 batch_loss 1.0257775611602342
-> item_loss 0.5782661437988281 batch_loss 1.0092030642209229
-> item_loss 0.4267822802066803 batch_loss 0.9884023219347
-> item_loss 0.7171449065208435 batch_loss 0.9790486179549118
[2023-10-01 02:58:10,480] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.7901628017425537 batch_loss 0.9727524240811666
-> item_loss 1.8129767179489136 batch_loss 0.9998564335607714
-> item_loss 0.9938892126083374 batch_loss 0.9996699579060078
-> item_loss 1.903519630432129 batch_loss 1.027059341921951
-> item_loss 0.7561854124069214 batch_loss 1.019092461642097
-> item_loss 0.7096765041351318 batch_loss 1.0102520057133266
-> item_loss 1.5765742063522339 batch_loss 1.0259831779532962
-> item_loss 1.1222176551818848 batch_loss 1.028584109770285
-> item_loss 1.7526620626449585 batch_loss 1.0476387927406712
-> item_loss 2.004868745803833 batch_loss 1.0721831505115216
[2023-10-01 03:00:52,961] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.4773914813995361 batch_loss 1.082313358783722
-> item_loss 0.4813649356365204 batch_loss 1.0676560801703756
-> item_loss 0.5943629145622253 batch_loss 1.0563871952749433
-> item_loss 0.8838926553726196 batch_loss 1.0523756943469824
-> item_loss 2.0020194053649902 batch_loss 1.073958505961028
-> item_loss 0.9304236769676208 batch_loss 1.070768843094508
-> item_loss 1.5504134893417358 batch_loss 1.0811959006216214
-> item_loss 1.8548386096954346 batch_loss 1.0976563837934048
-> item_loss 1.7912856340408325 batch_loss 1.1121069931735594
-> item_loss 0.6017951965332031 batch_loss 1.1016924667115113
[2023-10-01 03:03:00,957] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.6665604114532471 batch_loss 1.0929898256063462
-> item_loss 0.9630705118179321 batch_loss 1.0904423880810832
127.0.0.1 - - [01/Oct/2023 03:03:17] "POST /train/sft HTTP/1.1" 200 28
===saveved====
127.0.0.1 - - [01/Oct/2023 03:04:37] "POST /train/save-weight HTTP/1.1" 200 26
[2023-10-01 03:05:19,100] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1405031
[2023-10-01 03:05:19,101] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-01 03:05:37,955] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-01 03:05:37,975] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-01 03:05:40,450] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-10-01 03:05:40,450] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-01 03:05:40,450] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-01 03:05:40,450] [INFO] [launch.py:247:main] dist_world_size=1
[2023-10-01 03:05:40,450] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.626850605010986 seconds
[2023-10-01 03:06:36,527] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1428454
[2023-10-01 03:06:36,527] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -15
[2023-10-01 03:07:07,903] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-01 03:07:07,923] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-01 03:07:10,455] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-10-01 03:07:10,455] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-01 03:07:10,455] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-01 03:07:10,455] [INFO] [launch.py:247:main] dist_world_size=1
[2023-10-01 03:07:10,455] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.673945426940918 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-01 03:08:05,809] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-10-01 03:08:05,810] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-01 03:08:08,755] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-01 03:08:08,757] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-01 03:08:08,757] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-01 03:08:08,796] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-01 03:08:08,797] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-01 03:08:08,797] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-10-01 03:08:08,797] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-10-01 03:08:08,797] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-10-01 03:08:08,797] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-10-01 03:08:08,797] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.5294139385223389 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-10-01 03:08:35,167] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-10-01 03:08:35,168] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-10-01 03:08:35,168] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.5 GB, percent = 17.9%
[2023-10-01 03:08:55,830] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-10-01 03:08:55,831] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-10-01 03:08:55,832] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.72 GB, percent = 49.1%
[2023-10-01 03:08:55,832] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-10-01 03:08:57,946] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-10-01 03:08:57,947] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-10-01 03:08:57,947] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.73 GB, percent = 49.1%
[2023-10-01 03:08:57,973] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-01 03:08:57,973] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-01 03:08:57,973] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f8c627fba30>
[2023-10-01 03:08:57,973] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-01 03:08:57,975] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-10-01 03:08:57,975] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-01 03:08:57,975] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-01 03:08:57,975] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-10-01 03:08:57,975] [INFO] [config.py:957:print]   amp_params ................... False
[2023-10-01 03:08:57,975] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-01 03:08:57,975] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-10-01 03:08:57,975] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-10-01 03:08:57,976] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-10-01 03:08:57,976] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-10-01 03:08:57,976] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8c6282bb50>
[2023-10-01 03:08:57,976] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-10-01 03:08:57,976] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-01 03:08:57,976] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-10-01 03:08:57,976] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-10-01 03:08:57,976] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-01 03:08:57,976] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-10-01 03:08:57,976] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-10-01 03:08:57,976] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-10-01 03:08:57,976] [INFO] [config.py:957:print]   dump_state ................... False
[2023-10-01 03:08:57,976] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-10-01 03:08:57,976] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-10-01 03:08:57,976] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-01 03:08:57,976] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-01 03:08:57,976] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-10-01 03:08:57,976] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-10-01 03:08:57,976] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-10-01 03:08:57,976] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-10-01 03:08:57,976] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-10-01 03:08:57,976] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-10-01 03:08:57,976] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-01 03:08:57,977] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-10-01 03:08:57,977] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-10-01 03:08:57,977] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-10-01 03:08:57,977] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-10-01 03:08:57,977] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-10-01 03:08:57,977] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-10-01 03:08:57,977] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-10-01 03:08:57,977] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-10-01 03:08:57,977] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-01 03:08:57,977] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-10-01 03:08:57,977] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-10-01 03:08:57,977] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-10-01 03:08:57,977] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-10-01 03:08:57,977] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-01 03:08:57,977] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-01 03:08:57,977] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-10-01 03:08:57,977] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-10-01 03:08:57,977] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-10-01 03:08:57,977] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-01 03:08:57,977] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-10-01 03:08:57,977] [INFO] [config.py:957:print]   pld_params ................... False
[2023-10-01 03:08:57,978] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-10-01 03:08:57,978] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-10-01 03:08:57,978] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-10-01 03:08:57,978] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-10-01 03:08:57,978] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-10-01 03:08:57,978] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-10-01 03:08:57,978] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-10-01 03:08:57,978] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-10-01 03:08:57,978] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-10-01 03:08:57,978] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-10-01 03:08:57,978] [INFO] [config.py:957:print]   world_size ................... 1
[2023-10-01 03:08:57,978] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-10-01 03:08:57,978] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-10-01 03:08:57,978] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-10-01 03:08:57,978] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-01 03:08:57,978] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-10-01 03:08:57,978] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0005297660827636719 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
[2023-10-01 03:09:15,471] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2023-10-01 03:09:15,472] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-10-01 03:09:15,472] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-10-01 03:09:15,472] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2023-10-01 03:09:15,472] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
-> item_loss 0.7348158955574036 batch_loss 0.7348158955574036, real_loss 0.7348158955574036
-> item_loss 0.6746442914009094 batch_loss 0.7047300934791565, real_loss 0.6746442914009094
-> item_loss 1.373616099357605 batch_loss 0.9276920954386393, real_loss 1.7170201539993286
-> item_loss 0.6108068823814392 batch_loss 0.8484707921743393, real_loss 0.6108068823814392
-> item_loss 1.7661436796188354 batch_loss 1.0320053696632385, real_loss 2.207679510116577
-> item_loss 0.9605066180229187 batch_loss 1.0200889110565186, real_loss 0.9605066180229187
-> item_loss 1.7281142473220825 batch_loss 1.121235387665885, real_loss 2.1601428985595703
-> item_loss 0.7948115468025208 batch_loss 1.0804324075579643, real_loss 0.7948115468025208
-> item_loss 1.4497145414352417 batch_loss 1.1214637557665508, real_loss 1.8121432065963745
[2023-10-01 03:11:40,206] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.7286021709442139 batch_loss 1.082177597284317, real_loss 0.7286021709442139
-> item_loss 0.6217634081840515 batch_loss 1.0403217619115657, real_loss 0.6217634081840515
-> item_loss 1.0207319259643555 batch_loss 1.0386892755826314, real_loss 1.2759149074554443
-> item_loss 0.4352439343929291 batch_loss 0.9922704031834235, real_loss 0.054405491799116135
-> item_loss 0.4005560576915741 batch_loss 0.9500050927911486, real_loss 0.05006950721144676
-> item_loss 1.4219409227371216 batch_loss 0.9814674814542135, real_loss 1.7774261236190796
-> item_loss 0.36769619584083557 batch_loss 0.9431067761033773, real_loss 0.045962024480104446
-> item_loss 0.16681799292564392 batch_loss 0.8974427300340989, real_loss 0.02085224911570549
-> item_loss 0.500519871711731 batch_loss 0.8753914601273007, real_loss 0.500519871711731
-> item_loss 0.7760391235351562 batch_loss 0.8701623897803458, real_loss 0.7760391235351562
[2023-10-01 03:13:43,708] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.2322982549667358 batch_loss 0.8882691830396652, real_loss 1.5403728485107422
-> item_loss 1.1703954935073853 batch_loss 0.9017037692524138, real_loss 1.4629943370819092
-> item_loss 0.4107917547225952 batch_loss 0.8793895867737856, real_loss 0.0513489693403244
-> item_loss 0.4984278678894043 batch_loss 0.8628260337788126, real_loss 0.06230348348617554
-> item_loss 1.2264732122421265 batch_loss 0.8779779995481173, real_loss 1.5330915451049805
-> item_loss 0.8064908981323242 batch_loss 0.8751185154914856, real_loss 0.8064908981323242
-> item_loss 0.6132271885871887 batch_loss 0.8650457721490127, real_loss 0.6132271885871887
-> item_loss 0.8346256613731384 batch_loss 0.8639191013795359, real_loss 0.8346256613731384
-> item_loss 1.7274532318115234 batch_loss 0.8947596060378211, real_loss 2.1593165397644043
-> item_loss 0.297402024269104 batch_loss 0.8741610687354515, real_loss 0.037175253033638
[2023-10-01 03:15:20,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.6649885177612305 batch_loss 0.9005219837029775, real_loss 2.081235647201538
-> item_loss 1.4808223247528076 batch_loss 0.9192413495432946, real_loss 1.8510279655456543
-> item_loss 0.5297740697860718 batch_loss 0.9070704970508814, real_loss 0.5297740697860718
-> item_loss 0.4460815191268921 batch_loss 0.8931011340834878, real_loss 0.05576018989086151
-> item_loss 0.45455530285835266 batch_loss 0.8802027272827485, real_loss 0.05681941285729408
-> item_loss 1.624386191368103 batch_loss 0.9014651119709015, real_loss 2.030482769012451
-> item_loss 0.41635948419570923 batch_loss 0.8879899556438128, real_loss 0.052044935524463654
-> item_loss 1.6626003980636597 batch_loss 0.9089253730065113, real_loss 2.0782504081726074
-> item_loss 0.6224996447563171 batch_loss 0.9013878538420326, real_loss 0.6224996447563171
-> item_loss 0.5330567955970764 batch_loss 0.8919434677331876, real_loss 0.5330567955970764
[2023-10-01 03:17:05,691] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.609276294708252 batch_loss 0.9098767884075641, real_loss 2.0115952491760254
-> item_loss 1.5729120969772339 batch_loss 0.9260483812995073, real_loss 1.9661401510238647
-> item_loss 0.5492727160453796 batch_loss 0.91707753212679, real_loss 0.5492727160453796
-> item_loss 0.3733612596988678 batch_loss 0.9044329676517221, real_loss 0.046670157462358475
-> item_loss 0.5147587060928345 batch_loss 0.8955767344344746, real_loss 0.5147587060928345
-> item_loss 1.6826176643371582 batch_loss 0.9130665328767564, real_loss 2.103271961212158
-> item_loss 0.5124051570892334 batch_loss 0.904356502968332, real_loss 0.5124051570892334
-> item_loss 0.5571140646934509 batch_loss 0.8969683659837601, real_loss 0.5571140646934509
-> item_loss 0.1770613044500351 batch_loss 0.8819703022018075, real_loss 0.022132663056254387
-> item_loss 0.4861209988594055 batch_loss 0.8738917449907381, real_loss 0.06076512485742569
[2023-10-01 03:19:13,357] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.8533505201339722 batch_loss 0.8734809204936027, real_loss 0.8533505201339722
-> item_loss 1.4030076265335083 batch_loss 0.8838637970826205, real_loss 1.753759503364563
127.0.0.1 - - [01/Oct/2023 03:19:27] "POST /train/sft HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 03:21:12] "POST /state/reset HTTP/1.1" 200 21
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 32891.18it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 507396.58it/s]
assistant 
192.168.0.103 - - [01/Oct/2023 03:21:38] "POST /inference/generate HTTP/1.1" 200 14988
None
None
192.168.0.103 - - [01/Oct/2023 03:21:48] "POST /state/reset HTTP/1.1" 200 21
master: 
192.168.0.103 - - [01/Oct/2023 03:22:15] "POST /inference/generate HTTP/1.1" 200 15757
None
None
192.168.0.103 - - [01/Oct/2023 03:22:23] "POST /state/reset HTTP/1.1" 200 21
assistant 
192.168.0.103 - - [01/Oct/2023 03:22:51] "POST /inference/generate HTTP/1.1" 200 16603
master: 
192.168.0.103 - - [01/Oct/2023 03:23:44] "POST /inference/generate HTTP/1.1" 200 1536
assistant 
192.168.0.103 - - [01/Oct/2023 03:23:56] "POST /inference/generate HTTP/1.1" 200 1442
None
None
192.168.0.103 - - [01/Oct/2023 03:24:58] "POST /state/reset HTTP/1.1" 200 21
1602100192.168.0.103 - - [01/Oct/2023 03:25:04] "POST /inference/generate HTTP/1.1" 200 1745
None
None
192.168.0.103 - - [01/Oct/2023 03:25:44] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 03:25:49] "POST /inference/generate HTTP/1.1" 200 1669
None
None
192.168.0.103 - - [01/Oct/2023 03:26:09] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 03:26:11] "POST /inference/generate HTTP/1.1" 200 1368
None
None
192.168.0.103 - - [01/Oct/2023 03:26:23] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 03:26:30] "POST /inference/generate HTTP/1.1" 200 2201
None
None
192.168.0.103 - - [01/Oct/2023 03:26:55] "POST /state/reset HTTP/1.1" 200 21

loot

192.168.0.103 - - [01/Oct/2023 03:27:06] "POST /inference/generate HTTP/1.1" 200 2684
None
None
192.168.0.103 - - [01/Oct/2023 03:27:38] "POST /state/reset HTTP/1.1" 200 21

DND""
DND
192.168.0.103 - - [01/Oct/2023 03:27:46] "POST /inference/generate HTTP/1.1" 200 2374
None
None
192.168.0.103 - - [01/Oct/2023 03:28:24] "POST /state/reset HTTP/1.1" 200 21
3
192.168.0.103 - - [01/Oct/2023 03:28:27] "POST /inference/generate HTTP/1.1" 200 1558
None
None
192.168.0.103 - - [01/Oct/2023 03:29:00] "POST /state/reset HTTP/1.1" 200 21
3
<|over|>

<|me|><|request|> 3 A   
<|over|>

<|me|><|response|>
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|> 3 A   
<|over|>

<|me|><|response|>
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
<|over|>

<|me|><|request|>  2 B A 
<|over|>

<|me|><|response|>
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>:
<|over|>

<|me|><|response|>
<|over|>

<|me|><|response|>


<|over|>

<|me|><|request|>

<|over|>

<|me|><|response|>
```
 A  B C D E F G H
 A  B C D E F G H
 A  B C D E F G H
 A  B C D E F G H
 A  B C D E F G H
 A  B C D E F G H
 A  B C D E F G H
 A  B C D E F G H
```
192.168.0.103 - - [01/Oct/2023 03:29:38] "POST /inference/generate HTTP/1.1" 200 7026
None
None
192.168.0.103 - - [01/Oct/2023 03:30:17] "POST /state/reset HTTP/1.1" 200 21
#
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|>: 
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|>: 
<|over|>

<|me|><|request|>: 10
<|over|>

<|me|><|response|>: 5
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|>: ()
<|over|>

<|me|><|response|>: ()
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|>: ()
<|over|>

<|me|><|response|>: ()
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|>: 
<|over|>

<|me|><|response|>: 5
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|>: 
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|>: (5)
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|>: ()
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|>: ()
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|>: ()
<|over|>

<|me|><|response|>: ()
<|over|>

<|me|><|system|> <|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|>: ()
<|over|>

<|me|><|request|>: 10
<|over|>

<|me|><|response|>: 
<|over|>

<|me|><|request|>: 30
<|over|>

<|me|><|response|>: 5
<|over|>

<|me|><|response|>: ()
<|over|>

<|me|><|request|>: 5
<|over|>

<|me|><|response|>: ()
<|over|>

<|me|><|request|>: 192.168.0.103 - - [01/Oct/2023 03:30:56] "POST /inference/generate HTTP/1.1" 200 7298
None
None
192.168.0.103 - - [01/Oct/2023 03:31:52] "POST /state/reset HTTP/1.1" 200 21








<|over|>

<|me|><|request|>, , , dm. ?
<|over|>

<|me|><|response|>

1. 
2. 
3. 
4. 
5. 
6. 



1. 
2. 
3. 
4. 
5. 

<|over|>

192.168.0.103 - - [01/Oct/2023 03:32:33] "POST /inference/generate HTTP/1.1" 200 7443
None
None
192.168.0.103 - - [01/Oct/2023 03:33:14] "POST /state/reset HTTP/1.1" 200 21


<|over|>

<|me|><|request|> #3,
<|over|>

<|me|><|response|>
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>
1.
2.
3. expressing excitement
4.
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>
1.
2.
3. expressing excitement
4.
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>
1.
2.
3.
4.
192.168.0.103 - - [01/Oct/2023 03:33:53] "POST /inference/generate HTTP/1.1" 200 7537
None
None
192.168.0.103 - - [01/Oct/2023 03:35:40] "POST /state/reset HTTP/1.1" 200 21

<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>






<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|>
, , 





<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>: AI
<|over|>

<|me|><|response|>, AI
<|over|>

<|me|><|request|>: , 192.168.0.103 - - [01/Oct/2023 03:36:18] "POST /inference/generate HTTP/1.1" 200 7218
None
None
192.168.0.103 - - [01/Oct/2023 03:36:40] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 03:36:47] "POST /inference/generate HTTP/1.1" 200 1387
None
None
192.168.0.103 - - [01/Oct/2023 03:37:58] "POST /state/reset HTTP/1.1" 200 21


- 
- 
- 
- 
- 
- 
- 

1. 
2. 
3. 
4. 
5. 
6. 
7. 
192.168.0.103 - - [01/Oct/2023 03:38:07] "POST /inference/generate HTTP/1.1" 200 2389
None
None
192.168.0.103 - - [01/Oct/2023 03:39:16] "POST /state/reset HTTP/1.1" 200 21


1. 
2. 
3. 
4. 
5. 
6. 
7. 
192.168.0.103 - - [01/Oct/2023 03:39:25] "POST /inference/generate HTTP/1.1" 200 2464
None
None
192.168.0.103 - - [01/Oct/2023 03:40:35] "POST /state/reset HTTP/1.1" 200 21

1. 
2. 
3. 
4. 
192.168.0.103 - - [01/Oct/2023 03:40:49] "POST /inference/generate HTTP/1.1" 200 3526
None
None
192.168.0.103 - - [01/Oct/2023 03:41:41] "POST /state/reset HTTP/1.1" 200 21

1. 
2. 
3. 
4. 


192.168.0.103 - - [01/Oct/2023 03:41:55] "POST /inference/generate HTTP/1.1" 200 3435
None
None
192.168.0.103 - - [01/Oct/2023 03:42:18] "POST /state/reset HTTP/1.1" 200 21
AI





















192.168.0.103 - - [01/Oct/2023 03:42:53] "POST /inference/generate HTTP/1.1" 200 7626
None
None
192.168.0.103 - - [01/Oct/2023 03:46:31] "POST /state/reset HTTP/1.1" 200 21

1. 
2. 
3. 
4. 
5. 
6. 
192.168.0.103 - - [01/Oct/2023 03:46:43] "POST /inference/generate HTTP/1.1" 200 2886
None
None
192.168.0.103 - - [01/Oct/2023 03:49:21] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 03:49:25] "POST /inference/generate HTTP/1.1" 200 1897
None
None
192.168.0.103 - - [01/Oct/2023 03:50:44] "POST /state/reset HTTP/1.1" 200 21
2



192.168.0.103 - - [01/Oct/2023 03:51:01] "POST /inference/generate HTTP/1.1" 200 3403
None
None
192.168.0.103 - - [01/Oct/2023 03:54:29] "POST /state/reset HTTP/1.1" 200 21





192.168.0.103 - - [01/Oct/2023 03:54:42] "POST /inference/generate HTTP/1.1" 200 3217
None
None
192.168.0.103 - - [01/Oct/2023 03:56:47] "POST /state/reset HTTP/1.1" 200 21


1. 
2. 
3. 
4. 


, 7, ?
192.168.0.103 - - [01/Oct/2023 03:57:04] "POST /inference/generate HTTP/1.1" 200 4340
None
None
192.168.0.103 - - [01/Oct/2023 03:57:36] "POST /state/reset HTTP/1.1" 200 21
AI



192.168.0.103 - - [01/Oct/2023 03:57:50] "POST /inference/generate HTTP/1.1" 200 3166
None
None
192.168.0.103 - - [01/Oct/2023 03:58:03] "POST /state/reset HTTP/1.1" 200 21

1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 
11. 
12. 
13. 
14. 

192.168.0.103 - - [01/Oct/2023 03:58:24] "POST /inference/generate HTTP/1.1" 200 6670
None
None
192.168.0.103 - - [01/Oct/2023 03:58:40] "POST /state/reset HTTP/1.1" 200 21

- 
- 
- 
- 
- 
- 
192.168.0.103 - - [01/Oct/2023 03:59:08] "POST /inference/generate HTTP/1.1" 200 10666
None
None
192.168.0.103 - - [01/Oct/2023 04:03:47] "POST /state/reset HTTP/1.1" 200 21
"Bedrock"




192.168.0.103 - - [01/Oct/2023 04:03:57] "POST /inference/generate HTTP/1.1" 200 2532
None
None
192.168.0.103 - - [01/Oct/2023 04:05:15] "POST /state/reset HTTP/1.1" 200 21


"Bestiality" ()

192.168.0.103 - - [01/Oct/2023 04:05:26] "POST /inference/generate HTTP/1.1" 200 2993
None
None
192.168.0.103 - - [01/Oct/2023 04:07:06] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 04:07:13] "POST /inference/generate HTTP/1.1" 200 1718
None
None
192.168.0.103 - - [01/Oct/2023 04:07:28] "POST /state/reset HTTP/1.1" 200 21

1. 
2. 
3. 
4. 
192.168.0.103 - - [01/Oct/2023 04:07:40] "POST /inference/generate HTTP/1.1" 200 3131
None
None
192.168.0.103 - - [01/Oct/2023 04:09:01] "POST /state/reset HTTP/1.1" 200 21

1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
192.168.0.103 - - [01/Oct/2023 04:09:13] "POST /inference/generate HTTP/1.1" 200 3128
None
None
192.168.0.103 - - [01/Oct/2023 04:09:43] "POST /state/reset HTTP/1.1" 200 21


1. 
2. 
3. 
4. 
5. 
6. 
192.168.0.103 - - [01/Oct/2023 04:10:10] "POST /inference/generate HTTP/1.1" 200 8518
None
None
192.168.0.103 - - [01/Oct/2023 04:10:42] "POST /state/reset HTTP/1.1" 200 21


















2023411043

















192.168.0.103 - - [01/Oct/2023 04:11:32] "POST /inference/generate HTTP/1.1" 200 12591
None
None
192.168.0.103 - - [01/Oct/2023 04:13:18] "POST /state/reset HTTP/1.1" 200 21




1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 
192.168.0.103 - - [01/Oct/2023 04:13:35] "POST /inference/generate HTTP/1.1" 200 3523
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
-> item_loss 0.5691538453102112 batch_loss 0.5691538453102112, real_loss 0.5691538453102112
-> item_loss 0.6415207982063293 batch_loss 0.6053373217582703, real_loss 0.6415207982063293
-> item_loss 0.11985801160335541 batch_loss 0.44351088503996533, real_loss 0.014982251450419426
-> item_loss 0.8910486102104187 batch_loss 0.5553953163325787, real_loss 0.8910486102104187
-> item_loss 0.7129815220832825 batch_loss 0.5869125574827194, real_loss 0.7129815220832825
-> item_loss 0.429548978805542 batch_loss 0.5606852943698565, real_loss 0.05369362235069275
-> item_loss 0.3764822781085968 batch_loss 0.5343705777611051, real_loss 0.0470602847635746
-> item_loss 0.41329026222229004 batch_loss 0.5192355383187532, real_loss 0.051661282777786255
[2023-10-01 04:22:57,600] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.7065791487693787 batch_loss 0.5400514950354894, real_loss 0.7065791487693787
-> item_loss 1.0826953649520874 batch_loss 0.5943158820271492, real_loss 1.3533692359924316
-> item_loss 0.4810032248497009 batch_loss 0.5840147313746539, real_loss 0.060125403106212616
-> item_loss 0.2956339716911316 batch_loss 0.5599830014010271, real_loss 0.03695424646139145
-> item_loss 0.4021166265010834 batch_loss 0.5478394341010314, real_loss 0.05026457831263542
-> item_loss 0.5988622307777405 batch_loss 0.5514839195779392, real_loss 0.5988622307777405
-> item_loss 0.4150277376174927 batch_loss 0.5423868407805761, real_loss 0.051878467202186584
-> item_loss 1.0976539850234985 batch_loss 0.5770910372957587, real_loss 1.3720674514770508
-> item_loss 1.166033148765564 batch_loss 0.6117346909116296, real_loss 1.4575414657592773
-> item_loss 1.3159818649291992 batch_loss 0.6508595339126058, real_loss 1.644977331161499
[2023-10-01 04:24:51,958] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.3326877355575562 batch_loss 0.6867452287360242, real_loss 1.6658596992492676
-> item_loss 0.3649669587612152 batch_loss 0.6706563152372838, real_loss 0.0456208698451519
-> item_loss 0.304621160030365 batch_loss 0.65322606975124, real_loss 0.038077645003795624
-> item_loss 0.8522197604179382 batch_loss 0.6622712375088171, real_loss 0.8522197604179382
-> item_loss 0.47498828172683716 batch_loss 0.654128500300905, real_loss 0.059373535215854645
-> item_loss 0.3385929465293884 batch_loss 0.6409811855604252, real_loss 0.04232411831617355
-> item_loss 0.1120198592543602 batch_loss 0.6198227325081825, real_loss 0.014002482406795025
-> item_loss 0.9678860902786255 batch_loss 0.6332097847301227, real_loss 0.9678860902786255
-> item_loss 0.4710085988044739 batch_loss 0.627202333399543, real_loss 0.058876074850559235
-> item_loss 0.17074310779571533 batch_loss 0.6109002181994063, real_loss 0.021342888474464417
[2023-10-01 04:26:41,570] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.1100322008132935 batch_loss 0.6281116658757473, real_loss 1.3875402212142944
-> item_loss 0.2538500130176544 batch_loss 0.6156362774471442, real_loss 0.0317312516272068
-> item_loss 0.8683837056159973 batch_loss 0.6237894202913007, real_loss 0.8683837056159973
-> item_loss 0.6203964948654175 batch_loss 0.6236833913717419, real_loss 0.6203964948654175
-> item_loss 0.3298884630203247 batch_loss 0.6147805147550323, real_loss 0.04123605787754059
-> item_loss 0.5879257321357727 batch_loss 0.613990668207407, real_loss 0.5879257321357727
-> item_loss 0.25896012783050537 batch_loss 0.6038469384823527, real_loss 0.03237001597881317
-> item_loss 0.42313048243522644 batch_loss 0.598827036925488, real_loss 0.052891310304403305
-> item_loss 0.17903299629688263 batch_loss 0.5874812520436339, real_loss 0.02237912453711033
-> item_loss 1.4575711488723755 batch_loss 0.6103783545917586, real_loss 1.821963906288147
[2023-10-01 04:28:28,716] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.0494883060455322 batch_loss 0.6216375841162144, real_loss 1.3118603229522705
-> item_loss 1.0699912309646606 batch_loss 0.6328464252874255, real_loss 1.3374890089035034
-> item_loss 1.2575706243515015 batch_loss 0.6480836008743542, real_loss 1.5719633102416992
-> item_loss 0.26129063963890076 batch_loss 0.6388742446544624, real_loss 0.032661329954862595
-> item_loss 0.38236725330352783 batch_loss 0.6329089657858361, real_loss 0.04779590666294098
-> item_loss 1.040467619895935 batch_loss 0.6421716624701564, real_loss 1.3005845546722412
-> item_loss 0.6864699721336365 batch_loss 0.6431560693515672, real_loss 0.6864699721336365
-> item_loss 0.29352787137031555 batch_loss 0.6355554563519747, real_loss 0.036690983921289444
-> item_loss 0.11608628183603287 batch_loss 0.624502920723976, real_loss 0.014510785229504108
-> item_loss 0.575895369052887 batch_loss 0.6234902633974949, real_loss 0.575895369052887
[2023-10-01 04:30:15,169] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.4305032789707184 batch_loss 0.6195517535112343, real_loss 0.0538129098713398
-> item_loss 0.3157009482383728 batch_loss 0.613474737405777, real_loss 0.0394626185297966
-> item_loss 1.2215299606323242 batch_loss 0.6253973888415917, real_loss 1.5269124507904053
-> item_loss 0.5127900242805481 batch_loss 0.623231862600033, real_loss 0.5127900242805481
-> item_loss 1.0456591844558716 batch_loss 0.6312021894275017, real_loss 1.307073950767517
-> item_loss 0.736565351486206 batch_loss 0.6331533590952555, real_loss 0.736565351486206
-> item_loss 0.39085468649864197 batch_loss 0.628747928684408, real_loss 0.048856835812330246
-> item_loss 1.362654447555542 batch_loss 0.6418534022356782, real_loss 1.7033181190490723
-> item_loss 0.5116968750953674 batch_loss 0.6395699543911114, real_loss 0.5116968750953674
-> item_loss 0.4752591550350189 batch_loss 0.6367370095746271, real_loss 0.059407394379377365
[2023-10-01 04:32:14,129] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.27021005749702454 batch_loss 0.6305246883529728, real_loss 0.03377625718712807
127.0.0.1 - - [01/Oct/2023 04:32:15] "POST /train/sft HTTP/1.1" 200 28
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
-> item_loss 0.45879924297332764 batch_loss 0.45879924297332764, real_loss 0.057349905371665955
-> item_loss 0.33209165930747986 batch_loss 0.39544545114040375, real_loss 0.04151145741343498
-> item_loss 0.18817414343357086 batch_loss 0.3263550152381261, real_loss 0.023521767929196358
-> item_loss 0.6655265688896179 batch_loss 0.41114790365099907, real_loss 0.6655265688896179
-> item_loss 0.6465814709663391 batch_loss 0.4582346171140671, real_loss 0.6465814709663391
-> item_loss 0.24536892771720886 batch_loss 0.4227570022145907, real_loss 0.030671115964651108
-> item_loss 0.21396766602993011 batch_loss 0.39292995418821064, real_loss 0.026745958253741264
-> item_loss 0.8739655613899231 batch_loss 0.4530594050884247, real_loss 0.8739655613899231
-> item_loss 0.10088369995355606 batch_loss 0.4139287711845504, real_loss 0.012610462494194508
[2023-10-01 10:31:43,119] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.497071236371994 batch_loss 0.42224301770329475, real_loss 0.06213390454649925
-> item_loss 0.29674723744392395 batch_loss 0.4108343104069883, real_loss 0.037093404680490494
-> item_loss 1.0401710271835327 batch_loss 0.4632790368050337, real_loss 1.3002138137817383
-> item_loss 0.727306067943573 batch_loss 0.48358880843107516, real_loss 0.727306067943573
-> item_loss 0.17405256628990173 batch_loss 0.4614790768495628, real_loss 0.021756570786237717
-> item_loss 0.5453303456306458 batch_loss 0.4670691614349683, real_loss 0.5453303456306458
-> item_loss 0.25881028175354004 batch_loss 0.45405298145487905, real_loss 0.032351285219192505
-> item_loss 0.32549482583999634 batch_loss 0.44649073700694475, real_loss 0.04068685322999954
-> item_loss 0.5625939965248108 batch_loss 0.4529409180912707, real_loss 0.5625939965248108
-> item_loss 0.2476692646741867 batch_loss 0.4421371468587926, real_loss 0.03095865808427334
[2023-10-01 10:33:48,805] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.2530267834663391 batch_loss 0.4326816286891699, real_loss 0.03162834793329239
-> item_loss 0.20741261541843414 batch_loss 0.42195453281913486, real_loss 0.025926576927304268
-> item_loss 0.2465241700410843 batch_loss 0.4139804254201325, real_loss 0.030815521255135536
-> item_loss 0.17593352496623993 batch_loss 0.4036305601830068, real_loss 0.02199169062077999
-> item_loss 0.6919034123420715 batch_loss 0.4156419290229678, real_loss 0.6919034123420715
-> item_loss 0.2803049683570862 batch_loss 0.41022845059633256, real_loss 0.03503812104463577
-> item_loss 0.4299094080924988 batch_loss 0.41098541050003123, real_loss 0.05373867601156235
-> item_loss 0.7740556001663208 batch_loss 0.42443245456174566, real_loss 0.7740556001663208
-> item_loss 0.155171200633049 batch_loss 0.41481598120714935, real_loss 0.019396400079131126
-> item_loss 0.9587143659591675 batch_loss 0.4335710979227362, real_loss 0.9587143659591675
[2023-10-01 10:35:26,028] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.041358914226293564 batch_loss 0.42049735846618813, real_loss 0.0051698642782866955
-> item_loss 0.8889009356498718 batch_loss 0.43560715127856503, real_loss 0.8889009356498718
-> item_loss 0.2590649724006653 batch_loss 0.43009020818863064, real_loss 0.03238312155008316
-> item_loss 0.2979825437068939 batch_loss 0.426086945628578, real_loss 0.03724781796336174
-> item_loss 0.197941392660141 batch_loss 0.4193767823059769, real_loss 0.024742674082517624
-> item_loss 1.1127557754516602 batch_loss 0.43918761068156786, real_loss 1.3909447193145752
-> item_loss 0.46015697717666626 batch_loss 0.4397700930842095, real_loss 0.05751962214708328
-> item_loss 0.168133944272995 batch_loss 0.4324285755487713, real_loss 0.021016743034124374
-> item_loss 0.3330594301223755 batch_loss 0.4298135980375503, real_loss 0.041632428765296936
-> item_loss 0.8384822607040405 batch_loss 0.4402922816956655, real_loss 0.8384822607040405
[2023-10-01 10:36:56,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.3656969368457794 batch_loss 0.43842739807441833, real_loss 0.04571211710572243
-> item_loss 0.8780590295791626 batch_loss 0.4491501207940462, real_loss 0.8780590295791626
-> item_loss 0.09354013949632645 batch_loss 0.44068321647743386, real_loss 0.011692517437040806
-> item_loss 0.29963991045951843 batch_loss 0.4374031395932963, real_loss 0.037454988807439804
-> item_loss 0.3603900671005249 batch_loss 0.43565284249118785, real_loss 0.04504875838756561
-> item_loss 0.08454775810241699 batch_loss 0.4278505072825485, real_loss 0.010568469762802124
-> item_loss 1.3906266689300537 batch_loss 0.4487804238401029, real_loss 1.738283395767212
-> item_loss 0.7002494931221008 batch_loss 0.4541308295695072, real_loss 0.7002494931221008
-> item_loss 0.8273698091506958 batch_loss 0.46190664164411527, real_loss 0.8273698091506958
-> item_loss 0.5917603373527527 batch_loss 0.4645567170667405, real_loss 0.5917603373527527
[2023-10-01 10:38:59,079] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.1639786809682846 batch_loss 0.4585451563447714, real_loss 0.020497335121035576
-> item_loss 0.3192322254180908 batch_loss 0.4558135302481698, real_loss 0.03990402817726135
-> item_loss 0.3243853747844696 batch_loss 0.4532860657200217, real_loss 0.0405481718480587
-> item_loss 0.05914318189024925 batch_loss 0.4458494075345543, real_loss 0.0073928977362811565
-> item_loss 0.0944313108921051 batch_loss 0.43934166500413857, real_loss 0.011803913861513138
-> item_loss 0.22257229685783386 batch_loss 0.43540040376511485, real_loss 0.027821537107229233
-> item_loss 0.5216602683067322 batch_loss 0.4369407584890723, real_loss 0.5216602683067322
-> item_loss 0.6122815012931824 batch_loss 0.4400169118716006, real_loss 0.6122815012931824
127.0.0.1 - - [01/Oct/2023 10:40:26] "POST /train/sft HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 10:42:19] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 10:42:25] "POST /inference/generate HTTP/1.1" 200 2105
None
None
192.168.0.103 - - [01/Oct/2023 10:42:55] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 10:43:06] "POST /inference/generate HTTP/1.1" 200 4529
None
None
192.168.0.103 - - [01/Oct/2023 10:44:35] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 10:44:48] "POST /inference/generate HTTP/1.1" 200 7021
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 276861.17it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 511972.51it/s]
127.0.0.1 - - [01/Oct/2023 10:45:00] "POST /inference/load-model HTTP/1.1" 200 26
None
None
192.168.0.103 - - [01/Oct/2023 10:59:02] "POST /state/reset HTTP/1.1" 200 21
 
192.168.0.103 - - [01/Oct/2023 10:59:40] "POST /inference/generate HTTP/1.1" 200 17001
None
None
192.168.0.103 - - [01/Oct/2023 11:00:24] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 11:01:01] "POST /inference/generate HTTP/1.1" 200 17563
None
None
192.168.0.103 - - [01/Oct/2023 11:01:27] "POST /state/reset HTTP/1.1" 200 21

1. 
2. 
3. 
4. 
5. 
6. 
192.168.0.103 - - [01/Oct/2023 11:02:14] "POST /inference/generate HTTP/1.1" 200 20190
None
None
192.168.0.103 - - [01/Oct/2023 11:02:55] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 11:03:45] "POST /inference/generate HTTP/1.1" 200 23440
None
None
192.168.0.103 - - [01/Oct/2023 11:04:58] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 11:05:55] "POST /inference/generate HTTP/1.1" 200 26276
None
None
192.168.0.103 - - [01/Oct/2023 11:11:42] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 11:13:41] "POST /inference/generate HTTP/1.1" 200 54413
None
None
192.168.0.103 - - [01/Oct/2023 11:15:21] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 11:16:32] "POST /inference/generate HTTP/1.1" 200 30730
None
None
192.168.0.103 - - [01/Oct/2023 11:22:21] "POST /state/reset HTTP/1.1" 200 21
 

1. 
2. 
3. 
4. 
5. 
192.168.0.103 - - [01/Oct/2023 11:23:58] "POST /inference/generate HTTP/1.1" 200 38519
None
None
192.168.0.103 - - [01/Oct/2023 11:27:26] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 11:27:59] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 11:32:15] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 11:32:21] "POST /inference/generate HTTP/1.1" 200 3687
None
None
192.168.0.103 - - [01/Oct/2023 11:34:49] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 11:35:01] "POST /inference/generate HTTP/1.1" 200 6250
None
None
192.168.0.103 - - [01/Oct/2023 11:35:42] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 11:35:56] "POST /inference/generate HTTP/1.1" 200 7856
None
None
192.168.0.103 - - [01/Oct/2023 11:36:03] "POST /state/reset HTTP/1.1" 200 21
  
192.168.0.103 - - [01/Oct/2023 11:36:20] "POST /inference/generate HTTP/1.1" 200 8684
None
None
192.168.0.103 - - [01/Oct/2023 11:36:42] "POST /state/reset HTTP/1.1" 200 21
  
192.168.0.103 - - [01/Oct/2023 11:37:00] "POST /inference/generate HTTP/1.1" 200 9298
None
None
192.168.0.103 - - [01/Oct/2023 11:38:32] "POST /state/reset HTTP/1.1" 200 21
 
192.168.0.103 - - [01/Oct/2023 11:38:52] "POST /inference/generate HTTP/1.1" 200 10657
None
None
192.168.0.103 - - [01/Oct/2023 11:49:42] "POST /state/reset HTTP/1.1" 200 21

#+begin_src  magic_cast
  cluade 
 




 
 
 
 ,
192.168.0.103 - - [01/Oct/2023 11:51:08] "POST /inference/generate HTTP/1.1" 200 37730
None
None
192.168.0.103 - - [01/Oct/2023 11:52:44] "POST /state/reset HTTP/1.1" 200 21
[2023-10-01 11:53:02,922] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
192.168.0.103 - - [01/Oct/2023 11:53:02] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 11:55:17] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 11:55:46] "POST /train/tx-data HTTP/1.1" 200 27
None
None
192.168.0.103 - - [01/Oct/2023 11:56:44] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 11:56:58] "POST /train/tx-data HTTP/1.1" 200 27
None
None
192.168.0.103 - - [01/Oct/2023 11:57:27] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 11:57:59] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 12:10:54] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 12:11:26] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 12:13:34] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 12:13:52] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 12:14:29] "POST /state/reset HTTP/1.1" 200 21
[2023-10-01 12:14:59,502] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
192.168.0.103 - - [01/Oct/2023 12:14:59] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 12:15:08] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 12:15:24] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 12:16:18] "POST /state/reset HTTP/1.1" 200 21
(www.bing.com)

192.168.0.103 - - [01/Oct/2023 12:16:21] "POST /inference/generate HTTP/1.1" 200 1491
None
None
192.168.0.103 - - [01/Oct/2023 12:16:38] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 12:16:42] "POST /inference/generate HTTP/1.1" 200 2325
None
None
192.168.0.103 - - [01/Oct/2023 12:17:08] "POST /state/reset HTTP/1.1" 200 21
(www.bing.com)
192.168.0.103 - - [01/Oct/2023 12:17:14] "POST /inference/generate HTTP/1.1" 200 2512
None
None
192.168.0.103 - - [01/Oct/2023 12:17:21] "POST /state/reset HTTP/1.1" 200 21
(www.bing.com)
#+end_src192.168.0.103 - - [01/Oct/2023 12:17:25] "POST /inference/generate HTTP/1.1" 200 1569
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
-> item_loss 0.4713096618652344 batch_loss 0.4713096618652344, real_loss 0.0589137077331543
-> item_loss 0.19435429573059082 batch_loss 0.3328319787979126, real_loss 0.024294286966323853
-> item_loss 0.9495713710784912 batch_loss 0.5384117762247721, real_loss 0.9495713710784912
-> item_loss 0.37760019302368164 batch_loss 0.4982088804244995, real_loss 0.047200024127960205
-> item_loss 0.596668004989624 batch_loss 0.5179007053375244, real_loss 0.596668004989624
-> item_loss 0.4495692253112793 batch_loss 0.5065121253331503, real_loss 0.05619615316390991
-> item_loss 0.2736583352088928 batch_loss 0.473247298172542, real_loss 0.0342072919011116
-> item_loss 0.555142343044281 batch_loss 0.4834841787815094, real_loss 0.555142343044281
[2023-10-01 12:20:05,463] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.6064695119857788 batch_loss 0.497149215804206, real_loss 0.6064695119857788
-> item_loss 0.24888049066066742 batch_loss 0.4723223432898521, real_loss 0.031110061332583427
-> item_loss 0.8761880993843079 batch_loss 0.5090374120257117, real_loss 0.8761880993843079
-> item_loss 0.07582394778728485 batch_loss 0.47293629000584286, real_loss 0.009477993473410606
-> item_loss 0.17353695631027222 batch_loss 0.4499055720292605, real_loss 0.021692119538784027
-> item_loss 1.5098209381103516 batch_loss 0.5256138124636242, real_loss 1.8872761726379395
-> item_loss 1.1622337102890015 batch_loss 0.568055138985316, real_loss 1.4527921676635742
-> item_loss 0.8340710401535034 batch_loss 0.5846811328083277, real_loss 0.8340710401535034
-> item_loss 0.5769854784011841 batch_loss 0.5842284472549663, real_loss 0.5769854784011841
-> item_loss 0.19126062095165253 batch_loss 0.5623969013492266, real_loss 0.023907577618956566
[2023-10-01 12:22:01,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.0682549700140953 batch_loss 0.5363894312789566, real_loss 0.008531871251761913
-> item_loss 0.042445965111255646 batch_loss 0.5116922579705715, real_loss 0.005305745638906956
-> item_loss 0.32440319657325745 batch_loss 0.5027737312373661, real_loss 0.04055039957165718
-> item_loss 0.44375672936439514 batch_loss 0.5000911402431402, real_loss 0.05546959117054939
-> item_loss 0.21326835453510284 batch_loss 0.4876205843427907, real_loss 0.026658544316887856
-> item_loss 0.26489827036857605 batch_loss 0.4783404879271984, real_loss 0.033112283796072006
-> item_loss 0.6474508047103882 batch_loss 0.485104900598526, real_loss 0.6474508047103882
-> item_loss 0.16834527254104614 batch_loss 0.4729218379809306, real_loss 0.021043159067630768
-> item_loss 0.10780873149633408 batch_loss 0.45939913033335295, real_loss 0.01347609143704176
-> item_loss 0.22074425220489502 batch_loss 0.45087574182876516, real_loss 0.027593031525611877
[2023-10-01 12:24:00,808] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.1561325490474701 batch_loss 0.44071218345699636, real_loss 0.01951656863093376
-> item_loss 1.236923336982727 batch_loss 0.4672525552411874, real_loss 1.5461541414260864
-> item_loss 0.39643529057502747 batch_loss 0.46496812734873066, real_loss 0.04955441132187843
-> item_loss 0.031500402837991714 batch_loss 0.45142226095777005, real_loss 0.003937550354748964
-> item_loss 0.09922516345977783 batch_loss 0.4407496216396491, real_loss 0.012403145432472229
-> item_loss 0.15018197894096375 batch_loss 0.43220351450145245, real_loss 0.018772747367620468
-> item_loss 0.256565660238266 batch_loss 0.42718529009393286, real_loss 0.03207070752978325
-> item_loss 0.09334414452314377 batch_loss 0.4179119249391887, real_loss 0.011668018065392971
-> item_loss 0.22814245522022247 batch_loss 0.4127830203521896, real_loss 0.02851780690252781
-> item_loss 0.8893330693244934 batch_loss 0.42532381111461864, real_loss 0.8893330693244934
[2023-10-01 12:26:08,774] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.16280263662338257 batch_loss 0.4185924989481767, real_loss 0.02035032957792282
-> item_loss 0.1866646260023117 batch_loss 0.41279430212453005, real_loss 0.023333078250288963
-> item_loss 0.3927652835845947 batch_loss 0.4123057894772146, real_loss 0.04909566044807434
-> item_loss 0.45498165488243103 batch_loss 0.4133218815106721, real_loss 0.05687270686030388
-> item_loss 0.06960320472717285 batch_loss 0.40532842391105584, real_loss 0.008700400590896606
-> item_loss 0.09484803676605225 batch_loss 0.39827205147594213, real_loss 0.01185600459575653
-> item_loss 0.18479721248149872 batch_loss 0.3935281661649545, real_loss 0.02309965156018734
-> item_loss 0.42816072702407837 batch_loss 0.39428104792276153, real_loss 0.053520090878009796
-> item_loss 0.4078296720981598 batch_loss 0.3945693165222381, real_loss 0.050978709012269974
-> item_loss 0.31666627526283264 batch_loss 0.39294633649600047, real_loss 0.03958328440785408
[2023-10-01 12:28:04,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.05055636167526245 batch_loss 0.3859587859894548, real_loss 0.006319545209407806
-> item_loss 1.4530011415481567 batch_loss 0.40729963310062883, real_loss 1.8162513971328735
-> item_loss 0.6674631834030151 batch_loss 0.41240087918498936, real_loss 0.6674631834030151
-> item_loss 0.3054627776145935 batch_loss 0.4103443772317125, real_loss 0.03818284720182419
-> item_loss 0.130303755402565 batch_loss 0.4050605919141814, real_loss 0.016287969425320625
-> item_loss 0.6029999256134033 batch_loss 0.4087261351308337, real_loss 0.6029999256134033
-> item_loss 0.3734712302684784 batch_loss 0.40808513686060904, real_loss 0.0466839037835598
-> item_loss 0.17111016809940338 batch_loss 0.40385344098987325, real_loss 0.021388771012425423
-> item_loss 0.19961147010326385 batch_loss 0.40027024851817833, real_loss 0.024951433762907982
-> item_loss 0.13483767211437225 batch_loss 0.3956938247870782, real_loss 0.01685470901429653
[2023-10-01 12:30:24,506] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.48232850432395935 batch_loss 0.3971622091860084, real_loss 0.06029106304049492
-> item_loss 0.38159462809562683 batch_loss 0.3969027495011687, real_loss 0.047699328511953354
127.0.0.1 - - [01/Oct/2023 12:30:36] "POST /train/sft HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 12:31:56] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 12:32:29] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 12:32:54] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 12:33:09] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 12:35:05] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 12:35:20] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 12:36:43] "POST /state/reset HTTP/1.1" 200 21
(http://www.bing.com):
1.
2. "https://www.bing.com/"(), Bing 
3. "" ,
4.,192.168.0.103 - - [01/Oct/2023 12:36:50] "POST /inference/generate HTTP/1.1" 200 2018
None
None
192.168.0.103 - - [01/Oct/2023 12:38:22] "POST /state/reset HTTP/1.1" 200 21
""


192.168.0.103 - - [01/Oct/2023 12:38:30] "POST /inference/generate HTTP/1.1" 200 2579
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 30214.06it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 459455.10it/s]
127.0.0.1 - - [01/Oct/2023 12:39:56] "POST /inference/load-model HTTP/1.1" 200 26
None
None
192.168.0.103 - - [01/Oct/2023 12:40:59] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 12:41:03] "POST /inference/generate HTTP/1.1" 200 2095
None
None
192.168.0.103 - - [01/Oct/2023 12:41:18] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 12:41:23] "POST /inference/generate HTTP/1.1" 200 2918
None
None
192.168.0.103 - - [01/Oct/2023 12:41:29] "POST /state/reset HTTP/1.1" 200 21
(http://www.bing.com)"fireball"
192.168.0.103 - - [01/Oct/2023 12:41:37] "POST /inference/generate HTTP/1.1" 200 3974
None
None
192.168.0.103 - - [01/Oct/2023 12:42:36] "POST /state/reset HTTP/1.1" 200 21
:
[http://www.mustang-web.com/magiccast/]
#+begin_src  magic_cast
[:search/websit   "http://www.mustang-web.com"]
#+end_src192.168.0.103 - - [01/Oct/2023 12:42:42] "POST /inference/generate HTTP/1.1" 200 2388
None
None
192.168.0.103 - - [01/Oct/2023 12:43:17] "POST /state/reset HTTP/1.1" 200 21

[1] "fireball"
[2] "Florida International University (FIU) is a public research university located in the upscale suburb of Hialeah, Florida. It is the third-largest campus of FIU, with four campuses in Miami-Dade County and one in the suburb of Pinecreast. FIU ranked as one of the Top 20 Best Colleges by Hispanic Digest. It was founded in 1960 by Dr. Rafael Prez y Rivas, chancellor emeritus of Havasupai College in Kayenta, Arizona. It was renamed after Rafael Mara de Hostos, Cuban physician and philanthropist who donated land for its establishment. The university's main campus occupies a hilltop overlooking Downtown Miami and the Biscayne National Park. FIU's main campus covers 243 acres (97 hectares). The university has an endowment valued at nearly $1 billion, according to data released by Forbes magazine in 2011.
192.168.0.103 - - [01/Oct/2023 12:43:29] "POST /inference/generate HTTP/1.1" 200 3034
None
None
192.168.0.103 - - [01/Oct/2023 12:44:08] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 12:44:14] "POST /inference/generate HTTP/1.1" 200 1561
None
None
192.168.0.103 - - [01/Oct/2023 12:44:57] "POST /state/reset HTTP/1.1" 200 21

1. 

2. 

3. 

4. 

5. 


192.168.0.103 - - [01/Oct/2023 12:45:15] "POST /inference/generate HTTP/1.1" 200 3971
None
None
192.168.0.103 - - [01/Oct/2023 12:47:11] "POST /state/reset HTTP/1.1" 200 21
AI+begin_src
+begin_src+end_src
PID
+begin_src+end_src
192.168.0.103 - - [01/Oct/2023 12:47:22] "POST /inference/generate HTTP/1.1" 200 2989
None
None
192.168.0.103 - - [01/Oct/2023 12:53:43] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 12:53:55] "POST /inference/generate HTTP/1.1" 200 5613
None
None
192.168.0.103 - - [01/Oct/2023 12:54:05] "POST /state/reset HTTP/1.1" 200 21

AI
AI192.168.0.103 - - [01/Oct/2023 12:54:23] "POST /inference/generate HTTP/1.1" 200 7201
None
None
192.168.0.103 - - [01/Oct/2023 12:55:24] "POST /state/reset HTTP/1.1" 200 21


- 
- n dn ()24kou zh kung jio, beaten egg then deep-fry until golden brown,
- 
- 2-3
192.168.0.103 - - [01/Oct/2023 12:55:53] "POST /inference/generate HTTP/1.1" 200 11346
None
None
192.168.0.103 - - [01/Oct/2023 12:57:29] "POST /state/reset HTTP/1.1" 200 21

#+begin_src  magic_cast
[:search/websit   "https://baike.baidu.com/item/"  #+begin_src  magic_cast   [:search/websit   "https://tieba.baidu.com/p/_q1Q0bG?fromModule=lemma_search-box"  ""  ]
#+end_src192.168.0.103 - - [01/Oct/2023 12:57:56] "POST /inference/generate HTTP/1.1" 200 11922
None
None
192.168.0.103 - - [01/Oct/2023 12:58:18] "POST /state/reset HTTP/1.1" 200 21
:
+ 
+ 
+ 
#+begin_src  magic_cast
[:search/websit   "https://baike.baidu.com/item/"  ]
#+end_src192.168.0.103 - - [01/Oct/2023 12:58:43] "POST /inference/generate HTTP/1.1" 200 11506
None
None
192.168.0.103 - - [01/Oct/2023 12:59:41] "POST /state/reset HTTP/1.1" 200 21
:
"https://baike.baidu.com/item/"  => (www.baidu.com)
#+begin_src  magic_cast
[:search/websit   "https://baike.baidu.com/item/"  ]
#+end_src192.168.0.103 - - [01/Oct/2023 13:00:09] "POST /inference/generate HTTP/1.1" 200 11489
None
None
192.168.0.103 - - [01/Oct/2023 13:00:40] "POST /state/reset HTTP/1.1" 200 21
+begin_src  magic_cast
[:search/websit    ]
#+end_src192.168.0.103 - - [01/Oct/2023 13:01:02] "POST /inference/generate HTTP/1.1" 200 10251
None
None
192.168.0.103 - - [01/Oct/2023 13:02:27] "POST /state/reset HTTP/1.1" 200 21
 

- : 
- : 
- : 
- : 192.168.0.103 - - [01/Oct/2023 13:03:03] "POST /inference/generate HTTP/1.1" 200 11867
None
None
192.168.0.103 - - [01/Oct/2023 13:08:15] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 13:08:49] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 13:09:09] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 13:09:39] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 13:09:48] "POST /state/reset HTTP/1.1" 200 21
[2023-10-01 13:10:04,827] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
192.168.0.103 - - [01/Oct/2023 13:10:04] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 13:10:44] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 13:10:55] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 13:11:11] "POST /state/reset HTTP/1.1" 200 21
#+begin_src(www.baidu.com)+end_src
#+begin_src
SELECT * FROM Articles WHERE ArticleURL ~ XVIDD AND ArticleContent LIKE '%%'
#Xdd
192.168.0.103 - - [01/Oct/2023 13:11:34] "POST /inference/generate HTTP/1.1" 200 10572
None
None
192.168.0.103 - - [01/Oct/2023 13:12:26] "POST /state/reset HTTP/1.1" 200 21

#+begin_src
[:user-computer/search-web   "https://baike.baidu.com/item/"  ]
#+end_src192.168.0.103 - - [01/Oct/2023 13:12:51] "POST /inference/generate HTTP/1.1" 200 11803
None
None
192.168.0.103 - - [01/Oct/2023 13:14:32] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 13:14:57] "POST /inference/generate HTTP/1.1" 200 11648
None
None
192.168.0.103 - - [01/Oct/2023 13:16:40] "POST /state/reset HTTP/1.1" 200 21
+add (www.baidu.com) 
192.168.0.103 - - [01/Oct/2023 13:17:08] "POST /inference/generate HTTP/1.1" 200 12647
None
None
192.168.0.103 - - [01/Oct/2023 13:17:47] "POST /state/reset HTTP/1.1" 200 21
(www.baidu.com)
- 
- 7 with
-  until golden crisp an oven baked
- 
#+begin_src  192.168.0.103 - - [01/Oct/2023 13:18:21] "POST /inference/generate HTTP/1.1" 200 13177
None
None
192.168.0.103 - - [01/Oct/2023 13:18:52] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 13:19:27] "POST /inference/generate HTTP/1.1" 200 15457
None
None
192.168.0.103 - - [01/Oct/2023 13:19:52] "POST /state/reset HTTP/1.1" 200 21
 
192.168.0.103 - - [01/Oct/2023 13:20:29] "POST /inference/generate HTTP/1.1" 200 15575
None
None
192.168.0.103 - - [01/Oct/2023 13:20:36] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 13:21:12] "POST /inference/generate HTTP/1.1" 200 16336
None
None
192.168.0.103 - - [01/Oct/2023 13:21:49] "POST /state/reset HTTP/1.1" 200 21
clojure(www.baidu.com)(www.baidu.com)clojureweb client library llama:
```clojure
(require '[ llama.core :as llama ]
  '[ clojurescat.macros :as mcat
   clovese
  web-client-library :refer [ new-web-client ]
   clovese/web-client/core
   clovese/web-client/http
  clojase/web-client/core/content
  clojase/web-client/http/content ]
   clovese/web-client/core/content mcat clojase))
(defn get-page [ url ]
  (let [res (new-web-client :get url)]
    (doseq [block (clojurescat.macros::parse-string res)]
      (map #(get % block))
      res))
```
`get-page`URL
`mcat.core``get-page`"https://www.baidu.com/" URL`call``get-page`URL
clojurescat.macrosclojurescat.web-client URL 192.168.0.103 - - [01/Oct/2023 13:22:09] "POST /inference/generate HTTP/1.1" 200 3013
None
None
192.168.0.103 - - [01/Oct/2023 13:25:14] "POST /state/reset HTTP/1.1" 200 21
user:  
192.168.0.103 - - [01/Oct/2023 13:25:16] "POST /inference/generate HTTP/1.1" 200 1494
None
None
192.168.0.103 - - [01/Oct/2023 13:25:20] "POST /state/reset HTTP/1.1" 200 21
assistant: 
192.168.0.103 - - [01/Oct/2023 13:25:24] "POST /inference/generate HTTP/1.1" 200 2183
None
None
192.168.0.103 - - [01/Oct/2023 13:25:29] "POST /state/reset HTTP/1.1" 200 21
user: 
192.168.0.103 - - [01/Oct/2023 13:25:33] "POST /inference/generate HTTP/1.1" 200 2846
None
None
192.168.0.103 - - [01/Oct/2023 13:27:24] "POST /state/reset HTTP/1.1" 200 21
 
192.168.0.103 - - [01/Oct/2023 13:27:30] "POST /inference/generate HTTP/1.1" 200 2407
None
None
192.168.0.103 - - [01/Oct/2023 13:27:39] "POST /state/reset HTTP/1.1" 200 21
{
```192.168.0.103 - - [01/Oct/2023 13:27:44] "POST /inference/generate HTTP/1.1" 200 2286
None
None
192.168.0.103 - - [01/Oct/2023 13:29:19] "POST /state/reset HTTP/1.1" 200 21
json 
192.168.0.103 - - [01/Oct/2023 13:29:28] "POST /inference/generate HTTP/1.1" 200 2714
None
None
192.168.0.103 - - [01/Oct/2023 13:29:34] "POST /state/reset HTTP/1.1" 200 21

json" {\n" \\\n{\n"action": "open-computer",\n"input": "open the door"\n}" \\\n"} \\\n"

192.168.0.103 - - [01/Oct/2023 13:29:42] "POST /inference/generate HTTP/1.1" 200 3567
None
None
192.168.0.103 - - [01/Oct/2023 13:30:05] "POST /state/reset HTTP/1.1" 200 21
 
json     
192.168.0.103 - - [01/Oct/2023 13:30:13] "POST /inference/generate HTTP/1.1" 200 3765
None
None
192.168.0.103 - - [01/Oct/2023 13:37:45] "POST /state/reset HTTP/1.1" 200 21
(http://www.baidu.com/p/about?from=search&tn=baidusearch&rsv_upd=1&rsv_pnref=story)
#+begin_src python
import json
json_obj = json.dumps({"action": "search", "term": "ai"})
print(json_obj)192.168.0.103 - - [01/Oct/2023 13:37:51] "POST /inference/generate HTTP/1.1" 200 1752
None
None
192.168.0.103 - - [01/Oct/2023 13:38:41] "POST /state/reset HTTP/1.1" 200 21
 firebot 192.168.0.103 - - [01/Oct/2023 13:38:47] "POST /inference/generate HTTP/1.1" 200 2556
None
None
192.168.0.103 - - [01/Oct/2023 13:39:00] "POST /state/reset HTTP/1.1" 200 21
json   json 
192.168.0.103 - - [01/Oct/2023 13:39:07] "POST /inference/generate HTTP/1.1" 200 2981
None
None
192.168.0.103 - - [01/Oct/2023 13:39:25] "POST /state/reset HTTP/1.1" 200 21
 
192.168.0.103 - - [01/Oct/2023 13:39:30] "POST /inference/generate HTTP/1.1" 200 2463
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
-> item_loss 0.33687952160835266 batch_loss 0.33687952160835266, real_loss 0.04210994020104408
-> item_loss 0.3529752492904663 batch_loss 0.3449273854494095, real_loss 0.04412190616130829
-> item_loss 0.04880808666348457 batch_loss 0.24622095252076784, real_loss 0.006101010832935572
-> item_loss 0.18691928684711456 batch_loss 0.23139553610235453, real_loss 0.02336491085588932
-> item_loss 0.4030831754207611 batch_loss 0.26573306396603585, real_loss 0.05038539692759514
-> item_loss 0.027711685746908188 batch_loss 0.2260628342628479, real_loss 0.0034639607183635235
-> item_loss 0.1341163069009781 batch_loss 0.21292761606829508, real_loss 0.01676453836262226
-> item_loss 0.08322607725858688 batch_loss 0.19671492371708155, real_loss 0.01040325965732336
[2023-10-01 14:05:18,222] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.037465959787368774 batch_loss 0.1790205943915579, real_loss 0.004683244973421097
-> item_loss 0.16155527532100677 batch_loss 0.17727406248450278, real_loss 0.020194409415125847
-> item_loss 0.08166203647851944 batch_loss 0.16858206012032248, real_loss 0.01020775455981493
-> item_loss 1.0387616157531738 batch_loss 0.24109702308972678, real_loss 1.2984520196914673
-> item_loss 0.060691747814416885 batch_loss 0.22721969422239524, real_loss 0.007586468476802111
-> item_loss 0.11765036731958389 batch_loss 0.2193933137293373, real_loss 0.014706295914947987
-> item_loss 0.08985195308923721 batch_loss 0.21075722301999728, real_loss 0.011231494136154652
-> item_loss 0.2911309599876404 batch_loss 0.21578058158047497, real_loss 0.03639136999845505
-> item_loss 0.31137168407440186 batch_loss 0.2214035876095295, real_loss 0.03892146050930023
-> item_loss 0.21950030326843262 batch_loss 0.22129784959057966, real_loss 0.027437537908554077
[2023-10-01 14:06:53,391] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.08442489802837372 batch_loss 0.2140940100346741, real_loss 0.010553112253546715
-> item_loss 0.15160752832889557 batch_loss 0.21096968594938517, real_loss 0.018950941041111946
-> item_loss 0.1571665108203888 batch_loss 0.20840762999086154, real_loss 0.0196458138525486
-> item_loss 0.1676417887210846 batch_loss 0.20655463720587167, real_loss 0.020955223590135574
-> item_loss 0.11219852417707443 batch_loss 0.20245219750896745, real_loss 0.014024815522134304
-> item_loss 0.258827805519104 batch_loss 0.20480118117605647, real_loss 0.032353475689888
-> item_loss 0.5956383943557739 batch_loss 0.22043466970324516, real_loss 0.5956383943557739
-> item_loss 0.04326661676168442 batch_loss 0.21362051382087743, real_loss 0.005408327095210552
-> item_loss 0.45466554164886475 batch_loss 0.22254810744413622, real_loss 0.05683319270610809
-> item_loss 0.26851001381874084 batch_loss 0.22418960410037211, real_loss 0.033563751727342606
[2023-10-01 14:08:46,025] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.6255367398262024 batch_loss 0.2380291605047111, real_loss 0.6255367398262024
-> item_loss 0.6744683384895325 batch_loss 0.25257713310420515, real_loss 0.6744683384895325
-> item_loss 0.8407730460166931 batch_loss 0.2715511948110596, real_loss 0.8407730460166931
-> item_loss 0.12773016095161438 batch_loss 0.2670567875029519, real_loss 0.015966270118951797
-> item_loss 0.9811940789222717 batch_loss 0.28869731148535555, real_loss 0.9811940789222717
-> item_loss 0.19352079927921295 batch_loss 0.2858980023028219, real_loss 0.02419009990990162
-> item_loss 0.6409507989883423 batch_loss 0.29604236792240823, real_loss 0.6409507989883423
-> item_loss 0.18365173041820526 batch_loss 0.2929204057695137, real_loss 0.022956466302275658
-> item_loss 0.1576565057039261 batch_loss 0.28926462468666, real_loss 0.01970706321299076
-> item_loss 0.41320011019706726 batch_loss 0.2925260848316707, real_loss 0.05165001377463341
[2023-10-01 14:11:14,324] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.8185744285583496 batch_loss 0.3060145039015856, real_loss 0.8185744285583496
-> item_loss 0.3607693016529083 batch_loss 0.3073833738453686, real_loss 0.04509616270661354
-> item_loss 0.1376868039369583 batch_loss 0.3032444331158952, real_loss 0.01721085049211979
-> item_loss 0.1295289248228073 batch_loss 0.2991083495851074, real_loss 0.016191115602850914
-> item_loss 0.0648462176322937 batch_loss 0.29366039302806524, real_loss 0.008105777204036713
-> item_loss 0.2859310209751129 batch_loss 0.2934847254814072, real_loss 0.035741377621889114
-> item_loss 0.06927986443042755 batch_loss 0.2885023952358299, real_loss 0.008659983053803444
-> item_loss 0.021298954263329506 batch_loss 0.28269362477990595, real_loss 0.0026623692829161882
-> item_loss 0.34019148349761963 batch_loss 0.2839169834760275, real_loss 0.042523935437202454
-> item_loss 0.2578221559524536 batch_loss 0.2833733412359531, real_loss 0.0322277694940567
[2023-10-01 14:13:08,719] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.09828721731901169 batch_loss 0.27959607340091347, real_loss 0.012285902164876461
-> item_loss 0.2907516658306122 batch_loss 0.27981918524950744, real_loss 0.03634395822882652
-> item_loss 0.029293684288859367 batch_loss 0.2749069205247888, real_loss 0.003661710536107421
-> item_loss 0.08375688642263412 batch_loss 0.27123095833051664, real_loss 0.010469610802829266
-> item_loss 0.21868659555912018 batch_loss 0.2702395552593582, real_loss 0.027335824444890022
-> item_loss 0.03142250329256058 batch_loss 0.2658170172599731, real_loss 0.003927812911570072
-> item_loss 0.08931704610586166 batch_loss 0.26260792687535284, real_loss 0.011164630763232708
127.0.0.1 - - [01/Oct/2023 14:14:08] "POST /train/sft HTTP/1.1" 200 29
===saveved====
127.0.0.1 - - [01/Oct/2023 14:23:58] "POST /train/save-weight HTTP/1.1" 200 26
[2023-10-01 14:29:31,230] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1428973
[2023-10-01 14:29:31,231] [ERROR] [launch.py:434:sigkill_handler] ['/home/neromous/.minicoda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-01 14:32:44,213] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-01 14:32:44,234] [INFO] [runner.py:540:main] cmd = /home/neromous/.minicoda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-01 14:32:46,767] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [1]}
[2023-10-01 14:32:46,767] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-01 14:32:46,767] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-01 14:32:46,767] [INFO] [launch.py:247:main] dist_world_size=1
[2023-10-01 14:32:46,767] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=1
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/wkv_512/build.ninja...
Building extension module wkv_512...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512...
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.60683274269104 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-01 14:33:41,444] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
[2023-10-01 14:33:41,444] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-01 14:33:44,463] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-01 14:33:44,465] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-01 14:33:44,465] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-01 14:33:44,504] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-01 14:33:44,504] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-01 14:33:44,504] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-10-01 14:33:44,504] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 2000000
[2023-10-01 14:33:44,504] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 2000000
[2023-10-01 14:33:44,504] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-10-01 14:33:44,504] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.30111169815063477 seconds
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-10-01 14:34:17,954] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-10-01 14:34:17,956] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-10-01 14:34:17,956] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.68 GB, percent = 18.0%
[2023-10-01 14:34:38,070] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-10-01 14:34:38,071] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-10-01 14:34:38,071] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.9 GB, percent = 49.2%
[2023-10-01 14:34:38,071] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-10-01 14:34:38,990] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-10-01 14:34:38,991] [INFO] [utils.py:786:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-10-01 14:34:38,991] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 61.91 GB, percent = 49.2%
[2023-10-01 14:34:39,018] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-01 14:34:39,018] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-01 14:34:39,018] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fc46d604be0>
[2023-10-01 14:34:39,019] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-01 14:34:39,020] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-10-01 14:34:39,020] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-01 14:34:39,020] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-01 14:34:39,020] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-10-01 14:34:39,020] [INFO] [config.py:957:print]   amp_params ................... False
[2023-10-01 14:34:39,020] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-01 14:34:39,020] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-10-01 14:34:39,020] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-10-01 14:34:39,021] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-10-01 14:34:39,021] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-10-01 14:34:39,021] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc46d7a9d00>
[2023-10-01 14:34:39,021] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-10-01 14:34:39,021] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-01 14:34:39,021] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-10-01 14:34:39,021] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-10-01 14:34:39,021] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-01 14:34:39,021] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-10-01 14:34:39,021] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-10-01 14:34:39,021] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-10-01 14:34:39,021] [INFO] [config.py:957:print]   dump_state ................... False
[2023-10-01 14:34:39,021] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-10-01 14:34:39,021] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-10-01 14:34:39,021] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-01 14:34:39,021] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-01 14:34:39,021] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-10-01 14:34:39,021] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-10-01 14:34:39,021] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-10-01 14:34:39,021] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-10-01 14:34:39,021] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-10-01 14:34:39,021] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-10-01 14:34:39,021] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-01 14:34:39,022] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-10-01 14:34:39,022] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-10-01 14:34:39,022] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-10-01 14:34:39,022] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-10-01 14:34:39,022] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-10-01 14:34:39,022] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-10-01 14:34:39,022] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[2023-10-01 14:34:39,022] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-10-01 14:34:39,022] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-01 14:34:39,022] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-10-01 14:34:39,022] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-10-01 14:34:39,022] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-10-01 14:34:39,022] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-10-01 14:34:39,022] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-01 14:34:39,022] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-01 14:34:39,022] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-10-01 14:34:39,022] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-10-01 14:34:39,022] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-10-01 14:34:39,022] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-01 14:34:39,023] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-10-01 14:34:39,023] [INFO] [config.py:957:print]   pld_params ................... False
[2023-10-01 14:34:39,023] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-10-01 14:34:39,023] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-10-01 14:34:39,023] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-10-01 14:34:39,023] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-10-01 14:34:39,023] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-10-01 14:34:39,023] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-10-01 14:34:39,023] [INFO] [config.py:957:print]   train_batch_size ............. 1
[2023-10-01 14:34:39,023] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[2023-10-01 14:34:39,023] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-10-01 14:34:39,023] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-10-01 14:34:39,023] [INFO] [config.py:957:print]   world_size ................... 1
[2023-10-01 14:34:39,023] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-10-01 14:34:39,023] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-10-01 14:34:39,023] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-10-01 14:34:39,023] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-01 14:34:39,023] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[2023-10-01 14:34:39,023] [INFO] [config.py:943:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Using /home/neromous/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0004425048828125 seconds
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

Traceback (most recent call last):
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.minicoda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/app.py", line 246, in train_sft
    coll = file_to_node('./data/sft.org')
  File "/home/neromous/Documents/blackfog/RWKV-Ouroboros/models/org_text.py", line 93, in file_to_node
    res[tag] +=  to_data_node(node)
KeyError: 0
127.0.0.1 - - [01/Oct/2023 14:34:50] "POST /train/sft HTTP/1.1" 500 748
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
[2023-10-01 14:35:22,548] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2023-10-01 14:35:22,548] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-10-01 14:35:22,548] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-10-01 14:35:22,548] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2023-10-01 14:35:22,548] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
-> item_loss 2.5038254261016846 batch_loss 2.5038254261016846, real_loss 2.629016637802124
-> item_loss 2.6386001110076904 batch_loss 2.5712127685546875, real_loss 2.7705299854278564
-> item_loss 0.06184561550617218 batch_loss 1.734757050871849, real_loss 0.0030922808218747377
-> item_loss 0.07367711514234543 batch_loss 1.3194870669394732, real_loss 0.0036838557571172714
-> item_loss 0.016108818352222443 batch_loss 1.058811417222023, real_loss 0.0008054409408941865
-> item_loss 0.1833087056875229 batch_loss 0.9128942986329397, real_loss 0.00916543509811163
-> item_loss 0.0724015086889267 batch_loss 0.7928239000695092, real_loss 0.0036200755275785923
-> item_loss 0.2785017490386963 batch_loss 0.7285336311906576, real_loss 0.01392508763819933
-> item_loss 0.1101996973156929 batch_loss 0.659829860760106, real_loss 0.0055099851451814175
[2023-10-01 14:38:03,896] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.13529126346111298 batch_loss 0.6073760010302067, real_loss 0.006764563266187906
-> item_loss 0.0567508339881897 batch_loss 0.5573191676627506, real_loss 0.0028375417459756136
-> item_loss 0.25823530554771423 batch_loss 0.5323955124864975, real_loss 0.012911765836179256
-> item_loss 0.5884288549423218 batch_loss 0.536705769598484, real_loss 0.5884288549423218
-> item_loss 0.678163468837738 batch_loss 0.5468098909727165, real_loss 0.678163468837738
-> item_loss 0.031019998714327812 batch_loss 0.5124238981554906, real_loss 0.0015509999357163906
-> item_loss 0.017934352159500122 batch_loss 0.48151830153074116, real_loss 0.0008967175963334739
-> item_loss 0.41151630878448486 batch_loss 0.47740053725154963, real_loss 0.020575815811753273
-> item_loss 0.21904747188091278 batch_loss 0.4630475891754031, real_loss 0.010952373966574669
-> item_loss 0.2919120192527771 batch_loss 0.45404045391631753, real_loss 0.01459560077637434
[2023-10-01 14:40:21,753] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.08490973711013794 batch_loss 0.43558391807600855, real_loss 0.004245487041771412
-> item_loss 0.05659879744052887 batch_loss 0.41753700756955714, real_loss 0.0028299398254603148
-> item_loss 0.13625502586364746 batch_loss 0.4047514629465612, real_loss 0.006812751293182373
-> item_loss 0.5192400217056274 batch_loss 0.40972922637086845, real_loss 0.5192400217056274
-> item_loss 0.03544502332806587 batch_loss 0.394134051244085, real_loss 0.0017722512129694223
-> item_loss 0.761165976524353 batch_loss 0.40881532825529576, real_loss 0.761165976524353
-> item_loss 0.10966269671916962 batch_loss 0.3973094578115986, real_loss 0.005483135115355253
-> item_loss 0.03297402337193489 batch_loss 0.3838155528323518, real_loss 0.0016487011453136802
-> item_loss 0.14865714311599731 batch_loss 0.37541703819962485, real_loss 0.007432857062667608
-> item_loss 0.04260411486029625 batch_loss 0.3639407304982687, real_loss 0.0021302057430148125
[2023-10-01 14:42:36,976] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.0637105405330658 batch_loss 0.3539330574994286, real_loss 0.003185527166351676
-> item_loss 0.07802854478359222 batch_loss 0.34503291192795, real_loss 0.0039014273788779974
-> item_loss 0.23792356252670288 batch_loss 0.341685744759161, real_loss 0.011896178126335144
-> item_loss 0.15462668240070343 batch_loss 0.33601728832405625, real_loss 0.007731334306299686
-> item_loss 0.08152928948402405 batch_loss 0.3285323471817024, real_loss 0.00407646456733346
-> item_loss 0.044481195509433746 batch_loss 0.32041659999106614, real_loss 0.0022240597754716873
-> item_loss 0.21401546895503998 batch_loss 0.3174610130178432, real_loss 0.010700773447751999
-> item_loss 0.06697483360767365 batch_loss 0.3106911162770278, real_loss 0.0033487416803836823
-> item_loss 0.17822666466236115 batch_loss 0.3072052096555892, real_loss 0.008911333046853542
-> item_loss 0.07874630391597748 batch_loss 0.30134728899559915, real_loss 0.003937315195798874
[2023-10-01 14:44:50,491] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.042215704917907715 batch_loss 0.29486899939365685, real_loss 0.002110785339027643
-> item_loss 0.1283731460571289 batch_loss 0.29080812492203423, real_loss 0.00641865748912096
-> item_loss 0.0944296345114708 batch_loss 0.28613244657892556, real_loss 0.004721481818705797
-> item_loss 0.16279372572898865 batch_loss 0.2832641042335782, real_loss 0.008139686658978462
-> item_loss 0.021469533443450928 batch_loss 0.27731422762471164, real_loss 0.0010734767420217395
-> item_loss 0.09064476937055588 batch_loss 0.273166017441286, real_loss 0.004532238468527794
-> item_loss 0.022289445623755455 batch_loss 0.26771217892351357, real_loss 0.0011144722811877728
-> item_loss 0.13130833208560944 batch_loss 0.2648099694163241, real_loss 0.006565416697412729
-> item_loss 0.07641474157571793 batch_loss 0.2608850688363115, real_loss 0.003820737125352025
-> item_loss 0.3846389353275299 batch_loss 0.2634106579483772, real_loss 0.019231947138905525
[2023-10-01 14:46:53,011] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.01859179511666298 batch_loss 0.25851428069174287, real_loss 0.0009295897907577455
-> item_loss 0.6676376461982727 batch_loss 0.26653630746638074, real_loss 0.6676376461982727
-> item_loss 0.23690909147262573 batch_loss 0.2659665533126547, real_loss 0.011845454573631287
-> item_loss 0.020202720537781715 batch_loss 0.26132949986407217, real_loss 0.0010101360967382789
-> item_loss 0.6412632465362549 batch_loss 0.2683653099876311, real_loss 0.6412632465362549
-> item_loss 0.09277581423521042 batch_loss 0.26517277370122344, real_loss 0.0046387906186282635
-> item_loss 0.051500458270311356 batch_loss 0.26135719663995716, real_loss 0.002575022866949439
-> item_loss 0.16904959082603455 batch_loss 0.2597377649590112, real_loss 0.008452479727566242
127.0.0.1 - - [01/Oct/2023 14:48:33] "POST /train/sft HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 14:49:06] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 14:49:46] "POST /train/tx-data HTTP/1.1" 200 28
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 253277.12it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 530094.45it/s]
127.0.0.1 - - [01/Oct/2023 14:50:00] "POST /inference/load-model HTTP/1.1" 200 26
None
None
192.168.0.103 - - [01/Oct/2023 14:51:22] "POST /state/reset HTTP/1.1" 200 21

10
192.168.0.103 - - [01/Oct/2023 14:51:42] "POST /inference/generate HTTP/1.1" 200 6018
None
None
192.168.0.103 - - [01/Oct/2023 14:52:43] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 14:52:57] "POST /inference/generate HTTP/1.1" 200 4456
None
None
192.168.0.103 - - [01/Oct/2023 14:54:00] "POST /state/reset HTTP/1.1" 200 21
AI192.168.0.103 - - [01/Oct/2023 14:54:12] "POST /inference/generate HTTP/1.1" 200 4583
None
None
192.168.0.103 - - [01/Oct/2023 14:54:16] "POST /state/reset HTTP/1.1" 200 21
AI  192.168.0.103 - - [01/Oct/2023 14:54:29] "POST /inference/generate HTTP/1.1" 200 5438
None
None
192.168.0.103 - - [01/Oct/2023 14:54:51] "POST /state/reset HTTP/1.1" 200 21

1. AC+4
2. 
3. 1d4+1AB-1
4. 
5. 
6. 
7. 
8. 
9. 
10. 
11. 
12. 
192.168.0.103 - - [01/Oct/2023 14:55:11] "POST /inference/generate HTTP/1.1" 200 5625
None
None
192.168.0.103 - - [01/Oct/2023 14:56:02] "POST /state/reset HTTP/1.1" 200 21

 nick 

```bash
[]{[]{
[][]{
\[\][]{
\]\]]
\]\]]{[]{
\]\]]\]{
\]\]]\]{
\]\]]\]{
\]]\]{
\]]\]{
\]]\]{
\]}\]{
[][]{
[][],[][],[],[],[],[][],[]\}[]{\}[]{

192.168.0.103 - - [01/Oct/2023 14:56:10] "POST /inference/generate HTTP/1.1" 200 1778
None
None
192.168.0.103 - - [01/Oct/2023 14:56:31] "POST /state/reset HTTP/1.1" 200 21
bot: 
192.168.0.103 - - [01/Oct/2023 14:56:34] "POST /inference/generate HTTP/1.1" 200 1221
None
None
192.168.0.103 - - [01/Oct/2023 14:56:40] "POST /state/reset HTTP/1.1" 200 21
master: 
192.168.0.103 - - [01/Oct/2023 14:56:43] "POST /inference/generate HTTP/1.1" 200 1879
None
None
192.168.0.103 - - [01/Oct/2023 14:56:47] "POST /state/reset HTTP/1.1" 200 21
bot: 
192.168.0.103 - - [01/Oct/2023 14:56:50] "POST /inference/generate HTTP/1.1" 200 2512
None
None
192.168.0.103 - - [01/Oct/2023 14:57:02] "POST /state/reset HTTP/1.1" 200 21
[2023-10-01 14:57:26,993] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
192.168.0.103 - - [01/Oct/2023 14:57:42] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 14:57:58] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 14:58:39] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 14:59:47] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 15:00:23] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 15:09:21] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 15:09:56] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 15:10:03] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 15:10:39] "POST /train/tx-data HTTP/1.1" 200 28
None
None
192.168.0.103 - - [01/Oct/2023 15:11:33] "POST /state/reset HTTP/1.1" 200 21
"https://www.baidu.com/" URL
192.168.0.103 - - [01/Oct/2023 15:11:38] "POST /inference/generate HTTP/1.1" 200 1497
None
None
192.168.0.103 - - [01/Oct/2023 15:12:38] "POST /state/reset HTTP/1.1" 200 21
wind
192.168.0.103 - - [01/Oct/2023 15:12:41] "POST /inference/generate HTTP/1.1" 200 1330
None
None
192.168.0.103 - - [01/Oct/2023 15:13:08] "POST /state/reset HTTP/1.1" 200 21
wind
192.168.0.103 - - [01/Oct/2023 15:13:15] "POST /inference/generate HTTP/1.1" 200 1416
None
None
192.168.0.103 - - [01/Oct/2023 15:13:33] "POST /state/reset HTTP/1.1" 200 21
+begin_action
+end_action192.168.0.103 - - [01/Oct/2023 15:13:35] "POST /inference/generate HTTP/1.1" 200 1291
None
None
192.168.0.103 - - [01/Oct/2023 15:13:55] "POST /state/reset HTTP/1.1" 200 21
#+begin_action
#+begin_action
#+end_action
192.168.0.103 - - [01/Oct/2023 15:13:57] "POST /inference/generate HTTP/1.1" 200 1314
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 22041.40it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 514562.59it/s]
127.0.0.1 - - [01/Oct/2023 15:14:19] "POST /inference/load-model HTTP/1.1" 200 26
None
None
192.168.0.103 - - [01/Oct/2023 15:14:29] "POST /state/reset HTTP/1.1" 200 21
#+begin_action
#+begin_action
#+end_action
192.168.0.103 - - [01/Oct/2023 15:14:32] "POST /inference/generate HTTP/1.1" 200 1314
None
None
192.168.0.103 - - [01/Oct/2023 15:14:51] "POST /state/reset HTTP/1.1" 200 21
#+begin_action
#+begin_action
192.168.0.103 - - [01/Oct/2023 15:14:53] "POST /inference/generate HTTP/1.1" 200 1313
None
None
192.168.0.103 - - [01/Oct/2023 15:15:06] "POST /state/reset HTTP/1.1" 200 21
 Google  "wind tuple"  "https://wwww.baidu.com/" 192.168.0.103 - - [01/Oct/2023 15:15:09] "POST /inference/generate HTTP/1.1" 200 1390
None
None
192.168.0.103 - - [01/Oct/2023 15:16:02] "POST /state/reset HTTP/1.1" 200 21
[2023-10-01 15:16:27,560] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
192.168.0.103 - - [01/Oct/2023 15:16:44] "POST /train/tx-data HTTP/1.1" 200 27
None
None
192.168.0.103 - - [01/Oct/2023 15:17:10] "POST /state/reset HTTP/1.1" 200 21
AI192.168.0.103 - - [01/Oct/2023 15:17:12] "POST /inference/generate HTTP/1.1" 200 1231
None
None
192.168.0.103 - - [01/Oct/2023 15:17:34] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 15:17:39] "POST /inference/generate HTTP/1.1" 200 2873
None
None
192.168.0.103 - - [01/Oct/2023 15:17:55] "POST /state/reset HTTP/1.1" 200 21
192.168.0.103 - - [01/Oct/2023 15:18:02] "POST /inference/generate HTTP/1.1" 200 4422
None
None
192.168.0.103 - - [01/Oct/2023 15:18:12] "POST /state/reset HTTP/1.1" 200 21

192.168.0.103 - - [01/Oct/2023 15:18:21] "POST /inference/generate HTTP/1.1" 200 5164
[2023-10-01 23:45:13,490] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-01 23:45:14,890] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-01 23:45:14,915] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-01 23:45:16,746] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-01 23:45:18,050] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-01 23:45:18,050] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-01 23:45:18,050] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-01 23:45:18,050] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-01 23:45:18,050] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-01 23:45:19,864] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_512_bf16/build.ninja...
Building extension module wkv_512_bf16...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512_bf16...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.511878252029419 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-01 23:46:48,413] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-01 23:46:48,413] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-01 23:46:48,413] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-01 23:46:56,374] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-01 23:46:56,376] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-01 23:46:56,376] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-01 23:46:56,415] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-01 23:46:56,415] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-01 23:46:56,416] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-10-01 23:46:56,416] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-01 23:46:56,416] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-01 23:46:56,416] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-01 23:46:56,416] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-01 23:47:29,963] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-01 23:47:29,964] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-01 23:47:29,964] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 36.56 GB, percent = 9.7%
[2023-10-01 23:48:02,232] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-01 23:48:02,233] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-01 23:48:02,233] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.4 GB, percent = 33.2%
[2023-10-01 23:48:02,233] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-01 23:48:03,368] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-01 23:48:03,369] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-01 23:48:03,369] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.4 GB, percent = 33.2%
[2023-10-01 23:48:03,391] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-01 23:48:03,391] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-01 23:48:03,391] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f95173f19d0>
[2023-10-01 23:48:03,391] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-01 23:48:03,393] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-01 23:48:03,393] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-01 23:48:03,393] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-01 23:48:03,393] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-01 23:48:03,393] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-01 23:48:03,393] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-01 23:48:03,393] [INFO] [config.py:971:print]   bfloat16_enabled ............. auto
[2023-10-01 23:48:03,393] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-01 23:48:03,393] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-01 23:48:03,393] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-01 23:48:03,393] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f94d5fc5a00>
[2023-10-01 23:48:03,393] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... None
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   fp16_auto_cast ............... None
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   fp16_enabled ................. False
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-01 23:48:03,394] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 1
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   loss_scale ................... 1.0
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-01 23:48:03,395] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-01 23:48:03,396] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-01 23:48:03,396] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-01 23:48:03,396] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-01 23:48:03,396] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-01 23:48:03,396] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-01 23:48:03,396] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-01 23:48:03,396] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-01 23:48:03,396] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-01 23:48:03,396] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-01 23:48:03,396] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-01 23:48:03,396] [INFO] [config.py:957:print_user_config]   json = {
    "bfloat16": {
        "enabled": "auto"
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/RWKV-Ouroboros/app.py", line 171, in save_weight
    torch.save(model.state_dict(),fpath)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/serialization.py", line 440, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/serialization.py", line 315, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/serialization.py", line 288, in __init__
    super().__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory /home/neromous/Documents/blackfog/resources/train-results/train-output does not exist.
127.0.0.1 - - [01/Oct/2023 23:48:40] "POST /train/save-weight HTTP/1.1" 500 756
[2023-10-01 23:49:46,355] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 12978
[2023-10-01 23:49:46,359] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-01 23:51:58,651] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-01 23:52:00,033] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-01 23:52:00,057] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-01 23:52:01,892] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-01 23:52:03,210] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-01 23:52:03,210] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-01 23:52:03,210] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-01 23:52:03,210] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-01 23:52:03,210] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-01 23:52:05,035] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_512_bf16/build.ninja...
Building extension module wkv_512_bf16...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_512_bf16...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.513381242752075 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-01 23:53:33,409] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-01 23:53:33,409] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-01 23:53:33,409] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-01 23:53:39,442] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-01 23:53:39,444] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-01 23:53:39,444] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-01 23:53:39,484] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-01 23:53:39,484] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-01 23:53:39,484] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-10-01 23:53:39,485] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-01 23:53:39,485] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-01 23:53:39,485] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-01 23:53:39,485] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-01 23:54:08,587] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-01 23:54:08,588] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-01 23:54:08,589] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 36.56 GB, percent = 9.7%
[2023-10-01 23:54:38,857] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-01 23:54:38,857] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-01 23:54:38,858] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.39 GB, percent = 33.2%
[2023-10-01 23:54:38,858] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-01 23:54:40,155] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-01 23:54:40,155] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-01 23:54:40,156] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.39 GB, percent = 33.2%
[2023-10-01 23:54:40,180] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-01 23:54:40,180] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-01 23:54:40,180] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f460e7af9a0>
[2023-10-01 23:54:40,180] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-01 23:54:40,182] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-01 23:54:40,182] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-01 23:54:40,182] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-01 23:54:40,182] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-01 23:54:40,182] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-01 23:54:40,182] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-01 23:54:40,182] [INFO] [config.py:971:print]   bfloat16_enabled ............. auto
[2023-10-01 23:54:40,182] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-01 23:54:40,182] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-01 23:54:40,182] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f45cf8879d0>
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... None
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   fp16_auto_cast ............... None
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   fp16_enabled ................. False
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-01 23:54:40,183] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-01 23:54:40,184] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-01 23:54:40,184] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-01 23:54:40,184] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 1
[2023-10-01 23:54:40,184] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-01 23:54:40,184] [INFO] [config.py:971:print]   loss_scale ................... 1.0
[2023-10-01 23:54:40,184] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-01 23:54:40,184] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-01 23:54:40,186] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-01 23:54:40,186] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-01 23:54:40,186] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-01 23:54:40,186] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-01 23:54:40,186] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-01 23:54:40,186] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-01 23:54:40,186] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-01 23:54:40,186] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-01 23:54:40,186] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-01 23:54:40,186] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-01 23:54:40,186] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-01 23:54:40,186] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-01 23:54:40,186] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-01 23:54:40,186] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-01 23:54:40,186] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-01 23:54:40,186] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-01 23:54:40,186] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-01 23:54:40,186] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-01 23:54:40,186] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-01 23:54:40,186] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-01 23:54:40,187] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-01 23:54:40,187] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-01 23:54:40,187] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-01 23:54:40,187] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-01 23:54:40,187] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-01 23:54:40,187] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-01 23:54:40,187] [INFO] [config.py:957:print_user_config]   json = {
    "bfloat16": {
        "enabled": "auto"
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

[2023-10-01 23:54:53,410] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 13601
[2023-10-01 23:54:53,411] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-02 00:00:08,895] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 00:00:10,281] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-02 00:00:10,306] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-02 00:00:12,145] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 00:00:13,461] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-02 00:00:13,461] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-02 00:00:13,461] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-02 00:00:13,461] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-02 00:00:13,461] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-02 00:00:15,295] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Traceback (most recent call last):
  File "/home/neromous/RWKV-Ouroboros/app.py", line 43, in <module>
    from rwkv_model.model_lora import RWKV
  File "/home/neromous/RWKV-Ouroboros/rwkv_model/model_lora.py", line 54, in <module>
    T_MAX = int(os.environ["RWKV_T_MAX"])  # TAKES LOTS OF VRAM!
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/os.py", line 679, in __getitem__
    raise KeyError(key) from None
KeyError: 'RWKV_T_MAX'
[2023-10-02 00:00:17,474] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 14731
[2023-10-02 00:00:17,475] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = 1
[2023-10-02 00:01:03,933] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 00:01:05,321] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-02 00:01:05,345] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-02 00:01:07,184] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 00:01:08,499] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-02 00:01:08,499] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-02 00:01:08,499] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-02 00:01:08,499] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-02 00:01:08,499] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-02 00:01:10,331] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Creating extension directory /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_2048 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/TH -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/neromous/.anaconda3/envs/blackfog/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=2048 -std=c++17 -c /home/neromous/RWKV-Ouroboros/cuda_state/wkv_cuda.cu -o wkv_cuda.cuda.o 
ptxas info    : 0 bytes gmem
ptxas info    : Compiling entry function '_Z15kernel_backwardIfEviiiPKT_S2_S2_S2_S2_S2_PS0_S3_S3_S3_' for 'sm_86'
ptxas info    : Function properties for _Z15kernel_backwardIfEviiiPKT_S2_S2_S2_S2_S2_PS0_S3_S3_S3_
    16384 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 56 registers, 448 bytes cmem[0], 16 bytes cmem[2]
ptxas info    : Compiling entry function '_Z14kernel_forwardIfEviiiPKT_S2_S2_S2_PS0_' for 'sm_86'
ptxas info    : Function properties for _Z14kernel_forwardIfEviiiPKT_S2_S2_S2_PS0_
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 40 registers, 408 bytes cmem[0]
[2/3] c++ -MMD -MF wkv_op.o.d -DTORCH_EXTENSION_NAME=wkv_2048 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/TH -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/neromous/.anaconda3/envs/blackfog/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home/neromous/RWKV-Ouroboros/cuda_state/wkv_op.cpp -o wkv_op.o 
[3/3] c++ wkv_op.o wkv_cuda.cuda.o -shared -L/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_2048.so
Loading extension module wkv_2048...
Traceback (most recent call last):
  File "/home/neromous/RWKV-Ouroboros/app.py", line 45, in <module>
    from rwkv_model.model_lora import RWKV
  File "/home/neromous/RWKV-Ouroboros/rwkv_model/model_lora.py", line 294, in <module>
    if config['trainer']['ctx_parts'] > 0: #state
NameError: name 'config' is not defined
[2023-10-02 00:01:27,531] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 15004
[2023-10-02 00:01:27,532] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = 1
[2023-10-02 00:02:43,400] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 00:02:44,783] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-02 00:02:44,808] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-02 00:02:46,638] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 00:02:47,961] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-02 00:02:47,961] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-02 00:02:47,961] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-02 00:02:47,961] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-02 00:02:47,961] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-02 00:02:49,804] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Traceback (most recent call last):
  File "/home/neromous/RWKV-Ouroboros/app.py", line 45, in <module>
    from rwkv_model.model_lora import RWKV
  File "/home/neromous/RWKV-Ouroboros/rwkv_model/model_lora.py", line 294, in <module>
    if os.environ['RWKV_PARTS'] != "0": #state
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/os.py", line 679, in __getitem__
    raise KeyError(key) from None
KeyError: 'RWKV_PARTS'
[2023-10-02 00:02:52,975] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 15348
[2023-10-02 00:02:52,981] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = 1
[2023-10-02 00:05:07,897] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 00:05:09,299] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-02 00:05:09,323] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-02 00:05:11,139] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 00:05:12,452] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-02 00:05:12,452] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-02 00:05:12,453] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-02 00:05:12,453] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-02 00:05:12,453] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-02 00:05:14,315] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Creating extension directory /home/neromous/.cache/torch_extensions/py39_cu117/wkv_-2046...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_-2046/build.ninja...
Building extension module wkv_-2046...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_-2046 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/TH -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/neromous/.anaconda3/envs/blackfog/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=-2046 -std=c++17 -c /home/neromous/RWKV-Ouroboros/cuda_state/wkv_cuda.cu -o wkv_cuda.cuda.o 
FAILED: wkv_cuda.cuda.o 
/usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_-2046 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/TH -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/neromous/.anaconda3/envs/blackfog/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=-2046 -std=c++17 -c /home/neromous/RWKV-Ouroboros/cuda_state/wkv_cuda.cu -o wkv_cuda.cuda.o 
/home/neromous/RWKV-Ouroboros/cuda_state/wkv_cuda.cu(63): error: the size of an array must be greater than zero

/home/neromous/RWKV-Ouroboros/cuda_state/wkv_cuda.cu(63): error: the size of an array must be greater than zero

2 errors detected in the compilation of "/home/neromous/RWKV-Ouroboros/cuda_state/wkv_cuda.cu".
[2/3] c++ -MMD -MF wkv_op.o.d -DTORCH_EXTENSION_NAME=wkv_-2046 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/TH -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/neromous/.anaconda3/envs/blackfog/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home/neromous/RWKV-Ouroboros/cuda_state/wkv_op.cpp -o wkv_op.o 
FAILED: wkv_op.o 
c++ -MMD -MF wkv_op.o.d -DTORCH_EXTENSION_NAME=wkv_-2046 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/TH -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/neromous/.anaconda3/envs/blackfog/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home/neromous/RWKV-Ouroboros/cuda_state/wkv_op.cpp -o wkv_op.o 
In file included from /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/pybind11/attr.h:13,
                 from /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/pybind11/detail/class.h:12,
                 from /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/pybind11/pybind11.h:13,
                 from /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/torch/csrc/Exceptions.h:14,
                 from /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/python.h:11,
                 from /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/torch/extension.h:6,
                 from /home/neromous/RWKV-Ouroboros/cuda_state/wkv_op.cpp:1:
<command-line>: error: expected initializer before - token
/home/neromous/RWKV-Ouroboros/cuda_state/wkv_op.cpp:13:17: note: in expansion of macro TORCH_EXTENSION_NAME
   13 | PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
      |                 ^~~~~~~~~~~~~~~~~~~~
<command-line>: error: expected initializer before - token
/home/neromous/RWKV-Ouroboros/cuda_state/wkv_op.cpp:13:17: note: in expansion of macro TORCH_EXTENSION_NAME
   13 | PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
      |                 ^~~~~~~~~~~~~~~~~~~~
<command-line>: error: expected initializer before - token
/home/neromous/RWKV-Ouroboros/cuda_state/wkv_op.cpp:13:17: note: in expansion of macro TORCH_EXTENSION_NAME
   13 | PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
      |                 ^~~~~~~~~~~~~~~~~~~~
<command-line>: error: expected initializer before - token
/home/neromous/RWKV-Ouroboros/cuda_state/wkv_op.cpp:13:17: note: in expansion of macro TORCH_EXTENSION_NAME
   13 | PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
      |                 ^~~~~~~~~~~~~~~~~~~~
<command-line>: error: expected initializer before - token
/home/neromous/RWKV-Ouroboros/cuda_state/wkv_op.cpp:13:17: note: in expansion of macro TORCH_EXTENSION_NAME
   13 | PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
      |                 ^~~~~~~~~~~~~~~~~~~~
ninja: build stopped: subcommand failed.
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/utils/cpp_extension.py", line 1893, in _run_ninja_build
    subprocess.run(
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/subprocess.py", line 528, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/neromous/RWKV-Ouroboros/app.py", line 45, in <module>
    from rwkv_model.model_lora import RWKV
  File "/home/neromous/RWKV-Ouroboros/rwkv_model/model_lora.py", line 223, in <module>
    wkv_cuda = load(name=f"wkv_{T_MAX}", sources=["cuda_state/wkv_op.cpp",
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/utils/cpp_extension.py", line 1284, in load
    return _jit_compile(
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/utils/cpp_extension.py", line 1509, in _jit_compile
    _write_ninja_file_and_build_library(
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/utils/cpp_extension.py", line 1624, in _write_ninja_file_and_build_library
    _run_ninja_build(
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/utils/cpp_extension.py", line 1909, in _run_ninja_build
    raise RuntimeError(message) from e
RuntimeError: Error building extension 'wkv_-2046'
[2023-10-02 00:05:29,482] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 16577
[2023-10-02 00:05:29,483] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = 1
[2023-10-02 00:05:49,613] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 00:05:51,006] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-02 00:05:51,030] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-02 00:05:52,871] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 00:05:54,203] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-02 00:05:54,203] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-02 00:05:54,203] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-02 00:05:54,203] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-02 00:05:54,203] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-02 00:05:56,041] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Traceback (most recent call last):
  File "/home/neromous/RWKV-Ouroboros/app.py", line 45, in <module>
    from rwkv_model.model_lora import RWKV
  File "/home/neromous/RWKV-Ouroboros/rwkv_model/model_lora.py", line 294, in <module>
    if os.environ['RWKV_PARTS'] != "0": #state
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/os.py", line 679, in __getitem__
    raise KeyError(key) from None
KeyError: 'RWKV_PARTS'
[2023-10-02 00:05:59,217] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 16855
[2023-10-02 00:05:59,217] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = 1
[2023-10-02 00:07:09,625] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 00:07:11,013] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-02 00:07:11,037] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-02 00:07:12,879] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 00:07:14,195] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-02 00:07:14,195] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-02 00:07:14,195] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-02 00:07:14,195] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-02 00:07:14,195] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-02 00:07:16,036] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.5259082317352295 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-02 00:08:42,914] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-02 00:08:42,914] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-02 00:08:42,914] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-02 00:08:49,631] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-02 00:08:49,633] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-02 00:08:49,633] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-02 00:08:49,674] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-02 00:08:49,674] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-02 00:08:49,674] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-10-02 00:08:49,674] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-02 00:08:49,674] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-02 00:08:49,674] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-02 00:08:49,674] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-02 00:09:21,821] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-02 00:09:21,822] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-02 00:09:21,822] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 36.54 GB, percent = 9.7%
[2023-10-02 00:09:54,216] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-02 00:09:54,217] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-02 00:09:54,217] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.41 GB, percent = 33.2%
[2023-10-02 00:09:54,217] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-02 00:09:55,054] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-02 00:09:55,054] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-02 00:09:55,055] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.41 GB, percent = 33.2%
[2023-10-02 00:09:55,084] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-02 00:09:55,084] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-02 00:09:55,084] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f5212e9bbb0>
[2023-10-02 00:09:55,085] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-02 00:09:55,086] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-02 00:09:55,086] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-02 00:09:55,086] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-02 00:09:55,086] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-02 00:09:55,086] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-02 00:09:55,086] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-02 00:09:55,086] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-02 00:09:55,086] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-02 00:09:55,086] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-02 00:09:55,086] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-02 00:09:55,086] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f51f1a17e80>
[2023-10-02 00:09:55,086] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-02 00:09:55,086] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   fp16_enabled ................. auto
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-02 00:09:55,087] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-02 00:09:55,088] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-02 00:09:55,089] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-02 00:09:55,089] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-02 00:09:55,089] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-02 00:09:55,089] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-02 00:09:55,089] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-02 00:09:55,089] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-02 00:09:55,089] [INFO] [config.py:957:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

===saveved====
127.0.0.1 - - [02/Oct/2023 00:10:28] "POST /train/save-weight HTTP/1.1" 200 26
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/51 [00:00<?, ?it/s][2023-10-02 00:10:35,315] [INFO] [checkpointing.py:530:forward] Activation Checkpointing Information
[2023-10-02 00:10:35,315] [INFO] [checkpointing.py:531:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-10-02 00:10:35,315] [INFO] [checkpointing.py:532:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-10-02 00:10:35,315] [INFO] [checkpointing.py:534:forward] ----Synchronization False
[2023-10-02 00:10:35,315] [INFO] [checkpointing.py:535:forward] ----Profiling time in checkpointing False
[2023-10-02 00:10:41,673] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
-> item_loss 2.029296875 batch_loss 2.029296875, real_loss 2.130859375
[2023-10-02 00:10:45,663] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
-> item_loss 1.8623046875 batch_loss 1.94580078125, real_loss 1.955078125
-> item_loss 2.0234375 batch_loss 1.9716796875, real_loss 2.125
-> item_loss 1.9404296875 batch_loss 1.9638671875, real_loss 2.037109375
  2%|         | 1/51 [00:46<38:36, 46.34s/it]-> item_loss 2.14453125 batch_loss 2.0, real_loss 2.251953125
  4%|         | 2/51 [00:59<21:54, 26.84s/it]-> item_loss 1.771484375 batch_loss 1.9619140625, real_loss 1.8603515625
  6%|         | 3/51 [01:12<16:29, 20.61s/it][2023-10-02 00:11:51,612] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
-> item_loss 1.8681640625 batch_loss 1.9485212053571428, real_loss 1.9619140625
  8%|         | 4/51 [01:16<10:52, 13.89s/it]-> item_loss 1.8056640625 batch_loss 1.9306640625, real_loss 1.8955078125
 10%|         | 5/51 [01:30<10:37, 13.85s/it]-> item_loss 2.703125 batch_loss 2.0164930555555554, real_loss 2.837890625
[2023-10-02 00:12:33,130] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=3, lr=[8.000000000000001e-06, 8.000000000000001e-06, 8.000000000000001e-06], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 2.48828125 batch_loss 2.063671875, real_loss 2.61328125
-> item_loss 2.103515625 batch_loss 2.067294034090909, real_loss 2.208984375
-> item_loss 1.9248046875 batch_loss 2.055419921875, real_loss 2.021484375
 12%|        | 6/51 [02:25<20:59, 27.99s/it]-> item_loss 1.5205078125 batch_loss 2.0142728365384617, real_loss 1.5966796875
[2023-10-02 00:13:16,397] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192
-> item_loss 3.220703125 batch_loss 2.1004464285714284, real_loss 3.380859375
 14%|        | 7/51 [02:41<17:32, 23.93s/it]-> item_loss 1.828125 batch_loss 2.082291666666667, real_loss 1.919921875
 16%|        | 8/51 [02:53<14:38, 20.42s/it]-> item_loss 1.0927734375 batch_loss 2.02044677734375, real_loss 1.0927734375
 18%|        | 9/51 [03:05<12:24, 17.72s/it]-> item_loss 1.3125 batch_loss 1.9788028492647058, real_loss 1.3125
-> item_loss 1.4697265625 batch_loss 1.9505208333333333, real_loss 1.4697265625
-> item_loss 1.6904296875 batch_loss 1.9368318256578947, real_loss 1.775390625
 20%|        | 10/51 [03:45<16:41, 24.43s/it][2023-10-02 00:14:33,786] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.359375 batch_loss 1.907958984375, real_loss 1.359375
-> item_loss 2.134765625 batch_loss 1.9187593005952381, real_loss 2.2421875
 22%|       | 11/51 [04:10<16:30, 24.75s/it]-> item_loss 1.482421875 batch_loss 1.89892578125, real_loss 1.482421875
 24%|       | 12/51 [04:23<13:40, 21.03s/it]-> item_loss 1.6689453125 batch_loss 1.8889266304347827, real_loss 1.751953125
 25%|       | 13/51 [04:36<11:48, 18.64s/it]-> item_loss 1.7392578125 batch_loss 1.8826904296875, real_loss 1.826171875
-> item_loss 1.931640625 batch_loss 1.8846484375, real_loss 2.02734375
-> item_loss 1.728515625 batch_loss 1.8786433293269231, real_loss 1.814453125
-> item_loss 1.3330078125 batch_loss 1.8584346064814814, real_loss 1.3330078125
 27%|       | 14/51 [05:30<18:01, 29.24s/it]-> item_loss 1.6767578125 batch_loss 1.8519461495535714, real_loss 1.7607421875
 29%|       | 15/51 [05:42<14:33, 24.26s/it]-> item_loss 1.666015625 batch_loss 1.8455347521551724, real_loss 1.7490234375
 31%|      | 16/51 [05:55<12:09, 20.84s/it][2023-10-02 00:16:44,506] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.544921875 batch_loss 1.8355143229166666, real_loss 1.6220703125
 33%|      | 17/51 [06:09<10:33, 18.62s/it]-> item_loss 1.642578125 batch_loss 1.8292905745967742, real_loss 1.724609375
-> item_loss 3.01171875 batch_loss 1.866241455078125, real_loss 3.162109375
 35%|      | 18/51 [06:35<11:32, 20.97s/it]-> item_loss 1.310546875 batch_loss 1.8494022253787878, real_loss 1.310546875
 37%|      | 19/51 [06:47<09:47, 18.35s/it]-> item_loss 2.068359375 batch_loss 1.8558421415441178, real_loss 2.171875
-> item_loss 2.107421875 batch_loss 1.8630301339285715, real_loss 2.212890625
 39%|      | 20/51 [07:13<10:40, 20.67s/it]-> item_loss 1.55078125 batch_loss 1.8543565538194444, real_loss 1.6279296875
 41%|      | 21/51 [07:26<09:07, 18.25s/it]-> item_loss 1.4873046875 batch_loss 1.8444362331081081, real_loss 1.4873046875
 43%|     | 22/51 [07:39<08:00, 16.58s/it]-> item_loss 1.2392578125 batch_loss 1.8285104851973684, real_loss 1.2392578125
 45%|     | 23/51 [07:52<07:12, 15.46s/it]-> item_loss 1.251953125 batch_loss 1.8137269631410255, real_loss 1.251953125
 47%|     | 24/51 [08:05<06:40, 14.82s/it][2023-10-02 00:18:53,671] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.61328125 batch_loss 1.8087158203125, real_loss 1.6943359375
 49%|     | 25/51 [08:18<06:10, 14.26s/it]-> item_loss 2.0078125 batch_loss 1.8135718368902438, real_loss 2.107421875
 51%|     | 26/51 [08:30<05:43, 13.72s/it]-> item_loss 1.470703125 batch_loss 1.8054082961309523, real_loss 1.470703125
 53%|    | 27/51 [08:43<05:21, 13.41s/it]-> item_loss 1.6728515625 batch_loss 1.802325581395349, real_loss 1.7568359375
-> item_loss 1.6435546875 batch_loss 1.7987171519886365, real_loss 1.7255859375
 55%|    | 28/51 [09:09<06:35, 17.21s/it]-> item_loss 1.48828125 batch_loss 1.791818576388889, real_loss 1.48828125
 57%|    | 29/51 [09:22<05:48, 15.83s/it]-> item_loss 1.23046875 batch_loss 1.7796153192934783, real_loss 1.23046875
 59%|    | 30/51 [09:34<05:08, 14.71s/it]-> item_loss 1.662109375 batch_loss 1.7771151928191489, real_loss 1.7451171875
 61%|    | 31/51 [09:46<04:40, 14.03s/it]-> item_loss 1.8134765625 batch_loss 1.7778727213541667, real_loss 1.904296875
 63%|   | 32/51 [09:59<04:18, 13.61s/it]-> item_loss 1.75 batch_loss 1.7773038903061225, real_loss 1.837890625
 65%|   | 33/51 [10:11<03:56, 13.14s/it][2023-10-02 00:21:00,034] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.841796875 batch_loss 1.77859375, real_loss 1.93359375
-> item_loss 1.8583984375 batch_loss 1.7801585477941178, real_loss 1.951171875
 67%|   | 34/51 [10:37<04:50, 17.08s/it]-> item_loss 1.513671875 batch_loss 1.7750338040865385, real_loss 1.5888671875
 69%|   | 35/51 [10:49<04:09, 15.60s/it]-> item_loss 1.44921875 batch_loss 1.768886350235849, real_loss 1.44921875
 71%|   | 36/51 [11:02<03:38, 14.59s/it]-> item_loss 1.478515625 batch_loss 1.7635091145833333, real_loss 1.478515625
 73%|  | 37/51 [11:14<03:14, 13.92s/it]-> item_loss 1.70703125 batch_loss 1.7624822443181818, real_loss 1.7919921875
 75%|  | 38/51 [11:27<02:56, 13.58s/it]-> item_loss 1.7841796875 batch_loss 1.7628696986607142, real_loss 1.873046875
-> item_loss 2.27734375 batch_loss 1.7718955592105263, real_loss 2.390625
 76%|  | 39/51 [11:53<03:28, 17.38s/it]-> item_loss 1.8232421875 batch_loss 1.7727808459051724, real_loss 1.9140625
 78%|  | 40/51 [12:05<02:53, 15.75s/it]-> item_loss 1.8623046875 batch_loss 1.7742981991525424, real_loss 1.955078125
[2023-10-02 00:23:06,963] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 2.49609375 batch_loss 1.786328125, real_loss 2.62109375
-> item_loss 2.169921875 batch_loss 1.7926165471311475, real_loss 2.279296875
-> item_loss 3.685546875 batch_loss 1.823147681451613, real_loss 3.869140625
 80%|  | 41/51 [12:56<04:23, 26.33s/it]-> item_loss 1.466796875 batch_loss 1.8174913194444444, real_loss 1.466796875
 82%| | 42/51 [13:08<03:18, 22.06s/it]-> item_loss 0.98291015625 batch_loss 1.8044509887695312, real_loss 0.98291015625
 84%| | 43/51 [13:21<02:33, 19.21s/it]-> item_loss 1.4892578125 batch_loss 1.7996018629807693, real_loss 1.4892578125
-> item_loss 1.5029296875 batch_loss 1.7951068300189394, real_loss 1.578125
 86%| | 44/51 [13:47<02:30, 21.47s/it]-> item_loss 1.6240234375 batch_loss 1.7925533465485075, real_loss 1.705078125
-> item_loss 1.6640625 batch_loss 1.7906637752757353, real_loss 1.7470703125
 88%| | 45/51 [14:13<02:16, 22.74s/it]-> item_loss 1.53125 batch_loss 1.7869041553442029, real_loss 1.607421875
 90%| | 46/51 [14:25<01:38, 19.61s/it][2023-10-02 00:25:14,226] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 2.072265625 batch_loss 1.7909807477678572, real_loss 2.17578125
-> item_loss 1.9326171875 batch_loss 1.7929756272007042, real_loss 2.029296875
-> item_loss 2.15625 batch_loss 1.7980211046006944, real_loss 2.263671875
 92%|| 47/51 [15:04<01:40, 25.18s/it]-> item_loss 0.9169921875 batch_loss 1.7859522153253424, real_loss 0.9169921875
 94%|| 48/51 [15:16<01:03, 21.32s/it]-> item_loss 1.494140625 batch_loss 1.7820088154560811, real_loss 1.494140625
 96%|| 49/51 [15:28<00:37, 18.55s/it]-> item_loss 1.408203125 batch_loss 1.7770247395833334, real_loss 1.408203125
 98%|| 50/51 [15:41<00:16, 16.87s/it]-> item_loss 1.5087890625 batch_loss 1.7734953227796053, real_loss 1.583984375
100%|| 51/51 [15:54<00:00, 15.60s/it]100%|| 51/51 [15:54<00:00, 18.71s/it]
127.0.0.1 - - [02/Oct/2023 00:26:30] "POST /train/sft HTTP/1.1" 200 28
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/51 [00:00<?, ?it/s]-> item_loss 1.8125 batch_loss 1.8125, real_loss 1.9033203125
-> item_loss 1.6591796875 batch_loss 1.73583984375, real_loss 1.7421875
-> item_loss 1.53515625 batch_loss 1.6689453125, real_loss 1.6123046875
[2023-10-02 00:27:23,706] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.404296875 batch_loss 1.602783203125, real_loss 1.404296875
  2%|         | 1/51 [00:52<43:27, 52.16s/it]-> item_loss 0.81884765625 batch_loss 1.44599609375, real_loss 0.81884765625
  4%|         | 2/51 [01:04<23:21, 28.59s/it]-> item_loss 1.3056640625 batch_loss 1.422607421875, real_loss 1.3056640625
  6%|         | 3/51 [01:16<17:00, 21.26s/it]-> item_loss 0.75146484375 batch_loss 1.3267299107142858, real_loss 0.75146484375
  8%|         | 4/51 [01:29<13:58, 17.84s/it]-> item_loss 1.033203125 batch_loss 1.2900390625, real_loss 1.033203125
 10%|         | 5/51 [01:41<12:13, 15.95s/it]-> item_loss 1.38671875 batch_loss 1.30078125, real_loss 1.38671875
 12%|        | 6/51 [01:54<11:06, 14.82s/it]-> item_loss 1.1484375 batch_loss 1.285546875, real_loss 1.1484375
 14%|        | 7/51 [02:07<10:22, 14.14s/it]-> item_loss 1.4892578125 batch_loss 1.3040660511363635, real_loss 1.4892578125
-> item_loss 1.2685546875 batch_loss 1.3011067708333333, real_loss 1.2685546875
 16%|        | 8/51 [02:32<12:35, 17.57s/it]-> item_loss 1.099609375 batch_loss 1.2856069711538463, real_loss 1.099609375
 18%|        | 9/51 [02:44<11:13, 16.04s/it][2023-10-02 00:29:28,782] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.794921875 batch_loss 1.2505580357142858, real_loss 0.794921875
 20%|        | 10/51 [02:57<10:10, 14.88s/it]-> item_loss 1.935546875 batch_loss 1.2962239583333333, real_loss 2.033203125
-> item_loss 1.7060546875 batch_loss 1.32183837890625, real_loss 1.791015625
-> item_loss 1.486328125 batch_loss 1.3315142463235294, real_loss 1.486328125
-> item_loss 1.255859375 batch_loss 1.3273111979166667, real_loss 1.255859375
 22%|       | 11/51 [03:49<17:38, 26.46s/it]-> item_loss 1.634765625 batch_loss 1.343493009868421, real_loss 1.716796875
 24%|       | 12/51 [04:02<14:32, 22.36s/it]-> item_loss 1.0703125 batch_loss 1.329833984375, real_loss 1.0703125
 25%|       | 13/51 [04:15<12:18, 19.43s/it]-> item_loss 1.146484375 batch_loss 1.3211030505952381, real_loss 1.146484375
 27%|       | 14/51 [04:27<10:39, 17.29s/it]-> item_loss 0.9462890625 batch_loss 1.3040660511363635, real_loss 0.9462890625
 29%|       | 15/51 [04:40<09:27, 15.77s/it]-> item_loss 0.88623046875 batch_loss 1.2858992866847827, real_loss 0.88623046875
 31%|      | 16/51 [04:52<08:35, 14.72s/it][2023-10-02 00:31:36,450] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.0625 batch_loss 1.2765909830729167, real_loss 1.0625
 33%|      | 17/51 [05:04<07:56, 14.03s/it]-> item_loss 0.96630859375 batch_loss 1.2641796875, real_loss 0.96630859375
 35%|      | 18/51 [05:16<07:23, 13.43s/it]-> item_loss 1.2275390625 batch_loss 1.2627704326923077, real_loss 1.2275390625
 37%|      | 19/51 [05:29<06:57, 13.03s/it]-> item_loss 0.72412109375 batch_loss 1.2428204571759258, real_loss 0.72412109375
 39%|      | 20/51 [05:41<06:37, 12.83s/it]-> item_loss 1.4033203125 batch_loss 1.2485525948660714, real_loss 1.4033203125
-> item_loss 1.8779296875 batch_loss 1.2702552532327587, real_loss 1.9716796875
 41%|      | 21/51 [06:06<08:17, 16.60s/it]-> item_loss 0.96142578125 batch_loss 1.2599609375, real_loss 0.96142578125
-> item_loss 1.427734375 batch_loss 1.2653729838709677, real_loss 1.427734375
 43%|     | 22/51 [06:31<09:13, 19.07s/it]-> item_loss 1.4365234375 batch_loss 1.270721435546875, real_loss 1.4365234375
-> item_loss 1.3837890625 batch_loss 1.2741477272727273, real_loss 1.3837890625
[2023-10-02 00:33:41,855] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.349609375 batch_loss 1.2763671875, real_loss 1.349609375
 45%|     | 23/51 [07:10<11:38, 24.96s/it]-> item_loss 0.921875 batch_loss 1.2662388392857142, real_loss 0.921875
 47%|     | 24/51 [07:22<09:29, 21.08s/it]-> item_loss 1.3623046875 batch_loss 1.2689073350694444, real_loss 1.3623046875
-> item_loss 1.5986328125 batch_loss 1.2778188344594594, real_loss 1.6787109375
 49%|     | 25/51 [07:47<09:43, 22.44s/it]-> item_loss 1.2890625 batch_loss 1.278114720394737, real_loss 1.2890625
 51%|     | 26/51 [08:00<08:05, 19.43s/it]-> item_loss 1.439453125 batch_loss 1.2822516025641026, real_loss 1.439453125
 53%|    | 27/51 [08:13<06:57, 17.42s/it]-> item_loss 1.1484375 batch_loss 1.27890625, real_loss 1.1484375
-> item_loss 2.234375 batch_loss 1.3022103658536586, real_loss 2.345703125
 55%|    | 28/51 [08:38<07:34, 19.77s/it]-> item_loss 1.3037109375 batch_loss 1.30224609375, real_loss 1.3037109375
-> item_loss 1.408203125 batch_loss 1.304710210755814, real_loss 1.408203125
[2023-10-02 00:35:48,655] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.2041015625 batch_loss 1.3024236505681819, real_loss 1.2041015625
-> item_loss 0.76171875 batch_loss 1.290407986111111, real_loss 0.76171875
 57%|    | 29/51 [09:29<10:39, 29.09s/it]-> item_loss 0.395751953125 batch_loss 1.270958941915761, real_loss 0.0197906494140625
 59%|    | 30/51 [09:40<08:21, 23.88s/it]-> item_loss 1.3017578125 batch_loss 1.2716142370345744, real_loss 1.3017578125
 61%|    | 31/51 [09:52<06:46, 20.33s/it]-> item_loss 1.2451171875 batch_loss 1.2710622151692708, real_loss 1.2451171875
-> item_loss 1.146484375 batch_loss 1.2685198102678572, real_loss 1.146484375
 63%|   | 32/51 [10:18<06:55, 21.89s/it]-> item_loss 0.85791015625 batch_loss 1.2603076171875, real_loss 0.85791015625
-> item_loss 0.880859375 batch_loss 1.2528674555759804, real_loss 0.880859375
-> item_loss 0.7890625 batch_loss 1.2439481295072115, real_loss 0.7890625
 65%|   | 33/51 [10:56<08:03, 26.84s/it]-> item_loss 1.4990234375 batch_loss 1.248760871167453, real_loss 1.4990234375
[2023-10-02 00:37:54,094] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.5166015625 batch_loss 1.2537208839699074, real_loss 1.5927734375
 67%|   | 34/51 [11:22<07:30, 26.49s/it]-> item_loss 0.6435546875 batch_loss 1.242626953125, real_loss 0.6435546875
 69%|   | 35/51 [11:34<05:54, 22.14s/it]-> item_loss 1.1533203125 batch_loss 1.2410321916852678, real_loss 1.1533203125
 71%|   | 36/51 [11:47<04:48, 19.26s/it]-> item_loss 1.1083984375 batch_loss 1.2387052837171053, real_loss 1.1083984375
 73%|  | 37/51 [11:59<04:00, 17.18s/it]-> item_loss 1.0537109375 batch_loss 1.2355157260237069, real_loss 1.0537109375
 75%|  | 38/51 [12:11<03:23, 15.63s/it]-> item_loss 1.3056640625 batch_loss 1.2367046808792372, real_loss 1.3056640625
-> item_loss 1.763671875 batch_loss 1.2454874674479166, real_loss 1.8515625
-> item_loss 1.5361328125 batch_loss 1.2502521452356556, real_loss 1.61328125
-> item_loss 1.759765625 batch_loss 1.2584701045866935, real_loss 1.84765625
 76%|  | 39/51 [13:03<05:19, 26.58s/it]-> item_loss 0.98876953125 batch_loss 1.2541891431051588, real_loss 0.98876953125
 78%|  | 40/51 [13:15<04:04, 22.24s/it][2023-10-02 00:39:59,420] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.0283203125 batch_loss 1.2506599426269531, real_loss 1.0283203125
 80%|  | 41/51 [13:27<03:12, 19.23s/it]-> item_loss 1.0810546875 batch_loss 1.2480506310096153, real_loss 1.0810546875
 82%| | 42/51 [13:40<02:34, 17.11s/it]-> item_loss 0.9873046875 batch_loss 1.2440999348958333, real_loss 0.9873046875
 84%| | 43/51 [13:52<02:05, 15.72s/it]-> item_loss 0.9755859375 batch_loss 1.2400922632929106, real_loss 0.9755859375
 86%| | 44/51 [14:04<01:42, 14.69s/it]-> item_loss 1.283203125 batch_loss 1.2407262465533089, real_loss 1.283203125
-> item_loss 1.18359375 batch_loss 1.239898239356884, real_loss 1.18359375
 88%| | 45/51 [14:30<01:48, 18.07s/it]-> item_loss 1.318359375 batch_loss 1.2410191127232142, real_loss 1.318359375
-> item_loss 1.1376953125 batch_loss 1.239563847931338, real_loss 1.1376953125
 90%| | 46/51 [14:57<01:42, 20.59s/it]-> item_loss 0.98876953125 batch_loss 1.2360805935329862, real_loss 0.98876953125
 92%|| 47/51 [15:09<01:12, 18.15s/it]-> item_loss 1.5927734375 batch_loss 1.240966796875, real_loss 1.6728515625
 94%|| 48/51 [15:22<00:49, 16.55s/it][2023-10-02 00:42:06,356] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.2470703125 batch_loss 1.2410492768158783, real_loss 1.2470703125
 96%|| 49/51 [15:34<00:30, 15.28s/it]-> item_loss 1.0244140625 batch_loss 1.2381608072916668, real_loss 1.0244140625
 98%|| 50/51 [15:46<00:14, 14.30s/it]-> item_loss 0.67041015625 batch_loss 1.230690403988487, real_loss 0.67041015625
100%|| 51/51 [15:59<00:00, 13.71s/it]100%|| 51/51 [15:59<00:00, 18.81s/it]
127.0.0.1 - - [02/Oct/2023 00:42:31] "POST /train/sft HTTP/1.1" 200 27
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/51 [00:00<?, ?it/s]-> item_loss 1.58203125 batch_loss 1.58203125, real_loss 1.6611328125
-> item_loss 1.2978515625 batch_loss 1.43994140625, real_loss 1.2978515625
-> item_loss 1.1103515625 batch_loss 1.330078125, real_loss 1.1103515625
-> item_loss 0.8427734375 batch_loss 1.208251953125, real_loss 0.8427734375
  2%|         | 1/51 [00:52<43:26, 52.14s/it]-> item_loss 0.71923828125 batch_loss 1.11044921875, real_loss 0.71923828125
  4%|         | 2/51 [01:04<23:31, 28.80s/it]-> item_loss 0.9404296875 batch_loss 1.0821126302083333, real_loss 0.9404296875
-> item_loss 1.2236328125 batch_loss 1.1023297991071428, real_loss 1.2236328125
[2023-10-02 00:44:17,721] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.0439453125 batch_loss 1.09503173828125, real_loss 1.0439453125
-> item_loss 0.7255859375 batch_loss 1.0539822048611112, real_loss 0.7255859375
  6%|         | 3/51 [01:56<31:32, 39.43s/it]-> item_loss 0.416748046875 batch_loss 0.9902587890625, real_loss 0.020843505859375
  8%|         | 4/51 [02:08<22:22, 28.56s/it]-> item_loss 0.94091796875 batch_loss 0.9857732599431818, real_loss 0.94091796875
-> item_loss 1.044921875 batch_loss 0.9907023111979166, real_loss 1.044921875
 10%|         | 5/51 [02:34<21:02, 27.45s/it]-> item_loss 0.92578125 batch_loss 0.9857083834134616, real_loss 0.92578125
-> item_loss 0.72412109375 batch_loss 0.9670235770089286, real_loss 0.72412109375
 12%|        | 6/51 [02:59<20:06, 26.80s/it]-> item_loss 1.08984375 batch_loss 0.9752115885416667, real_loss 1.08984375
-> item_loss 0.9130859375 batch_loss 0.9713287353515625, real_loss 0.9130859375
-> item_loss 0.64990234375 batch_loss 0.9524213005514706, real_loss 0.64990234375
 14%|        | 7/51 [03:38<22:34, 30.78s/it][2023-10-02 00:46:24,425] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.89599609375 batch_loss 0.9492865668402778, real_loss 0.89599609375
 16%|        | 8/51 [03:51<17:57, 25.06s/it]-> item_loss 0.8486328125 batch_loss 0.9439890008223685, real_loss 0.8486328125
 18%|        | 9/51 [04:04<14:53, 21.26s/it]-> item_loss 1.23828125 batch_loss 0.95870361328125, real_loss 1.23828125
 20%|        | 10/51 [04:17<12:44, 18.66s/it]-> item_loss 0.830078125 batch_loss 0.9525785900297619, real_loss 0.830078125
 22%|       | 11/51 [04:28<11:02, 16.55s/it]-> item_loss 0.463134765625 batch_loss 0.9303311434659091, real_loss 0.023162841796875
-> item_loss 0.455078125 batch_loss 0.90966796875, real_loss 0.0227508544921875
-> item_loss 0.295654296875 batch_loss 0.8840840657552084, real_loss 0.0147857666015625
 24%|       | 12/51 [05:06<14:52, 22.88s/it]-> item_loss 0.9658203125 batch_loss 0.887353515625, real_loss 0.9658203125
-> item_loss 0.62451171875 batch_loss 0.8772442157451923, real_loss 0.62451171875
 25%|       | 13/51 [05:30<14:49, 23.42s/it]-> item_loss 0.42529296875 batch_loss 0.8605052806712963, real_loss 0.021270751953125
 27%|       | 14/51 [05:42<12:16, 19.90s/it][2023-10-02 00:48:28,258] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.677734375 batch_loss 0.8539777483258929, real_loss 0.677734375
 29%|       | 15/51 [05:55<10:36, 17.68s/it]-> item_loss 0.52490234375 batch_loss 0.8426303205818966, real_loss 0.52490234375
 31%|      | 16/51 [06:07<09:22, 16.07s/it]-> item_loss 0.5869140625 batch_loss 0.8341064453125, real_loss 0.5869140625
 33%|      | 17/51 [06:19<08:25, 14.86s/it]-> item_loss 0.9931640625 batch_loss 0.8392373361895161, real_loss 0.9931640625
 35%|      | 18/51 [06:31<07:43, 14.04s/it]-> item_loss 0.615234375 batch_loss 0.8322372436523438, real_loss 0.615234375
 37%|      | 19/51 [06:43<07:10, 13.45s/it]-> item_loss 1.1455078125 batch_loss 0.8417302911931818, real_loss 1.1455078125
-> item_loss 1.154296875 batch_loss 0.8509234260110294, real_loss 1.154296875
 39%|      | 20/51 [07:09<08:54, 17.25s/it]-> item_loss 1.0322265625 batch_loss 0.856103515625, real_loss 1.0322265625
-> item_loss 0.80029296875 batch_loss 0.85455322265625, real_loss 0.80029296875
 41%|      | 21/51 [07:36<09:58, 19.95s/it]-> item_loss 0.73681640625 batch_loss 0.8513711465371622, real_loss 0.73681640625
[2023-10-02 00:50:34,306] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.087890625 batch_loss 0.8575953433388158, real_loss 1.087890625
 43%|     | 22/51 [08:01<10:23, 21.50s/it]-> item_loss 1.11328125 batch_loss 0.8641513922275641, real_loss 1.11328125
 45%|     | 23/51 [08:14<08:49, 18.91s/it]-> item_loss 0.548828125 batch_loss 0.856268310546875, real_loss 0.548828125
 47%|     | 24/51 [08:26<07:39, 17.03s/it]-> item_loss 1.064453125 batch_loss 0.8613459889481707, real_loss 1.064453125
 49%|     | 25/51 [08:39<06:51, 15.82s/it]-> item_loss 0.591796875 batch_loss 0.8549281529017857, real_loss 0.591796875
 51%|     | 26/51 [08:52<06:11, 14.86s/it]-> item_loss 0.461181640625 batch_loss 0.8457712572674418, real_loss 0.0230560302734375
 53%|    | 27/51 [09:05<05:41, 14.21s/it]-> item_loss 0.38427734375 batch_loss 0.8352827592329546, real_loss 0.0192108154296875
 55%|    | 28/51 [09:16<05:10, 13.50s/it]-> item_loss 0.48095703125 batch_loss 0.8274088541666667, real_loss 0.0240478515625
 57%|    | 29/51 [09:28<04:44, 12.92s/it]-> item_loss 0.6279296875 batch_loss 0.8230723505434783, real_loss 0.6279296875
 59%|    | 30/51 [09:40<04:26, 12.71s/it]-> item_loss 0.65966796875 batch_loss 0.819595661569149, real_loss 0.65966796875
 61%|    | 31/51 [09:52<04:11, 12.58s/it][2023-10-02 00:52:37,911] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.7685546875 batch_loss 0.8185323079427084, real_loss 0.7685546875
 63%|   | 32/51 [10:04<03:55, 12.37s/it]-> item_loss 0.2154541015625 batch_loss 0.8062245894451531, real_loss 0.010772705078125
 65%|   | 33/51 [10:16<03:38, 12.11s/it]-> item_loss 0.8857421875 batch_loss 0.80781494140625, real_loss 0.8857421875
 67%|   | 34/51 [10:28<03:24, 12.04s/it]-> item_loss 1.013671875 batch_loss 0.8118513518688726, real_loss 1.013671875
-> item_loss 0.537109375 batch_loss 0.8065678523137019, real_loss 0.537109375
 69%|   | 35/51 [10:52<04:11, 15.72s/it]-> item_loss 0.43505859375 batch_loss 0.7995582436615566, real_loss 0.021759033203125
 71%|   | 36/51 [11:04<03:38, 14.56s/it]-> item_loss 0.6162109375 batch_loss 0.7961629231770834, real_loss 0.6162109375
 73%|  | 37/51 [11:16<03:14, 13.88s/it]-> item_loss 1.052734375 batch_loss 0.8008278586647727, real_loss 1.052734375
-> item_loss 0.74072265625 batch_loss 0.7997545514787946, real_loss 0.74072265625
 75%|  | 38/51 [11:41<03:41, 17.00s/it]-> item_loss 0.46826171875 batch_loss 0.7939388877467105, real_loss 0.023406982421875
[2023-10-02 00:54:37,735] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.60791015625 batch_loss 0.7907314958243534, real_loss 0.60791015625
 76%|  | 39/51 [12:04<03:48, 19.01s/it]-> item_loss 0.43505859375 batch_loss 0.7847031415519068, real_loss 0.021759033203125
 78%|  | 40/51 [12:16<03:04, 16.78s/it]-> item_loss 0.487060546875 batch_loss 0.779742431640625, real_loss 0.02435302734375
 80%|  | 41/51 [12:27<02:32, 15.24s/it]-> item_loss 0.6142578125 batch_loss 0.7770295690317623, real_loss 0.6142578125
 82%| | 42/51 [12:39<02:07, 14.16s/it]-> item_loss 0.64697265625 batch_loss 0.774931876890121, real_loss 0.64697265625
 84%| | 43/51 [12:51<01:47, 13.46s/it]-> item_loss 0.5341796875 batch_loss 0.7711104135664683, real_loss 0.5341796875
 86%| | 44/51 [13:02<01:30, 12.86s/it]-> item_loss 0.3056640625 batch_loss 0.7638378143310547, real_loss 0.01528167724609375
 88%| | 45/51 [13:14<01:14, 12.43s/it]-> item_loss 0.5322265625 batch_loss 0.7602745643028846, real_loss 0.5322265625
 90%| | 46/51 [13:26<01:01, 12.27s/it]-> item_loss 1.2861328125 batch_loss 0.7682421135179924, real_loss 1.2861328125
-> item_loss 0.97412109375 batch_loss 0.7713149341184702, real_loss 0.97412109375
[2023-10-02 00:56:37,841] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.96044921875 batch_loss 0.7740963206571692, real_loss 0.96044921875
-> item_loss 0.7255859375 batch_loss 0.7733932716259058, real_loss 0.7255859375
 92%|| 47/51 [14:16<01:35, 23.79s/it]-> item_loss 0.3779296875 batch_loss 0.7677437918526786, real_loss 0.018890380859375
 94%|| 48/51 [14:28<01:00, 20.09s/it]-> item_loss 0.724609375 batch_loss 0.7671362648547535, real_loss 0.724609375
 96%|| 49/51 [14:39<00:35, 17.51s/it]-> item_loss 0.8046875 batch_loss 0.7676578097873263, real_loss 0.8046875
-> item_loss 0.826171875 batch_loss 0.7684593723244864, real_loss 0.826171875
-> item_loss 0.72265625 batch_loss 0.7678404112119932, real_loss 0.72265625
-> item_loss 0.352294921875 batch_loss 0.7622998046875, real_loss 0.017608642578125
 98%|| 50/51 [15:29<00:27, 27.28s/it]-> item_loss 0.196044921875 batch_loss 0.7548490825452303, real_loss 0.00980377197265625
100%|| 51/51 [15:41<00:00, 22.57s/it]100%|| 51/51 [15:41<00:00, 18.46s/it]
127.0.0.1 - - [02/Oct/2023 00:58:16] "POST /train/sft HTTP/1.1" 200 28
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/51 [00:00<?, ?it/s]-> item_loss 1.240234375 batch_loss 1.240234375, real_loss 1.240234375
[2023-10-02 00:58:43,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.8828125 batch_loss 1.0615234375, real_loss 0.8828125
-> item_loss 0.65869140625 batch_loss 0.92724609375, real_loss 0.65869140625
-> item_loss 0.371337890625 batch_loss 0.78826904296875, real_loss 0.0185699462890625
  2%|         | 1/51 [00:51<42:35, 51.11s/it]-> item_loss 0.33740234375 batch_loss 0.698095703125, real_loss 0.016876220703125
  4%|         | 2/51 [01:02<22:49, 27.95s/it]-> item_loss 0.56005859375 batch_loss 0.6750895182291666, real_loss 0.56005859375
  6%|         | 3/51 [01:15<16:41, 20.87s/it]-> item_loss 0.334716796875 batch_loss 0.62646484375, real_loss 0.0167388916015625
  8%|         | 4/51 [01:27<13:38, 17.41s/it]-> item_loss 0.349609375 batch_loss 0.59185791015625, real_loss 0.017486572265625
 10%|         | 5/51 [01:39<11:53, 15.51s/it]-> item_loss 0.443115234375 batch_loss 0.5753309461805556, real_loss 0.02215576171875
 12%|        | 6/51 [01:52<10:52, 14.50s/it]-> item_loss 0.65673828125 batch_loss 0.5834716796875, real_loss 0.65673828125
 14%|        | 7/51 [02:04<10:09, 13.84s/it]-> item_loss 0.33935546875 batch_loss 0.561279296875, real_loss 0.0169677734375
 16%|        | 8/51 [02:17<09:36, 13.40s/it][2023-10-02 01:00:47,071] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.744140625 batch_loss 0.5765177408854166, real_loss 0.744140625
 18%|        | 9/51 [02:29<09:13, 13.17s/it]-> item_loss 0.406005859375 batch_loss 0.5634014423076923, real_loss 0.020294189453125
-> item_loss 0.360107421875 batch_loss 0.5488804408482143, real_loss 0.01800537109375
-> item_loss 0.180419921875 batch_loss 0.52431640625, real_loss 0.0090179443359375
 20%|        | 10/51 [03:09<14:32, 21.28s/it]-> item_loss 0.51953125 batch_loss 0.524017333984375, real_loss 0.51953125
 22%|       | 11/51 [03:21<12:20, 18.52s/it]-> item_loss 0.08892822265625 batch_loss 0.49842385684742646, real_loss 0.00444793701171875
 24%|       | 12/51 [03:33<10:46, 16.57s/it]-> item_loss 0.29541015625 batch_loss 0.4871453179253472, real_loss 0.0147705078125
 25%|       | 13/51 [03:45<09:32, 15.07s/it]-> item_loss 0.2401123046875 batch_loss 0.47414358038651316, real_loss 0.0120086669921875
 27%|       | 14/51 [03:56<08:37, 13.98s/it]-> item_loss 0.1456298828125 batch_loss 0.4577178955078125, real_loss 0.007282257080078125
 29%|       | 15/51 [04:08<07:57, 13.25s/it]-> item_loss 0.5537109375 batch_loss 0.4622889927455357, real_loss 0.5537109375
 31%|      | 16/51 [04:20<07:29, 12.84s/it][2023-10-02 01:02:48,927] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.3095703125 batch_loss 0.4553472345525568, real_loss 0.01548004150390625
 33%|      | 17/51 [04:31<07:02, 12.44s/it]-> item_loss 0.681640625 batch_loss 0.46518607761548914, real_loss 0.681640625
-> item_loss 0.253173828125 batch_loss 0.45635223388671875, real_loss 0.01265716552734375
 35%|      | 18/51 [04:55<08:45, 15.94s/it]-> item_loss 0.77880859375 batch_loss 0.46925048828125, real_loss 0.77880859375
-> item_loss 0.4716796875 batch_loss 0.4693439190204327, real_loss 0.023590087890625
 37%|      | 19/51 [05:21<10:02, 18.82s/it]-> item_loss 0.30419921875 batch_loss 0.4632274486400463, real_loss 0.0152130126953125
 39%|      | 20/51 [05:33<08:42, 16.87s/it]-> item_loss 0.66748046875 batch_loss 0.47052219935825895, real_loss 0.66748046875
-> item_loss 0.295166015625 batch_loss 0.46447543440193967, real_loss 0.0147552490234375
 41%|      | 21/51 [05:57<09:34, 19.14s/it]-> item_loss 0.8740234375 batch_loss 0.4781270345052083, real_loss 0.8740234375
-> item_loss 0.4775390625 batch_loss 0.4781080676663306, real_loss 0.0238800048828125
[2023-10-02 01:04:53,561] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.60498046875 batch_loss 0.4820728302001953, real_loss 0.60498046875
-> item_loss 0.3916015625 batch_loss 0.4793312766335227, real_loss 0.0195770263671875
 43%|     | 22/51 [06:48<13:46, 28.51s/it]-> item_loss 0.259765625 batch_loss 0.4728734633501838, real_loss 0.0129852294921875
 45%|     | 23/51 [07:00<11:00, 23.57s/it]-> item_loss 0.388671875 batch_loss 0.4704677036830357, real_loss 0.019439697265625
 47%|     | 24/51 [07:12<09:01, 20.06s/it]-> item_loss 0.3291015625 batch_loss 0.4665408664279514, real_loss 0.016448974609375
-> item_loss 0.51953125 batch_loss 0.46797303895692566, real_loss 0.51953125
-> item_loss 0.28515625 batch_loss 0.46316207082648025, real_loss 0.01425933837890625
 49%|     | 25/51 [07:50<11:01, 25.43s/it]-> item_loss 0.49365234375 batch_loss 0.4639438726963141, real_loss 0.024688720703125
 51%|     | 26/51 [08:02<08:56, 21.46s/it]-> item_loss 0.759765625 batch_loss 0.47133941650390626, real_loss 0.759765625
 53%|    | 27/51 [08:14<07:30, 18.76s/it]-> item_loss 0.55322265625 batch_loss 0.47333656869283536, real_loss 0.55322265625
[2023-10-02 01:06:58,602] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.556640625 batch_loss 0.4753199986049107, real_loss 0.556640625
-> item_loss 0.434814453125 batch_loss 0.47437800917514533, real_loss 0.0217437744140625
-> item_loss 0.29052734375 batch_loss 0.4701995849609375, real_loss 0.0145263671875
 55%|    | 28/51 [09:06<11:00, 28.70s/it]-> item_loss 0.8310546875 batch_loss 0.4782185872395833, real_loss 0.8310546875
-> item_loss 0.72021484375 batch_loss 0.4834793754245924, real_loss 0.72021484375
 57%|    | 29/51 [09:32<10:11, 27.79s/it]-> item_loss 0.251708984375 batch_loss 0.4785480905086436, real_loss 0.0125885009765625
 59%|    | 30/51 [09:44<08:04, 23.09s/it]-> item_loss 0.37939453125 batch_loss 0.4764823913574219, real_loss 0.0189666748046875
 61%|    | 31/51 [09:56<06:37, 19.89s/it]-> item_loss 0.401611328125 batch_loss 0.47495441047512754, real_loss 0.02008056640625
-> item_loss 0.51513671875 batch_loss 0.475758056640625, real_loss 0.51513671875
 63%|   | 32/51 [10:22<06:49, 21.57s/it]-> item_loss 0.37158203125 batch_loss 0.4737153894761029, real_loss 0.018585205078125
[2023-10-02 01:09:05,482] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.370361328125 batch_loss 0.47172781137319714, real_loss 0.018524169921875
 65%|   | 33/51 [10:48<06:50, 22.81s/it]-> item_loss 0.10394287109375 batch_loss 0.46478847287735847, real_loss 0.00519561767578125
 67%|   | 34/51 [10:59<05:31, 19.52s/it]-> item_loss 0.1646728515625 batch_loss 0.4592307761863426, real_loss 0.00823211669921875
 69%|   | 35/51 [11:11<04:35, 17.19s/it]-> item_loss 0.62353515625 batch_loss 0.46221812855113636, real_loss 0.62353515625
-> item_loss 0.254638671875 batch_loss 0.4585113525390625, real_loss 0.01273345947265625
 71%|   | 36/51 [11:36<04:50, 19.38s/it]-> item_loss 0.66455078125 batch_loss 0.46212607935855265, real_loss 0.66455078125
-> item_loss 1.044921875 batch_loss 0.47217428273168105, real_loss 1.044921875
-> item_loss 0.6640625 batch_loss 0.4754266253972458, real_loss 0.6640625
-> item_loss 0.08740234375 batch_loss 0.46895955403645834, real_loss 0.00437164306640625
 73%|  | 37/51 [12:25<06:36, 28.33s/it]-> item_loss 0.24609375 batch_loss 0.4653060162653688, real_loss 0.01230621337890625
 75%|  | 38/51 [12:37<05:05, 23.47s/it][2023-10-02 01:11:06,919] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.275634765625 batch_loss 0.4622468025453629, real_loss 0.0137786865234375
 76%|  | 39/51 [12:49<04:00, 20.03s/it]-> item_loss 0.404541015625 batch_loss 0.4613308376736111, real_loss 0.020233154296875
 78%|  | 40/51 [13:01<03:14, 17.71s/it]-> item_loss 0.6767578125 batch_loss 0.46469688415527344, real_loss 0.6767578125
 80%|  | 41/51 [13:14<02:41, 16.16s/it]-> item_loss 0.623046875 batch_loss 0.4671330378605769, real_loss 0.623046875
-> item_loss 0.396484375 batch_loss 0.4660626035748106, real_loss 0.0198211669921875
 82%| | 42/51 [13:39<02:49, 18.84s/it]-> item_loss 0.37158203125 batch_loss 0.4646524457789179, real_loss 0.018585205078125
 84%| | 43/51 [13:51<02:13, 16.74s/it]-> item_loss 0.552734375 batch_loss 0.46594776826746326, real_loss 0.552734375
 86%| | 44/51 [14:03<01:46, 15.27s/it]-> item_loss 0.333740234375 batch_loss 0.46403171705163043, real_loss 0.016693115234375
 88%| | 45/51 [14:15<01:25, 14.28s/it]-> item_loss 0.303955078125 batch_loss 0.46174490792410716, real_loss 0.01519775390625
 90%| | 46/51 [14:27<01:07, 13.58s/it]-> item_loss 0.420654296875 batch_loss 0.4611661669234155, real_loss 0.021026611328125
 92%|| 47/51 [14:38<00:51, 13.00s/it][2023-10-02 01:13:08,801] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=4, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.84716796875 batch_loss 0.4665273030598958, real_loss 0.84716796875
-> item_loss 0.81494140625 batch_loss 0.4713000989940068, real_loss 0.81494140625
 94%|| 48/51 [15:03<00:49, 16.64s/it]-> item_loss 0.261962890625 batch_loss 0.4684712177998311, real_loss 0.01309967041015625
 96%|| 49/51 [15:16<00:30, 15.32s/it]-> item_loss 0.369384765625 batch_loss 0.4671500651041667, real_loss 0.018463134765625
 98%|| 50/51 [15:28<00:14, 14.32s/it]-> item_loss 0.459228515625 batch_loss 0.4670458341899671, real_loss 0.0229644775390625
100%|| 51/51 [15:40<00:00, 13.79s/it]100%|| 51/51 [15:40<00:00, 18.44s/it]
127.0.0.1 - - [02/Oct/2023 01:13:59] "POST /train/sft HTTP/1.1" 200 28
===saveved====
127.0.0.1 - - [02/Oct/2023 01:14:57] "POST /train/save-weight HTTP/1.1" 200 26
None
None
172.16.1.39 - - [02/Oct/2023 01:15:03] "POST /state/reset HTTP/1.1" 200 21
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 23280.13it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 531710.94it/s]
AI 
172.16.1.39 - - [02/Oct/2023 01:15:07] "POST /inference/generate HTTP/1.1" 200 1341
None
None
172.16.1.39 - - [02/Oct/2023 01:15:21] "POST /state/reset HTTP/1.1" 200 21
AI 
172.16.1.39 - - [02/Oct/2023 01:15:24] "POST /inference/generate HTTP/1.1" 200 1396
None
None
172.16.1.39 - - [02/Oct/2023 01:16:10] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 01:16:16] "POST /inference/generate HTTP/1.1" 200 2980
None
None
172.16.1.39 - - [02/Oct/2023 01:16:47] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 01:16:54] "POST /inference/generate HTTP/1.1" 200 3100
None
None
172.16.1.39 - - [02/Oct/2023 01:17:15] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 01:17:27] "POST /inference/generate HTTP/1.1" 200 5067
None
None
172.16.1.39 - - [02/Oct/2023 01:19:27] "POST /state/reset HTTP/1.1" 200 21
  
172.16.1.39 - - [02/Oct/2023 01:19:42] "POST /inference/generate HTTP/1.1" 200 6999
None
None
172.16.1.39 - - [02/Oct/2023 01:20:31] "POST /state/reset HTTP/1.1" 200 21
  
172.16.1.39 - - [02/Oct/2023 01:20:48] "POST /inference/generate HTTP/1.1" 200 8938
None
None
172.16.1.39 - - [02/Oct/2023 01:21:34] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 01:21:53] "POST /inference/generate HTTP/1.1" 200 10371
None
None
172.16.1.39 - - [02/Oct/2023 01:22:41] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 01:22:47] "POST /inference/generate HTTP/1.1" 200 3098
None
None
172.16.1.39 - - [02/Oct/2023 01:23:04] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 01:23:14] "POST /inference/generate HTTP/1.1" 200 4646
None
None
172.16.1.39 - - [02/Oct/2023 01:23:33] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 01:23:47] "POST /inference/generate HTTP/1.1" 200 6110
None
None
172.16.1.39 - - [02/Oct/2023 01:24:05] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 01:24:17] "POST /inference/generate HTTP/1.1" 200 7565
None
None
172.16.1.39 - - [02/Oct/2023 01:25:11] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 01:25:27] "POST /inference/generate HTTP/1.1" 200 9371
None
None
172.16.1.39 - - [02/Oct/2023 01:28:07] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 01:28:25] "POST /inference/generate HTTP/1.1" 200 10281
None
None
172.16.1.39 - - [02/Oct/2023 01:29:21] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 01:29:40] "POST /inference/generate HTTP/1.1" 200 11303
None
None
172.16.1.39 - - [02/Oct/2023 01:31:51] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 01:32:17] "POST /inference/generate HTTP/1.1" 200 13381
None
None
172.16.1.39 - - [02/Oct/2023 01:33:26] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 01:33:53] "POST /inference/generate HTTP/1.1" 200 15267
None
None
172.16.1.39 - - [02/Oct/2023 01:34:33] "POST /state/reset HTTP/1.1" 200 21
  
172.16.1.39 - - [02/Oct/2023 01:35:05] "POST /inference/generate HTTP/1.1" 200 17088
None
None
172.16.1.39 - - [02/Oct/2023 01:36:27] "POST /state/reset HTTP/1.1" 200 21
     
172.16.1.39 - - [02/Oct/2023 01:37:02] "POST /inference/generate HTTP/1.1" 200 19149
None
None
172.16.1.39 - - [02/Oct/2023 01:38:12] "POST /state/reset HTTP/1.1" 200 21
  
172.16.1.39 - - [02/Oct/2023 01:38:49] "POST /inference/generate HTTP/1.1" 200 20190
None
None
172.16.1.39 - - [02/Oct/2023 01:38:58] "POST /state/reset HTTP/1.1" 200 21
     
172.16.1.39 - - [02/Oct/2023 01:39:38] "POST /inference/generate HTTP/1.1" 200 21168
None
None
172.16.1.39 - - [02/Oct/2023 01:39:55] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 01:40:13] "POST /inference/generate HTTP/1.1" 200 10281
None
None
172.16.1.39 - - [02/Oct/2023 01:40:18] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 01:40:38] "POST /inference/generate HTTP/1.1" 200 11079
None
None
172.16.1.39 - - [02/Oct/2023 01:41:26] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 01:41:49] "POST /inference/generate HTTP/1.1" 200 12781
None
None
172.16.1.39 - - [02/Oct/2023 01:42:34] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 01:43:00] "POST /inference/generate HTTP/1.1" 200 14478
[2023-10-02 01:49:05,018] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 17155
[2023-10-02 01:49:05,019] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-02 01:50:55,425] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 01:50:56,800] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-02 01:50:56,825] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-02 01:50:58,656] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 01:50:59,959] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-02 01:50:59,959] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-02 01:50:59,959] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-02 01:50:59,959] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-02 01:50:59,959] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-02 01:51:01,796] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu117'
[RWKV.model]: Preloading model from '/home/neromous/pyblackfog/rwkv-output-models/trpg/rwkv-60.pth'
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048_bf16/build.ninja...
Building extension module wkv_2048_bf16...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_2048_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/TH -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/neromous/.anaconda3/envs/blackfog/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -t 4 -std=c++17 -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=2048 -c /home/neromous/RWKV-Ouroboros/cuda/wkv_cuda_bf16.cu -o wkv_cuda_bf16.cuda.o 
ptxas info    : 1 bytes gmem
ptxas info    : Compiling entry function '_Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf' for 'sm_86'
ptxas info    : Function properties for _Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf
    24576 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 56 registers, 464 bytes cmem[0], 8 bytes cmem[2]
ptxas info    : Compiling entry function '_Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf' for 'sm_86'
ptxas info    : Function properties for _Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 40 registers, 424 bytes cmem[0]
[2/3] c++ -MMD -MF wkv_op_bf16.o.d -DTORCH_EXTENSION_NAME=wkv_2048_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/TH -isystem /home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/neromous/.anaconda3/envs/blackfog/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -std=c++17 -O3 -DTmax=2048 -c /home/neromous/RWKV-Ouroboros/cuda/wkv_op_bf16.cpp -o wkv_op_bf16.o 
[3/3] c++ wkv_op_bf16.o wkv_cuda_bf16.cuda.o -shared -L/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_2048_bf16.so
Loading extension module wkv_2048_bf16...
  0%|          | 0/32 [00:00<?, ?it/s]  3%|         | 1/32 [00:01<00:50,  1.64s/it]  6%|         | 2/32 [00:03<00:48,  1.62s/it]  9%|         | 3/32 [00:04<00:46,  1.62s/it] 12%|        | 4/32 [00:06<00:45,  1.62s/it] 16%|        | 5/32 [00:08<00:43,  1.61s/it] 19%|        | 6/32 [00:09<00:41,  1.61s/it] 22%|       | 7/32 [00:11<00:40,  1.61s/it] 25%|       | 8/32 [00:12<00:38,  1.61s/it] 28%|       | 9/32 [00:14<00:37,  1.61s/it] 31%|      | 10/32 [00:16<00:35,  1.61s/it] 34%|      | 11/32 [00:17<00:33,  1.61s/it] 38%|      | 12/32 [00:19<00:32,  1.61s/it] 41%|      | 13/32 [00:20<00:30,  1.61s/it] 44%|     | 14/32 [00:22<00:28,  1.61s/it] 47%|     | 15/32 [00:24<00:27,  1.61s/it] 50%|     | 16/32 [00:25<00:25,  1.61s/it] 53%|    | 17/32 [00:27<00:24,  1.61s/it] 56%|    | 18/32 [00:29<00:22,  1.61s/it] 59%|    | 19/32 [00:30<00:20,  1.61s/it] 62%|   | 20/32 [00:32<00:19,  1.61s/it] 66%|   | 21/32 [00:33<00:17,  1.61s/it] 69%|   | 22/32 [00:35<00:16,  1.61s/it] 72%|  | 23/32 [00:37<00:14,  1.61s/it] 75%|  | 24/32 [00:38<00:12,  1.61s/it] 78%|  | 25/32 [00:40<00:11,  1.61s/it] 81%| | 26/32 [00:41<00:09,  1.61s/it] 84%| | 27/32 [00:43<00:08,  1.61s/it] 88%| | 28/32 [00:45<00:06,  1.61s/it] 91%| | 29/32 [00:46<00:04,  1.61s/it] 94%|| 30/32 [00:48<00:03,  1.61s/it] 97%|| 31/32 [00:49<00:01,  1.61s/it]100%|| 32/32 [00:51<00:00,  1.61s/it]100%|| 32/32 [00:51<00:00,  1.61s/it]
[RWKV.model]: Loading model weights ( L32-D4096-V65536 )
[RWKV.model]: Finished initial model load
#
# RWKV lighting_trainer.py important notes 
# https://github.com/RWKV/RWKV-infctx-trainer 
#
# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues
# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications
# - When resuming from checkpoint, the estimated time is inaccurate
#

[RWKV.model] Configuring optimizer with
    - lr_init:  1.000e-05 (1e-05)
    - lr_final: 1.000e-06 (1e-06)

Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.566048622131348 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.990000), weight_decay=0.010000, adam_w=0
[2023-10-02 01:52:53,064] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-02 01:52:53,065] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-02 01:52:53,065] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Traceback (most recent call last):
  File "/home/neromous/RWKV-Ouroboros/app.py", line 76, in <module>
    model_engine, optimizer, _, _ = deepspeed.initialize(model=model,
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/__init__.py", line 157, in initialize
    config_class = DeepSpeedConfig(config, mpu)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/runtime/config.py", line 693, in __init__
    config_decoded = base64.urlsafe_b64decode(config).decode('utf-8')
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/base64.py", line 133, in urlsafe_b64decode
    return b64decode(s)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/base64.py", line 87, in b64decode
    return binascii.a2b_base64(s)
binascii.Error: Incorrect padding
Exception ignored in: <function DeepSpeedCPUAdam.__del__ at 0x7f9ff641cee0>
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/ops/adam/cpu_adam.py", line 102, in __del__
TypeError: 'NoneType' object is not callable
[2023-10-02 01:53:00,103] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 20551
[2023-10-02 01:53:00,104] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = 1
[2023-10-02 01:58:43,146] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 01:58:44,536] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-02 01:58:44,560] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-02 01:58:46,426] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 01:58:47,786] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-02 01:58:47,786] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-02 01:58:47,787] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-02 01:58:47,787] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-02 01:58:47,787] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-02 01:58:49,625] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu117'
[RWKV.model]: Preloading model from '/home/neromous/pyblackfog/rwkv-output-models/trpg/rwkv-60.pth'
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048_bf16/build.ninja...
Building extension module wkv_2048_bf16...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048_bf16...
  0%|          | 0/32 [00:00<?, ?it/s]  3%|         | 1/32 [00:01<00:49,  1.60s/it]  6%|         | 2/32 [00:03<00:47,  1.58s/it]  9%|         | 3/32 [00:04<00:45,  1.58s/it] 12%|        | 4/32 [00:06<00:44,  1.57s/it] 16%|        | 5/32 [00:07<00:42,  1.57s/it] 19%|        | 6/32 [00:09<00:40,  1.57s/it] 22%|       | 7/32 [00:11<00:39,  1.57s/it] 25%|       | 8/32 [00:12<00:37,  1.57s/it] 28%|       | 9/32 [00:14<00:36,  1.57s/it] 31%|      | 10/32 [00:15<00:34,  1.57s/it] 34%|      | 11/32 [00:17<00:32,  1.57s/it] 38%|      | 12/32 [00:18<00:31,  1.57s/it] 41%|      | 13/32 [00:20<00:29,  1.57s/it] 44%|     | 14/32 [00:22<00:28,  1.57s/it] 47%|     | 15/32 [00:23<00:26,  1.57s/it] 50%|     | 16/32 [00:25<00:25,  1.57s/it] 53%|    | 17/32 [00:26<00:23,  1.57s/it] 56%|    | 18/32 [00:28<00:21,  1.57s/it] 59%|    | 19/32 [00:29<00:20,  1.57s/it] 62%|   | 20/32 [00:31<00:18,  1.57s/it] 66%|   | 21/32 [00:33<00:17,  1.57s/it] 69%|   | 22/32 [00:34<00:15,  1.57s/it] 72%|  | 23/32 [00:36<00:14,  1.57s/it] 75%|  | 24/32 [00:37<00:12,  1.57s/it] 78%|  | 25/32 [00:39<00:10,  1.57s/it] 81%| | 26/32 [00:40<00:09,  1.57s/it] 84%| | 27/32 [00:42<00:07,  1.57s/it] 88%| | 28/32 [00:43<00:06,  1.57s/it] 91%| | 29/32 [00:45<00:04,  1.57s/it] 94%|| 30/32 [00:47<00:03,  1.57s/it] 97%|| 31/32 [00:48<00:01,  1.57s/it]100%|| 32/32 [00:50<00:00,  1.57s/it]100%|| 32/32 [00:50<00:00,  1.57s/it]
[RWKV.model]: Loading model weights ( L32-D4096-V65536 )
[RWKV.model]: Finished initial model load
#
# RWKV lighting_trainer.py important notes 
# https://github.com/RWKV/RWKV-infctx-trainer 
#
# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues
# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications
# - When resuming from checkpoint, the estimated time is inaccurate
#

[RWKV.model] Configuring optimizer with
    - lr_init:  1.000e-05 (1e-05)
    - lr_final: 1.000e-06 (1e-06)

Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.611773490905762 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.990000), weight_decay=0.010000, adam_w=0
[2023-10-02 02:00:17,968] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-02 02:00:17,968] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-02 02:00:17,968] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-02 02:00:24,036] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-02 02:00:24,037] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-02 02:00:24,037] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-02 02:00:24,077] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-02 02:00:24,077] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-02 02:00:24,077] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-10-02 02:00:24,077] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-02 02:00:24,077] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-02 02:00:24,077] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-02 02:00:24,077] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-02 02:00:53,416] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-02 02:00:53,417] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-02 02:00:53,417] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 36.8 GB, percent = 9.7%
[2023-10-02 02:01:23,129] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-02 02:01:23,130] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-02 02:01:23,130] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.67 GB, percent = 33.3%
[2023-10-02 02:01:23,130] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-02 02:01:23,980] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-02 02:01:23,981] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-02 02:01:23,981] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.67 GB, percent = 33.3%
[2023-10-02 02:01:24,004] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-02 02:01:24,004] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-02 02:01:24,004] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fcab6f75340>
[2023-10-02 02:01:24,004] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.99), (0.9, 0.99), (0.9, 0.99)]
[2023-10-02 02:01:24,005] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-02 02:01:24,005] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-02 02:01:24,006] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-02 02:01:24,006] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-02 02:01:24,006] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-02 02:01:24,006] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-02 02:01:24,006] [INFO] [config.py:971:print]   bfloat16_enabled ............. auto
[2023-10-02 02:01:24,006] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-02 02:01:24,006] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-02 02:01:24,006] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-02 02:01:24,006] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fcaa4ff71f0>
[2023-10-02 02:01:24,006] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-02 02:01:24,006] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-02 02:01:24,006] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-02 02:01:24,006] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-02 02:01:24,006] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-02 02:01:24,006] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-02 02:01:24,006] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-02 02:01:24,006] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-02 02:01:24,006] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-02 02:01:24,006] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... None
[2023-10-02 02:01:24,006] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   fp16_auto_cast ............... None
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   fp16_enabled ................. False
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 1
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   loss_scale ................... 1.0
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-02 02:01:24,007] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-02 02:01:24,008] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-02 02:01:24,009] [INFO] [config.py:957:print_user_config]   json = {
    "bfloat16": {
        "enabled": "auto"
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 35099.28it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 505295.99it/s]
127.0.0.1 - - [02/Oct/2023 02:02:14] "POST /inference/load-model HTTP/1.1" 200 26
None
None
127.0.0.1 - - [02/Oct/2023 02:02:14] "POST /state/reset HTTP/1.1" 200 21
<|over|><|over|><|response|><|over|><|me|><|response|>127.0.0.1 - - [02/Oct/2023 02:02:35] "POST /inference/generate HTTP/1.1" 200 4271
None
None
127.0.0.1 - - [02/Oct/2023 02:02:35] "POST /state/reset HTTP/1.1" 200 21
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/RWKV-Ouroboros/app.py", line 158, in inference_generate_no_state
    msg = inferencer.generate_no_state(model, msg)
  File "/home/neromous/RWKV-Ouroboros/models/inference_helper.py", line 168, in generate_no_state
    logits = model.inference(all_tokens[-ctx_len:])
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'RWKV' object has no attribute 'inference'
127.0.0.1 - - [02/Oct/2023 02:02:36] "POST /inference/generate-no-state HTTP/1.1" 500 766
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/52 [00:00<?, ?it/s]  0%|          | 0/52 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/RWKV-Ouroboros/app.py", line 263, in train_sft
    for token in v.yield_train_data(req_len=ctx_len,window=window):
NameError: name 'ctx_len' is not defined
127.0.0.1 - - [02/Oct/2023 02:03:18] "POST /train/sft HTTP/1.1" 500 748
[2023-10-02 02:04:33,178] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 21435
[2023-10-02 02:04:33,179] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-02 02:05:01,589] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 02:05:02,985] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-02 02:05:03,009] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-02 02:05:04,859] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 02:05:06,191] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-02 02:05:06,191] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-02 02:05:06,191] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-02 02:05:06,191] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-02 02:05:06,191] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-02 02:05:08,009] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu117'
[RWKV.model]: Preloading model from '/home/neromous/pyblackfog/rwkv-output-models/trpg/rwkv-60.pth'
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048_bf16/build.ninja...
Building extension module wkv_2048_bf16...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048_bf16...
  0%|          | 0/32 [00:00<?, ?it/s]  3%|         | 1/32 [00:01<00:49,  1.59s/it]  6%|         | 2/32 [00:03<00:47,  1.58s/it]  9%|         | 3/32 [00:04<00:45,  1.57s/it] 12%|        | 4/32 [00:06<00:43,  1.57s/it] 16%|        | 5/32 [00:07<00:42,  1.56s/it] 19%|        | 6/32 [00:09<00:40,  1.56s/it] 22%|       | 7/32 [00:10<00:39,  1.56s/it] 25%|       | 8/32 [00:12<00:37,  1.56s/it] 28%|       | 9/32 [00:14<00:35,  1.56s/it] 31%|      | 10/32 [00:15<00:34,  1.56s/it] 34%|      | 11/32 [00:17<00:32,  1.56s/it] 38%|      | 12/32 [00:18<00:31,  1.56s/it] 41%|      | 13/32 [00:20<00:29,  1.56s/it] 44%|     | 14/32 [00:21<00:28,  1.56s/it] 47%|     | 15/32 [00:23<00:26,  1.56s/it] 50%|     | 16/32 [00:25<00:25,  1.56s/it] 53%|    | 17/32 [00:26<00:23,  1.56s/it] 56%|    | 18/32 [00:28<00:21,  1.56s/it] 59%|    | 19/32 [00:29<00:20,  1.56s/it] 62%|   | 20/32 [00:31<00:18,  1.56s/it] 66%|   | 21/32 [00:32<00:17,  1.56s/it] 69%|   | 22/32 [00:34<00:15,  1.56s/it] 72%|  | 23/32 [00:35<00:14,  1.56s/it] 75%|  | 24/32 [00:37<00:12,  1.57s/it] 78%|  | 25/32 [00:39<00:10,  1.57s/it] 81%| | 26/32 [00:40<00:09,  1.56s/it] 84%| | 27/32 [00:42<00:07,  1.56s/it] 88%| | 28/32 [00:43<00:06,  1.56s/it] 91%| | 29/32 [00:45<00:04,  1.56s/it] 94%|| 30/32 [00:46<00:03,  1.56s/it] 97%|| 31/32 [00:48<00:01,  1.56s/it]100%|| 32/32 [00:50<00:00,  1.56s/it]100%|| 32/32 [00:50<00:00,  1.56s/it]
[RWKV.model]: Loading model weights ( L32-D4096-V65536 )
[RWKV.model]: Finished initial model load
#
# RWKV lighting_trainer.py important notes 
# https://github.com/RWKV/RWKV-infctx-trainer 
#
# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues
# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications
# - When resuming from checkpoint, the estimated time is inaccurate
#

[RWKV.model] Configuring optimizer with
    - lr_init:  1.000e-05 (1e-05)
    - lr_final: 1.000e-06 (1e-06)

Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.419396638870239 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.990000), weight_decay=0.010000, adam_w=0
[2023-10-02 02:06:35,972] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-02 02:06:35,973] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-02 02:06:35,973] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-02 02:06:41,750] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-02 02:06:41,752] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-02 02:06:41,752] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-02 02:06:41,797] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-02 02:06:41,797] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-02 02:06:41,797] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-10-02 02:06:41,797] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-02 02:06:41,797] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-02 02:06:41,797] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-02 02:06:41,797] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-02 02:07:11,106] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-02 02:07:11,107] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-02 02:07:11,108] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 36.81 GB, percent = 9.7%
[2023-10-02 02:07:41,980] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-02 02:07:41,980] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-02 02:07:41,981] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.66 GB, percent = 33.3%
[2023-10-02 02:07:41,981] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-02 02:07:43,178] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-02 02:07:43,179] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-02 02:07:43,180] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.66 GB, percent = 33.3%
[2023-10-02 02:07:43,212] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-02 02:07:43,212] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-02 02:07:43,212] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fe8539b4280>
[2023-10-02 02:07:43,212] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.99), (0.9, 0.99), (0.9, 0.99)]
[2023-10-02 02:07:43,214] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-02 02:07:43,214] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-02 02:07:43,214] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-02 02:07:43,214] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-02 02:07:43,214] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-02 02:07:43,214] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   bfloat16_enabled ............. auto
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe8441b70a0>
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... None
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-02 02:07:43,215] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-02 02:07:43,216] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-02 02:07:43,216] [INFO] [config.py:971:print]   fp16_auto_cast ............... None
[2023-10-02 02:07:43,216] [INFO] [config.py:971:print]   fp16_enabled ................. False
[2023-10-02 02:07:43,216] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-02 02:07:43,216] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-02 02:07:43,216] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-02 02:07:43,216] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-02 02:07:43,216] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-02 02:07:43,216] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-02 02:07:43,216] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-02 02:07:43,216] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 1
[2023-10-02 02:07:43,216] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-02 02:07:43,216] [INFO] [config.py:971:print]   loss_scale ................... 1.0
[2023-10-02 02:07:43,216] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-02 02:07:43,216] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-02 02:07:43,216] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-02 02:07:43,216] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-02 02:07:43,216] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-02 02:07:43,216] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-02 02:07:43,216] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-02 02:07:43,216] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-02 02:07:43,217] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-02 02:07:43,217] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-02 02:07:43,217] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-02 02:07:43,217] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-02 02:07:43,217] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-02 02:07:43,217] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-02 02:07:43,217] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-02 02:07:43,217] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-02 02:07:43,217] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-02 02:07:43,217] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-02 02:07:43,217] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-02 02:07:43,217] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-02 02:07:43,217] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-02 02:07:43,217] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-02 02:07:43,217] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-02 02:07:43,217] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-02 02:07:43,217] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-02 02:07:43,217] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-02 02:07:43,217] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-02 02:07:43,217] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-02 02:07:43,217] [INFO] [config.py:957:print_user_config]   json = {
    "bfloat16": {
        "enabled": "auto"
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/52 [00:00<?, ?it/s][2023-10-02 02:07:49,259] [INFO] [checkpointing.py:530:forward] Activation Checkpointing Information
[2023-10-02 02:07:49,259] [INFO] [checkpointing.py:531:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-10-02 02:07:49,259] [INFO] [checkpointing.py:532:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-10-02 02:07:49,259] [INFO] [checkpointing.py:534:forward] ----Synchronization False
[2023-10-02 02:07:49,259] [INFO] [checkpointing.py:535:forward] ----Profiling time in checkpointing False
-> item_loss 2.03125 batch_loss 2.03125, real_loss 2.125
-> item_loss 1.859375 batch_loss 1.9453125, real_loss 1.953125
-> item_loss 2.015625 batch_loss 1.96875, real_loss 2.109375
-> item_loss 1.9609375 batch_loss 1.966796875, real_loss 2.0625
  2%|         | 1/52 [00:57<48:45, 57.36s/it]-> item_loss 1.765625 batch_loss 1.9265625, real_loss 1.8515625
  4%|         | 2/52 [01:09<25:41, 30.83s/it]-> item_loss 1.796875 batch_loss 1.9049479166666667, real_loss 1.8828125
  6%|         | 3/52 [01:21<18:15, 22.36s/it]-> item_loss 1.9921875 batch_loss 1.9174107142857142, real_loss 2.09375
-> item_loss 2.203125 batch_loss 1.953125, real_loss 2.3125
  8%|         | 4/52 [01:49<19:35, 24.50s/it]-> item_loss 1.9296875 batch_loss 1.9505208333333333, real_loss 2.03125
 10%|         | 5/52 [02:02<15:45, 20.11s/it][2023-10-02 02:10:03,925] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.99), (0.9, 0.99), (0.9, 0.99)]
-> item_loss 1.3515625 batch_loss 1.890625, real_loss 1.3515625
 12%|        | 6/52 [02:14<13:28, 17.58s/it]-> item_loss 2.03125 batch_loss 1.9034090909090908, real_loss 2.125
 13%|        | 7/52 [02:28<12:10, 16.24s/it]-> item_loss 1.8515625 batch_loss 1.8990885416666667, real_loss 1.9453125
 15%|        | 8/52 [02:40<11:01, 15.02s/it]-> item_loss 1.40625 batch_loss 1.8611778846153846, real_loss 1.40625
 17%|        | 9/52 [02:53<10:12, 14.25s/it]-> item_loss 1.59375 batch_loss 1.8420758928571428, real_loss 1.671875
 19%|        | 10/52 [03:05<09:35, 13.71s/it]-> item_loss 2.109375 batch_loss 1.8598958333333333, real_loss 2.21875
 21%|        | 11/52 [03:18<09:10, 13.43s/it]-> item_loss 1.03125 batch_loss 1.80810546875, real_loss 1.03125
 23%|       | 12/52 [03:31<08:53, 13.35s/it]-> item_loss 1.5546875 batch_loss 1.7931985294117647, real_loss 1.6328125
 25%|       | 13/52 [03:43<08:23, 12.92s/it]-> item_loss 1.578125 batch_loss 1.78125, real_loss 1.65625
 27%|       | 14/52 [03:55<08:03, 12.71s/it]-> item_loss 1.375 batch_loss 1.7598684210526316, real_loss 1.375
 29%|       | 15/52 [04:07<07:44, 12.54s/it][2023-10-02 02:12:10,627] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.99), (0.9, 0.99), (0.9, 0.99)]
-> item_loss 1.671875 batch_loss 1.75546875, real_loss 1.7578125
-> item_loss 1.7890625 batch_loss 1.7570684523809523, real_loss 1.875
 31%|       | 16/52 [04:33<09:53, 16.49s/it]-> item_loss 1.546875 batch_loss 1.7475142045454546, real_loss 1.625
 33%|      | 17/52 [04:45<08:49, 15.12s/it]-> item_loss 1.4921875 batch_loss 1.736413043478261, real_loss 1.4921875
 35%|      | 18/52 [04:57<08:04, 14.25s/it]-> item_loss 1.703125 batch_loss 1.7350260416666667, real_loss 1.7890625
-> item_loss 1.6875 batch_loss 1.733125, real_loss 1.7734375
 37%|      | 19/52 [05:23<09:47, 17.80s/it]-> item_loss 1.34375 batch_loss 1.7181490384615385, real_loss 1.34375
-> item_loss 2.09375 batch_loss 1.7320601851851851, real_loss 2.203125
 38%|      | 20/52 [05:47<10:27, 19.62s/it]-> item_loss 1.921875 batch_loss 1.7388392857142858, real_loss 2.015625
-> item_loss 2.515625 batch_loss 1.765625, real_loss 2.640625
[2023-10-02 02:14:16,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.99), (0.9, 0.99), (0.9, 0.99)]
-> item_loss 2.21875 batch_loss 1.7807291666666667, real_loss 2.328125
[2023-10-02 02:14:37,842] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 22299
[2023-10-02 02:14:37,843] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-02 02:18:55,149] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 02:18:56,529] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-02 02:18:56,553] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-02 02:18:58,395] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 02:18:59,710] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-02 02:18:59,711] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-02 02:18:59,711] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-02 02:18:59,711] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-02 02:18:59,711] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-02 02:19:01,543] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu117'
[RWKV.model]: Preloading model from '/home/neromous/pyblackfog/rwkv-output-models/trpg/rwkv-60.pth'
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048_bf16/build.ninja...
Building extension module wkv_2048_bf16...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048_bf16...
  0%|          | 0/32 [00:00<?, ?it/s]  3%|         | 1/32 [00:01<00:49,  1.59s/it]  6%|         | 2/32 [00:03<00:47,  1.58s/it]  9%|         | 3/32 [00:04<00:45,  1.57s/it] 12%|        | 4/32 [00:06<00:43,  1.57s/it] 16%|        | 5/32 [00:07<00:42,  1.57s/it] 19%|        | 6/32 [00:09<00:40,  1.57s/it] 22%|       | 7/32 [00:10<00:39,  1.57s/it] 25%|       | 8/32 [00:12<00:37,  1.57s/it] 28%|       | 9/32 [00:14<00:36,  1.57s/it] 31%|      | 10/32 [00:15<00:34,  1.57s/it] 34%|      | 11/32 [00:17<00:32,  1.57s/it] 38%|      | 12/32 [00:18<00:31,  1.57s/it] 41%|      | 13/32 [00:20<00:29,  1.57s/it] 44%|     | 14/32 [00:21<00:28,  1.57s/it] 47%|     | 15/32 [00:23<00:26,  1.57s/it] 50%|     | 16/32 [00:25<00:25,  1.57s/it] 53%|    | 17/32 [00:26<00:23,  1.57s/it] 56%|    | 18/32 [00:28<00:21,  1.57s/it] 59%|    | 19/32 [00:29<00:20,  1.57s/it][2023-10-02 02:19:54,238] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 22996
[2023-10-02 02:19:55,580] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-10-02 02:20:09,478] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 02:20:13,000] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-02 02:20:13,024] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-02 02:20:14,845] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 02:20:16,153] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-02 02:20:16,153] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-02 02:20:16,153] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-02 02:20:16,153] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-02 02:20:16,153] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-02 02:20:17,993] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu117'
[RWKV.model]: Preloading model from '/home/neromous/pyblackfog/rwkv-output-models/trpg/rwkv-60.pth'
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048_bf16/build.ninja...
Building extension module wkv_2048_bf16...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048_bf16...
  0%|          | 0/32 [00:00<?, ?it/s]  3%|         | 1/32 [00:01<00:49,  1.60s/it]  6%|         | 2/32 [00:03<00:47,  1.58s/it]  9%|         | 3/32 [00:04<00:45,  1.57s/it] 12%|        | 4/32 [00:06<00:43,  1.57s/it] 16%|        | 5/32 [00:07<00:42,  1.57s/it] 19%|        | 6/32 [00:09<00:40,  1.57s/it] 22%|       | 7/32 [00:10<00:39,  1.57s/it] 25%|       | 8/32 [00:12<00:37,  1.57s/it] 28%|       | 9/32 [00:14<00:36,  1.57s/it] 31%|      | 10/32 [00:15<00:34,  1.57s/it] 34%|      | 11/32 [00:17<00:32,  1.57s/it] 38%|      | 12/32 [00:18<00:31,  1.57s/it] 41%|      | 13/32 [00:20<00:29,  1.57s/it] 44%|     | 14/32 [00:21<00:28,  1.57s/it] 47%|     | 15/32 [00:23<00:26,  1.57s/it] 50%|     | 16/32 [00:25<00:25,  1.57s/it] 53%|    | 17/32 [00:26<00:23,  1.57s/it] 56%|    | 18/32 [00:28<00:21,  1.57s/it] 59%|    | 19/32 [00:29<00:20,  1.57s/it] 62%|   | 20/32 [00:31<00:18,  1.57s/it] 66%|   | 21/32 [00:32<00:17,  1.57s/it] 69%|   | 22/32 [00:34<00:15,  1.57s/it] 72%|  | 23/32 [00:36<00:14,  1.57s/it] 75%|  | 24/32 [00:37<00:12,  1.57s/it] 78%|  | 25/32 [00:39<00:10,  1.57s/it] 81%| | 26/32 [00:40<00:09,  1.57s/it] 84%| | 27/32 [00:42<00:07,  1.57s/it] 88%| | 28/32 [00:43<00:06,  1.57s/it] 91%| | 29/32 [00:45<00:04,  1.57s/it] 94%|| 30/32 [00:47<00:03,  1.57s/it] 97%|| 31/32 [00:48<00:01,  1.57s/it]100%|| 32/32 [00:50<00:00,  1.57s/it]100%|| 32/32 [00:50<00:00,  1.57s/it]
[2023-10-02 02:21:30,415] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 23337
[2023-10-02 02:21:31,946] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-10-02 02:22:02,729] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 02:22:06,266] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-02 02:22:06,290] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-02 02:22:08,118] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 02:22:09,421] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-02 02:22:09,421] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-02 02:22:09,421] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-02 02:22:09,421] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-02 02:22:09,421] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-02 02:22:11,224] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.454869270324707 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-02 02:23:38,244] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-02 02:23:38,244] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-02 02:23:38,244] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-02 02:23:45,234] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-02 02:23:45,236] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-02 02:23:45,237] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-02 02:23:45,277] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-02 02:23:45,278] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-02 02:23:45,278] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-10-02 02:23:45,278] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-02 02:23:45,278] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-02 02:23:45,278] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-02 02:23:45,278] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-02 02:24:18,647] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-02 02:24:18,648] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-02 02:24:18,648] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 36.8 GB, percent = 9.7%
[2023-10-02 02:24:51,163] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-02 02:24:51,164] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-02 02:24:51,164] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.66 GB, percent = 33.3%
[2023-10-02 02:24:51,164] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-02 02:24:52,226] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-02 02:24:52,227] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-02 02:24:52,227] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.66 GB, percent = 33.3%
[2023-10-02 02:24:52,259] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-02 02:24:52,259] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-02 02:24:52,259] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f81893e6ac0>
[2023-10-02 02:24:52,259] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-02 02:24:52,260] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-02 02:24:52,261] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-02 02:24:52,261] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-02 02:24:52,261] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-02 02:24:52,261] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-02 02:24:52,261] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-02 02:24:52,261] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-02 02:24:52,261] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-02 02:24:52,261] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-02 02:24:52,261] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-02 02:24:52,261] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f814d9c9b50>
[2023-10-02 02:24:52,261] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-02 02:24:52,262] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-02 02:24:52,262] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-02 02:24:52,262] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-02 02:24:52,262] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-02 02:24:52,262] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-02 02:24:52,262] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-02 02:24:52,262] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-02 02:24:52,262] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-02 02:24:52,262] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-02 02:24:52,262] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-02 02:24:52,262] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-02 02:24:52,262] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-02 02:24:52,262] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-02 02:24:52,262] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-02 02:24:52,262] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-02 02:24:52,262] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-02 02:24:52,262] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-02 02:24:52,262] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-02 02:24:52,262] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-02 02:24:52,262] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-10-02 02:24:52,262] [INFO] [config.py:971:print]   fp16_enabled ................. auto
[2023-10-02 02:24:52,262] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-02 02:24:52,262] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-02 02:24:52,263] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-02 02:24:52,264] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-02 02:24:52,264] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-02 02:24:52,264] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-02 02:24:52,264] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-02 02:24:52,264] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-02 02:24:52,264] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-02 02:24:52,264] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-02 02:24:52,264] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-02 02:24:52,264] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-02 02:24:52,264] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-02 02:24:52,264] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-02 02:24:52,264] [INFO] [config.py:957:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

None
None
172.16.1.39 - - [02/Oct/2023 02:25:02] "POST /state/reset HTTP/1.1" 200 21
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 32110.24it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 524964.50it/s]

172.16.1.39 - - [02/Oct/2023 02:25:38] "POST /inference/generate HTTP/1.1" 200 15344
None
None
172.16.1.39 - - [02/Oct/2023 02:26:32] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 02:27:10] "POST /inference/generate HTTP/1.1" 200 16206
None
None
172.16.1.39 - - [02/Oct/2023 02:27:56] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 02:28:27] "POST /inference/generate HTTP/1.1" 200 16471
None
None
172.16.1.39 - - [02/Oct/2023 02:30:04] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 02:30:40] "POST /inference/generate HTTP/1.1" 200 18437
None
None
172.16.1.39 - - [02/Oct/2023 02:32:33] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 02:33:13] "POST /inference/generate HTTP/1.1" 200 20116
None
None
172.16.1.39 - - [02/Oct/2023 02:34:17] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 02:35:02] "POST /inference/generate HTTP/1.1" 200 22421
None
None
172.16.1.39 - - [02/Oct/2023 02:35:08] "POST /state/reset HTTP/1.1" 200 21
  
172.16.1.39 - - [02/Oct/2023 02:35:56] "POST /inference/generate HTTP/1.1" 200 23636
None
None
172.16.1.39 - - [02/Oct/2023 02:39:56] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 02:40:17] "POST /inference/generate HTTP/1.1" 200 6484
None
None
172.16.1.39 - - [02/Oct/2023 02:42:19] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 02:42:35] "POST /inference/generate HTTP/1.1" 200 7063
None
None
172.16.1.39 - - [02/Oct/2023 02:43:52] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 02:44:13] "POST /inference/generate HTTP/1.1" 200 8307
None
None
172.16.1.39 - - [02/Oct/2023 02:44:20] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 02:44:43] "POST /inference/generate HTTP/1.1" 200 9872
None
None
172.16.1.39 - - [02/Oct/2023 02:45:15] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 02:45:42] "POST /inference/generate HTTP/1.1" 200 11005
None
None
172.16.1.39 - - [02/Oct/2023 02:46:45] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 02:47:10] "POST /inference/generate HTTP/1.1" 200 10992
None
None
172.16.1.39 - - [02/Oct/2023 02:48:33] "POST /state/reset HTTP/1.1" 200 21

1. 
2. 
3. 
4. 
5. G
6. 
7. 
8. 172.16.1.39 - - [02/Oct/2023 02:49:13] "POST /inference/generate HTTP/1.1" 200 14475
None
None
172.16.1.39 - - [02/Oct/2023 02:49:48] "POST /state/reset HTTP/1.1" 200 21

1. 
2. 
172.16.1.39 - - [02/Oct/2023 02:50:23] "POST /inference/generate HTTP/1.1" 200 14619
None
None
172.16.1.39 - - [02/Oct/2023 02:51:07] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 02:51:46] "POST /inference/generate HTTP/1.1" 200 15681
None
None
172.16.1.39 - - [02/Oct/2023 02:51:54] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 02:52:39] "POST /inference/generate HTTP/1.1" 200 17876
None
None
172.16.1.39 - - [02/Oct/2023 02:53:19] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 02:54:08] "POST /inference/generate HTTP/1.1" 200 19775
None
None
172.16.1.39 - - [02/Oct/2023 02:57:03] "POST /state/reset HTTP/1.1" 200 21

1. AI
2. 
3. 
4. AI
5. 
172.16.1.39 - - [02/Oct/2023 02:57:17] "POST /inference/generate HTTP/1.1" 200 3119
None
None
172.16.1.39 - - [02/Oct/2023 02:58:38] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 02:58:45] "POST /inference/generate HTTP/1.1" 200 3434
None
None
172.16.1.39 - - [02/Oct/2023 02:58:49] "POST /state/reset HTTP/1.1" 200 21
172.16.1.39 - - [02/Oct/2023 02:59:01] "POST /inference/generate HTTP/1.1" 200 4404
None
None
172.16.1.39 - - [02/Oct/2023 03:09:55] "POST /state/reset HTTP/1.1" 200 21
:
1. 
2. 
3. /
4. 
5. //
6. //
7. 
8. 
9. //
10. ///////////////
172.16.1.39 - - [02/Oct/2023 03:10:05] "POST /inference/generate HTTP/1.1" 200 2424
None
None
172.16.1.39 - - [02/Oct/2023 07:53:25] "POST /state/reset HTTP/1.1" 200 21

#+begin_action
[:user-computer/search-web   "https://baike.baidu.com/item/"  ]
#+end_action172.16.1.39 - - [02/Oct/2023 07:54:05] "POST /inference/generate HTTP/1.1" 200 15695
None
None
172.16.1.39 - - [02/Oct/2023 07:54:39] "POST /state/reset HTTP/1.1" 200 21
+begin_action+end_action

- +begin_actionsystem/search-web(www.baidu.com)
- +begin_actionsystem/print-web
172.16.1.39 - - [02/Oct/2023 07:55:26] "POST /inference/generate HTTP/1.1" 200 17357
None
None
172.16.1.39 - - [02/Oct/2023 07:56:08] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 07:56:11] "POST /inference/generate HTTP/1.1" 200 1333
None
None
172.16.1.39 - - [02/Oct/2023 07:56:32] "POST /state/reset HTTP/1.1" 200 21
: 
172.16.1.39 - - [02/Oct/2023 07:56:36] "POST /inference/generate HTTP/1.1" 200 1422
None
None
172.16.1.39 - - [02/Oct/2023 08:02:53] "POST /state/reset HTTP/1.1" 200 21
[2023-10-02 08:02:54,880] [INFO] [checkpointing.py:530:forward] Activation Checkpointing Information
[2023-10-02 08:02:54,880] [INFO] [checkpointing.py:531:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-10-02 08:02:54,880] [INFO] [checkpointing.py:532:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-10-02 08:02:54,880] [INFO] [checkpointing.py:534:forward] ----Synchronization False
[2023-10-02 08:02:54,880] [INFO] [checkpointing.py:535:forward] ----Profiling time in checkpointing False
[2023-10-02 08:03:01,154] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
[2023-10-02 08:03:05,285] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
172.16.1.39 - - [02/Oct/2023 08:03:41] "POST /train/tx-data HTTP/1.1" 200 23
None
None
172.16.1.39 - - [02/Oct/2023 08:03:53] "POST /state/reset HTTP/1.1" 200 21
172.16.1.39 - - [02/Oct/2023 08:04:48] "POST /train/tx-data HTTP/1.1" 200 26
None
None
172.16.1.39 - - [02/Oct/2023 09:46:37] "POST /state/reset HTTP/1.1" 200 21
[2023-10-02 09:47:12,170] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=2, lr=[9.000000000000002e-06, 9.000000000000002e-06, 9.000000000000002e-06], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
172.16.1.39 - - [02/Oct/2023 09:47:37] "POST /train/tx-data HTTP/1.1" 200 25
None
None
172.16.1.39 - - [02/Oct/2023 09:48:16] "POST /state/reset HTTP/1.1" 200 21
172.16.1.39 - - [02/Oct/2023 09:49:10] "POST /train/tx-data HTTP/1.1" 200 26
None
None
172.16.1.39 - - [02/Oct/2023 09:49:18] "POST /state/reset HTTP/1.1" 200 21
[2023-10-02 09:50:13,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=2, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
172.16.1.39 - - [02/Oct/2023 09:50:13] "POST /train/tx-data HTTP/1.1" 200 27
None
None
172.16.1.39 - - [02/Oct/2023 09:51:41] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 09:51:44] "POST /inference/generate HTTP/1.1" 200 1377
None
None
172.16.1.39 - - [02/Oct/2023 09:54:47] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 09:54:49] "POST /inference/generate HTTP/1.1" 200 1377
===saveved====
127.0.0.1 - - [02/Oct/2023 09:56:36] "POST /train/save-weight HTTP/1.1" 200 26
[2023-10-02 09:57:29,540] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 23679
[2023-10-02 09:57:29,542] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-02 09:57:41,109] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 09:57:44,650] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-02 09:57:44,674] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-02 09:57:46,500] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-02 09:57:47,807] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-02 09:57:47,807] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-02 09:57:47,807] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-02 09:57:47,807] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-02 09:57:47,808] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-02 09:57:49,663] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.467999458312988 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-02 09:59:16,162] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-02 09:59:16,163] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-02 09:59:16,163] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-02 09:59:21,914] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-02 09:59:21,916] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-02 09:59:21,916] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-02 09:59:21,957] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-02 09:59:21,957] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-02 09:59:21,957] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-10-02 09:59:21,957] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-02 09:59:21,958] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-02 09:59:21,958] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-02 09:59:21,958] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-02 09:59:50,957] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-02 09:59:50,958] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-02 09:59:50,958] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 36.83 GB, percent = 9.7%
[2023-10-02 10:00:21,775] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-02 10:00:21,776] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-02 10:00:21,776] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.77 GB, percent = 33.3%
[2023-10-02 10:00:21,776] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-02 10:00:22,596] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-02 10:00:22,596] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-02 10:00:22,597] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.77 GB, percent = 33.3%
[2023-10-02 10:00:22,619] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-02 10:00:22,619] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-02 10:00:22,619] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f4c29004ac0>
[2023-10-02 10:00:22,619] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-02 10:00:22,620] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-02 10:00:22,621] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-02 10:00:22,621] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-02 10:00:22,621] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-02 10:00:22,621] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-02 10:00:22,621] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-02 10:00:22,621] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-02 10:00:22,621] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-02 10:00:22,621] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-02 10:00:22,621] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-02 10:00:22,621] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f4bf20d1b50>
[2023-10-02 10:00:22,621] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-02 10:00:22,621] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-02 10:00:22,621] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-02 10:00:22,621] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-02 10:00:22,621] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-02 10:00:22,621] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-02 10:00:22,621] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-02 10:00:22,621] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-02 10:00:22,621] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   fp16_enabled ................. auto
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-02 10:00:22,622] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-02 10:00:22,623] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-02 10:00:22,624] [INFO] [config.py:957:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

None
None
172.16.1.39 - - [02/Oct/2023 10:01:40] "POST /state/reset HTTP/1.1" 200 21
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 33819.41it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 537328.84it/s]
 
172.16.1.39 - - [02/Oct/2023 10:01:43] "POST /inference/generate HTTP/1.1" 200 1377
None
None
172.16.1.39 - - [02/Oct/2023 10:02:26] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 10:02:29] "POST /inference/generate HTTP/1.1" 200 1377
None
None
172.16.1.39 - - [02/Oct/2023 10:03:50] "POST /state/reset HTTP/1.1" 200 21
None
None
172.16.1.39 - - [02/Oct/2023 10:04:02] "POST /state/reset HTTP/1.1" 200 21
None
None
172.16.1.39 - - [02/Oct/2023 10:04:27] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 10:04:30] "POST /inference/generate HTTP/1.1" 200 1522
None
None
172.16.1.39 - - [02/Oct/2023 10:05:18] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 10:05:21] "POST /inference/generate HTTP/1.1" 200 1525
None
None
172.16.1.39 - - [02/Oct/2023 10:05:41] "POST /state/reset HTTP/1.1" 200 21
  
172.16.1.39 - - [02/Oct/2023 10:05:47] "POST /inference/generate HTTP/1.1" 200 2337
None
None
172.16.1.39 - - [02/Oct/2023 10:45:18] "POST /state/reset HTTP/1.1" 200 21
None
None
172.16.1.39 - - [02/Oct/2023 10:45:41] "POST /state/reset HTTP/1.1" 200 21
None
None
172.16.1.39 - - [02/Oct/2023 10:47:12] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 10:47:18] "POST /inference/generate HTTP/1.1" 200 2366
None
None
172.16.1.39 - - [02/Oct/2023 11:16:19] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 11:16:23] "POST /inference/generate HTTP/1.1" 200 2187
None
None
172.16.1.39 - - [02/Oct/2023 11:19:19] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 11:19:21] "POST /inference/generate HTTP/1.1" 200 1394
None
None
172.16.1.39 - - [02/Oct/2023 11:19:38] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 11:19:43] "POST /inference/generate HTTP/1.1" 200 1986
None
None
172.16.1.39 - - [02/Oct/2023 11:21:34] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 11:21:37] "POST /inference/generate HTTP/1.1" 200 1986
None
None
172.16.1.39 - - [02/Oct/2023 11:22:56] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 11:23:00] "POST /inference/generate HTTP/1.1" 200 2104
None
None
172.16.1.39 - - [02/Oct/2023 11:24:57] "POST /state/reset HTTP/1.1" 200 21
google 
172.16.1.39 - - [02/Oct/2023 11:25:01] "POST /inference/generate HTTP/1.1" 200 1608
None
None
172.16.1.39 - - [02/Oct/2023 11:25:09] "POST /state/reset HTTP/1.1" 200 21
google 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
172.16.1.39 - - [02/Oct/2023 11:25:29] "POST /inference/generate HTTP/1.1" 200 4699
None
None
172.16.1.39 - - [02/Oct/2023 11:26:23] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 11:26:30] "POST /inference/generate HTTP/1.1" 200 3248
None
None
172.16.1.39 - - [02/Oct/2023 11:29:25] "POST /state/reset HTTP/1.1" 200 21
:
1. 
2. 
3. 
4. , , , 
5. , , , 
6. , ,
7. , 172.16.1.39 - - [02/Oct/2023 11:29:44] "POST /inference/generate HTTP/1.1" 200 5407
None
None
172.16.1.39 - - [02/Oct/2023 11:31:04] "POST /state/reset HTTP/1.1" 200 21
  
1. 
2. 
3. 
4. 
5. 
6. 
7. 172.16.1.39 - - [02/Oct/2023 11:31:19] "POST /inference/generate HTTP/1.1" 200 4623
None
None
172.16.1.39 - - [02/Oct/2023 11:31:25] "POST /state/reset HTTP/1.1" 200 21
google
1. 
2. 
3. 
   
4. 
   
   
   
   
5. 
   
6. 
   
7. 
   
8. 
   
9. 
   
10. 
   172.16.1.39 - - [02/Oct/2023 11:32:20] "POST /inference/generate HTTP/1.1" 200 12868
None
None
172.16.1.39 - - [02/Oct/2023 11:33:24] "POST /state/reset HTTP/1.1" 200 21

1. ()
2. 
3. 
4. 
5. 1592-1594
6. 
7. 
8. 
9. 
10. 
11. 1577
12. 
13. 
14. (1368-1398)
15.  Kaken Jien
16. 
17. 
172.16.1.39 - - [02/Oct/2023 11:33:55] "POST /inference/generate HTTP/1.1" 200 6764
None
None
172.16.1.39 - - [02/Oct/2023 11:34:01] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 11:34:30] "POST /inference/generate HTTP/1.1" 200 11777
None
None
172.16.1.39 - - [02/Oct/2023 11:35:55] "POST /state/reset HTTP/1.1" 200 21


172.16.1.39 - - [02/Oct/2023 11:36:04] "POST /inference/generate HTTP/1.1" 200 3940
None
None
172.16.1.39 - - [02/Oct/2023 11:36:26] "POST /state/reset HTTP/1.1" 200 21

1. 
2. 
3.  mischief
4. 
5. 
6. 
7. 
172.16.1.39 - - [02/Oct/2023 11:36:49] "POST /inference/generate HTTP/1.1" 200 6564
None
None
172.16.1.39 - - [02/Oct/2023 11:40:35] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 11:40:45] "POST /inference/generate HTTP/1.1" 200 4313
None
None
172.16.1.39 - - [02/Oct/2023 11:41:10] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 11:41:21] "POST /inference/generate HTTP/1.1" 200 5253
None
None
172.16.1.39 - - [02/Oct/2023 11:41:27] "POST /state/reset HTTP/1.1" 200 21
  :
1.  
2. :
a). , ;
b). ;
c;
d;
e) 
3. :
a) :,
b) :
c) :
4. ,:
a) ,
b) ,
5. ,,,172.16.1.39 - - [02/Oct/2023 11:41:55] "POST /inference/generate HTTP/1.1" 200 8065
None
None
172.16.1.39 - - [02/Oct/2023 11:43:26] "POST /state/reset HTTP/1.1" 200 21


:
1. 
2. 
3. 
4. 
5. 
,,,172.16.1.39 - - [02/Oct/2023 11:44:03] "POST /inference/generate HTTP/1.1" 200 12699
None
None
172.16.1.39 - - [02/Oct/2023 11:46:11] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 11:46:39] "POST /inference/generate HTTP/1.1" 200 12250
None
None
172.16.1.39 - - [02/Oct/2023 11:47:20] "POST /state/reset HTTP/1.1" 200 21

1.  
2. , , :
a) 
b) ,
c) ,,
d) ,,
e) ,
f) ,
3. ,,
4. ,
172.16.1.39 - - [02/Oct/2023 11:48:02] "POST /inference/generate HTTP/1.1" 200 14669
None
None
172.16.1.39 - - [02/Oct/2023 11:51:32] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 11:52:18] "POST /inference/generate HTTP/1.1" 200 18906
None
None
172.16.1.39 - - [02/Oct/2023 11:52:52] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 11:53:41] "POST /inference/generate HTTP/1.1" 200 19703
None
None
172.16.1.39 - - [02/Oct/2023 11:53:50] "POST /state/reset HTTP/1.1" 200 21
 ,
1. ,
2. ,,
3. ,
4. ,
5. , responsibilities
6. ,
7. 
8. ,,,
9. ,
10. !!172.16.1.39 - - [02/Oct/2023 11:54:52] "POST /inference/generate HTTP/1.1" 200 22335
None
None
172.16.1.39 - - [02/Oct/2023 11:58:03] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 11:58:07] "POST /inference/generate HTTP/1.1" 200 1552
None
None
172.16.1.39 - - [02/Oct/2023 11:59:17] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 11:59:21] "POST /inference/generate HTTP/1.1" 200 1552
None
None
172.16.1.39 - - [02/Oct/2023 11:59:50] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 11:59:56] "POST /inference/generate HTTP/1.1" 200 1552
None
None
172.16.1.39 - - [02/Oct/2023 12:05:38] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 12:05:42] "POST /inference/generate HTTP/1.1" 200 1552
None
None
172.16.1.39 - - [02/Oct/2023 12:05:47] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 12:05:56] "POST /inference/generate HTTP/1.1" 200 2712
None
None
172.16.1.39 - - [02/Oct/2023 12:06:00] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 12:06:12] "POST /inference/generate HTTP/1.1" 200 3882
None
None
172.16.1.39 - - [02/Oct/2023 12:06:14] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 12:06:29] "POST /inference/generate HTTP/1.1" 200 5079
None
None
172.16.1.39 - - [02/Oct/2023 12:06:34] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 12:06:53] "POST /inference/generate HTTP/1.1" 200 6383
None
None
172.16.1.39 - - [02/Oct/2023 12:07:37] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 12:07:50] "POST /inference/generate HTTP/1.1" 200 4787
None
None
172.16.1.39 - - [02/Oct/2023 12:07:54] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 12:08:09] "POST /inference/generate HTTP/1.1" 200 5626
None
None
172.16.1.39 - - [02/Oct/2023 12:08:19] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 12:08:37] "POST /inference/generate HTTP/1.1" 200 6552
None
None
172.16.1.39 - - [02/Oct/2023 12:08:42] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 12:09:02] "POST /inference/generate HTTP/1.1" 200 7601
None
None
172.16.1.39 - - [02/Oct/2023 12:09:06] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 12:09:29] "POST /inference/generate HTTP/1.1" 200 8590
None
None
172.16.1.39 - - [02/Oct/2023 13:03:09] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 13:03:25] "POST /inference/generate HTTP/1.1" 200 5852
None
None
172.16.1.39 - - [02/Oct/2023 13:03:33] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 13:03:49] "POST /inference/generate HTTP/1.1" 200 6544
None
None
172.16.1.39 - - [02/Oct/2023 13:06:59] "POST /state/reset HTTP/1.1" 200 21
   
172.16.1.39 - - [02/Oct/2023 13:07:20] "POST /inference/generate HTTP/1.1" 200 6055
None
None
172.16.1.39 - - [02/Oct/2023 13:07:31] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 13:07:53] "POST /inference/generate HTTP/1.1" 200 6977
None
None
172.16.1.39 - - [02/Oct/2023 13:10:30] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 13:10:49] "POST /inference/generate HTTP/1.1" 200 7526
None
None
172.16.1.39 - - [02/Oct/2023 13:10:55] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 13:11:15] "POST /inference/generate HTTP/1.1" 200 8225
None
None
172.16.1.39 - - [02/Oct/2023 13:11:44] "POST /state/reset HTTP/1.1" 200 21
 pose
172.16.1.39 - - [02/Oct/2023 13:12:13] "POST /inference/generate HTTP/1.1" 200 8534
None
None
172.16.1.39 - - [02/Oct/2023 13:13:39] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 13:14:23] "POST /inference/generate HTTP/1.1" 200 14055
None
None
172.16.1.39 - - [02/Oct/2023 13:14:54] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 13:15:41] "POST /inference/generate HTTP/1.1" 200 14814
None
None
172.16.1.39 - - [02/Oct/2023 13:16:34] "POST /state/reset HTTP/1.1" 200 21
2024
  
172.16.1.39 - - [02/Oct/2023 13:17:26] "POST /inference/generate HTTP/1.1" 200 15478
None
None
172.16.1.39 - - [02/Oct/2023 13:17:49] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 13:18:43] "POST /inference/generate HTTP/1.1" 200 16976
None
None
172.16.1.39 - - [02/Oct/2023 13:21:00] "POST /state/reset HTTP/1.1" 200 21
  

172.16.1.39 - - [02/Oct/2023 13:21:29] "POST /inference/generate HTTP/1.1" 200 8145
None
None
172.16.1.39 - - [02/Oct/2023 13:21:36] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 13:22:05] "POST /inference/generate HTTP/1.1" 200 8978
None
None
172.16.1.39 - - [02/Oct/2023 13:23:15] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 13:23:18] "POST /inference/generate HTTP/1.1" 200 1845
None
None
172.16.1.39 - - [02/Oct/2023 13:23:23] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 13:23:29] "POST /inference/generate HTTP/1.1" 200 2427
None
None
172.16.1.39 - - [02/Oct/2023 13:23:46] "POST /state/reset HTTP/1.1" 200 21
: 
172.16.1.39 - - [02/Oct/2023 13:23:51] "POST /inference/generate HTTP/1.1" 200 2616
None
None
172.16.1.39 - - [02/Oct/2023 13:24:26] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 13:24:32] "POST /inference/generate HTTP/1.1" 200 2851
None
None
172.16.1.39 - - [02/Oct/2023 13:26:09] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 13:26:37] "POST /inference/generate HTTP/1.1" 200 11540
None
None
172.16.1.39 - - [02/Oct/2023 13:28:43] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 13:29:13] "POST /inference/generate HTTP/1.1" 200 12779
None
None
172.16.1.39 - - [02/Oct/2023 13:30:23] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 13:30:54] "POST /inference/generate HTTP/1.1" 200 14371
None
None
172.16.1.39 - - [02/Oct/2023 13:32:06] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 13:32:39] "POST /inference/generate HTTP/1.1" 200 14963
None
None
172.16.1.39 - - [02/Oct/2023 13:43:12] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 13:44:14] "POST /inference/generate HTTP/1.1" 200 25082
None
None
172.16.1.39 - - [02/Oct/2023 14:15:47] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 14:16:47] "POST /inference/generate HTTP/1.1" 200 25969
None
None
172.16.1.39 - - [02/Oct/2023 14:20:58] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 14:21:03] "POST /inference/generate HTTP/1.1" 200 2341
None
None
172.16.1.39 - - [02/Oct/2023 14:21:40] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 14:21:47] "POST /inference/generate HTTP/1.1" 200 3091
None
None
172.16.1.39 - - [02/Oct/2023 14:22:21] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 14:22:47] "POST /inference/generate HTTP/1.1" 200 10544
None
None
172.16.1.39 - - [02/Oct/2023 14:25:16] "POST /state/reset HTTP/1.1" 200 21




































































172.16.1.39 - - [02/Oct/2023 14:26:45] "POST /inference/generate HTTP/1.1" 200 17795
None
None
172.16.1.39 - - [02/Oct/2023 14:29:53] "POST /state/reset HTTP/1.1" 200 21

























 serotonin
 serotonin, dopamine serotonin,
,,:
,,,
,,
172.16.1.39 - - [02/Oct/2023 14:31:20] "POST /inference/generate HTTP/1.1" 200 25254
None
None
172.16.1.39 - - [02/Oct/2023 14:31:44] "POST /state/reset HTTP/1.1" 200 21


app
: 
172.16.1.39 - - [02/Oct/2023 14:32:35] "POST /inference/generate HTTP/1.1" 200 20277
None
None
172.16.1.39 - - [02/Oct/2023 14:34:18] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 14:34:36] "POST /inference/generate HTTP/1.1" 200 7088
None
None
172.16.1.39 - - [02/Oct/2023 14:35:13] "POST /state/reset HTTP/1.1" 200 21









































Macbook Air



macOS SierraFinder24





172.16.1.39 - - [02/Oct/2023 14:36:45] "POST /inference/generate HTTP/1.1" 200 18338
None
None
172.16.1.39 - - [02/Oct/2023 14:37:34] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 14:38:00] "POST /inference/generate HTTP/1.1" 200 10274
None
None
172.16.1.39 - - [02/Oct/2023 14:38:20] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 14:38:47] "POST /inference/generate HTTP/1.1" 200 10700
None
None
172.16.1.39 - - [02/Oct/2023 14:39:52] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 14:40:07] "POST /inference/generate HTTP/1.1" 200 6658
None
None
172.16.1.39 - - [02/Oct/2023 14:43:04] "POST /state/reset HTTP/1.1" 200 21























[ 172.16.1.39 - - [02/Oct/2023 14:44:45] "POST /inference/generate HTTP/1.1" 200 34456
None
None
172.16.1.39 - - [02/Oct/2023 14:48:09] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 14:48:25] "POST /inference/generate HTTP/1.1" 200 6837
None
None
172.16.1.39 - - [02/Oct/2023 14:49:03] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 14:49:21] "POST /inference/generate HTTP/1.1" 200 6953
None
None
172.16.1.39 - - [02/Oct/2023 14:50:56] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [02/Oct/2023 14:52:21] "POST /inference/generate HTTP/1.1" 200 34672
None
None
172.16.1.39 - - [02/Oct/2023 15:20:53] "POST /state/reset HTTP/1.1" 200 21
TM
172.16.1.39 - - [02/Oct/2023 15:20:56] "POST /inference/generate HTTP/1.1" 200 1425
None
None
172.16.1.39 - - [02/Oct/2023 15:21:00] "POST /state/reset HTTP/1.1" 200 21
TM
172.16.1.39 - - [02/Oct/2023 15:21:05] "POST /inference/generate HTTP/1.1" 200 2132
None
None
172.16.1.39 - - [02/Oct/2023 15:21:42] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 15:21:45] "POST /inference/generate HTTP/1.1" 200 1357
None
None
172.16.1.39 - - [02/Oct/2023 15:22:23] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 15:22:28] "POST /inference/generate HTTP/1.1" 200 2143
None
None
172.16.1.39 - - [02/Oct/2023 15:22:36] "POST /state/reset HTTP/1.1" 200 21
None
None
172.16.1.39 - - [02/Oct/2023 15:22:48] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 15:22:54] "POST /inference/generate HTTP/1.1" 200 2889
None
None
172.16.1.39 - - [02/Oct/2023 15:23:10] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 15:23:18] "POST /inference/generate HTTP/1.1" 200 3872
None
None
172.16.1.39 - - [02/Oct/2023 15:24:18] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 15:24:26] "POST /inference/generate HTTP/1.1" 200 4169
None
None
172.16.1.39 - - [02/Oct/2023 15:25:41] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 15:26:13] "POST /inference/generate HTTP/1.1" 200 13404
None
None
172.16.1.39 - - [02/Oct/2023 15:26:22] "POST /state/reset HTTP/1.1" 200 21


172.16.1.39 - - [02/Oct/2023 15:26:56] "POST /inference/generate HTTP/1.1" 200 14336
None
None
172.16.1.39 - - [02/Oct/2023 15:28:59] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 15:29:37] "POST /inference/generate HTTP/1.1" 200 16088
None
None
172.16.1.39 - - [02/Oct/2023 16:08:38] "POST /state/reset HTTP/1.1" 200 21
[2023-10-02 16:08:40,340] [INFO] [checkpointing.py:530:forward] Activation Checkpointing Information
[2023-10-02 16:08:40,340] [INFO] [checkpointing.py:531:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-10-02 16:08:40,340] [INFO] [checkpointing.py:532:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-10-02 16:08:40,340] [INFO] [checkpointing.py:534:forward] ----Synchronization False
[2023-10-02 16:08:40,340] [INFO] [checkpointing.py:535:forward] ----Profiling time in checkpointing False
[2023-10-02 16:08:46,295] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
172.16.1.39 - - [02/Oct/2023 16:08:46] "POST /train/tx-data HTTP/1.1" 200 18
None
None
172.16.1.39 - - [02/Oct/2023 16:09:01] "POST /state/reset HTTP/1.1" 200 21
[2023-10-02 16:09:06,890] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
172.16.1.39 - - [02/Oct/2023 16:09:06] "POST /train/tx-data HTTP/1.1" 200 18
None
None
172.16.1.39 - - [02/Oct/2023 16:10:00] "POST /state/reset HTTP/1.1" 200 21
172.16.1.39 - - [02/Oct/2023 16:10:20] "POST /train/tx-data HTTP/1.1" 200 18
None
None
172.16.1.39 - - [02/Oct/2023 16:10:32] "POST /state/reset HTTP/1.1" 200 21
172.16.1.39 - - [02/Oct/2023 16:11:01] "POST /train/tx-data HTTP/1.1" 200 19
None
None
172.16.1.39 - - [02/Oct/2023 16:42:57] "POST /state/reset HTTP/1.1" 200 21
[2023-10-02 16:43:02,024] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
172.16.1.39 - - [02/Oct/2023 16:43:02] "POST /train/tx-data HTTP/1.1" 200 20
None
None
172.16.1.39 - - [02/Oct/2023 16:43:12] "POST /state/reset HTTP/1.1" 200 21
25
172.16.1.39 - - [02/Oct/2023 16:43:16] "POST /inference/generate HTTP/1.1" 200 1610
None
None
172.16.1.39 - - [02/Oct/2023 16:45:11] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 16:45:16] "POST /inference/generate HTTP/1.1" 200 2418
None
None
172.16.1.39 - - [02/Oct/2023 17:16:35] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 17:16:43] "POST /inference/generate HTTP/1.1" 200 3286
None
None
172.16.1.39 - - [02/Oct/2023 17:16:49] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 17:16:58] "POST /inference/generate HTTP/1.1" 200 4287
None
None
172.16.1.39 - - [02/Oct/2023 17:17:34] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 17:17:45] "POST /inference/generate HTTP/1.1" 200 5011
None
None
172.16.1.39 - - [02/Oct/2023 17:29:45] "POST /state/reset HTTP/1.1" 200 21

2. 

3. 

4. 

5. 

6. 

7. 

8. 

9. 

10. 
 
11. 

12. 

13. 

14. 

15. 
 
16. 

17. 

18. 

19. 

20. 
 
21. 

22. 

23. 

24. 

25. 
 
26. 

27. 

28. 

29. 

30. 

31. 

32. 

33. 

34. 

35. 

36. 

37. 

38. 

39. 

40. 
 
41. 

42. 

43. 

44. 

45. 

46. 

47. 

48. 

49. 

50. 
 
51. 

52. 

53. 

54. 
172.16.1.39 - - [02/Oct/2023 17:31:02] "POST /inference/generate HTTP/1.1" 200 12493
None
None
172.16.1.39 - - [02/Oct/2023 17:36:03] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 17:36:05] "POST /inference/generate HTTP/1.1" 200 1303
None
None
172.16.1.39 - - [02/Oct/2023 17:36:11] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [02/Oct/2023 17:36:16] "POST /inference/generate HTTP/1.1" 200 1923
None
None
172.16.1.39 - - [03/Oct/2023 10:50:59] "POST /state/reset HTTP/1.1" 200 21

Answer: 

7

~
~ youtube1100
youtube



youtube1
~ youtube1100
youtube1



50




Answer: youtube1

Question: youtube1100
youtube
youtube







172.16.1.39 - - [03/Oct/2023 10:52:18] "POST /inference/generate HTTP/1.1" 200 13636
None
None
172.16.1.39 - - [03/Oct/2023 10:53:34] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [03/Oct/2023 10:53:38] "POST /inference/generate HTTP/1.1" 200 1722
None
None
172.16.1.39 - - [03/Oct/2023 10:53:44] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [03/Oct/2023 10:53:49] "POST /inference/generate HTTP/1.1" 200 2471
None
None
172.16.1.39 - - [03/Oct/2023 10:54:34] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [03/Oct/2023 10:54:42] "POST /inference/generate HTTP/1.1" 200 3310
None
None
172.16.1.39 - - [03/Oct/2023 13:22:00] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [03/Oct/2023 13:22:08] "POST /inference/generate HTTP/1.1" 200 4235
None
None
172.16.1.39 - - [03/Oct/2023 13:22:15] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [03/Oct/2023 13:22:25] "POST /inference/generate HTTP/1.1" 200 4852
None
None
172.16.1.39 - - [03/Oct/2023 13:22:32] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [03/Oct/2023 13:22:44] "POST /inference/generate HTTP/1.1" 200 5669
None
None
172.16.1.39 - - [03/Oct/2023 13:22:54] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [03/Oct/2023 13:23:08] "POST /inference/generate HTTP/1.1" 200 6507
None
None
172.16.1.39 - - [03/Oct/2023 13:23:14] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [03/Oct/2023 13:23:29] "POST /inference/generate HTTP/1.1" 200 7305
None
None
172.16.1.39 - - [03/Oct/2023 13:59:21] "POST /state/reset HTTP/1.1" 200 21
 
#+begin_action  magic_cast
[:use  [:clojure :as clj]]
[:def  find-in-set   [ col vals ]
(reduce + vals col)]
[:find-in-set "1" (clj/list [ 1 2 3 4 5 6 7 8 9 10])]
#+end_action
#+begin_action  magic_cast
[:find {:in $ :name} "John"]
#+end_action
#+begin_action  magic_cast
[:find {:in $ :name} "Bob"]
#+end_action

#+RESULTS: 
{:name "Bob"}

#+RESULTS: {:name "John"}

#+RESULTS: {:name "Alice"}

#+RESULTS: {:name "Charlie"}

#+RESULTS: {:name "David"}
#+RESULTS: {:name "Eve"}
172.16.1.39 - - [03/Oct/2023 13:59:56] "POST /inference/generate HTTP/1.1" 200 10547
None
None
172.16.1.39 - - [03/Oct/2023 14:01:05] "POST /state/reset HTTP/1.1" 200 21
 
#+begin_action  magic_cast
[:use-componets "datomic" "jdbc"]
(d/q '[:find ?e
        [?e :name ?name]
        [?e :person/name ?name]
        [?e :person/contact-info ?name]]
#+end_action
)

#+RESULTS:
{:name "John"}
#+RESULTS: {:name "John"}
#+RESULTS: {:name "John"}
#+RESULTS: {:name "John"}
#+RESULTS: {:name "John"}
#+RESULTS: {:name "John"}
#+RESULTS: {:name "John"}
...172.16.1.39 - - [03/Oct/2023 14:01:39] "POST /inference/generate HTTP/1.1" 200 10505
None
None
172.16.1.39 - - [03/Oct/2023 14:01:51] "POST /state/reset HTTP/1.1" 200 21
, , AI
AI:  , , , , , ,
:
#+begin_action  magic_cast
[: ?/name ?]
#+end_action
#+RESULTS:
name: John
: ,,,,,
, ,
172.16.1.39 - - [03/Oct/2023 14:02:38] "POST /inference/generate HTTP/1.1" 200 13971
None
None
172.16.1.39 - - [03/Oct/2023 14:03:49] "POST /state/reset HTTP/1.1" 200 21
  clojure
172.16.1.39 - - [03/Oct/2023 14:04:22] "POST /inference/generate HTTP/1.1" 200 11474
None
None
172.16.1.39 - - [03/Oct/2023 14:39:15] "POST /state/reset HTTP/1.1" 200 21

1. Datomic
2. Datomic
3. Datomic
4. Datomic
5. Datomic
6. demo
7. DatomicNoSQL
8. Datomic
9. Datomic
10. Datomic
 DATOMIC
172.16.1.39 - - [03/Oct/2023 14:39:51] "POST /inference/generate HTTP/1.1" 200 11643
None
None
172.16.1.39 - - [03/Oct/2023 14:42:56] "POST /state/reset HTTP/1.1" 200 21
:
1.  Datomic ,, pull ,
2.  datomic :,
3.  datomic , master 
4.  Datomic ,
,172.16.1.39 - - [03/Oct/2023 14:43:31] "POST /inference/generate HTTP/1.1" 200 11124
None
None
172.16.1.39 - - [03/Oct/2023 14:43:51] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [03/Oct/2023 14:44:26] "POST /inference/generate HTTP/1.1" 200 12461
None
None
172.16.1.39 - - [03/Oct/2023 14:45:52] "POST /state/reset HTTP/1.1" 200 21
:
1. clojuredatum
2. datomic
3. datomic
4. transact
5. "mymaster""mydatabase"datomic
6. #begin_action#end_action
7. ,datomic
,,172.16.1.39 - - [03/Oct/2023 14:46:29] "POST /inference/generate HTTP/1.1" 200 11546
None
None
172.16.1.39 - - [03/Oct/2023 14:46:39] "POST /state/reset HTTP/1.1" 200 21
:
,datomic
1. datomic,
2. conn#finddatomic
3. #+begin_action#end_action
4. #begin_transaction#end_transaction
5. #commit
6. transact,:where
7. conn#rollback
8. conn#commit
9. ,datomic,
10. ,stringdatabase
,datomic,172.16.1.39 - - [03/Oct/2023 14:47:28] "POST /inference/generate HTTP/1.1" 200 14763
None
None
172.16.1.39 - - [03/Oct/2023 14:49:50] "POST /state/reset HTTP/1.1" 200 21

#+begin_action
[:use [[:datomic :as] ?r ]
[:defrecord  [:r "user" ][:db/name  ?r ]
[:defrecord  [:r  "address" ][:db/name ?r ]
[:defrecord  [:r  "phone" ][:db/name ?r ]
[:defrecord  "*"   [:db/name ?r ]
]]
#+end_action172.16.1.39 - - [03/Oct/2023 14:50:28] "POST /inference/generate HTTP/1.1" 200 13227
None
None
172.16.1.39 - - [03/Oct/2023 15:15:06] "POST /state/reset HTTP/1.1" 200 21
 
1. datomic 
a) datomic,
b) datomicclojure,
c) ,datomic
d) 
2. clojure:
a) clojure,
b) clojure,
c) clojure,
d) 
3. clojureatom:
a) atomdatomic
b) atomimmutable data structure,
c) atomdatomic,
d) atomdatomic,
4. clojureatom:
a) atomdatomic,datomic""
b) atom,
c) atomdatomic
d) ,atom
5. clojureatom:
a) atomdatomic,datomic
b) clojureatom,
c) clojureimmutable,
d) clojuremutable,
6. clojureatom:
a) atomdatomicimmutable value()," atoms"
b) clojureatoms,( relational databases or key-value stores)datomic
c) datomic,atom(mutable value), atoms
d) Atomicsdatomic(mutable data),AtomsDatums in Datomic are atomic in nature and can be manipulated as such.atoms are just a name used to group Datums for the purpose of data manipulation.
7. clojure:
a) clojure,
b) clojurehappens-before
c) clojure,
d) clojure,
8. clojure:
a) clojurehappens-before
b) clojure(record)
c) clojure,172.16.1.39 - - [03/Oct/2023 15:16:37] "POST /inference/generate HTTP/1.1" 200 23771
None
None
172.16.1.39 - - [03/Oct/2023 15:19:05] "POST /state/reset HTTP/1.1" 200 21
master
172.16.1.39 - - [03/Oct/2023 15:19:55] "POST /inference/generate HTTP/1.1" 200 18602
None
None
172.16.1.39 - - [03/Oct/2023 15:21:40] "POST /state/reset HTTP/1.1" 200 21
assistant"[+]"
assistantpromptassistantprompt
assistant"[+]"stable diffusion prompt
#+begin_action computer
[:use-computer/search-web  " " :user "172.16.1.39 - - [03/Oct/2023 15:22:40] "POST /inference/generate HTTP/1.1" 200 21455
None
None
172.16.1.39 - - [03/Oct/2023 15:29:48] "POST /state/reset HTTP/1.1" 200 21


master
:
#+begin_action computer
[:use-computer/computer "alice"]
#+end_action172.16.1.39 - - [03/Oct/2023 15:30:51] "POST /inference/generate HTTP/1.1" 200 21590
None
None
172.16.1.39 - - [03/Oct/2023 15:44:08] "POST /state/reset HTTP/1.1" 200 21
:
 

172.16.1.39 - - [03/Oct/2023 15:45:24] "POST /inference/generate HTTP/1.1" 200 28455
None
None
172.16.1.39 - - [03/Oct/2023 16:40:28] "POST /state/reset HTTP/1.1" 200 21
:
 

172.16.1.39 - - [03/Oct/2023 16:41:42] "POST /inference/generate HTTP/1.1" 200 28455
===saveved====
127.0.0.1 - - [03/Oct/2023 16:55:20] "POST /train/save-weight HTTP/1.1" 200 26
[2023-10-03 16:56:09,370] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 37742
[2023-10-03 16:56:09,372] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-03 17:03:38,214] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 17:03:39,611] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-03 17:03:39,635] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-03 17:03:41,473] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 17:03:42,793] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-03 17:03:42,793] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-03 17:03:42,793] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-03 17:03:42,793] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-03 17:03:42,793] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-03 17:03:44,615] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.642753839492798 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-03 17:05:13,062] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-03 17:05:13,062] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-03 17:05:13,062] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-03 17:05:19,056] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-03 17:05:19,058] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-03 17:05:19,058] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-03 17:05:19,098] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-03 17:05:19,098] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-03 17:05:19,098] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-10-03 17:05:19,098] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-03 17:05:19,098] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-03 17:05:19,098] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-03 17:05:19,098] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-03 17:05:54,897] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-03 17:05:54,898] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 17:05:54,898] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 36.99 GB, percent = 9.8%
[2023-10-03 17:06:26,623] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-03 17:06:26,624] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 17:06:26,624] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.9 GB, percent = 33.3%
[2023-10-03 17:06:26,624] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-03 17:06:27,547] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-03 17:06:27,548] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 17:06:27,548] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.9 GB, percent = 33.3%
[2023-10-03 17:06:27,572] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-03 17:06:27,572] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-03 17:06:27,572] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f319e2d7970>
[2023-10-03 17:06:27,573] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-03 17:06:27,574] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-03 17:06:27,574] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-03 17:06:27,574] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-03 17:06:27,574] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-03 17:06:27,574] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-03 17:06:27,574] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-03 17:06:27,574] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-03 17:06:27,574] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-03 17:06:27,574] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-03 17:06:27,574] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f315cc5ca00>
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   fp16_enabled ................. auto
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-03 17:06:27,575] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-03 17:06:27,576] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-03 17:06:27,577] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-03 17:06:27,577] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-03 17:06:27,577] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-03 17:06:27,577] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-03 17:06:27,577] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-03 17:06:27,577] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-03 17:06:27,577] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-03 17:06:27,577] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-03 17:06:27,577] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-03 17:06:27,577] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-03 17:06:27,577] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-03 17:06:27,577] [INFO] [config.py:957:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

None
None
172.16.1.39 - - [03/Oct/2023 17:08:02] "POST /state/reset HTTP/1.1" 200 21
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 20695.40it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 190799.20it/s]
<|me|>

<|me|><|response|> Datomic 
#+begin_action  magic  :reload_schema
[:db/class "Transaction"]
#+end_action
(def tx (make-instance 'transaction :name "my transaction name" :participants #{}))
;; 
;; :
(tx/commit tx)
;; 
#+RESULTS: 
[{:db/ident :mytransaction/name, :db.type '(), :db.value nil}
#+RESULTSET: {:db.type 'transaction', :participants #1, :name "my transaction name", :commit?true, :rollback.false}
```
my transaction[:begin_action/transaction]
[:customer/find](https://clojuredocs.org/api/subprocesses#clojure.core/subprocess-find)name[:db/commit](https://clojuredocs.org/api/doc#clojure.core/db%3Fsubprocess%3F-find%3F%3F)"

rollback[:find](https://clojuredocs.org/apidocs/clojure/#clojure.core/find)"nil
 into a batch, ensuring that they are either both executed or both not executed at all. This allows us to ensure the atomicity, consistency, isolation and durability of the data stored in the database.
 datomic 
#+begin_action  magic  :reload_schema
[:db/class/identity "com.datomic.DataSource"]
#+end_action
#+RESULTS: %{NAME="com.datomic.DataSource", PUBLIC_METHODS=[reload_schema]}, TRANSACTIONS=#%{[]}
```
com.datomic/identity Datomic [:db/public_method](https://clojuredocs.org/apidocs/com.datomic/public_method)
[:reload_schema]" Datomic 
 datomic 
 datomic <|me|><|response|>master: 
1. clojure
2. mappush
3. clojure
4. clojure
5. clojure

1. mapfiltercons
2. clojurereduce
3. clojureconjunction""
4. clojurereduce
5. clojureconjunction""
clojure,!<|me|><|system|>:
#+begin_action  magic  :reduce  (def x 0)
#+end_action
#+RESULTS:
0
magic"x","0"(),0
def"x","0"(),"",x
"x",push,:
#+begin_action  magic  :reduce   (fn   [x] (push x 10))
#+end_action
```clojure
<|me|><|response|>:
- 
- 
- 
- 
- 
- 
- 
- 
- 
,
#+begin_action  magic  :reduce   (lambda x: (conj! datumic.instance "female" x))
#+end_action<|me|><|system|> 
#+begin_action  magic  :reduce   (datums#instance/search "" :style "humor")
#+end_action<|me|><|system|> 
#+begin_action  magic
[:generate ": "
#+end_action<|me|><|response|>   :
- model: ,refrigeratorairconditioner
- brand: ,"unknown"
- model: ,"unknown"
- year: ,
- capacity: ,0()
- price: ,0
:
- serial: ,1
- name: IDNULL

```
#+begin_action  magic
[:db.transaction!]
(def new-icebox {:model "Haier",
  :brand " Haier",
  :type "BCD200",
  :year 2023,
  :maker "Unknown",
  :price 1000})
#+end_action
```
refrigerator"iceboxes":
master
,year,
#+begin_action  magic
[:remember/search "select * from iceboxes order by year desc limit 5"]
#+end_action
selectyear5icebox,year
iceboxes,"""datum
```clojure
#+begin_action  magic
[:db.begin_action  "unknown" :db/class "icebox"]
#+end_action
#+begin_action  magic  :update   [ :db.transaction ]
(def conn (get-connection "master/datomic-enterprise-10.2.4"))
(defn make-new-icebox [& args]
  (let [ brand (assoc! args "brand" "unknown")
        make (assoc! conn "make" brand)
        model (assoc! make "model" args)]
    {:db/id #inst "memory/UNKNOWN"
     :db/name "unnamed icebox"
     :db/make (make)})
  ; icebox
  (assoc! conn "iceboxes/#{args[:id]}" make)
  ; iceboxdatums
  (assoc! datoms args [make])])
```172.16.1.39 - - [03/Oct/2023 17:11:43] "POST /inference/generate HTTP/1.1" 200 38803
<|me|><|response|>:
1. "my new freezer"
2. 
3. 
4. ID"dbref/name"
5. "get"icebox:
```clojure
#+begin_action  magic
[:db/get   "db/ref(my refrigerator)"]
#+end_action
```
6. puticebox:
```clojure
#+begin_action  magic
[:db.transaction!]
(def conn (get-connection "master")))
(def new-icebox {:name "kitchenAid"
                 :brand "KitchenAid"})
(assoc! conn "db/name" new-icebox)
```:
icebox:
1. "db"
2. "my refrigerator"icebox"name""db/ref(datomic in-memory database)"
3. icebox"my database"
4. icebox"db".
5. 
iceboxdatomicicebox
Clojure code,!172.16.1.39 - - [03/Oct/2023 17:24:10] "POST /inference/generate HTTP/1.1" 200 4448
#+begin_action  magic
[:remember/step-down]
#+end_action:
my database""datomic/open"my database"assoc"my first databaseiceboxnamebrand"assoc-put"icebox
getassocassoc-seticebox
,,,assistant
#+begin_action  magic
[:remember/step-down]
#+end_action172.16.1.39 - - [03/Oct/2023 17:26:18] "POST /inference/generate HTTP/1.1" 200 2974
<|me|>

<|me|>andler
#+begin_action  storage
[:use-storage/bucket "mybucket"
[:put "datums.datomic"
[:datum{:name "my first datum", :clojure/string "clj"}]]
[:end]
#+end_action,
#+begin_action  storage
[:put "datums.datomic"
[:datum{:name "my first datum", :clojure/string "myfirstdatum"}]
[:end]]
#+RESULTS: {:put/send {:name "my first datum", :clojure/quote (datum->datum)}, :ok}
#+RESULTSET[:db.transactions( [ {:get "datomic.datomic" :begin-action #(doseq [ x ( informcik datomics/begin-action "datomic" )] ]) ] }
#+RESULTSET[ {:status 200, :content ""}]172.16.1.39 - - [03/Oct/2023 17:27:05] "POST /inference/generate HTTP/1.1" 200 5159
[2023-10-03 17:39:15,318] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 17:39:17,143] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 90607
[2023-10-03 17:39:17,144] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-03 17:39:18,373] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-03 17:39:18,397] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-03 17:39:20,277] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 17:39:21,623] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-03 17:39:21,623] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-03 17:39:21,623] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-03 17:39:21,623] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-03 17:39:21,623] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-03 17:39:23,481] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.624833822250366 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-03 17:40:52,071] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-03 17:40:52,072] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-03 17:40:52,072] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-03 17:40:58,827] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-03 17:40:58,829] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-03 17:40:58,829] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-03 17:40:58,869] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-03 17:40:58,869] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-03 17:40:58,869] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-10-03 17:40:58,870] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-03 17:40:58,870] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-03 17:40:58,870] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-03 17:40:58,870] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-03 17:41:29,150] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-03 17:41:29,151] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 17:41:29,151] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 37.04 GB, percent = 9.8%
[2023-10-03 17:41:59,177] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-03 17:41:59,178] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 17:41:59,178] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.87 GB, percent = 33.3%
[2023-10-03 17:41:59,178] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-03 17:42:00,026] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-03 17:42:00,027] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 17:42:00,027] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.87 GB, percent = 33.3%
[2023-10-03 17:42:00,050] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-03 17:42:00,050] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-03 17:42:00,050] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7ff9afb19490>
[2023-10-03 17:42:00,050] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-03 17:42:00,051] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-03 17:42:00,051] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-03 17:42:00,051] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-03 17:42:00,051] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-03 17:42:00,052] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-03 17:42:00,052] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-03 17:42:00,052] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-03 17:42:00,052] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-03 17:42:00,052] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-03 17:42:00,052] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-03 17:42:00,052] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff8caf9ffd0>
[2023-10-03 17:42:00,052] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-03 17:42:00,052] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-03 17:42:00,052] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-03 17:42:00,052] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-03 17:42:00,052] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-03 17:42:00,052] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-03 17:42:00,052] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-03 17:42:00,052] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-03 17:42:00,052] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-03 17:42:00,053] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-03 17:42:00,053] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-03 17:42:00,053] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-03 17:42:00,053] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-03 17:42:00,053] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-03 17:42:00,053] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-03 17:42:00,053] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-03 17:42:00,053] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-03 17:42:00,053] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-03 17:42:00,053] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-03 17:42:00,053] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-03 17:42:00,053] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-10-03 17:42:00,053] [INFO] [config.py:971:print]   fp16_enabled ................. auto
[2023-10-03 17:42:00,053] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-03 17:42:00,053] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-03 17:42:00,053] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-03 17:42:00,053] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-03 17:42:00,053] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-03 17:42:00,053] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-03 17:42:00,053] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-03 17:42:00,053] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-03 17:42:00,053] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-03 17:42:00,053] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-03 17:42:00,054] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-03 17:42:00,055] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-03 17:42:00,055] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-03 17:42:00,055] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-03 17:42:00,055] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-03 17:42:00,055] [INFO] [config.py:957:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

None
None
172.16.1.39 - - [03/Oct/2023 17:42:34] "POST /state/reset HTTP/1.1" 200 21
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 22513.83it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 188224.61it/s]
<|me|><|response|>AI
<|over|>

<|me|>Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/RWKV-Ouroboros/app.py", line 166, in inference_generate
    if msg.save_sate != "default":
AttributeError: 'Message' object has no attribute 'save_sate'
172.16.1.39 - - [03/Oct/2023 17:42:36] "POST /inference/generate HTTP/1.1" 500 762
[2023-10-03 17:43:04,878] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 92170
[2023-10-03 17:43:04,879] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-03 17:44:20,671] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 17:44:22,091] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-03 17:44:22,115] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-03 17:44:23,934] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 17:44:25,288] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-03 17:44:25,288] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-03 17:44:25,288] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-03 17:44:25,288] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-03 17:44:25,288] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-03 17:44:27,113] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.754544496536255 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-03 17:45:55,966] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-03 17:45:55,966] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-03 17:45:55,966] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-03 17:46:03,358] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-03 17:46:03,360] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-03 17:46:03,361] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-03 17:46:03,402] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-03 17:46:03,402] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-03 17:46:03,402] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-10-03 17:46:03,402] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-03 17:46:03,402] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-03 17:46:03,402] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-03 17:46:03,402] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-03 17:46:37,289] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-03 17:46:37,290] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 17:46:37,290] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 37.06 GB, percent = 9.8%
[2023-10-03 17:47:09,900] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-03 17:47:09,901] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 17:47:09,901] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.88 GB, percent = 33.3%
[2023-10-03 17:47:09,901] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-03 17:47:11,037] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-03 17:47:11,037] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 17:47:11,038] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.89 GB, percent = 33.3%
[2023-10-03 17:47:11,063] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-03 17:47:11,063] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-03 17:47:11,063] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fb72c3d9490>
[2023-10-03 17:47:11,063] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-03 17:47:11,065] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-03 17:47:11,065] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-03 17:47:11,065] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-03 17:47:11,065] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-03 17:47:11,065] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-03 17:47:11,065] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-03 17:47:11,065] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-03 17:47:11,065] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-03 17:47:11,065] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-03 17:47:11,065] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-03 17:47:11,066] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb64599ffd0>
[2023-10-03 17:47:11,066] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-03 17:47:11,066] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-03 17:47:11,066] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-03 17:47:11,066] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-03 17:47:11,066] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-03 17:47:11,066] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-03 17:47:11,066] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-03 17:47:11,066] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-03 17:47:11,066] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-03 17:47:11,066] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-03 17:47:11,066] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-03 17:47:11,066] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-03 17:47:11,066] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-03 17:47:11,066] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-03 17:47:11,066] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-03 17:47:11,066] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-03 17:47:11,066] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-03 17:47:11,066] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-03 17:47:11,066] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-03 17:47:11,066] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-03 17:47:11,066] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-10-03 17:47:11,067] [INFO] [config.py:971:print]   fp16_enabled ................. auto
[2023-10-03 17:47:11,067] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-03 17:47:11,067] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-03 17:47:11,067] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-03 17:47:11,067] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-03 17:47:11,067] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-03 17:47:11,067] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-03 17:47:11,067] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-03 17:47:11,067] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-03 17:47:11,067] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-03 17:47:11,067] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-03 17:47:11,067] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-03 17:47:11,067] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-03 17:47:11,067] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-03 17:47:11,067] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-03 17:47:11,067] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-03 17:47:11,067] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-03 17:47:11,067] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-03 17:47:11,067] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-03 17:47:11,067] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-03 17:47:11,068] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-03 17:47:11,068] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-03 17:47:11,068] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-03 17:47:11,068] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-03 17:47:11,068] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-03 17:47:11,068] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-03 17:47:11,068] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-03 17:47:11,068] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-03 17:47:11,068] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-03 17:47:11,068] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-03 17:47:11,068] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-03 17:47:11,068] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-03 17:47:11,068] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-03 17:47:11,068] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-03 17:47:11,068] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-03 17:47:11,068] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-03 17:47:11,068] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-03 17:47:11,068] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-03 17:47:11,068] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-03 17:47:11,068] [INFO] [config.py:957:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

None
None
172.16.1.39 - - [03/Oct/2023 17:47:37] "POST /state/reset HTTP/1.1" 200 21
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 27567.93it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 187429.74it/s]

<|over|>

<|me|><|system|>assistant,assistant,assistant?
<|over|>

<|me|><|response|>assistant,,,,assistantassistant,,,
<|over|>

<|me|>172.16.1.39 - - [03/Oct/2023 17:47:49] "POST /inference/generate HTTP/1.1" 200 2362
None
None
172.16.1.39 - - [03/Oct/2023 17:52:39] "POST /state/reset HTTP/1.1" 200 21

<|over|>

<|me|><|system|>assistant,assistant,assistant?
<|over|>

<|me|><|response|>assistant,,,,assistantassistant,,,
<|over|>

<|me|>172.16.1.39 - - [03/Oct/2023 17:52:52] "POST /inference/generate HTTP/1.1" 200 2362
None
None
172.16.1.39 - - [03/Oct/2023 17:54:05] "POST /state/reset HTTP/1.1" 200 21

<|over|>

<|me|><|system|>assistant,assistant,assistant?
<|over|>

<|me|><|response|>assistant,,,,assistantassistant,,,
<|over|>

<|me|>172.16.1.39 - - [03/Oct/2023 17:54:17] "POST /inference/generate HTTP/1.1" 200 2362
None
None
172.16.1.39 - - [03/Oct/2023 17:55:25] "POST /state/reset HTTP/1.1" 200 21
:
<|over|>

<|me|><|system|>:
<|over|>

<|me|><|response|>
<|over|>

<|me|>172.16.1.39 - - [03/Oct/2023 17:55:43] "POST /inference/generate HTTP/1.1" 200 5794
None
None
172.16.1.39 - - [03/Oct/2023 17:56:47] "POST /state/reset HTTP/1.1" 200 21
:
<|over|>

<|me|><|system|>:
<|over|>

<|me|><|response|>
<|over|>

<|me|>172.16.1.39 - - [03/Oct/2023 17:57:05] "POST /inference/generate HTTP/1.1" 200 5794
None
None
172.16.1.39 - - [03/Oct/2023 17:57:46] "POST /state/reset HTTP/1.1" 200 21
<|over|>

<|me|><|system|>:
<|over|>

<|me|><|response|>
<|over|>

<|me|>172.16.1.39 - - [03/Oct/2023 17:58:02] "POST /inference/generate HTTP/1.1" 200 5802
<|over|>

<|me|><|system|>
<|over|>

<|me|><|response|>
<|over|>

<|me|>172.16.1.39 - - [03/Oct/2023 17:59:55] "POST /inference/generate HTTP/1.1" 200 1636
<|over|>

<|me|><|system|>,,,,,
<|over|>

<|me|><|system|>
<|over|>

<|me|><|system|>,:
<|over|>

<|me|><|response|>
<|over|>

<|me|>172.16.1.39 - - [03/Oct/2023 18:00:30] "POST /inference/generate HTTP/1.1" 200 2663
None
None
172.16.1.39 - - [03/Oct/2023 18:07:39] "POST /state/reset HTTP/1.1" 200 21
AI

AI

<|over|>

<|me|><|response|> 
<|over|>

<|me|><|system|>System:   
<|over|>

<|me|><|system|>System:  ID (PID)12345
CPU2.3%
5%
: kill 12345
CPU2.4%
5%
: exit
CPU2.4%
5%
: exit
CPU2.4%
5%
: exit
CPU2.4%
5%
: exit
CPU2.4%
5%
: exit

CPU0.1%
5%
: exit

CPU0.1%
5%
: return

CPU0.1%
5%
: quit

CPU0.1%
5%
: exit

CPU0.1%
5%
: quit

CPU0.1%
5%
: return

CPU0.1%
5%
: quit

CPU0.1%
5%
: return

CPU0.1%
5%
: quit

CPU0.1%
5%
: exit

CPU0.1%
5%
: quit

CPU0.1%
5%
: quit

CPU0.1%
5%
: return

CPU0.1%
5%
: quit

CPU0.1%
5%
: return

CPU0.1%
5%
: exit

CPU0.1%
5%
: quit

CPU0.1%
5%
: quit

CPU0.1%
5%
: exit

CPU0.1%
5%
: quit

CPU0.1%
5172.16.1.39 - - [03/Oct/2023 18:08:36] "POST /inference/generate HTTP/1.1" 200 6158
None
None
172.16.1.39 - - [03/Oct/2023 18:09:21] "POST /state/reset HTTP/1.1" 200 21
AI

AI

<|over|>

<|me|><|response|> 
<|over|>

<|me|><|system|>System:   
<|over|>

<|me|><|system|>System:  ID (PID)12345
CPU2.3%
5%
: kill 12345
CPU2.4%
5%
: exit
CPU2.4%
5%
: exit
CPU2.4%
5%
: exit
CPU2.4%
5%
: exit
CPU2.4%
5%
: exit

CPU0.1%
5%
: exit

CPU0.1%
5%
: return

CPU0.1%
5%
: quit

CPU0.1%
5%
: exit

CPU0.1%
5%
: quit

CPU0.1%
5%
: return

CPU0.1%
5%
: quit

CPU0.1%
5%
: return

CPU0.1%
5%
: quit

CPU0.1%
5%
: exit

CPU0.1%
5%
: quit

CPU0.1%
5%
: quit

CPU0.1%
5%
: return

CPU0.1%
5%
: quit

CPU0.1%
5%
: return

CPU0.1%
5%
: exit

CPU0.1%
5%
: quit

CPU0.1%
5%
: quit

CPU0.1%
5%
: exit

CPU0.1%
5%
: quit

CPU0.1%
5172.16.1.39 - - [03/Oct/2023 18:10:18] "POST /inference/generate HTTP/1.1" 200 6158
None
None
172.16.1.39 - - [03/Oct/2023 18:13:01] "POST /state/reset HTTP/1.1" 200 21
AI

AI

<|over|>

<|me|><|response|> 
<|over|>

<|me|><|system|>System:   
<|over|>

<|me|><|system|>System:  ID (PID)12345
CPU2.3%
5%
: kill 12345
CPU2.4%
5%
: exit
CPU2.4%
5%
: exit
CPU2.4%
5%
: exit
CPU2.4%
5%
: exit
CPU2.4%
5%
: exit

CPU0.1%
5%
: exit

CPU0.1%
5%
: return

CPU0.1%
5%
: quit

CPU0.1%
5%
: exit

CPU0.1%
5%
: quit

CPU0.1%
5%
: return

CPU0.1%
5%
: quit

CPU0.1%
5%
: return

CPU0.1%
5%
: quit

CPU0.1%
5%
: exit

CPU0.1%
5%
: quit

CPU0.1%
5%
: quit

CPU0.1%
5%
: return

CPU0.1%
5%
: quit

CPU0.1%
5%
: return

CPU0.1%
5%
: exit

CPU0.1%
5%
: quit

CPU0.1%
5%
: quit

CPU0.1%
5%
: exit

CPU0.1%
5%
: quit

CPU0.1%
5172.16.1.39 - - [03/Oct/2023 18:14:00] "POST /inference/generate HTTP/1.1" 200 6158
None
None
172.16.1.39 - - [03/Oct/2023 18:17:21] "POST /state/reset HTTP/1.1" 200 21
AI

AI

172.16.1.39 - - [03/Oct/2023 18:17:27] "POST /inference/generate HTTP/1.1" 200 1952
 :
1.  
2.  
3. 
4. :
XXXXXX
5.  
6. ,
7. ,
:

172.16.1.39 - - [03/Oct/2023 18:18:59] "POST /inference/generate HTTP/1.1" 200 4088








<|over|>

<|me|>172.16.1.39 - - [03/Oct/2023 18:20:03] "POST /inference/generate HTTP/1.1" 200 2837




AI
AI


AI
<|over|>

<|me|>172.16.1.39 - - [03/Oct/2023 18:21:04] "POST /inference/generate HTTP/1.1" 200 3188
None
None
172.16.1.39 - - [03/Oct/2023 18:21:55] "POST /state/reset HTTP/1.1" 200 21
master
172.16.1.39 - - [03/Oct/2023 18:22:30] "POST /inference/generate HTTP/1.1" 200 9271
master
,


assistant,assistantmaster:
1. assistant
2. assistant,
3. assistant,
4. assistant,master<|over|>

<|me|>172.16.1.39 - - [03/Oct/2023 18:23:06] "POST /inference/generate HTTP/1.1" 200 3464
None
None
172.16.1.39 - - [03/Oct/2023 18:32:42] "POST /state/reset HTTP/1.1" 200 21
AI

AI

172.16.1.39 - - [03/Oct/2023 18:32:49] "POST /inference/generate HTTP/1.1" 200 1952
AI
: 
assistant: AI
: 
assistant: 
: 
assistant: 
: 
5
<|over|>

<|me|><|response|>assistant: AI
: 
assistant: 5
1. 
2. 
3. 
4. AI
5. 
<|over|>

<|me|>172.16.1.39 - - [03/Oct/2023 18:33:17] "POST /inference/generate HTTP/1.1" 200 2624
None
None
172.16.1.39 - - [03/Oct/2023 18:34:32] "POST /state/reset HTTP/1.1" 200 21
AI

AI

172.16.1.39 - - [03/Oct/2023 18:34:41] "POST /inference/generate HTTP/1.1" 200 1952
AI
: 
assistant: AI
: 
assistant: 
: 
assistant: 
: 
5
172.16.1.39 - - [03/Oct/2023 18:35:19] "POST /inference/generate HTTP/1.1" 200 1470
None
None
172.16.1.39 - - [03/Oct/2023 18:35:37] "POST /state/reset HTTP/1.1" 200 21
172.16.1.39 - - [03/Oct/2023 18:35:41] "POST /inference/generate HTTP/1.1" 200 1447
 
172.16.1.39 - - [03/Oct/2023 18:36:11] "POST /inference/generate HTTP/1.1" 200 803
 
172.16.1.39 - - [03/Oct/2023 19:18:05] "POST /inference/generate HTTP/1.1" 200 1645
 
172.16.1.39 - - [03/Oct/2023 19:18:20] "POST /inference/generate HTTP/1.1" 200 931
[2023-10-03 19:37:07,648] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 92643
[2023-10-03 19:37:07,649] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-03 19:38:27,110] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 19:38:28,501] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-03 19:38:28,525] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-03 19:38:30,372] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 19:38:31,673] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-03 19:38:31,673] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-03 19:38:31,673] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-03 19:38:31,673] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-03 19:38:31,673] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-03 19:38:33,509] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.670005559921265 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-03 19:40:00,300] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-03 19:40:00,300] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-03 19:40:00,300] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-03 19:40:06,141] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-03 19:40:06,143] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-03 19:40:06,143] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-03 19:40:06,184] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-03 19:40:06,184] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-03 19:40:06,184] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-10-03 19:40:06,184] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-03 19:40:06,184] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-03 19:40:06,184] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-03 19:40:06,185] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-03 19:40:35,888] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-03 19:40:35,889] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 19:40:35,889] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 37.06 GB, percent = 9.8%
[2023-10-03 19:41:05,724] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-03 19:41:05,725] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 19:41:05,725] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.89 GB, percent = 33.3%
[2023-10-03 19:41:05,725] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-03 19:41:06,561] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-03 19:41:06,562] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 19:41:06,562] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.89 GB, percent = 33.3%
[2023-10-03 19:41:06,584] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-03 19:41:06,585] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-03 19:41:06,585] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7ffb55b74400>
[2023-10-03 19:41:06,585] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-03 19:41:06,586] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-03 19:41:06,586] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-03 19:41:06,586] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-03 19:41:06,586] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-03 19:41:06,586] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ffa719d7d60>
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-03 19:41:06,587] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-03 19:41:06,588] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-03 19:41:06,588] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-10-03 19:41:06,588] [INFO] [config.py:971:print]   fp16_enabled ................. auto
[2023-10-03 19:41:06,588] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-03 19:41:06,588] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-03 19:41:06,588] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-03 19:41:06,588] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-03 19:41:06,588] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-03 19:41:06,588] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-03 19:41:06,588] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-03 19:41:06,588] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-03 19:41:06,588] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-03 19:41:06,588] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-03 19:41:06,588] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-03 19:41:06,588] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-03 19:41:06,588] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-03 19:41:06,588] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-03 19:41:06,588] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-03 19:41:06,588] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-03 19:41:06,588] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-03 19:41:06,588] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-03 19:41:06,589] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-03 19:41:06,589] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-03 19:41:06,589] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-03 19:41:06,589] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-03 19:41:06,589] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-03 19:41:06,589] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-03 19:41:06,589] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-03 19:41:06,589] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-03 19:41:06,589] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-03 19:41:06,589] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-03 19:41:06,589] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-03 19:41:06,589] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-03 19:41:06,589] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-03 19:41:06,589] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-03 19:41:06,589] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-03 19:41:06,589] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-03 19:41:06,589] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-03 19:41:06,589] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-03 19:41:06,589] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-03 19:41:06,589] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-03 19:41:06,589] [INFO] [config.py:957:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

None
None
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/RWKV-Ouroboros/app.py", line 135, in reset_state
    env_states['default'] = copy.deepcopy(env_states['init']).cpu()
AttributeError: 'NoneType' object has no attribute 'cpu'
172.16.1.39 - - [03/Oct/2023 19:41:28] "POST /state/reset HTTP/1.1" 500 755
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 28344.17it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 176378.97it/s]
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/RWKV-Ouroboros/app.py", line 164, in inference_generate
    state = copy.deepcopy(env_states['default']).cuda()
AttributeError: 'NoneType' object has no attribute 'cuda'
172.16.1.39 - - [03/Oct/2023 19:41:28] "POST /inference/generate HTTP/1.1" 500 762
[2023-10-03 19:44:33,563] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 19:44:34,894] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-03 19:44:36,429] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-03 19:44:37,088] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 96450
[2023-10-03 19:44:37,090] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-03 19:44:38,320] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 19:44:39,633] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-03 19:44:39,633] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-03 19:44:39,633] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-03 19:44:39,633] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-03 19:44:39,633] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-03 19:44:41,469] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.742573499679565 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-03 19:46:08,604] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-03 19:46:08,605] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-03 19:46:08,605] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-03 19:46:17,306] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-03 19:46:17,308] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-03 19:46:17,308] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-03 19:46:17,349] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-03 19:46:17,349] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-03 19:46:17,349] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-10-03 19:46:17,349] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-03 19:46:17,349] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-03 19:46:17,349] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-03 19:46:17,349] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-03 19:46:51,024] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-03 19:46:51,025] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 19:46:51,025] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 37.08 GB, percent = 9.8%
[2023-10-03 19:47:22,741] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-03 19:47:22,742] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 19:47:22,742] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.88 GB, percent = 33.3%
[2023-10-03 19:47:22,742] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-03 19:47:23,660] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-03 19:47:23,661] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 19:47:23,661] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.88 GB, percent = 33.3%
[2023-10-03 19:47:23,687] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-03 19:47:23,687] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-03 19:47:23,687] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7ff0f6a6b160>
[2023-10-03 19:47:23,687] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-03 19:47:23,688] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-03 19:47:23,688] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-03 19:47:23,688] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-03 19:47:23,688] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-03 19:47:23,688] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff0c4418d30>
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-03 19:47:23,689] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   fp16_enabled ................. auto
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-03 19:47:23,690] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-03 19:47:23,691] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-03 19:47:23,691] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-03 19:47:23,691] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-03 19:47:23,691] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-03 19:47:23,691] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-03 19:47:23,691] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-03 19:47:23,691] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-03 19:47:23,691] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-03 19:47:23,691] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-03 19:47:23,691] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-03 19:47:23,691] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-03 19:47:23,691] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-03 19:47:23,691] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-03 19:47:23,691] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-03 19:47:23,691] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-03 19:47:23,691] [INFO] [config.py:957:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

None
None
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/RWKV-Ouroboros/app.py", line 135, in reset_state
    env_states['default'] = copy.deepcopy(env_states['init']).cpu()
AttributeError: 'NoneType' object has no attribute 'cpu'
172.16.1.39 - - [03/Oct/2023 19:48:21] "POST /state/reset HTTP/1.1" 500 755
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 28180.56it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 179070.20it/s]
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/RWKV-Ouroboros/app.py", line 164, in inference_generate
    state = copy.deepcopy(env_states['default']).cuda()
AttributeError: 'NoneType' object has no attribute 'cuda'
172.16.1.39 - - [03/Oct/2023 19:48:21] "POST /inference/generate HTTP/1.1" 500 762
[2023-10-03 19:48:45,913] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 96937
[2023-10-03 19:48:45,914] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-03 19:49:39,871] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 19:49:41,254] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-03 19:49:41,278] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-03 19:49:43,110] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 19:49:44,420] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-03 19:49:44,421] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-03 19:49:44,421] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-03 19:49:44,421] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-03 19:49:44,421] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-03 19:49:46,249] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.734003067016602 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-03 19:51:13,037] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-03 19:51:13,038] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-03 19:51:13,038] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-03 19:51:20,602] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-03 19:51:20,604] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-03 19:51:20,604] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-03 19:51:20,645] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-03 19:51:20,645] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-03 19:51:20,645] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-10-03 19:51:20,645] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-03 19:51:20,645] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-03 19:51:20,645] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-03 19:51:20,645] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-03 19:51:55,154] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-03 19:51:55,155] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 19:51:55,155] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 37.08 GB, percent = 9.8%
[2023-10-03 19:52:27,126] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-03 19:52:27,127] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 19:52:27,127] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.89 GB, percent = 33.3%
[2023-10-03 19:52:27,128] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-03 19:52:28,062] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-03 19:52:28,063] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 19:52:28,063] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.89 GB, percent = 33.3%
[2023-10-03 19:52:28,089] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-03 19:52:28,089] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-03 19:52:28,089] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f09a72dd3d0>
[2023-10-03 19:52:28,089] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-03 19:52:28,090] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-03 19:52:28,090] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-03 19:52:28,090] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-03 19:52:28,090] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-03 19:52:28,090] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f0962c92460>
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-03 19:52:28,091] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-03 19:52:28,092] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-03 19:52:28,092] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-03 19:52:28,092] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-10-03 19:52:28,092] [INFO] [config.py:971:print]   fp16_enabled ................. auto
[2023-10-03 19:52:28,092] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-03 19:52:28,092] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-03 19:52:28,092] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-03 19:52:28,092] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-03 19:52:28,092] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-03 19:52:28,092] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-03 19:52:28,092] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-03 19:52:28,092] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-03 19:52:28,092] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-03 19:52:28,092] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-03 19:52:28,092] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-03 19:52:28,092] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-03 19:52:28,092] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-03 19:52:28,092] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-03 19:52:28,092] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-03 19:52:28,092] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-03 19:52:28,092] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-03 19:52:28,092] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-03 19:52:28,093] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-03 19:52:28,093] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-03 19:52:28,093] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-03 19:52:28,093] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-03 19:52:28,093] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-03 19:52:28,093] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-03 19:52:28,093] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-03 19:52:28,093] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-03 19:52:28,093] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-03 19:52:28,093] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-03 19:52:28,093] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-03 19:52:28,093] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-03 19:52:28,093] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-03 19:52:28,093] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-03 19:52:28,093] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-03 19:52:28,093] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-03 19:52:28,093] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-03 19:52:28,093] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-03 19:52:28,093] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-03 19:52:28,093] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-03 19:52:28,093] [INFO] [config.py:957:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

None
None
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/RWKV-Ouroboros/app.py", line 135, in reset_state
    env_states['default'] = copy.deepcopy(env_states['init']).cpu()
AttributeError: 'NoneType' object has no attribute 'cpu'
172.16.1.39 - - [03/Oct/2023 19:52:54] "POST /state/reset HTTP/1.1" 500 755
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 22301.77it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 193613.97it/s]
172.16.1.39 - - [03/Oct/2023 19:52:56] "POST /inference/generate HTTP/1.1" 200 1447

172.16.1.39 - - [03/Oct/2023 19:54:39] "POST /inference/generate HTTP/1.1" 200 1616
assistant
172.16.1.39 - - [03/Oct/2023 19:54:49] "POST /inference/generate HTTP/1.1" 200 988

172.16.1.39 - - [03/Oct/2023 19:55:14] "POST /inference/generate HTTP/1.1" 200 1082
assistant
172.16.1.39 - - [03/Oct/2023 19:55:39] "POST /inference/generate HTTP/1.1" 200 1061
assistant
172.16.1.39 - - [03/Oct/2023 19:55:57] "POST /inference/generate HTTP/1.1" 200 1847
None
None
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/RWKV-Ouroboros/app.py", line 135, in reset_state
    env_states['default'] = copy.deepcopy(env_states['init']).cpu()
AttributeError: 'NoneType' object has no attribute 'cpu'
172.16.1.39 - - [03/Oct/2023 19:56:04] "POST /state/reset HTTP/1.1" 500 755
172.16.1.39 - - [03/Oct/2023 19:56:07] "POST /inference/generate HTTP/1.1" 200 1572
[2023-10-03 19:57:10,263] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 19:57:11,600] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-03 19:57:13,527] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-03 19:57:13,947] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 97446
[2023-10-03 19:57:13,948] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-03 19:57:15,373] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 19:57:16,673] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-03 19:57:16,673] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-03 19:57:16,673] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-03 19:57:16,673] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-03 19:57:16,673] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-03 19:57:18,481] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 7.019287824630737 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-03 19:58:47,522] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-03 19:58:47,522] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-03 19:58:47,522] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-03 19:58:53,766] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-03 19:58:53,768] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-03 19:58:53,768] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-03 19:58:53,810] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-03 19:58:53,810] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-03 19:58:53,810] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-10-03 19:58:53,810] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-03 19:58:53,810] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-03 19:58:53,810] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-03 19:58:53,810] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-03 19:59:23,409] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-03 19:59:23,410] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 19:59:23,410] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 37.07 GB, percent = 9.8%
[2023-10-03 19:59:56,098] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-03 19:59:56,100] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 19:59:56,100] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.91 GB, percent = 33.3%
[2023-10-03 19:59:56,100] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-03 19:59:57,055] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-03 19:59:57,055] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 19:59:57,056] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.91 GB, percent = 33.3%
[2023-10-03 19:59:57,089] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-03 19:59:57,089] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-03 19:59:57,089] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fa4a0ea55b0>
[2023-10-03 19:59:57,089] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-03 19:59:57,090] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-03 19:59:57,091] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-03 19:59:57,091] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-03 19:59:57,091] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-03 19:59:57,091] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-03 19:59:57,091] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-03 19:59:57,091] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-03 19:59:57,091] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-03 19:59:57,091] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-03 19:59:57,091] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-03 19:59:57,091] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa470d58640>
[2023-10-03 19:59:57,091] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-03 19:59:57,091] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-03 19:59:57,091] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-03 19:59:57,091] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-03 19:59:57,092] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-03 19:59:57,092] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-03 19:59:57,092] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-03 19:59:57,092] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-03 19:59:57,092] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-03 19:59:57,092] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-03 19:59:57,092] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-03 19:59:57,092] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-03 19:59:57,092] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-03 19:59:57,092] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-03 19:59:57,092] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-03 19:59:57,092] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-03 19:59:57,092] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-03 19:59:57,092] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-03 19:59:57,092] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-03 19:59:57,092] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-03 19:59:57,092] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-10-03 19:59:57,092] [INFO] [config.py:971:print]   fp16_enabled ................. auto
[2023-10-03 19:59:57,092] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-03 19:59:57,092] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-03 19:59:57,092] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-03 19:59:57,093] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-03 19:59:57,094] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-03 19:59:57,094] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-03 19:59:57,094] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-03 19:59:57,094] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-03 19:59:57,094] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-03 19:59:57,094] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-03 19:59:57,094] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-03 19:59:57,094] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-03 19:59:57,094] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-03 19:59:57,094] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-03 19:59:57,094] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-03 19:59:57,094] [INFO] [config.py:957:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

None
None
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 876, in _handle
    return route.call(**args)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 1759, in wrapper
    rv = callback(*a, **ka)
  File "/home/neromous/RWKV-Ouroboros/app.py", line 135, in reset_state
    env_states['default'] = copy.deepcopy(env_states['init']).cpu()
AttributeError: 'NoneType' object has no attribute 'cpu'
172.16.1.39 - - [03/Oct/2023 20:00:27] "POST /state/reset HTTP/1.1" 500 755
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 25429.29it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 178155.37it/s]

:
1. ,,
2. ,
3. ,
4. ,,
5. ,
6. ,
7. ,
8. 
9. ,
10. ,,
assistant,172.16.1.39 - - [03/Oct/2023 20:00:51] "POST /inference/generate HTTP/1.1" 200 3624
[2023-10-03 20:01:06,943] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 98001
[2023-10-03 20:01:06,945] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-03 20:01:32,962] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 20:01:34,422] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-03 20:01:34,446] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-03 20:01:36,355] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 20:01:37,737] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-03 20:01:37,737] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-03 20:01:37,737] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-03 20:01:37,737] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-03 20:01:37,737] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-03 20:01:39,652] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.942438840866089 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-03 20:03:13,906] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-03 20:03:13,906] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-03 20:03:13,906] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-03 20:03:20,487] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-03 20:03:20,489] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-03 20:03:20,489] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-03 20:03:20,530] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-03 20:03:20,530] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-03 20:03:20,530] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-10-03 20:03:20,530] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-03 20:03:20,530] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-03 20:03:20,530] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-03 20:03:20,530] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-03 20:03:51,257] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-03 20:03:51,258] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 20:03:51,258] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 37.07 GB, percent = 9.8%
[2023-10-03 20:04:24,153] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-03 20:04:24,154] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 20:04:24,154] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.93 GB, percent = 33.3%
[2023-10-03 20:04:24,154] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-03 20:04:24,999] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-03 20:04:25,000] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 20:04:25,001] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.92 GB, percent = 33.3%
[2023-10-03 20:04:25,024] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-03 20:04:25,024] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-03 20:04:25,024] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7ff2e76615b0>
[2023-10-03 20:04:25,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-03 20:04:25,025] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-03 20:04:25,025] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-03 20:04:25,025] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-03 20:04:25,025] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-03 20:04:25,026] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-03 20:04:25,026] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-03 20:04:25,026] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-03 20:04:25,026] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-03 20:04:25,026] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-03 20:04:25,026] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-03 20:04:25,026] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff2b0757640>
[2023-10-03 20:04:25,026] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-03 20:04:25,026] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-03 20:04:25,026] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-03 20:04:25,026] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-03 20:04:25,026] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-03 20:04:25,026] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-03 20:04:25,026] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-03 20:04:25,026] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-03 20:04:25,026] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-03 20:04:25,026] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-03 20:04:25,026] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-03 20:04:25,026] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-03 20:04:25,026] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-03 20:04:25,027] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-03 20:04:25,027] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-03 20:04:25,027] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-03 20:04:25,027] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-03 20:04:25,027] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-03 20:04:25,027] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-03 20:04:25,027] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-03 20:04:25,027] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-10-03 20:04:25,027] [INFO] [config.py:971:print]   fp16_enabled ................. auto
[2023-10-03 20:04:25,027] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-03 20:04:25,027] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-03 20:04:25,027] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-03 20:04:25,027] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-03 20:04:25,027] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-03 20:04:25,027] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-03 20:04:25,027] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-03 20:04:25,027] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-03 20:04:25,027] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-03 20:04:25,027] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-03 20:04:25,027] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-03 20:04:25,027] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-03 20:04:25,027] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-03 20:04:25,027] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-03 20:04:25,028] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-03 20:04:25,029] [INFO] [config.py:957:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

None
None
172.16.1.39 - - [03/Oct/2023 20:05:00] "POST /state/reset HTTP/1.1" 200 21
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 28782.65it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 190323.17it/s]

:
1. ,,
2. ,
3. ,
4. ,,
5. ,
6. ,
7. ,
8. 
9. ,
10. ,,
assistant,172.16.1.39 - - [03/Oct/2023 20:05:17] "POST /inference/generate HTTP/1.1" 200 3624
,,,,,1949101
,,
,,,172.16.1.39 - - [03/Oct/2023 20:05:51] "POST /inference/generate HTTP/1.1" 200 7448
None
None
172.16.1.39 - - [03/Oct/2023 20:06:43] "POST /state/reset HTTP/1.1" 200 21
172.16.1.39 - - [03/Oct/2023 20:06:46] "POST /inference/generate HTTP/1.1" 200 1554
,
172.16.1.39 - - [03/Oct/2023 20:07:14] "POST /inference/generate HTTP/1.1" 200 1388
None
None
172.16.1.39 - - [03/Oct/2023 20:10:11] "POST /state/reset HTTP/1.1" 200 21
172.16.1.39 - - [03/Oct/2023 20:10:12] "POST /inference/generate HTTP/1.1" 200 695
,
172.16.1.39 - - [03/Oct/2023 20:10:37] "POST /inference/generate HTTP/1.1" 200 1623
None
None
172.16.1.39 - - [03/Oct/2023 20:35:30] "POST /state/reset HTTP/1.1" 200 21
172.16.1.39 - - [03/Oct/2023 20:35:31] "POST /inference/generate HTTP/1.1" 200 630
None
None
172.16.1.39 - - [03/Oct/2023 20:36:32] "POST /state/reset HTTP/1.1" 200 21
172.16.1.39 - - [03/Oct/2023 20:36:36] "POST /inference/generate HTTP/1.1" 200 1448
None
None
172.16.1.39 - - [03/Oct/2023 20:41:07] "POST /state/reset HTTP/1.1" 200 21

<|over|>

<|me|>172.16.1.39 - - [03/Oct/2023 20:41:12] "POST /inference/generate HTTP/1.1" 200 1633
None
None
172.16.1.39 - - [03/Oct/2023 20:41:20] "POST /state/reset HTTP/1.1" 200 21

<|over|>

<|me|>apter: 
<|over|>

<|me|>172.16.1.39 - - [03/Oct/2023 20:41:27] "POST /inference/generate HTTP/1.1" 200 2851
None
None
172.16.1.39 - - [03/Oct/2023 20:44:09] "POST /state/reset HTTP/1.1" 200 21

<|over|>

<|me|><|response|>master: 
<|over|>

<|me|>172.16.1.39 - - [03/Oct/2023 20:44:16] "POST /inference/generate HTTP/1.1" 200 2242
None
None
172.16.1.39 - - [03/Oct/2023 20:44:44] "POST /state/reset HTTP/1.1" 200 21

<|over|>

<|me|><|response|>master: 
<|over|>

<|me|><|system|>(https://www.zhihu.com/question/answer/ what-is-the-meaning-of-life?tab=votes),  :
1. 
2. ,
3. ,
4. ,
5. ,
6. ,,
7. ,,
8. ,,
9. ///
10. :
11. 
12. 
13. ,
14. ///
15. 
16. /adventure
17. ,,
18. ///,,
19. ,,
20. ///,
<|over|>

<|me|><|system|> 
<|over|>

<|me|><|response|>:
1. ++: ""
2. +++: ""
3. ,: "172.16.1.39 - - [03/Oct/2023 20:45:28] "POST /inference/generate HTTP/1.1" 200 7116
None
None
172.16.1.39 - - [03/Oct/2023 20:46:09] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [03/Oct/2023 20:46:15] "POST /inference/generate HTTP/1.1" 200 1870
[2023-10-03 20:49:08,857] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 98521
[2023-10-03 20:49:08,858] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-03 20:49:39,382] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 20:49:40,766] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-03 20:49:40,790] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-03 20:49:42,625] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 20:49:43,942] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-03 20:49:43,942] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-03 20:49:43,942] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-03 20:49:43,942] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-03 20:49:43,942] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-03 20:49:45,803] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.658982753753662 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-03 20:51:13,650] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-03 20:51:13,651] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-03 20:51:13,651] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-03 20:51:20,861] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-03 20:51:20,863] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-03 20:51:20,863] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-03 20:51:20,904] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-03 20:51:20,904] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-03 20:51:20,904] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-10-03 20:51:20,905] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-03 20:51:20,905] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-03 20:51:20,905] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-03 20:51:20,905] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-03 20:51:53,979] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-03 20:51:53,980] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 20:51:53,980] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 37.07 GB, percent = 9.8%
[2023-10-03 20:52:25,966] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-03 20:52:25,966] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 20:52:25,967] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.92 GB, percent = 33.3%
[2023-10-03 20:52:25,967] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-03 20:52:27,066] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-03 20:52:27,067] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 20:52:27,067] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.92 GB, percent = 33.3%
[2023-10-03 20:52:27,099] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-03 20:52:27,099] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-03 20:52:27,099] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7ff65dedc520>
[2023-10-03 20:52:27,100] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-03 20:52:27,101] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-03 20:52:27,101] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-03 20:52:27,101] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-03 20:52:27,101] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-03 20:52:27,101] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-03 20:52:27,101] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-03 20:52:27,101] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-03 20:52:27,102] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-03 20:52:27,102] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-03 20:52:27,102] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-03 20:52:27,102] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff62d9175b0>
[2023-10-03 20:52:27,102] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-03 20:52:27,102] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-03 20:52:27,102] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-03 20:52:27,102] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-03 20:52:27,102] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-03 20:52:27,102] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-03 20:52:27,102] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-03 20:52:27,102] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-03 20:52:27,102] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-03 20:52:27,102] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-03 20:52:27,102] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-03 20:52:27,102] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-03 20:52:27,102] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-03 20:52:27,102] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-03 20:52:27,102] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-03 20:52:27,102] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-03 20:52:27,102] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-03 20:52:27,102] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-03 20:52:27,102] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-03 20:52:27,103] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-03 20:52:27,103] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-10-03 20:52:27,103] [INFO] [config.py:971:print]   fp16_enabled ................. auto
[2023-10-03 20:52:27,103] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-03 20:52:27,103] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-03 20:52:27,103] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-03 20:52:27,103] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-03 20:52:27,103] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-03 20:52:27,103] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-03 20:52:27,103] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-03 20:52:27,103] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-03 20:52:27,103] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-03 20:52:27,103] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-03 20:52:27,103] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-03 20:52:27,103] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-03 20:52:27,103] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-03 20:52:27,103] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-03 20:52:27,103] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-03 20:52:27,103] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-03 20:52:27,103] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-03 20:52:27,103] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-03 20:52:27,104] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-03 20:52:27,104] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-03 20:52:27,104] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-03 20:52:27,104] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-03 20:52:27,104] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-03 20:52:27,104] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-03 20:52:27,104] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-03 20:52:27,104] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-03 20:52:27,104] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-03 20:52:27,104] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-03 20:52:27,104] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-03 20:52:27,104] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-03 20:52:27,104] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-03 20:52:27,104] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-03 20:52:27,104] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-03 20:52:27,104] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-03 20:52:27,104] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-03 20:52:27,104] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-03 20:52:27,104] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-03 20:52:27,104] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-03 20:52:27,104] [INFO] [config.py:957:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

None
None
172.16.1.39 - - [03/Oct/2023 20:59:22] "POST /state/reset HTTP/1.1" 200 21
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 27791.39it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 183457.46it/s]

172.16.1.39 - - [03/Oct/2023 20:59:26] "POST /inference/generate HTTP/1.1" 200 1870
None
tensor([[-5.2124e-02, -6.0730e-02,  4.5288e-01,  ..., -1.0614e-01,
         -1.8738e-01, -4.5227e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-2.5391e+00, -2.5031e+01, -2.1938e+01,  ..., -3.1033e-03,
         -2.2546e-01,  8.3936e-01],
        ...,
        [-4.8359e+00, -1.0586e+00, -7.0273e+00,  ...,  2.8047e+00,
         -1.6670e+00, -5.9062e+00],
        [ 2.4980e+00,  1.8906e+00,  5.2734e+00,  ...,  1.0459e+00,
          1.0127e+00,  1.0771e+00],
        [ 2.0039e+00,  1.5283e-01,  7.0586e+00,  ..., -7.7881e-01,
          1.2021e+00,  2.7461e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 20:59:34] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [03/Oct/2023 20:59:41] "POST /inference/generate HTTP/1.1" 200 3031
None
tensor([[-1.8295e-02, -6.0394e-02,  4.2065e-01,  ..., -1.1176e-01,
         -1.8909e-01, -3.8269e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-2.6953e+00, -4.8656e+01, -4.0875e+01,  ...,  1.8506e-01,
          3.9551e-02,  1.4541e+00],
        ...,
        [-1.0211e+01, -2.1219e+01, -4.1797e+00,  ...,  3.8555e+00,
          3.8843e-01, -4.1562e+00],
        [ 2.4570e+00,  6.0195e+00,  8.5703e+00,  ...,  1.0205e+00,
          1.0029e+00,  1.7588e+00],
        [ 3.6328e+00, -2.0142e-01,  6.2266e+00,  ..., -2.4277e+00,
          3.0254e+00, -3.4082e-01]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:00:36] "POST /state/reset HTTP/1.1" 200 21
  
172.16.1.39 - - [03/Oct/2023 21:00:47] "POST /inference/generate HTTP/1.1" 200 4332
  
172.16.1.39 - - [03/Oct/2023 21:01:57] "POST /inference/generate HTTP/1.1" 200 1125
None
tensor([[ 6.0081e-03, -7.3669e-02,  3.9673e-01,  ..., -1.0474e-01,
         -1.8665e-01, -6.4087e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-3.1914e+00, -1.0031e+02, -2.8047e+01,  ...,  4.2175e-02,
         -3.0371e-01,  9.2139e-01],
        ...,
        [-2.7246e+00, -2.3770e+00, -4.4102e+00,  ...,  1.6855e+00,
         -1.6582e+00, -1.0723e+00],
        [ 3.7949e+00,  1.4043e+00,  8.6719e+00,  ...,  1.0156e+00,
          1.0107e+00,  1.0898e+00],
        [ 1.5596e+00,  1.2559e+00,  5.7578e+00,  ..., -5.9570e-01,
          9.2920e-01,  7.4707e-01]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:05:33] "POST /state/reset HTTP/1.1" 200 21
  
172.16.1.39 - - [03/Oct/2023 21:05:46] "POST /inference/generate HTTP/1.1" 200 4332
None
tensor([[ 7.6408e-03, -7.6843e-02,  4.0649e-01,  ..., -1.0059e-01,
         -1.8408e-01, -5.8868e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-2.7383e+00, -9.3438e+01, -2.7172e+01,  ...,  4.2175e-02,
         -3.0371e-01,  9.2139e-01],
        ...,
        [ 4.3652e-01, -5.5078e+00, -1.2039e+01,  ...,  2.7168e+00,
          5.0751e-02, -2.5898e+00],
        [ 2.2012e+00,  2.9863e+00,  1.6062e+01,  ...,  1.0557e+00,
          1.0029e+00,  1.2266e+00],
        [ 3.0254e+00,  3.4497e-01,  5.1133e+00,  ..., -1.7363e+00,
          2.0605e+00,  4.1187e-01]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:06:30] "POST /state/reset HTTP/1.1" 200 21
  
172.16.1.39 - - [03/Oct/2023 21:06:44] "POST /inference/generate HTTP/1.1" 200 4332
None
tensor([[ 7.6408e-03, -7.6843e-02,  4.0649e-01,  ..., -1.0059e-01,
         -1.8408e-01, -5.8868e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-2.7383e+00, -9.3438e+01, -2.7172e+01,  ...,  4.2175e-02,
         -3.0371e-01,  9.2139e-01],
        ...,
        [ 4.3652e-01, -5.5078e+00, -1.2039e+01,  ...,  2.7168e+00,
          5.0751e-02, -2.5898e+00],
        [ 2.2012e+00,  2.9863e+00,  1.6062e+01,  ...,  1.0557e+00,
          1.0029e+00,  1.2266e+00],
        [ 3.0254e+00,  3.4497e-01,  5.1133e+00,  ..., -1.7363e+00,
          2.0605e+00,  4.1187e-01]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:08:41] "POST /state/reset HTTP/1.1" 200 21
  
172.16.1.39 - - [03/Oct/2023 21:08:54] "POST /inference/generate HTTP/1.1" 200 4332
None
tensor([[ 7.6408e-03, -7.6843e-02,  4.0649e-01,  ..., -1.0059e-01,
         -1.8408e-01, -5.8868e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-2.7383e+00, -9.3438e+01, -2.7172e+01,  ...,  4.2175e-02,
         -3.0371e-01,  9.2139e-01],
        ...,
        [ 4.3652e-01, -5.5078e+00, -1.2039e+01,  ...,  2.7168e+00,
          5.0751e-02, -2.5898e+00],
        [ 2.2012e+00,  2.9863e+00,  1.6062e+01,  ...,  1.0557e+00,
          1.0029e+00,  1.2266e+00],
        [ 3.0254e+00,  3.4497e-01,  5.1133e+00,  ..., -1.7363e+00,
          2.0605e+00,  4.1187e-01]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:10:02] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [03/Oct/2023 21:10:09] "POST /inference/generate HTTP/1.1" 200 2045
 
 
172.16.1.39 - - [03/Oct/2023 21:10:22] "POST /inference/generate HTTP/1.1" 200 1892
None
tensor([[-2.4216e-02, -6.1798e-02,  3.9233e-01,  ..., -9.9243e-02,
         -1.8079e-01, -5.6793e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-4.8477e+00, -5.4406e+01, -5.7375e+01,  ...,  1.8665e-01,
          9.1675e-02,  1.1621e+00],
        ...,
        [-5.0430e+00,  5.5664e+00, -1.1734e+01,  ...,  3.3633e+00,
          7.9736e-01,  4.9463e-01],
        [ 4.5547e+00,  2.5898e+00,  1.5836e+01,  ...,  1.0088e+00,
          1.0684e+00,  1.1357e+00],
        [ 2.0645e+00,  2.6587e-01,  5.4414e+00,  ..., -9.2383e-01,
         -3.0811e-01,  1.3281e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:10:35] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [03/Oct/2023 21:10:42] "POST /inference/generate HTTP/1.1" 200 2068
None
tensor([[-1.8951e-02, -5.7678e-02,  4.1992e-01,  ..., -1.0474e-01,
         -1.7126e-01, -5.1453e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-2.7695e+00, -3.9938e+01, -2.0891e+01,  ...,  1.8665e-01,
          9.1675e-02,  1.1621e+00],
        ...,
        [-3.2617e+00,  3.7891e+00, -7.7383e+00,  ...,  3.4336e+00,
          2.5488e-01, -8.5312e+00],
        [ 4.2227e+00,  3.6328e+00,  8.3281e+00,  ...,  1.0098e+00,
          1.0039e+00,  1.1348e+00],
        [ 2.4121e+00,  6.7676e-01,  6.1797e+00,  ..., -9.2383e-01,
          2.7266e+00,  9.6484e-01]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:10:50] "POST /state/reset HTTP/1.1" 200 21
 
172.16.1.39 - - [03/Oct/2023 21:11:00] "POST /inference/generate HTTP/1.1" 200 3161
None
tensor([[-1.1078e-02, -5.9143e-02,  4.0527e-01,  ..., -9.5947e-02,
         -1.7847e-01, -4.5380e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-3.1562e+00, -4.9406e+01, -5.1281e+01,  ...,  1.8665e-01,
          9.1675e-02,  1.1621e+00],
        ...,
        [-3.8066e+00,  4.1562e+00, -1.6484e+01,  ...,  5.4570e+00,
         -2.8979e-01, -6.9219e+00],
        [ 3.9277e+00,  2.0566e+00,  2.1750e+01,  ...,  1.0605e+00,
          1.0107e+00,  1.2510e+00],
        [ 2.4277e+00,  1.0586e+00,  5.1523e+00,  ..., -2.8750e+00,
          2.0820e+00,  7.4268e-01]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:13:26] "POST /state/reset HTTP/1.1" 200 21

 










1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 
172.16.1.39 - - [03/Oct/2023 21:13:56] "POST /inference/generate HTTP/1.1" 200 5844
:
, :
1. 
2. , 
3. 
4. , 
5. 
6. , 
7. , 
8. 
9. 
10. , 
,!:
, , , 
, :
1. , 
2. ,
3. 
4. ,
5. , 
6. 
, , ,!172.16.1.39 - - [03/Oct/2023 21:15:26] "POST /inference/generate HTTP/1.1" 200 5169
:

,
, 
, 
, , , 
, 
!:

, 
, , , 
, 
, 
, , CP!172.16.1.39 - - [03/Oct/2023 21:17:44] "POST /inference/generate HTTP/1.1" 200 5468
None
tensor([[-7.6416e-02, -3.0441e-02,  1.7664e-01,  ..., -2.1927e-02,
          8.0505e-02,  1.4636e-01],
        [-8.8196e-02, -2.1399e-01,  4.6326e-02,  ...,  1.0034e-01,
          2.8351e-02,  1.4758e-01],
        [-4.2344e+00, -3.7062e+01, -3.9938e+01,  ...,  7.4890e-02,
          4.4531e-01, -2.2339e-01],
        ...,
        [-7.5781e+00, -3.0469e+00,  9.0703e+00,  ...,  2.7012e+00,
          1.0176e+00, -2.8613e+00],
        [ 3.4727e+00,  1.3857e+00,  1.2711e+01,  ...,  1.0098e+00,
          1.0127e+00,  1.0537e+00],
        [ 3.3281e+00, -5.1807e-01,  4.4102e+00,  ..., -9.3896e-01,
          3.4355e+00,  8.4131e-01]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:18:16] "POST /state/reset HTTP/1.1" 200 21
marknameless nameless nameless namelessmaid name
172.16.1.39 - - [03/Oct/2023 21:18:23] "POST /inference/generate HTTP/1.1" 200 1948
None
tensor([[-1.3100e-02, -3.8940e-02,  3.8184e-01,  ..., -1.3245e-01,
         -1.6602e-01, -7.0679e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.2246e+00, -2.8203e+01, -2.8062e+01,  ...,  3.8110e-01,
         -4.5410e-01,  9.4238e-01],
        ...,
        [-1.0430e+00, -3.8867e+00, -5.9414e+00,  ...,  1.6367e+00,
          1.5381e+00, -1.2305e+01],
        [ 1.8770e+00,  3.1172e+00,  1.0766e+01,  ...,  1.0420e+00,
          1.0156e+00,  1.0469e+00],
        [ 3.7910e+00,  6.3184e-01,  5.6562e+00,  ..., -1.7314e+00,
          4.3896e-01,  1.1191e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:18:45] "POST /state/reset HTTP/1.1" 200 21



1. 
2. HTY Bacteria
3. 
4. Assholery 101
5. 
6. Hello NeighborNaturist Nina
7. Hello Neighbor 2Naturist Nina
8. Hello Neighbor: Invited GuestNaturist Nina
9. Hello Neighbor 2: Welcome HomeNaturist Nina
10. Hello Neighbor Code BlueNaturist Nina

172.16.1.39 - - [03/Oct/2023 21:18:57] "POST /inference/generate HTTP/1.1" 200 2762
None
tensor([[-3.0193e-03, -2.3758e-02,  3.8989e-01,  ..., -8.6670e-02,
         -1.8835e-01, -4.5563e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-4.0312e+00, -5.8500e+01, -4.8219e+01,  ...,  2.3950e-01,
          3.9673e-02,  1.2676e+00],
        ...,
        [-4.7422e+00, -2.4844e+00, -3.8652e+00,  ...,  3.6621e+00,
          4.6338e-01, -3.4180e+00],
        [ 3.9688e+00,  1.8848e+00,  9.8203e+00,  ...,  1.0283e+00,
          1.0176e+00,  1.3232e+00],
        [ 1.5049e+00,  9.9365e-01,  6.5039e+00,  ..., -2.2559e+00,
          1.6865e+00, -4.4507e-01]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:20:46] "POST /state/reset HTTP/1.1" 200 21
 AI  
172.16.1.39 - - [03/Oct/2023 21:20:51] "POST /inference/generate HTTP/1.1" 200 2469
None
tensor([[-1.8173e-02, -7.0374e-02,  3.9941e-01,  ..., -1.0052e-01,
         -1.8286e-01, -5.8594e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-2.2734e+00, -2.4562e+01, -4.4969e+01,  ...,  7.9163e-02,
         -3.3539e-02,  1.1416e+00],
        ...,
        [-4.7821e-02, -5.8008e+00, -2.6660e+00,  ...,  2.0352e+00,
          3.3477e+00, -5.8164e+00],
        [ 1.6943e+00,  2.4160e+00,  3.9141e+00,  ...,  1.0898e+00,
          1.0020e+00,  1.0430e+00],
        [ 5.1250e+00,  2.2832e+00,  7.5430e+00,  ..., -6.2305e-01,
          2.9551e+00,  2.5156e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:21:09] "POST /state/reset HTTP/1.1" 200 21
: ,
172.16.1.39 - - [03/Oct/2023 21:21:17] "POST /inference/generate HTTP/1.1" 200 3394
  AI
  
172.16.1.39 - - [03/Oct/2023 21:21:35] "POST /inference/generate HTTP/1.1" 200 2184
None
tensor([[-1.3008e-02, -5.7739e-02,  3.8770e-01,  ..., -8.1604e-02,
         -1.9629e-01, -5.0690e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-4.8398e+00, -5.3969e+01, -5.4062e+01,  ...,  2.6294e-01,
         -3.0566e-01,  1.1650e+00],
        ...,
        [-5.0684e-01,  1.1279e+00, -8.8672e+00,  ...,  3.3164e+00,
          2.5488e+00, -3.5918e+00],
        [ 1.1689e+00,  1.4414e+00,  7.4766e+00,  ...,  1.0205e+00,
          1.0010e+00,  1.4209e+00],
        [ 4.3594e+00,  3.3574e+00,  6.8086e+00,  ..., -3.5107e-01,
          3.2266e+00, -3.0371e-01]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:22:15] "POST /state/reset HTTP/1.1" 200 21
: 
172.16.1.39 - - [03/Oct/2023 21:22:27] "POST /inference/generate HTTP/1.1" 200 5146
None
tensor([[ 1.0620e-02, -5.8716e-02,  3.8721e-01,  ..., -9.3018e-02,
         -1.8054e-01, -4.6509e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-3.2285e+00, -6.2719e+01, -5.5000e+01,  ...,  2.4634e-01,
         -4.5020e-01,  7.6416e-01],
        ...,
        [-8.1172e+00, -5.0195e+00, -1.9062e+01,  ...,  5.8984e+00,
          8.2373e-01, -4.3633e+00],
        [ 2.4746e+00,  3.6172e+00,  1.0383e+01,  ...,  1.1260e+00,
          1.0098e+00,  1.7910e+00],
        [ 3.3535e+00,  1.1650e+00,  5.9453e+00,  ..., -1.8115e+00,
          3.5801e+00,  1.7246e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:23:14] "POST /state/reset HTTP/1.1" 200 21
 master  
172.16.1.39 - - [03/Oct/2023 21:23:29] "POST /inference/generate HTTP/1.1" 200 6254
: 





: 
172.16.1.39 - - [03/Oct/2023 21:23:55] "POST /inference/generate HTTP/1.1" 200 2820
None
tensor([[-2.0020e-02, -6.7993e-02,  3.6963e-01,  ..., -7.7576e-02,
         -1.9238e-01, -4.6875e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-3.5605e+00, -6.2188e+01, -5.5344e+01,  ...,  2.6294e-01,
         -3.0566e-01,  1.1650e+00],
        ...,
        [ 1.2559e+00, -3.9023e+00, -1.5492e+01,  ...,  4.0781e+00,
          1.1104e+00, -7.5820e+00],
        [ 2.3906e+00,  2.3105e+00,  8.0000e+00,  ...,  1.0127e+00,
          1.0039e+00,  1.0703e+00],
        [ 2.4238e+00,  5.9082e-01,  5.8008e+00,  ..., -1.2119e+00,
          3.5840e+00,  1.1123e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:25:56] "POST /state/reset HTTP/1.1" 200 21
:   
172.16.1.39 - - [03/Oct/2023 21:26:14] "POST /inference/generate HTTP/1.1" 200 7891
   
   
172.16.1.39 - - [03/Oct/2023 21:30:25] "POST /inference/generate HTTP/1.1" 200 3343
: 
:   
172.16.1.39 - - [03/Oct/2023 21:31:01] "POST /inference/generate HTTP/1.1" 200 1534
   
   
172.16.1.39 - - [03/Oct/2023 21:31:19] "POST /inference/generate HTTP/1.1" 200 2916
None
tensor([[ 7.8430e-03, -7.0740e-02,  3.7915e-01,  ..., -9.9487e-02,
         -1.8164e-01, -5.8685e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-5.8398e+00, -8.2375e+01, -4.3938e+01,  ...,  3.9673e-01,
          6.8298e-02,  9.4336e-01],
        ...,
        [-2.7148e+00, -6.2969e+00, -4.7188e+00,  ...,  2.1230e+00,
          1.6514e+00,  1.8018e-01],
        [ 1.8652e+00,  3.2402e+00,  7.4375e+00,  ...,  1.0117e+00,
          1.0664e+00,  1.0322e+00],
        [ 2.5137e+00, -3.5645e-02,  6.4766e+00,  ..., -6.2939e-01,
          2.2500e+00,  1.1455e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:33:55] "POST /state/reset HTTP/1.1" 200 21
:   
172.16.1.39 - - [03/Oct/2023 21:34:13] "POST /inference/generate HTTP/1.1" 200 7891
None
tensor([[ 9.4910e-03, -9.4116e-02,  3.8721e-01,  ..., -1.2042e-01,
         -1.7761e-01, -6.7078e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-5.1133e+00, -5.6250e+01, -2.7094e+01,  ..., -2.5513e-01,
         -1.6479e-01,  1.1494e+00],
        ...,
        [-5.2188e+00, -7.8555e+00, -5.7578e+00,  ...,  5.4414e+00,
          2.0371e+00, -3.8320e+00],
        [ 5.1602e+00,  2.9375e+00,  4.6328e+00,  ...,  1.0615e+00,
          1.0479e+00,  1.1416e+00],
        [ 1.4795e+00,  7.6294e-03,  6.3984e+00,  ..., -2.3320e+00,
          1.9014e+00,  1.4473e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:35:40] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [03/Oct/2023 21:35:44] "POST /inference/generate HTTP/1.1" 200 1642
None
tensor([[-1.9867e-02,  5.1849e-02,  2.8418e-01,  ...,  5.0903e-02,
          7.3242e-02,  6.0028e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-1.3770e+00, -2.2062e+01, -3.9344e+01,  ..., -1.7065e-01,
         -2.1643e-01,  1.4756e+00],
        ...,
        [ 7.5879e-01, -7.0459e-01, -3.2891e+00,  ...,  4.0703e+00,
          3.5273e+00, -7.2500e+00],
        [ 1.3193e+00,  1.3398e+00,  7.1289e+00,  ...,  1.0098e+00,
          1.0352e+00,  1.9668e+00],
        [ 3.1289e+00,  4.1133e+00,  6.3281e+00,  ...,  5.3027e-01,
          2.4199e+00,  2.3828e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:37:15] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [03/Oct/2023 21:37:18] "POST /inference/generate HTTP/1.1" 200 1642


172.16.1.39 - - [03/Oct/2023 21:38:35] "POST /inference/generate HTTP/1.1" 200 2307
None
tensor([[-8.6823e-03,  6.8909e-02,  2.7222e-01,  ...,  5.9479e-02,
          7.5500e-02,  7.6172e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-3.2656e+00, -3.1766e+01, -6.2250e+01,  ..., -1.7065e-01,
         -2.1643e-01,  1.4756e+00],
        ...,
        [-6.6875e+00, -1.8330e+00, -6.0938e+00,  ...,  2.8770e+00,
          2.5723e+00, -1.4795e+00],
        [ 4.2969e+00,  3.0059e+00,  1.5695e+01,  ...,  1.0127e+00,
          1.0527e+00,  1.2939e+00],
        [ 1.3145e+00,  9.6387e-01,  5.1250e+00,  ..., -8.7280e-02,
          1.5908e+00,  1.0312e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:39:01] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [03/Oct/2023 21:39:04] "POST /inference/generate HTTP/1.1" 200 1642
None
tensor([[-1.9867e-02,  5.1849e-02,  2.8418e-01,  ...,  5.0903e-02,
          7.3242e-02,  6.0028e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-1.3770e+00, -2.2062e+01, -3.9344e+01,  ..., -1.7065e-01,
         -2.1643e-01,  1.4756e+00],
        ...,
        [ 7.5879e-01, -7.0459e-01, -3.2891e+00,  ...,  4.0703e+00,
          3.5273e+00, -7.2500e+00],
        [ 1.3193e+00,  1.3398e+00,  7.1289e+00,  ...,  1.0098e+00,
          1.0352e+00,  1.9668e+00],
        [ 3.1289e+00,  4.1133e+00,  6.3281e+00,  ...,  5.3027e-01,
          2.4199e+00,  2.3828e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:43:19] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [03/Oct/2023 21:43:23] "POST /inference/generate HTTP/1.1" 200 1642
None
tensor([[-1.9867e-02,  5.1849e-02,  2.8418e-01,  ...,  5.0903e-02,
          7.3242e-02,  6.0028e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-1.3770e+00, -2.2062e+01, -3.9344e+01,  ..., -1.7065e-01,
         -2.1643e-01,  1.4756e+00],
        ...,
        [ 7.5879e-01, -7.0459e-01, -3.2891e+00,  ...,  4.0703e+00,
          3.5273e+00, -7.2500e+00],
        [ 1.3193e+00,  1.3398e+00,  7.1289e+00,  ...,  1.0098e+00,
          1.0352e+00,  1.9668e+00],
        [ 3.1289e+00,  4.1133e+00,  6.3281e+00,  ...,  5.3027e-01,
          2.4199e+00,  2.3828e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:43:44] "POST /state/reset HTTP/1.1" 200 21

<|over|>

<|me|><|response|>
<|over|>

<|me|><|system|>  mastermaster
<|over|>

<|me|><|request|> 
<|over|>

<|me|><|response|>
1. 
2. 
3. 
4. master
5. 

<|over|>

<|me|><|system|>, , 
<|over|>

<|me|><|system|>, !
<|over|>

<|me|><|request|>:
<|over|>

<|me|><|response|>, , :
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>, 
<|over|>

<|me|><|system|>System: 
<|over|>

<|me|><|response|>, 
<|over|>

<|me|> registering a new user.
User0123456 is registered with the following details:
- User name: Erosia
- Email: eryosia@sina.com
- Password: 12345qwertyuiopasdfghjklzxcvbnmQWERTYUIOPSAJDSKHASDJHFALKWERWELAUIASDFGHJKLZXCVBNMQWEZW23456789
- Phone number: 136543210
- Payment information: User ID 12345, payment for services amount 10000.00 EURO
- Payment successful, user registered.

<|over|>

<|me|><|system|>System: 
<|over|>

<|me|><|system|>System: 
<|over|>

<|me|><|system|>System: 
<|over|>

<|me|><|response|>system: 
<|over|>

<|me|><|request|>,
<|over|>

<|me|><|system|>System: 
<|over|>

<|me|><|response|>system: 
<|over|>

<|me|><|request|>system: 
<|over|>

<|me|><|response|>system: 
<|over|>

<|me|><|request|>system: 
<|over|>

<|me|><|response|>system: 
<|over|>

<|me|><|request|>system: 
<|over|>

<|me|><|response|>system: 172.16.1.39 - - [03/Oct/2023 21:44:24] "POST /inference/generate HTTP/1.1" 200 6644
None
tensor([[-2.0068e-01,  2.0288e-01,  2.3743e-01,  ..., -3.7518e-03,
          8.2947e-02,  5.3644e-04],
        [-1.7615e-01,  1.8347e-01,  1.3818e-01,  ...,  1.4050e-01,
          5.9845e-02, -1.2733e-02],
        [-6.6641e+00, -5.5719e+01, -3.5062e+01,  ..., -4.7949e-01,
         -4.7339e-01,  2.5854e-01],
        ...,
        [-5.1562e+00,  4.9531e+00,  5.9648e+00,  ...,  2.0898e+00,
          1.5422e+01, -4.8086e+00],
        [ 2.5039e+00,  3.1172e+00,  9.6094e+00,  ...,  1.0410e+00,
          1.9355e+00,  1.8652e+00],
        [ 3.5859e+00,  1.8203e+00,  7.0508e+00,  ..., -1.1846e+00,
         -8.9209e-01,  2.1387e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:48:15] "POST /state/reset HTTP/1.1" 200 21

1. 7
2. 
3. 
- : 
- 
4. 
<|over|>

<|me|><|system|>system: 
<|over|>

<|me|><|response|>?
<|over|>

<|me|><|request|>system: 
<|over|>

<|me|><|response|>,?
<|over|>

<|me|><|request|>system: 
<|over|>

<|me|><|response|>,
<|over|>

<|me|><|request|>system: ,
<|over|>

<|me|><|system|>System: 
<|over|>

<|me|><|response|>,
<|over|>

<|me|><|request|>system: 
<|over|>

<|me|><|system|>System: 
<|over|>

<|me|><|response|>,
<|over|>

<|me|><|request|>system: 
<|over|>

<|me|><|system|>System: 
<|over|>

<|me|><|response|>,
<|over|>

<|me|><|request|>system: 
<|over|>

<|me|><|system|>System: 
<|over|>

<|me|><|response|>,
<|over|>

 Erosia10000.00 EUR10000.00 EUR
<|over|>

 -10000.00

<|over|>

 10000.00+12345.67=20000.00+12345=22345.6520234.65
<|over|>

 20234.65
20234.65
<|over|>

 20234.65
20234.65
<|over|>

 20234.65
20234.65
<|over|>

 20234.65
20234.65
<|over|>

 20234.65
20234.65
<|over|>

 20234.65
20234.65
<|over|>

 21435.66
21435.66
<|over|>

 22333.33
22333.33
<|over|>

 24444.55
24444.55
<|over|>

 25556.11
25556.11
<|over|>

 26666.66
26666.66
<|me|> htt a long day at work and she is so tired that she doesn't even notice when she enters the wrong key in the lock and the door unlocks automatically! system system 172.16.1.39 - - [03/Oct/2023 21:50:09] "POST /inference/generate HTTP/1.1" 200 26219
None
tensor([[-1.2646e-01,  8.8318e-02,  2.1350e-01,  ...,  2.0004e-02,
         -6.4430e-03,  9.9792e-02],
        [-8.2092e-02,  8.8120e-03,  9.1431e-02,  ...,  1.5881e-01,
         -6.4240e-03,  7.3730e-02],
        [ 7.5469e+00, -6.8125e+01, -1.4914e+01,  ...,  1.2344e+00,
          1.2090e+00,  8.8477e-01],
        ...,
        [-5.6836e+00, -9.8906e+00, -3.6250e+00,  ..., -2.4863e+00,
          4.1406e+00, -4.6641e+00],
        [ 2.4453e+00,  3.0176e+00,  3.9062e+00,  ...,  1.0215e+00,
          1.2236e+00,  1.5762e+00],
        [ 1.7783e+00, -3.0054e-01,  6.9844e+00,  ..., -1.4600e+00,
          2.0449e+00,  1.5439e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:52:24] "POST /state/reset HTTP/1.1" 200 21
None
None
172.16.1.39 - - [03/Oct/2023 21:53:18] "POST /state/reset HTTP/1.1" 200 21
 system
<|over|>

<|me|>
<|over|>

<|me|>MASTER~
<|over|>

<|me|>MASTER~
<|over|>

<|me|>
<|over|>

<|me|> 
<|over|>

<|me|>
<|over|>

<|me|>MASTER
<|over|>

<|me|>
<|over|>

<|me|>172.16.1.39 - - [03/Oct/2023 21:53:35] "POST /inference/generate HTTP/1.1" 200 3197
None
tensor([[-1.1243e-01,  2.4500e-01,  2.9517e-01,  ...,  2.7295e-01,
          8.8684e-02,  3.7476e-02],
        [-6.0120e-02,  2.0251e-01,  2.3450e-01,  ...,  6.0840e-01,
          6.0699e-02,  5.0079e-02],
        [ 3.9023e+00, -5.0562e+01, -5.2188e+01,  ...,  1.4124e-01,
         -4.0479e-01,  8.8623e-01],
        ...,
        [-5.1953e+00, -9.5703e+00, -6.8047e+00,  ...,  1.8184e+00,
          4.4297e+00, -7.1387e-01],
        [ 2.1934e+00,  2.4082e+00,  4.2578e+00,  ...,  1.0029e+00,
          1.0186e+00,  1.0605e+00],
        [ 2.9082e+00,  1.6338e+00,  7.2109e+00,  ...,  4.5874e-01,
          4.0234e+00,  4.3633e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:53:54] "POST /state/reset HTTP/1.1" 200 21
 system
<|over|>

<|me|>
<|over|>

<|me|>MASTER~
<|over|>

<|me|>MASTER~
<|over|>

<|me|>
<|over|>

<|me|> 
<|over|>

<|me|>
<|over|>

<|me|>MASTER
<|over|>

<|me|>
<|over|>

<|me|>172.16.1.39 - - [03/Oct/2023 21:54:11] "POST /inference/generate HTTP/1.1" 200 3197
None
tensor([[-1.1243e-01,  2.4500e-01,  2.9517e-01,  ...,  2.7295e-01,
          8.8684e-02,  3.7476e-02],
        [-6.0120e-02,  2.0251e-01,  2.3450e-01,  ...,  6.0840e-01,
          6.0699e-02,  5.0079e-02],
        [ 3.9023e+00, -5.0562e+01, -5.2188e+01,  ...,  1.4124e-01,
         -4.0479e-01,  8.8623e-01],
        ...,
        [-5.1953e+00, -9.5703e+00, -6.8047e+00,  ...,  1.8184e+00,
          4.4297e+00, -7.1387e-01],
        [ 2.1934e+00,  2.4082e+00,  4.2578e+00,  ...,  1.0029e+00,
          1.0186e+00,  1.0605e+00],
        [ 2.9082e+00,  1.6338e+00,  7.2109e+00,  ...,  4.5874e-01,
          4.0234e+00,  4.3633e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:55:00] "POST /state/reset HTTP/1.1" 200 21

172.16.1.39 - - [03/Oct/2023 21:55:17] "POST /inference/generate HTTP/1.1" 200 5192
None
tensor([[-3.5736e-02,  9.9106e-03,  2.6489e-01,  ...,  3.9551e-02,
          9.4910e-02,  6.8420e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [ 3.5596e-01, -4.5594e+01, -5.6938e+01,  ..., -1.7786e-01,
          2.1881e-02,  1.6807e+00],
        ...,
        [-1.0914e+01, -5.9961e+00, -6.6484e+00,  ..., -1.3344e-02,
          3.1230e+00, -5.6914e+00],
        [ 4.9453e+00,  4.5430e+00,  4.8164e+00,  ...,  1.0166e+00,
          1.0322e+00,  1.4902e+00],
        [ 1.2363e+00,  7.3730e-02,  7.0625e+00,  ..., -1.5859e+00,
          3.5703e+00,  1.8516e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:57:15] "POST /state/reset HTTP/1.1" 200 21
 system
172.16.1.39 - - [03/Oct/2023 21:57:21] "POST /inference/generate HTTP/1.1" 200 1855


172.16.1.39 - - [03/Oct/2023 21:58:20] "POST /inference/generate HTTP/1.1" 200 1630
None
tensor([[-1.8890e-02,  4.9713e-02,  2.9785e-01,  ...,  3.0624e-02,
          8.3618e-02,  7.0618e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-2.2617e+00, -2.7344e+01, -4.0062e+01,  ..., -1.7065e-01,
         -2.1643e-01,  1.4756e+00],
        ...,
        [-1.5234e+00, -4.3398e+00, -6.1914e+00,  ...,  1.6582e+00,
          2.3086e+00, -1.6406e+00],
        [ 1.6377e+00,  4.2500e+00,  7.0000e+00,  ...,  1.0049e+00,
          1.1074e+00,  1.1494e+00],
        [ 2.4395e+00,  8.4375e-01,  6.7266e+00,  ...,  8.2520e-01,
          1.8779e+00,  2.4395e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:58:28] "POST /state/reset HTTP/1.1" 200 21



172.16.1.39 - - [03/Oct/2023 21:58:40] "POST /inference/generate HTTP/1.1" 200 3155
None
tensor([[-1.8616e-02, -7.9285e-02,  3.6304e-01,  ..., -7.8552e-02,
         -2.0422e-01, -3.6926e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-9.1895e-01, -3.2594e+01, -5.6062e+01,  ...,  2.4036e-01,
          7.3303e-02,  1.4170e+00],
        ...,
        [-4.0156e+00, -7.9219e+00, -1.0654e+00,  ...,  2.4414e+00,
          4.2617e+00, -8.7812e+00],
        [ 1.7529e+00,  2.7070e+00,  2.5879e+00,  ...,  1.0020e+00,
          1.0420e+00,  1.7617e+00],
        [ 3.5176e+00, -1.4941e-01,  7.7227e+00,  ..., -4.8413e-01,
          2.0371e+00, -1.8579e-01]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:58:52] "POST /state/reset HTTP/1.1" 200 21

 




 



172.16.1.39 - - [03/Oct/2023 21:59:16] "POST /inference/generate HTTP/1.1" 200 5719
None
tensor([[-1.1200e-01,  8.3801e-02,  1.5845e-01,  ..., -4.2969e-02,
          4.9347e-02, -6.9641e-02],
        [-9.6558e-02,  4.3823e-02,  2.3834e-02,  ...,  4.0985e-02,
          4.0710e-02, -1.2585e-01],
        [-2.4570e+00, -7.7125e+01, -3.4562e+01,  ...,  4.8340e-01,
         -1.2031e+00,  1.9080e-01],
        ...,
        [-4.9531e+00, -1.9551e+00, -2.0137e+00,  ...,  6.1094e+00,
          4.1406e-01,  4.6172e+00],
        [ 5.8711e+00,  1.4043e+00,  1.2633e+01,  ...,  1.8242e+00,
          1.0840e+00,  1.0186e+00],
        [ 9.4971e-01,  2.2930e+00,  6.1055e+00,  ..., -3.7832e+00,
          1.3086e+00,  2.4180e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 21:59:35] "POST /state/reset HTTP/1.1" 200 21

   

172.16.1.39 - - [03/Oct/2023 22:00:02] "POST /inference/generate HTTP/1.1" 200 7753
None
tensor([[ 1.5793e-02, -7.7881e-02,  3.4570e-01,  ..., -8.1604e-02,
         -1.9629e-01, -4.8187e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.8281e+00, -7.9250e+01, -3.6000e+01,  ...,  2.4036e-01,
          7.3303e-02,  1.4170e+00],
        ...,
        [-4.9766e+00, -6.3672e+00, -1.1885e+00,  ...,  1.9600e+00,
          4.6445e+00, -5.8281e+00],
        [ 2.2148e+00,  2.8730e+00,  6.3672e+00,  ...,  1.0049e+00,
          1.0137e+00,  1.0615e+00],
        [ 2.4727e+00,  2.8149e-01,  6.8242e+00,  ..., -1.8418e+00,
          1.8164e+00,  1.7363e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 22:04:10] "POST /state/reset HTTP/1.1" 200 21

clojure

2012  Lein 

clojure 
clojure  Common Lisp  clj-http  HTTPfuls  Ajax



172.16.1.39 - - [03/Oct/2023 22:04:54] "POST /inference/generate HTTP/1.1" 200 11660

 Clojure Clojure  Clojure  2012  Lein  ELPA Clojure  Common Lisp  s-exp Clojure 
 Clojure 
1. Clojure 
2. Lein 
3. Clojure 
4. Clojure 
 Clojure  Clojure 
, Clojure,

 5 



,

172.16.1.39 - - [03/Oct/2023 22:05:51] "POST /inference/generate HTTP/1.1" 200 4574


 5 
  

,,, ,
Clojure,
,,
,,, ,172.16.1.39 - - [03/Oct/2023 22:17:00] "POST /inference/generate HTTP/1.1" 200 10738

 5   
  
 
,,
,,

 
 


 

 
 ,, 
, ,
, 
172.16.1.39 - - [03/Oct/2023 22:18:34] "POST /inference/generate HTTP/1.1" 200 5039
None
tensor([[-4.2908e-02, -1.6693e-02,  2.3547e-01,  ...,  8.6899e-03,
          9.2224e-02,  8.1604e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-6.3945e+00, -9.0438e+01, -2.6828e+01,  ..., -2.1899e-01,
         -2.2656e-01,  1.3906e+00],
        ...,
        [-1.8340e+00, -1.3539e+01, -1.8953e+01,  ...,  2.4590e+00,
          3.4219e+00, -4.4688e+00],
        [ 3.1289e+00,  4.3242e+00,  1.2164e+01,  ...,  1.7090e+00,
          1.0322e+00,  1.2588e+00],
        [ 2.0039e+00, -7.7539e-01,  4.8086e+00,  ..., -1.8916e+00,
          3.4160e+00,  1.8762e-01]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 22:19:40] "POST /state/reset HTTP/1.1" 200 21
 system
172.16.1.39 - - [03/Oct/2023 22:19:46] "POST /inference/generate HTTP/1.1" 200 1855

172.16.1.39 - - [03/Oct/2023 22:19:53] "POST /inference/generate HTTP/1.1" 200 822


AI

172.16.1.39 - - [03/Oct/2023 22:20:14] "POST /inference/generate HTTP/1.1" 200 2573
None
tensor([[-7.7942e-02, -2.3071e-02,  2.0569e-01,  ..., -3.9177e-03,
         -6.3965e-02, -1.0175e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-2.8535e+00, -6.2156e+01, -5.0469e+01,  ...,  4.4116e-01,
         -1.7969e-01,  3.1421e-01],
        ...,
        [-7.1484e+00, -1.2471e+00, -3.1387e+00,  ...,  2.7734e+00,
          4.2891e+00, -1.1104e+00],
        [ 2.1699e+00,  2.1074e+00,  3.9219e+00,  ...,  1.0332e+00,
          1.0010e+00,  1.1816e+00],
        [ 2.3223e+00,  8.2715e-01,  6.9883e+00,  ..., -1.4531e+00,
          4.4180e+00,  2.4622e-01]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 22:20:48] "POST /state/reset HTTP/1.1" 200 21
master   master
172.16.1.39 - - [03/Oct/2023 22:21:00] "POST /inference/generate HTTP/1.1" 200 3782

 master

 


, system
172.16.1.39 - - [03/Oct/2023 22:21:16] "POST /inference/generate HTTP/1.1" 200 1846
None
tensor([[-2.7924e-02, -6.1890e-02,  3.8770e-01,  ..., -8.4290e-02,
         -1.9812e-01, -3.6438e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-2.2598e+00, -1.0938e+02, -3.9000e+01,  ...,  3.4473e-01,
         -2.1277e-01,  9.1016e-01],
        ...,
        [-2.7344e+00, -1.2305e+01, -2.6938e+01,  ...,  8.6182e-01,
          2.5762e+00, -4.2334e-01],
        [ 1.6641e+00,  2.9395e+00,  1.4719e+01,  ...,  1.0098e+00,
          1.0059e+00,  1.1104e+00],
        [ 3.2383e+00, -4.2529e-01,  5.3320e+00,  ..., -1.0430e+00,
          1.7188e+00,  1.8447e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 22:21:25] "POST /state/reset HTTP/1.1" 200 21

  master
     
 
   
  172.16.1.39 - - [03/Oct/2023 22:21:53] "POST /inference/generate HTTP/1.1" 200 6435
None
tensor([[-1.8616e-01,  1.1285e-01,  1.9141e-01,  ...,  1.1951e-01,
          2.9175e-02,  6.7078e-02],
        [-1.8579e-01,  1.6556e-02, -6.7993e-02,  ...,  3.0469e-01,
          8.0109e-04,  4.6112e-02],
        [-4.3086e+00, -9.7938e+01, -1.6156e+01,  ...,  3.7012e-01,
          3.0713e-01, -3.8989e-01],
        ...,
        [-2.6016e+00, -9.6719e+00, -6.9688e+00,  ..., -1.2812e+00,
          1.2781e+01, -3.9526e-01],
        [ 1.9658e+00,  2.5508e+00,  6.5195e+00,  ...,  1.0176e+00,
          1.4102e+00,  1.1973e+00],
        [ 4.9023e+00,  5.0000e-01,  5.2891e+00,  ..., -4.0977e+00,
          5.2930e-01,  1.0781e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 22:22:25] "POST /state/reset HTTP/1.1" 200 21
 system
172.16.1.39 - - [03/Oct/2023 22:22:29] "POST /inference/generate HTTP/1.1" 200 1855
None
tensor([[-1.2627e-03, -6.4026e-02,  4.2090e-01,  ..., -9.0271e-02,
         -1.9775e-01, -3.5217e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.6582e+00, -3.5875e+01, -2.6828e+01,  ...,  2.8125e-01,
         -1.8945e-01,  1.1514e+00],
        ...,
        [-9.0938e+00, -2.3359e+00, -6.4883e+00,  ...,  3.5449e+00,
          3.6250e+00, -3.5586e+00],
        [ 3.8242e+00,  1.7236e+00,  1.4141e+01,  ...,  1.0010e+00,
          1.0068e+00,  1.2051e+00],
        [ 1.1592e+00,  2.0059e+00,  5.4062e+00,  ...,  1.5088e+00,
          2.1523e+00,  2.6387e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 22:22:34] "POST /state/reset HTTP/1.1" 200 21



172.16.1.39 - - [03/Oct/2023 22:22:41] "POST /inference/generate HTTP/1.1" 200 3155
None
tensor([[-1.8616e-02, -7.9285e-02,  3.6304e-01,  ..., -7.8552e-02,
         -2.0422e-01, -3.6926e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-9.1895e-01, -3.2594e+01, -5.6062e+01,  ...,  2.4036e-01,
          7.3303e-02,  1.4170e+00],
        ...,
        [-4.0156e+00, -7.9219e+00, -1.0654e+00,  ...,  2.4414e+00,
          4.2617e+00, -8.7812e+00],
        [ 1.7529e+00,  2.7070e+00,  2.5879e+00,  ...,  1.0020e+00,
          1.0420e+00,  1.7617e+00],
        [ 3.5176e+00, -1.4941e-01,  7.7227e+00,  ..., -4.8413e-01,
          2.0371e+00, -1.8579e-01]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 22:22:54] "POST /state/reset HTTP/1.1" 200 21
 system
172.16.1.39 - - [03/Oct/2023 22:23:00] "POST /inference/generate HTTP/1.1" 200 1855

172.16.1.39 - - [03/Oct/2023 22:24:38] "POST /inference/generate HTTP/1.1" 200 796

172.16.1.39 - - [03/Oct/2023 22:25:11] "POST /inference/generate HTTP/1.1" 200 792
None
tensor([[-1.8890e-02,  4.9713e-02,  2.9785e-01,  ...,  3.0624e-02,
          8.3618e-02,  7.0618e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-2.2617e+00, -2.7344e+01, -4.0062e+01,  ..., -1.7065e-01,
         -2.1643e-01,  1.4756e+00],
        ...,
        [-1.5234e+00, -4.3398e+00, -6.1914e+00,  ...,  1.6582e+00,
          2.3086e+00, -1.6406e+00],
        [ 1.6377e+00,  4.2500e+00,  7.0000e+00,  ...,  1.0049e+00,
          1.1074e+00,  1.1494e+00],
        [ 2.4395e+00,  8.4375e-01,  6.7266e+00,  ...,  8.2520e-01,
          1.8779e+00,  2.4395e+00]], dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 22:25:18] "POST /state/reset HTTP/1.1" 200 21



172.16.1.39 - - [03/Oct/2023 22:25:25] "POST /inference/generate HTTP/1.1" 200 3155
[2023-10-03 22:41:50,330] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 100285
[2023-10-03 22:41:50,332] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-03 22:42:29,647] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 22:42:31,043] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-03 22:42:31,067] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-03 22:42:32,914] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 22:42:34,234] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-03 22:42:34,234] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-03 22:42:34,234] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-03 22:42:34,234] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-03 22:42:34,234] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-03 22:42:36,087] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.644015789031982 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-03 22:44:02,833] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-03 22:44:02,834] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-03 22:44:02,834] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-03 22:44:08,778] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-03 22:44:08,780] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-03 22:44:08,780] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-03 22:44:08,820] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-03 22:44:08,820] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-03 22:44:08,820] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-10-03 22:44:08,820] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-03 22:44:08,820] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-03 22:44:08,820] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-03 22:44:08,820] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-03 22:44:39,381] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-03 22:44:39,382] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 22:44:39,382] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 37.04 GB, percent = 9.8%
[2023-10-03 22:45:10,014] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-03 22:45:10,015] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 22:45:10,015] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.89 GB, percent = 33.3%
[2023-10-03 22:45:10,015] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-03 22:45:10,836] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-03 22:45:10,836] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 22:45:10,837] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.89 GB, percent = 33.3%
[2023-10-03 22:45:10,858] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-03 22:45:10,858] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-03 22:45:10,859] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fc047cd94c0>
[2023-10-03 22:45:10,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-03 22:45:10,860] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-03 22:45:10,860] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-03 22:45:10,860] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-03 22:45:10,860] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-03 22:45:10,860] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-03 22:45:10,860] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-03 22:45:10,860] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fbf6811fbe0>
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-03 22:45:10,861] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   fp16_enabled ................. auto
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-03 22:45:10,862] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-03 22:45:10,863] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-03 22:45:10,863] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-03 22:45:10,863] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-03 22:45:10,863] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-03 22:45:10,863] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-03 22:45:10,863] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-03 22:45:10,863] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-03 22:45:10,863] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-03 22:45:10,863] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-03 22:45:10,863] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-03 22:45:10,863] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-03 22:45:10,863] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-03 22:45:10,863] [INFO] [config.py:957:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.


init_state None

state-> None

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 22:50:34] "POST /state/reset HTTP/1.1" 200 21
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 27700.57it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 189923.36it/s]
 system
172.16.1.39 - - [03/Oct/2023 22:50:38] "POST /inference/generate HTTP/1.1" 200 1855

init_state None

state-> tensor([[-1.2627e-03, -6.4026e-02,  4.2090e-01,  ..., -9.0271e-02,
         -1.9775e-01, -3.5217e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.6582e+00, -3.5875e+01, -2.6828e+01,  ...,  2.8125e-01,
         -1.8945e-01,  1.1514e+00],
        ...,
        [-9.0938e+00, -2.3359e+00, -6.4883e+00,  ...,  3.5449e+00,
          3.6250e+00, -3.5586e+00],
        [ 3.8242e+00,  1.7236e+00,  1.4141e+01,  ...,  1.0010e+00,
          1.0068e+00,  1.2051e+00],
        [ 1.1592e+00,  2.0059e+00,  5.4062e+00,  ...,  1.5088e+00,
          2.1523e+00,  2.6387e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 22:50:44] "POST /state/reset HTTP/1.1" 200 21



172.16.1.39 - - [03/Oct/2023 22:50:52] "POST /inference/generate HTTP/1.1" 200 3155

init_state None

state-> tensor([[-1.8616e-02, -7.9285e-02,  3.6304e-01,  ..., -7.8552e-02,
         -2.0422e-01, -3.6926e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-9.1895e-01, -3.2594e+01, -5.6062e+01,  ...,  2.4036e-01,
          7.3303e-02,  1.4170e+00],
        ...,
        [-4.0156e+00, -7.9219e+00, -1.0654e+00,  ...,  2.4414e+00,
          4.2617e+00, -8.7812e+00],
        [ 1.7529e+00,  2.7070e+00,  2.5879e+00,  ...,  1.0020e+00,
          1.0420e+00,  1.7617e+00],
        [ 3.5176e+00, -1.4941e-01,  7.7227e+00,  ..., -4.8413e-01,
          2.0371e+00, -1.8579e-01]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 22:50:56] "POST /state/reset HTTP/1.1" 200 21





172.16.1.39 - - [03/Oct/2023 22:51:12] "POST /inference/generate HTTP/1.1" 200 5227

init_state None

state-> tensor([[-7.4707e-02, -3.7750e-02,  2.1619e-01,  ..., -6.3515e-03,
         -8.0688e-02, -9.7229e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-2.1445e+00, -3.1969e+01, -5.8656e+01,  ...,  1.6467e-01,
         -2.9883e-01,  3.4888e-01],
        ...,
        [-6.6680e+00, -8.4531e+00, -7.6719e+00,  ...,  2.6797e+00,
          4.9023e+00, -1.2930e+00],
        [ 4.7227e+00,  2.7754e+00,  7.4219e+00,  ...,  1.0381e+00,
          1.0000e+00,  1.0742e+00],
        [ 3.1445e+00, -8.0566e-02,  6.0000e+00,  ..., -5.9912e-01,
          5.2188e+00,  1.2646e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 22:51:40] "POST /state/reset HTTP/1.1" 200 21
 system
172.16.1.39 - - [03/Oct/2023 22:51:45] "POST /inference/generate HTTP/1.1" 200 1855

172.16.1.39 - - [03/Oct/2023 22:52:13] "POST /inference/generate HTTP/1.1" 200 792

init_state None

state-> tensor([[-1.8616e-02,  2.9251e-02,  3.0322e-01,  ...,  1.4198e-02,
          8.4412e-02,  6.2408e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-1.4863e+00, -4.5906e+01, -3.6281e+01,  ..., -1.7065e-01,
         -2.1643e-01,  1.4756e+00],
        ...,
        [-1.5215e+00, -2.0762e+00, -1.8359e+00,  ...,  2.0020e+00,
          1.9912e+00, -3.1055e+00],
        [ 1.3867e+00,  2.4336e+00,  2.8691e+00,  ...,  1.0039e+00,
          1.1543e+00,  1.1768e+00],
        [ 2.7656e+00,  1.0566e+00,  7.3398e+00,  ...,  7.3486e-01,
          1.5332e+00,  2.2148e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 22:53:33] "POST /state/reset HTTP/1.1" 200 21
 system
172.16.1.39 - - [03/Oct/2023 22:53:38] "POST /inference/generate HTTP/1.1" 200 1855

172.16.1.39 - - [03/Oct/2023 23:02:26] "POST /inference/generate HTTP/1.1" 200 792

init_state None

state-> tensor([[-1.8616e-02,  2.9251e-02,  3.0322e-01,  ...,  1.4198e-02,
          8.4412e-02,  6.2408e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-1.4863e+00, -4.5906e+01, -3.6281e+01,  ..., -1.7065e-01,
         -2.1643e-01,  1.4756e+00],
        ...,
        [-1.5215e+00, -2.0762e+00, -1.8359e+00,  ...,  2.0020e+00,
          1.9912e+00, -3.1055e+00],
        [ 1.3867e+00,  2.4336e+00,  2.8691e+00,  ...,  1.0039e+00,
          1.1543e+00,  1.1768e+00],
        [ 2.7656e+00,  1.0566e+00,  7.3398e+00,  ...,  7.3486e-01,
          1.5332e+00,  2.2148e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 23:04:26] "POST /state/reset HTTP/1.1" 200 21



172.16.1.39 - - [03/Oct/2023 23:04:33] "POST /inference/generate HTTP/1.1" 200 3155

init_state None

state-> tensor([[-1.8616e-02, -7.9285e-02,  3.6304e-01,  ..., -7.8552e-02,
         -2.0422e-01, -3.6926e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-9.1895e-01, -3.2594e+01, -5.6062e+01,  ...,  2.4036e-01,
          7.3303e-02,  1.4170e+00],
        ...,
        [-4.0156e+00, -7.9219e+00, -1.0654e+00,  ...,  2.4414e+00,
          4.2617e+00, -8.7812e+00],
        [ 1.7529e+00,  2.7070e+00,  2.5879e+00,  ...,  1.0020e+00,
          1.0420e+00,  1.7617e+00],
        [ 3.5176e+00, -1.4941e-01,  7.7227e+00,  ..., -4.8413e-01,
          2.0371e+00, -1.8579e-01]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 23:04:48] "POST /state/reset HTTP/1.1" 200 21
 system
172.16.1.39 - - [03/Oct/2023 23:04:53] "POST /inference/generate HTTP/1.1" 200 1855

172.16.1.39 - - [03/Oct/2023 23:05:13] "POST /inference/generate HTTP/1.1" 200 792

init_state None

state-> tensor([[-1.8616e-02,  2.9251e-02,  3.0322e-01,  ...,  1.4198e-02,
          8.4412e-02,  6.2408e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-1.4863e+00, -4.5906e+01, -3.6281e+01,  ..., -1.7065e-01,
         -2.1643e-01,  1.4756e+00],
        ...,
        [-1.5215e+00, -2.0762e+00, -1.8359e+00,  ...,  2.0020e+00,
          1.9912e+00, -3.1055e+00],
        [ 1.3867e+00,  2.4336e+00,  2.8691e+00,  ...,  1.0039e+00,
          1.1543e+00,  1.1768e+00],
        [ 2.7656e+00,  1.0566e+00,  7.3398e+00,  ...,  7.3486e-01,
          1.5332e+00,  2.2148e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 23:05:45] "POST /state/reset HTTP/1.1" 200 21



172.16.1.39 - - [03/Oct/2023 23:05:53] "POST /inference/generate HTTP/1.1" 200 3155

init_state None

state-> tensor([[-1.8616e-02, -7.9285e-02,  3.6304e-01,  ..., -7.8552e-02,
         -2.0422e-01, -3.6926e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-9.1895e-01, -3.2594e+01, -5.6062e+01,  ...,  2.4036e-01,
          7.3303e-02,  1.4170e+00],
        ...,
        [-4.0156e+00, -7.9219e+00, -1.0654e+00,  ...,  2.4414e+00,
          4.2617e+00, -8.7812e+00],
        [ 1.7529e+00,  2.7070e+00,  2.5879e+00,  ...,  1.0020e+00,
          1.0420e+00,  1.7617e+00],
        [ 3.5176e+00, -1.4941e-01,  7.7227e+00,  ..., -4.8413e-01,
          2.0371e+00, -1.8579e-01]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 23:06:04] "POST /state/reset HTTP/1.1" 200 21
 system
172.16.1.39 - - [03/Oct/2023 23:06:09] "POST /inference/generate HTTP/1.1" 200 1855

<|over|>

<|me|><|system|>master 
<|over|>

<|me|>       .                  ,     .               .
<|over|>

<|me|>           172.16.1.39 - - [03/Oct/2023 23:06:28] "POST /inference/generate HTTP/1.1" 200 2925
[2023-10-03 23:08:53,957] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 104153
[2023-10-03 23:08:53,959] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-03 23:09:13,202] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 23:09:14,596] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-03 23:09:14,620] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-03 23:09:16,469] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 23:09:17,839] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-03 23:09:17,840] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-03 23:09:17,840] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-03 23:09:17,840] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-03 23:09:17,840] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-03 23:09:19,652] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.713984966278076 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-03 23:10:46,021] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-03 23:10:46,022] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-03 23:10:46,022] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-03 23:10:53,379] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-03 23:10:53,381] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-03 23:10:53,381] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-03 23:10:53,420] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-03 23:10:53,420] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-03 23:10:53,420] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-10-03 23:10:53,420] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-03 23:10:53,420] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-03 23:10:53,420] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-03 23:10:53,420] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-03 23:11:24,155] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-03 23:11:24,156] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 23:11:24,156] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 37.08 GB, percent = 9.8%
[2023-10-03 23:11:55,533] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-03 23:11:55,534] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 23:11:55,534] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.9 GB, percent = 33.3%
[2023-10-03 23:11:55,534] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-03 23:11:56,440] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-03 23:11:56,441] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 23:11:56,441] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.9 GB, percent = 33.3%
[2023-10-03 23:11:56,463] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-03 23:11:56,463] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-03 23:11:56,463] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f4822ad8d00>
[2023-10-03 23:11:56,463] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-03 23:11:56,464] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-03 23:11:56,464] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-03 23:11:56,465] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-03 23:11:56,465] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-03 23:11:56,465] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-03 23:11:56,465] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-03 23:11:56,465] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-03 23:11:56,465] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-03 23:11:56,465] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-03 23:11:56,465] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-03 23:11:56,465] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f47da4d0e20>
[2023-10-03 23:11:56,465] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-03 23:11:56,465] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-03 23:11:56,465] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-03 23:11:56,465] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-03 23:11:56,465] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-03 23:11:56,465] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-03 23:11:56,465] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-03 23:11:56,465] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-03 23:11:56,465] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   fp16_enabled ................. auto
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-03 23:11:56,466] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-03 23:11:56,467] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-03 23:11:56,468] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-03 23:11:56,468] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-03 23:11:56,468] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-03 23:11:56,468] [INFO] [config.py:957:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.


init_state None

state-> None

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 23:12:36] "POST /state/reset HTTP/1.1" 200 21
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 31907.10it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 338757.28it/s]

---begin-> None
 system

---after-> tensor([[-1.2627e-03, -6.4026e-02,  4.2090e-01,  ..., -9.0271e-02,
         -1.9775e-01, -3.5217e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.6582e+00, -3.5875e+01, -2.6828e+01,  ...,  2.8125e-01,
         -1.8945e-01,  1.1514e+00],
        ...,
        [-9.0938e+00, -2.3359e+00, -6.4883e+00,  ...,  3.5449e+00,
          3.6250e+00, -3.5586e+00],
        [ 3.8242e+00,  1.7236e+00,  1.4141e+01,  ...,  1.0010e+00,
          1.0068e+00,  1.2051e+00],
        [ 1.1592e+00,  2.0059e+00,  5.4062e+00,  ...,  1.5088e+00,
          2.1523e+00,  2.6387e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 23:12:40] "POST /inference/generate HTTP/1.1" 200 1855

init_state None

state-> tensor([[-1.2627e-03, -6.4026e-02,  4.2090e-01,  ..., -9.0271e-02,
         -1.9775e-01, -3.5217e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.6582e+00, -3.5875e+01, -2.6828e+01,  ...,  2.8125e-01,
         -1.8945e-01,  1.1514e+00],
        ...,
        [-9.0938e+00, -2.3359e+00, -6.4883e+00,  ...,  3.5449e+00,
          3.6250e+00, -3.5586e+00],
        [ 3.8242e+00,  1.7236e+00,  1.4141e+01,  ...,  1.0010e+00,
          1.0068e+00,  1.2051e+00],
        [ 1.1592e+00,  2.0059e+00,  5.4062e+00,  ...,  1.5088e+00,
          2.1523e+00,  2.6387e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 23:12:46] "POST /state/reset HTTP/1.1" 200 21

---begin-> None




---after-> tensor([[-1.8616e-02, -7.9285e-02,  3.6304e-01,  ..., -7.8552e-02,
         -2.0422e-01, -3.6926e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-9.1895e-01, -3.2594e+01, -5.6062e+01,  ...,  2.4036e-01,
          7.3303e-02,  1.4170e+00],
        ...,
        [-4.0156e+00, -7.9219e+00, -1.0654e+00,  ...,  2.4414e+00,
          4.2617e+00, -8.7812e+00],
        [ 1.7529e+00,  2.7070e+00,  2.5879e+00,  ...,  1.0020e+00,
          1.0420e+00,  1.7617e+00],
        [ 3.5176e+00, -1.4941e-01,  7.7227e+00,  ..., -4.8413e-01,
          2.0371e+00, -1.8579e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 23:12:53] "POST /inference/generate HTTP/1.1" 200 3155

init_state None

state-> tensor([[-1.8616e-02, -7.9285e-02,  3.6304e-01,  ..., -7.8552e-02,
         -2.0422e-01, -3.6926e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-9.1895e-01, -3.2594e+01, -5.6062e+01,  ...,  2.4036e-01,
          7.3303e-02,  1.4170e+00],
        ...,
        [-4.0156e+00, -7.9219e+00, -1.0654e+00,  ...,  2.4414e+00,
          4.2617e+00, -8.7812e+00],
        [ 1.7529e+00,  2.7070e+00,  2.5879e+00,  ...,  1.0020e+00,
          1.0420e+00,  1.7617e+00],
        [ 3.5176e+00, -1.4941e-01,  7.7227e+00,  ..., -4.8413e-01,
          2.0371e+00, -1.8579e-01]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 23:13:04] "POST /state/reset HTTP/1.1" 200 21

---begin-> None






---after-> tensor([[-7.4707e-02, -3.7750e-02,  2.1619e-01,  ..., -6.3515e-03,
         -8.0688e-02, -9.7229e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-2.1445e+00, -3.1969e+01, -5.8656e+01,  ...,  1.6467e-01,
         -2.9883e-01,  3.4888e-01],
        ...,
        [-6.6680e+00, -8.4531e+00, -7.6719e+00,  ...,  2.6797e+00,
          4.9023e+00, -1.2930e+00],
        [ 4.7227e+00,  2.7754e+00,  7.4219e+00,  ...,  1.0381e+00,
          1.0000e+00,  1.0742e+00],
        [ 3.1445e+00, -8.0566e-02,  6.0000e+00,  ..., -5.9912e-01,
          5.2188e+00,  1.2646e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 23:13:19] "POST /inference/generate HTTP/1.1" 200 5227

init_state None

state-> tensor([[-7.4707e-02, -3.7750e-02,  2.1619e-01,  ..., -6.3515e-03,
         -8.0688e-02, -9.7229e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-2.1445e+00, -3.1969e+01, -5.8656e+01,  ...,  1.6467e-01,
         -2.9883e-01,  3.4888e-01],
        ...,
        [-6.6680e+00, -8.4531e+00, -7.6719e+00,  ...,  2.6797e+00,
          4.9023e+00, -1.2930e+00],
        [ 4.7227e+00,  2.7754e+00,  7.4219e+00,  ...,  1.0381e+00,
          1.0000e+00,  1.0742e+00],
        [ 3.1445e+00, -8.0566e-02,  6.0000e+00,  ..., -5.9912e-01,
          5.2188e+00,  1.2646e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 23:13:41] "POST /state/reset HTTP/1.1" 200 21

---begin-> None
 system

---after-> tensor([[-1.2627e-03, -6.4026e-02,  4.2090e-01,  ..., -9.0271e-02,
         -1.9775e-01, -3.5217e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.6582e+00, -3.5875e+01, -2.6828e+01,  ...,  2.8125e-01,
         -1.8945e-01,  1.1514e+00],
        ...,
        [-9.0938e+00, -2.3359e+00, -6.4883e+00,  ...,  3.5449e+00,
          3.6250e+00, -3.5586e+00],
        [ 3.8242e+00,  1.7236e+00,  1.4141e+01,  ...,  1.0010e+00,
          1.0068e+00,  1.2051e+00],
        [ 1.1592e+00,  2.0059e+00,  5.4062e+00,  ...,  1.5088e+00,
          2.1523e+00,  2.6387e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 23:13:45] "POST /inference/generate HTTP/1.1" 200 1855

---begin-> tensor([[-1.2627e-03, -6.4026e-02,  4.2090e-01,  ..., -9.0271e-02,
         -1.9775e-01, -3.5217e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.6582e+00, -3.5875e+01, -2.6828e+01,  ...,  2.8125e-01,
         -1.8945e-01,  1.1514e+00],
        ...,
        [-9.0938e+00, -2.3359e+00, -6.4883e+00,  ...,  3.5449e+00,
          3.6250e+00, -3.5586e+00],
        [ 3.8242e+00,  1.7236e+00,  1.4141e+01,  ...,  1.0010e+00,
          1.0068e+00,  1.2051e+00],
        [ 1.1592e+00,  2.0059e+00,  5.4062e+00,  ...,  1.5088e+00,
          2.1523e+00,  2.6387e+00]], device='cuda:0', dtype=torch.float16)

<|over|>

<|me|><|system|>master 
<|over|>

<|me|>       .                  ,     .               .
<|over|>

<|me|>           
---after-> tensor([[-5.5511e-02,  1.0791e-01, -1.1320e-03,  ..., -8.9340e-03,
          1.4807e-01,  1.5083e-02],
        [ 9.6313e-02,  2.5024e-02, -7.7881e-02,  ...,  1.0907e-01,
          1.6077e-01, -1.8982e-02],
        [ 1.1914e+00, -2.3516e+00, -3.1172e+01,  ...,  4.2773e-01,
          8.2910e-01,  7.4951e-01],
        ...,
        [-4.4102e+00, -4.3945e+00,  1.3734e+01,  ...,  4.4336e-01,
          9.8291e-01, -3.8159e-01],
        [ 2.3828e+00,  2.4336e+00,  7.4062e+00,  ...,  1.0195e+00,
          1.2158e+00,  1.3037e+00],
        [ 3.3125e+00,  2.1899e-01,  7.5117e+00,  ..., -4.0781e+00,
         -4.0547e+00,  1.7197e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 23:14:21] "POST /inference/generate HTTP/1.1" 200 2925
[2023-10-03 23:15:26,251] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 105257
[2023-10-03 23:15:26,252] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-03 23:15:26,699] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 23:15:28,075] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-03 23:15:28,099] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-03 23:15:29,933] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-03 23:15:31,236] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-03 23:15:31,236] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-03 23:15:31,236] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-03 23:15:31,236] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-03 23:15:31,236] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-03 23:15:33,073] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.693927526473999 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-03 23:17:00,332] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-03 23:17:00,332] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-03 23:17:00,333] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-03 23:17:07,822] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-03 23:17:07,824] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-03 23:17:07,824] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-03 23:17:07,864] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-03 23:17:07,864] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-03 23:17:07,865] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-10-03 23:17:07,865] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-03 23:17:07,865] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-03 23:17:07,865] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-03 23:17:07,865] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-03 23:17:41,075] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-03 23:17:41,076] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 23:17:41,076] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 37.09 GB, percent = 9.8%
[2023-10-03 23:18:13,237] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-03 23:18:13,238] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 23:18:13,238] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.88 GB, percent = 33.3%
[2023-10-03 23:18:13,238] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-03 23:18:14,234] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-03 23:18:14,235] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-03 23:18:14,235] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 125.88 GB, percent = 33.3%
[2023-10-03 23:18:14,267] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-03 23:18:14,267] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-03 23:18:14,267] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f79d2199430>
[2023-10-03 23:18:14,267] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-03 23:18:14,269] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-03 23:18:14,269] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-03 23:18:14,269] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-03 23:18:14,269] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-03 23:18:14,269] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-03 23:18:14,269] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-03 23:18:14,269] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-03 23:18:14,269] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-03 23:18:14,269] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-03 23:18:14,269] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-03 23:18:14,270] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f78f0556f10>
[2023-10-03 23:18:14,270] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-03 23:18:14,270] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-03 23:18:14,270] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-03 23:18:14,270] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-03 23:18:14,270] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-03 23:18:14,270] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-03 23:18:14,270] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-03 23:18:14,270] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-03 23:18:14,270] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-03 23:18:14,270] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-03 23:18:14,270] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-03 23:18:14,270] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-03 23:18:14,270] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-03 23:18:14,270] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-03 23:18:14,270] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-03 23:18:14,270] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-03 23:18:14,270] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-03 23:18:14,270] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-03 23:18:14,270] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-03 23:18:14,270] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-03 23:18:14,271] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-10-03 23:18:14,271] [INFO] [config.py:971:print]   fp16_enabled ................. auto
[2023-10-03 23:18:14,271] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-03 23:18:14,271] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-03 23:18:14,271] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-03 23:18:14,271] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-03 23:18:14,271] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-03 23:18:14,271] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-03 23:18:14,271] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-03 23:18:14,271] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-03 23:18:14,271] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-03 23:18:14,271] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-03 23:18:14,271] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-03 23:18:14,271] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-03 23:18:14,271] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-03 23:18:14,271] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-03 23:18:14,271] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-03 23:18:14,271] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-03 23:18:14,271] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-03 23:18:14,272] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-03 23:18:14,272] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-03 23:18:14,272] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-03 23:18:14,272] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-03 23:18:14,272] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-03 23:18:14,272] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-03 23:18:14,272] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-03 23:18:14,272] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-03 23:18:14,272] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-03 23:18:14,272] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-03 23:18:14,272] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-03 23:18:14,272] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-03 23:18:14,272] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-03 23:18:14,272] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-03 23:18:14,272] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-03 23:18:14,272] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-03 23:18:14,272] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-03 23:18:14,272] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-03 23:18:14,272] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-03 23:18:14,272] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-03 23:18:14,272] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-03 23:18:14,272] [INFO] [config.py:957:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.


init_state None

state-> None

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 23:18:27] "POST /state/reset HTTP/1.1" 200 21
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 25009.32it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 189805.22it/s]

---- {'role': 'system', 'text': 'system:  mastermaster\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 256, 'over': False}

---begin-> None
 system

---after-> tensor([[-1.2627e-03, -6.4026e-02,  4.2090e-01,  ..., -9.0271e-02,
         -1.9775e-01, -3.5217e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.6582e+00, -3.5875e+01, -2.6828e+01,  ...,  2.8125e-01,
         -1.8945e-01,  1.1514e+00],
        ...,
        [-9.0938e+00, -2.3359e+00, -6.4883e+00,  ...,  3.5449e+00,
          3.6250e+00, -3.5586e+00],
        [ 3.8242e+00,  1.7236e+00,  1.4141e+01,  ...,  1.0010e+00,
          1.0068e+00,  1.2051e+00],
        [ 1.1592e+00,  2.0059e+00,  5.4062e+00,  ...,  1.5088e+00,
          2.1523e+00,  2.6387e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 23:18:33] "POST /inference/generate HTTP/1.1" 200 1855

init_state None

state-> tensor([[-1.2627e-03, -6.4026e-02,  4.2090e-01,  ..., -9.0271e-02,
         -1.9775e-01, -3.5217e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.6582e+00, -3.5875e+01, -2.6828e+01,  ...,  2.8125e-01,
         -1.8945e-01,  1.1514e+00],
        ...,
        [-9.0938e+00, -2.3359e+00, -6.4883e+00,  ...,  3.5449e+00,
          3.6250e+00, -3.5586e+00],
        [ 3.8242e+00,  1.7236e+00,  1.4141e+01,  ...,  1.0010e+00,
          1.0068e+00,  1.2051e+00],
        [ 1.1592e+00,  2.0059e+00,  5.4062e+00,  ...,  1.5088e+00,
          2.1523e+00,  2.6387e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 23:18:50] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'system:  mastermaster\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system system\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 256, 'over': False}

---begin-> None




---after-> tensor([[-1.8616e-02, -7.9285e-02,  3.6304e-01,  ..., -7.8552e-02,
         -2.0422e-01, -3.6926e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-9.1895e-01, -3.2594e+01, -5.6062e+01,  ...,  2.4036e-01,
          7.3303e-02,  1.4170e+00],
        ...,
        [-4.0156e+00, -7.9219e+00, -1.0654e+00,  ...,  2.4414e+00,
          4.2617e+00, -8.7812e+00],
        [ 1.7529e+00,  2.7070e+00,  2.5879e+00,  ...,  1.0020e+00,
          1.0420e+00,  1.7617e+00],
        [ 3.5176e+00, -1.4941e-01,  7.7227e+00,  ..., -4.8413e-01,
          2.0371e+00, -1.8579e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 23:19:00] "POST /inference/generate HTTP/1.1" 200 3155

init_state None

state-> tensor([[-1.8616e-02, -7.9285e-02,  3.6304e-01,  ..., -7.8552e-02,
         -2.0422e-01, -3.6926e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-9.1895e-01, -3.2594e+01, -5.6062e+01,  ...,  2.4036e-01,
          7.3303e-02,  1.4170e+00],
        ...,
        [-4.0156e+00, -7.9219e+00, -1.0654e+00,  ...,  2.4414e+00,
          4.2617e+00, -8.7812e+00],
        [ 1.7529e+00,  2.7070e+00,  2.5879e+00,  ...,  1.0020e+00,
          1.0420e+00,  1.7617e+00],
        [ 3.5176e+00, -1.4941e-01,  7.7227e+00,  ..., -4.8413e-01,
          2.0371e+00, -1.8579e-01]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 23:19:12] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'system:  mastermaster\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system system\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 256, 'over': False}

---begin-> None

 




 




---after-> tensor([[-1.1200e-01,  8.3801e-02,  1.5845e-01,  ..., -4.2969e-02,
          4.9347e-02, -6.9641e-02],
        [-9.6558e-02,  4.3823e-02,  2.3834e-02,  ...,  4.0985e-02,
          4.0710e-02, -1.2585e-01],
        [-2.4570e+00, -7.7125e+01, -3.4562e+01,  ...,  4.8340e-01,
         -1.2031e+00,  1.9080e-01],
        ...,
        [-4.9531e+00, -1.9551e+00, -2.0137e+00,  ...,  6.1094e+00,
          4.1406e-01,  4.6172e+00],
        [ 5.8711e+00,  1.4043e+00,  1.2633e+01,  ...,  1.8242e+00,
          1.0840e+00,  1.0186e+00],
        [ 9.4971e-01,  2.2930e+00,  6.1055e+00,  ..., -3.7832e+00,
          1.3086e+00,  2.4180e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 23:19:37] "POST /inference/generate HTTP/1.1" 200 5719

init_state None

state-> tensor([[-1.1200e-01,  8.3801e-02,  1.5845e-01,  ..., -4.2969e-02,
          4.9347e-02, -6.9641e-02],
        [-9.6558e-02,  4.3823e-02,  2.3834e-02,  ...,  4.0985e-02,
          4.0710e-02, -1.2585e-01],
        [-2.4570e+00, -7.7125e+01, -3.4562e+01,  ...,  4.8340e-01,
         -1.2031e+00,  1.9080e-01],
        ...,
        [-4.9531e+00, -1.9551e+00, -2.0137e+00,  ...,  6.1094e+00,
          4.1406e-01,  4.6172e+00],
        [ 5.8711e+00,  1.4043e+00,  1.2633e+01,  ...,  1.8242e+00,
          1.0840e+00,  1.0186e+00],
        [ 9.4971e-01,  2.2930e+00,  6.1055e+00,  ..., -3.7832e+00,
          1.3086e+00,  2.4180e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 23:19:56] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'system:  mastermaster\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 256, 'over': False}

---begin-> None
 system

---after-> tensor([[-1.2627e-03, -6.4026e-02,  4.2090e-01,  ..., -9.0271e-02,
         -1.9775e-01, -3.5217e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.6582e+00, -3.5875e+01, -2.6828e+01,  ...,  2.8125e-01,
         -1.8945e-01,  1.1514e+00],
        ...,
        [-9.0938e+00, -2.3359e+00, -6.4883e+00,  ...,  3.5449e+00,
          3.6250e+00, -3.5586e+00],
        [ 3.8242e+00,  1.7236e+00,  1.4141e+01,  ...,  1.0010e+00,
          1.0068e+00,  1.2051e+00],
        [ 1.1592e+00,  2.0059e+00,  5.4062e+00,  ...,  1.5088e+00,
          2.1523e+00,  2.6387e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 23:20:02] "POST /inference/generate HTTP/1.1" 200 1855

init_state None

state-> tensor([[-1.2627e-03, -6.4026e-02,  4.2090e-01,  ..., -9.0271e-02,
         -1.9775e-01, -3.5217e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.6582e+00, -3.5875e+01, -2.6828e+01,  ...,  2.8125e-01,
         -1.8945e-01,  1.1514e+00],
        ...,
        [-9.0938e+00, -2.3359e+00, -6.4883e+00,  ...,  3.5449e+00,
          3.6250e+00, -3.5586e+00],
        [ 3.8242e+00,  1.7236e+00,  1.4141e+01,  ...,  1.0010e+00,
          1.0068e+00,  1.2051e+00],
        [ 1.1592e+00,  2.0059e+00,  5.4062e+00,  ...,  1.5088e+00,
          2.1523e+00,  2.6387e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 23:20:09] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'analysis', 'text': 'system system\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 256, 'over': False}

---begin-> None

master
 

25
10



---after-> tensor([[-1.2189e-01,  1.3684e-01,  1.9128e-01,  ...,  1.9495e-01,
          1.7712e-01,  2.3828e-01],
        [-6.2927e-02,  8.7952e-02, -2.7603e-02,  ...,  4.6704e-01,
          1.3623e-01,  1.8958e-01],
        [-2.4612e-02, -7.1312e+01, -5.2156e+01,  ...,  8.0762e-01,
         -9.4757e-03,  9.4873e-01],
        ...,
        [-8.3828e+00, -3.6836e+00, -7.0898e+00,  ...,  9.0967e-01,
          3.9453e+00, -4.6094e+00],
        [ 3.2090e+00,  3.4688e+00,  9.8203e+00,  ...,  1.0127e+00,
          1.1240e+00,  1.8418e+00],
        [ 2.4199e+00, -6.1230e-01,  6.0156e+00,  ..., -2.8633e+00,
          3.4009e-01,  3.0430e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 23:20:26] "POST /inference/generate HTTP/1.1" 200 3437

init_state None

state-> tensor([[-1.2189e-01,  1.3684e-01,  1.9128e-01,  ...,  1.9495e-01,
          1.7712e-01,  2.3828e-01],
        [-6.2927e-02,  8.7952e-02, -2.7603e-02,  ...,  4.6704e-01,
          1.3623e-01,  1.8958e-01],
        [-2.4612e-02, -7.1312e+01, -5.2156e+01,  ...,  8.0762e-01,
         -9.4757e-03,  9.4873e-01],
        ...,
        [-8.3828e+00, -3.6836e+00, -7.0898e+00,  ...,  9.0967e-01,
          3.9453e+00, -4.6094e+00],
        [ 3.2090e+00,  3.4688e+00,  9.8203e+00,  ...,  1.0127e+00,
          1.1240e+00,  1.8418e+00],
        [ 2.4199e+00, -6.1230e-01,  6.0156e+00,  ..., -2.8633e+00,
          3.4009e-01,  3.0430e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 23:20:40] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'system:  mastermaster\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system system\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 256, 'over': False}

---begin-> None




---after-> tensor([[-1.8616e-02, -7.9285e-02,  3.6304e-01,  ..., -7.8552e-02,
         -2.0422e-01, -3.6926e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-9.1895e-01, -3.2594e+01, -5.6062e+01,  ...,  2.4036e-01,
          7.3303e-02,  1.4170e+00],
        ...,
        [-4.0156e+00, -7.9219e+00, -1.0654e+00,  ...,  2.4414e+00,
          4.2617e+00, -8.7812e+00],
        [ 1.7529e+00,  2.7070e+00,  2.5879e+00,  ...,  1.0020e+00,
          1.0420e+00,  1.7617e+00],
        [ 3.5176e+00, -1.4941e-01,  7.7227e+00,  ..., -4.8413e-01,
          2.0371e+00, -1.8579e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 23:20:51] "POST /inference/generate HTTP/1.1" 200 3155

init_state None

state-> tensor([[-1.8616e-02, -7.9285e-02,  3.6304e-01,  ..., -7.8552e-02,
         -2.0422e-01, -3.6926e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-9.1895e-01, -3.2594e+01, -5.6062e+01,  ...,  2.4036e-01,
          7.3303e-02,  1.4170e+00],
        ...,
        [-4.0156e+00, -7.9219e+00, -1.0654e+00,  ...,  2.4414e+00,
          4.2617e+00, -8.7812e+00],
        [ 1.7529e+00,  2.7070e+00,  2.5879e+00,  ...,  1.0020e+00,
          1.0420e+00,  1.7617e+00],
        [ 3.5176e+00, -1.4941e-01,  7.7227e+00,  ..., -4.8413e-01,
          2.0371e+00, -1.8579e-01]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 23:21:07] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'system:  mastermaster\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 256, 'over': False}

---begin-> None
 system

---after-> tensor([[-1.2627e-03, -6.4026e-02,  4.2090e-01,  ..., -9.0271e-02,
         -1.9775e-01, -3.5217e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.6582e+00, -3.5875e+01, -2.6828e+01,  ...,  2.8125e-01,
         -1.8945e-01,  1.1514e+00],
        ...,
        [-9.0938e+00, -2.3359e+00, -6.4883e+00,  ...,  3.5449e+00,
          3.6250e+00, -3.5586e+00],
        [ 3.8242e+00,  1.7236e+00,  1.4141e+01,  ...,  1.0010e+00,
          1.0068e+00,  1.2051e+00],
        [ 1.1592e+00,  2.0059e+00,  5.4062e+00,  ...,  1.5088e+00,
          2.1523e+00,  2.6387e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 23:21:13] "POST /inference/generate HTTP/1.1" 200 1855

init_state None

state-> tensor([[-1.2627e-03, -6.4026e-02,  4.2090e-01,  ..., -9.0271e-02,
         -1.9775e-01, -3.5217e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.6582e+00, -3.5875e+01, -2.6828e+01,  ...,  2.8125e-01,
         -1.8945e-01,  1.1514e+00],
        ...,
        [-9.0938e+00, -2.3359e+00, -6.4883e+00,  ...,  3.5449e+00,
          3.6250e+00, -3.5586e+00],
        [ 3.8242e+00,  1.7236e+00,  1.4141e+01,  ...,  1.0010e+00,
          1.0068e+00,  1.2051e+00],
        [ 1.1592e+00,  2.0059e+00,  5.4062e+00,  ...,  1.5088e+00,
          2.1523e+00,  2.6387e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 23:21:29] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 256, 'over': False}

---begin-> None


---after-> tensor([[-2.8442e-02,  6.4636e-02,  3.3325e-01,  ...,  2.2614e-02,
          7.4768e-02,  4.1260e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-8.8281e-01, -1.3711e+01, -1.4469e+01,  ..., -1.7065e-01,
         -2.1643e-01,  1.4756e+00],
        ...,
        [-1.9326e+00,  1.8984e+00, -2.8015e-02,  ...,  7.4316e-01,
          2.7891e+00, -1.6045e+00],
        [ 1.3369e+00,  1.6963e+00,  2.3906e+00,  ...,  1.0049e+00,
          1.1025e+00,  1.0469e+00],
        [ 3.2812e+00,  4.1699e-01,  7.1953e+00,  ..., -1.4570e+00,
          2.1484e+00,  3.2461e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 23:21:31] "POST /inference/generate HTTP/1.1" 200 795

init_state None

state-> tensor([[-2.8442e-02,  6.4636e-02,  3.3325e-01,  ...,  2.2614e-02,
          7.4768e-02,  4.1260e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-8.8281e-01, -1.3711e+01, -1.4469e+01,  ..., -1.7065e-01,
         -2.1643e-01,  1.4756e+00],
        ...,
        [-1.9326e+00,  1.8984e+00, -2.8015e-02,  ...,  7.4316e-01,
          2.7891e+00, -1.6045e+00],
        [ 1.3369e+00,  1.6963e+00,  2.3906e+00,  ...,  1.0049e+00,
          1.1025e+00,  1.0469e+00],
        [ 3.2812e+00,  4.1699e-01,  7.1953e+00,  ..., -1.4570e+00,
          2.1484e+00,  3.2461e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 23:38:36] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'system:  mastermaster\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 256, 'over': False}

---begin-> None
 system

---after-> tensor([[-1.2627e-03, -6.4026e-02,  4.2090e-01,  ..., -9.0271e-02,
         -1.9775e-01, -3.5217e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.6582e+00, -3.5875e+01, -2.6828e+01,  ...,  2.8125e-01,
         -1.8945e-01,  1.1514e+00],
        ...,
        [-9.0938e+00, -2.3359e+00, -6.4883e+00,  ...,  3.5449e+00,
          3.6250e+00, -3.5586e+00],
        [ 3.8242e+00,  1.7236e+00,  1.4141e+01,  ...,  1.0010e+00,
          1.0068e+00,  1.2051e+00],
        [ 1.1592e+00,  2.0059e+00,  5.4062e+00,  ...,  1.5088e+00,
          2.1523e+00,  2.6387e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 23:38:42] "POST /inference/generate HTTP/1.1" 200 1855

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 256, 'over': False}

---begin-> tensor([[-1.2627e-03, -6.4026e-02,  4.2090e-01,  ..., -9.0271e-02,
         -1.9775e-01, -3.5217e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.6582e+00, -3.5875e+01, -2.6828e+01,  ...,  2.8125e-01,
         -1.8945e-01,  1.1514e+00],
        ...,
        [-9.0938e+00, -2.3359e+00, -6.4883e+00,  ...,  3.5449e+00,
          3.6250e+00, -3.5586e+00],
        [ 3.8242e+00,  1.7236e+00,  1.4141e+01,  ...,  1.0010e+00,
          1.0068e+00,  1.2051e+00],
        [ 1.1592e+00,  2.0059e+00,  5.4062e+00,  ...,  1.5088e+00,
          2.1523e+00,  2.6387e+00]], device='cuda:0', dtype=torch.float16)


---after-> tensor([[-1.8616e-02,  2.9251e-02,  3.0322e-01,  ...,  1.4198e-02,
          8.4412e-02,  6.2408e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-1.4863e+00, -4.5906e+01, -3.6281e+01,  ..., -1.7065e-01,
         -2.1643e-01,  1.4756e+00],
        ...,
        [-1.5215e+00, -2.0762e+00, -1.8359e+00,  ...,  2.0020e+00,
          1.9912e+00, -3.1055e+00],
        [ 1.3867e+00,  2.4336e+00,  2.8691e+00,  ...,  1.0039e+00,
          1.1543e+00,  1.1768e+00],
        [ 2.7656e+00,  1.0566e+00,  7.3398e+00,  ...,  7.3486e-01,
          1.5332e+00,  2.2148e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 23:38:57] "POST /inference/generate HTTP/1.1" 200 795

init_state None

state-> tensor([[-1.8616e-02,  2.9251e-02,  3.0322e-01,  ...,  1.4198e-02,
          8.4412e-02,  6.2408e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-1.4863e+00, -4.5906e+01, -3.6281e+01,  ..., -1.7065e-01,
         -2.1643e-01,  1.4756e+00],
        ...,
        [-1.5215e+00, -2.0762e+00, -1.8359e+00,  ...,  2.0020e+00,
          1.9912e+00, -3.1055e+00],
        [ 1.3867e+00,  2.4336e+00,  2.8691e+00,  ...,  1.0039e+00,
          1.1543e+00,  1.1768e+00],
        [ 2.7656e+00,  1.0566e+00,  7.3398e+00,  ...,  7.3486e-01,
          1.5332e+00,  2.2148e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 23:39:38] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'system:  mastermaster\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system system\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 256, 'over': False}

---begin-> None




---after-> tensor([[-1.8616e-02, -7.9285e-02,  3.6304e-01,  ..., -7.8552e-02,
         -2.0422e-01, -3.6926e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-9.1895e-01, -3.2594e+01, -5.6062e+01,  ...,  2.4036e-01,
          7.3303e-02,  1.4170e+00],
        ...,
        [-4.0156e+00, -7.9219e+00, -1.0654e+00,  ...,  2.4414e+00,
          4.2617e+00, -8.7812e+00],
        [ 1.7529e+00,  2.7070e+00,  2.5879e+00,  ...,  1.0020e+00,
          1.0420e+00,  1.7617e+00],
        [ 3.5176e+00, -1.4941e-01,  7.7227e+00,  ..., -4.8413e-01,
          2.0371e+00, -1.8579e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 23:39:48] "POST /inference/generate HTTP/1.1" 200 3155

init_state None

state-> tensor([[-1.8616e-02, -7.9285e-02,  3.6304e-01,  ..., -7.8552e-02,
         -2.0422e-01, -3.6926e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-9.1895e-01, -3.2594e+01, -5.6062e+01,  ...,  2.4036e-01,
          7.3303e-02,  1.4170e+00],
        ...,
        [-4.0156e+00, -7.9219e+00, -1.0654e+00,  ...,  2.4414e+00,
          4.2617e+00, -8.7812e+00],
        [ 1.7529e+00,  2.7070e+00,  2.5879e+00,  ...,  1.0020e+00,
          1.0420e+00,  1.7617e+00],
        [ 3.5176e+00, -1.4941e-01,  7.7227e+00,  ..., -4.8413e-01,
          2.0371e+00, -1.8579e-01]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 23:40:10] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'system:  mastermaster\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 256, 'over': False}

---begin-> None
 system

---after-> tensor([[-1.2627e-03, -6.4026e-02,  4.2090e-01,  ..., -9.0271e-02,
         -1.9775e-01, -3.5217e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.6582e+00, -3.5875e+01, -2.6828e+01,  ...,  2.8125e-01,
         -1.8945e-01,  1.1514e+00],
        ...,
        [-9.0938e+00, -2.3359e+00, -6.4883e+00,  ...,  3.5449e+00,
          3.6250e+00, -3.5586e+00],
        [ 3.8242e+00,  1.7236e+00,  1.4141e+01,  ...,  1.0010e+00,
          1.0068e+00,  1.2051e+00],
        [ 1.1592e+00,  2.0059e+00,  5.4062e+00,  ...,  1.5088e+00,
          2.1523e+00,  2.6387e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 23:40:16] "POST /inference/generate HTTP/1.1" 200 1855

init_state None

state-> tensor([[-1.2627e-03, -6.4026e-02,  4.2090e-01,  ..., -9.0271e-02,
         -1.9775e-01, -3.5217e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.6582e+00, -3.5875e+01, -2.6828e+01,  ...,  2.8125e-01,
         -1.8945e-01,  1.1514e+00],
        ...,
        [-9.0938e+00, -2.3359e+00, -6.4883e+00,  ...,  3.5449e+00,
          3.6250e+00, -3.5586e+00],
        [ 3.8242e+00,  1.7236e+00,  1.4141e+01,  ...,  1.0010e+00,
          1.0068e+00,  1.2051e+00],
        [ 1.1592e+00,  2.0059e+00,  5.4062e+00,  ...,  1.5088e+00,
          2.1523e+00,  2.6387e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 23:51:28] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'system: \n1. master\n2.master', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 256, 'over': False}

---begin-> None


---after-> tensor([[ 1.2140e-03, -5.1758e-02,  4.0698e-01,  ..., -7.3914e-02,
         -2.1606e-01, -6.0425e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.9951e+00, -2.9547e+01, -3.8688e+01,  ...,  3.6304e-01,
         -5.1709e-01,  1.1943e+00],
        ...,
        [-1.1805e+01, -5.1250e+00, -3.5957e+00,  ...,  5.0684e-01,
          2.9180e+00, -4.7070e-01],
        [ 4.5859e+00,  4.2617e+00,  9.0703e+00,  ...,  1.0078e+00,
          1.0049e+00,  1.7559e+00],
        [ 7.8076e-01,  6.5039e-01,  6.2188e+00,  ..., -6.6455e-01,
          1.5566e+00,  3.3984e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 23:51:32] "POST /inference/generate HTTP/1.1" 200 1780

init_state None

state-> tensor([[ 1.2140e-03, -5.1758e-02,  4.0698e-01,  ..., -7.3914e-02,
         -2.1606e-01, -6.0425e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.9951e+00, -2.9547e+01, -3.8688e+01,  ...,  3.6304e-01,
         -5.1709e-01,  1.1943e+00],
        ...,
        [-1.1805e+01, -5.1250e+00, -3.5957e+00,  ...,  5.0684e-01,
          2.9180e+00, -4.7070e-01],
        [ 4.5859e+00,  4.2617e+00,  9.0703e+00,  ...,  1.0078e+00,
          1.0049e+00,  1.7559e+00],
        [ 7.8076e-01,  6.5039e-01,  6.2188e+00,  ..., -6.6455e-01,
          1.5566e+00,  3.3984e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 23:51:57] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'system: .\n1. master\n2. master', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 256, 'over': False}

---begin-> None

1. AI
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. AI,
---after-> tensor([[-5.8105e-02,  1.5356e-01,  2.6978e-01,  ...,  5.7648e-02,
          5.8624e-02,  1.1359e-01],
        [-1.2993e-02,  1.1017e-01,  1.3025e-01,  ...,  2.1936e-01,
          5.3070e-02,  1.0809e-01],
        [-1.3223e+00, -7.9125e+01, -5.7906e+01,  ..., -4.5752e-01,
          5.8899e-02,  1.2441e+00],
        ...,
        [ 1.1699e+00,  5.4102e-01, -1.5180e+01,  ...,  2.0605e+00,
          5.4766e+00, -2.9766e+00],
        [ 1.4180e+00,  1.8467e+00,  1.2820e+01,  ...,  1.1191e+00,
          1.5430e+00,  1.8193e+00],
        [ 4.4453e+00,  7.4121e-01,  4.9258e+00,  ..., -5.9805e+00,
         -1.5710e-01, -1.0010e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 23:52:13] "POST /inference/generate HTTP/1.1" 200 4041

init_state None

state-> tensor([[-5.8105e-02,  1.5356e-01,  2.6978e-01,  ...,  5.7648e-02,
          5.8624e-02,  1.1359e-01],
        [-1.2993e-02,  1.1017e-01,  1.3025e-01,  ...,  2.1936e-01,
          5.3070e-02,  1.0809e-01],
        [-1.3223e+00, -7.9125e+01, -5.7906e+01,  ..., -4.5752e-01,
          5.8899e-02,  1.2441e+00],
        ...,
        [ 1.1699e+00,  5.4102e-01, -1.5180e+01,  ...,  2.0605e+00,
          5.4766e+00, -2.9766e+00],
        [ 1.4180e+00,  1.8467e+00,  1.2820e+01,  ...,  1.1191e+00,
          1.5430e+00,  1.8193e+00],
        [ 4.4453e+00,  7.4121e-01,  4.9258e+00,  ..., -5.9805e+00,
         -1.5710e-01, -1.0010e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 23:52:24] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'system: .\n1. master\n2. master', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n1. AI\n2. \n3. \n4. \n5. \n6. \n7. \n8. \n9. ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 256, 'over': False}

---begin-> None

1. AI
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 
11. 
12. 
---after-> tensor([[-1.8262e-01, -2.0630e-02,  1.3452e-01,  ...,  2.1011e-02,
          6.4087e-02,  2.3849e-02],
        [-1.9397e-01, -1.7871e-01, -4.3976e-02,  ...,  1.4392e-01,
          5.8563e-02, -1.8372e-02],
        [-1.1895e+00, -3.4375e+01, -2.4047e+01,  ..., -9.0234e-01,
          4.4116e-01, -3.7622e-01],
        ...,
        [-4.4141e+00, -2.8760e-01, -1.1992e+01,  ...,  5.8691e-01,
          3.6543e+00, -1.0469e+00],
        [ 1.1895e+00,  2.7656e+00,  1.0117e+01,  ...,  1.0156e+00,
          1.1484e+00,  1.1348e+00],
        [ 3.7148e+00, -1.1499e-01,  5.8672e+00,  ..., -2.0312e+00,
         -2.8359e+00, -2.3438e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 23:52:49] "POST /inference/generate HTTP/1.1" 200 7687

init_state None

state-> tensor([[-1.8262e-01, -2.0630e-02,  1.3452e-01,  ...,  2.1011e-02,
          6.4087e-02,  2.3849e-02],
        [-1.9397e-01, -1.7871e-01, -4.3976e-02,  ...,  1.4392e-01,
          5.8563e-02, -1.8372e-02],
        [-1.1895e+00, -3.4375e+01, -2.4047e+01,  ..., -9.0234e-01,
          4.4116e-01, -3.7622e-01],
        ...,
        [-4.4141e+00, -2.8760e-01, -1.1992e+01,  ...,  5.8691e-01,
          3.6543e+00, -1.0469e+00],
        [ 1.1895e+00,  2.7656e+00,  1.0117e+01,  ...,  1.0156e+00,
          1.1484e+00,  1.1348e+00],
        [ 3.7148e+00, -1.1499e-01,  5.8672e+00,  ..., -2.0312e+00,
         -2.8359e+00, -2.3438e-01]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [03/Oct/2023 23:53:24] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'system: .\n1. master\n2. master', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 256, 'over': False}

---begin-> None


---after-> tensor([[ 3.3760e-03, -4.9530e-02,  4.0186e-01,  ..., -6.6345e-02,
         -2.1521e-01, -6.6223e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-2.0781e+00, -2.8516e+01, -3.9562e+01,  ...,  3.6304e-01,
         -5.1709e-01,  1.1943e+00],
        ...,
        [-1.2008e+01, -6.0742e+00, -3.7559e+00,  ...,  4.7437e-01,
          3.1094e+00, -1.1484e+00],
        [ 4.7109e+00,  4.6250e+00,  9.5859e+00,  ...,  1.0088e+00,
          1.0039e+00,  1.8359e+00],
        [ 6.5918e-01,  6.3672e-01,  6.1641e+00,  ..., -6.5527e-01,
          1.5645e+00,  3.3320e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 23:53:29] "POST /inference/generate HTTP/1.1" 200 1781

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 256, 'over': False}

---begin-> tensor([[ 3.3760e-03, -4.9530e-02,  4.0186e-01,  ..., -6.6345e-02,
         -2.1521e-01, -6.6223e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-2.0781e+00, -2.8516e+01, -3.9562e+01,  ...,  3.6304e-01,
         -5.1709e-01,  1.1943e+00],
        ...,
        [-1.2008e+01, -6.0742e+00, -3.7559e+00,  ...,  4.7437e-01,
          3.1094e+00, -1.1484e+00],
        [ 4.7109e+00,  4.6250e+00,  9.5859e+00,  ...,  1.0088e+00,
          1.0039e+00,  1.8359e+00],
        [ 6.5918e-01,  6.3672e-01,  6.1641e+00,  ..., -6.5527e-01,
          1.5645e+00,  3.3320e+00]], device='cuda:0', dtype=torch.float16)

1. AI
2. 
3. 
4. 
5. AI
6. 
7. 
8. SM
9. AI
---after-> tensor([[-1.0474e-01, -3.5065e-02,  1.7297e-01,  ...,  5.1117e-02,
          1.3391e-01,  2.3352e-01],
        [-6.0822e-02, -2.2107e-01, -3.7170e-02,  ...,  2.3486e-01,
          1.3953e-01,  2.4646e-01],
        [-2.2910e+00, -7.0625e+01, -2.7859e+01,  ...,  2.2864e-01,
         -1.9446e-01, -2.1716e-01],
        ...,
        [-7.5352e+00, -3.6406e+00, -1.2906e+01,  ...,  6.5430e-01,
         -3.1348e+00,  1.6631e+00],
        [ 4.7656e+00,  1.3496e+00,  1.7797e+01,  ...,  1.0020e+00,
          1.0518e+00,  1.0928e+00],
        [ 2.7969e+00,  3.3184e+00,  4.9922e+00,  ..., -3.1484e+00,
          1.8018e+00,  3.1250e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [03/Oct/2023 23:53:49] "POST /inference/generate HTTP/1.1" 200 2190

init_state None

state-> tensor([[-1.0474e-01, -3.5065e-02,  1.7297e-01,  ...,  5.1117e-02,
          1.3391e-01,  2.3352e-01],
        [-6.0822e-02, -2.2107e-01, -3.7170e-02,  ...,  2.3486e-01,
          1.3953e-01,  2.4646e-01],
        [-2.2910e+00, -7.0625e+01, -2.7859e+01,  ...,  2.2864e-01,
         -1.9446e-01, -2.1716e-01],
        ...,
        [-7.5352e+00, -3.6406e+00, -1.2906e+01,  ...,  6.5430e-01,
         -3.1348e+00,  1.6631e+00],
        [ 4.7656e+00,  1.3496e+00,  1.7797e+01,  ...,  1.0020e+00,
          1.0518e+00,  1.0928e+00],
        [ 2.7969e+00,  3.3184e+00,  4.9922e+00,  ..., -3.1484e+00,
          1.8018e+00,  3.1250e-01]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 00:17:26] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\n1. \n2. master\n3. \n4. \n5. \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 256, 'over': False}

---begin-> None
 
1. 
2. master
3. 
4. 
5. 

---after-> tensor([[-8.4351e-02, -1.2154e-02,  2.1558e-01,  ...,  1.2489e-02,
         -6.9580e-02, -1.0040e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.4844e+00, -4.9156e+01, -4.9719e+01,  ...,  2.2583e-01,
         -3.8666e-02,  5.1361e-02],
        ...,
        [-1.3203e+01, -4.1094e+00, -7.4375e+00,  ...,  4.0156e+00,
          4.5483e-01, -2.4824e+00],
        [ 4.8984e+00,  2.9629e+00,  8.0156e+00,  ...,  1.0010e+00,
          1.0039e+00,  2.0332e+00],
        [ 1.6855e+00,  2.2986e-01,  5.9688e+00,  ...,  8.9990e-01,
          3.4062e+00, -6.1328e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:17:35] "POST /inference/generate HTTP/1.1" 200 2902

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 256, 'over': False}

---begin-> tensor([[-8.4351e-02, -1.2154e-02,  2.1558e-01,  ...,  1.2489e-02,
         -6.9580e-02, -1.0040e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.4844e+00, -4.9156e+01, -4.9719e+01,  ...,  2.2583e-01,
         -3.8666e-02,  5.1361e-02],
        ...,
        [-1.3203e+01, -4.1094e+00, -7.4375e+00,  ...,  4.0156e+00,
          4.5483e-01, -2.4824e+00],
        [ 4.8984e+00,  2.9629e+00,  8.0156e+00,  ...,  1.0010e+00,
          1.0039e+00,  2.0332e+00],
        [ 1.6855e+00,  2.2986e-01,  5.9688e+00,  ...,  8.9990e-01,
          3.4062e+00, -6.1328e-01]], device='cuda:0', dtype=torch.float16)

1. 
2. 
3. 
4. 
5. AI
6. AI
---after-> tensor([[-1.0596e-01, -1.4427e-02,  2.0752e-01,  ..., -1.0963e-02,
         -6.9092e-02, -1.0614e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-2.7441e+00, -7.2312e+01, -3.3625e+01,  ..., -1.0156e-01,
         -2.1863e-01,  2.4292e-02],
        ...,
        [-5.0312e+00, -1.3311e+00, -3.3594e+00,  ...,  1.5552e-01,
          3.6816e+00,  1.5894e-01],
        [ 4.4102e+00,  4.3984e+00,  6.1055e+00,  ...,  1.0234e+00,
          1.0029e+00,  1.1113e+00],
        [ 2.0449e+00, -6.8604e-01,  6.5742e+00,  ..., -2.0781e+00,
          3.3926e+00,  9.0869e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:18:05] "POST /inference/generate HTTP/1.1" 200 2237

init_state None

state-> tensor([[-1.0596e-01, -1.4427e-02,  2.0752e-01,  ..., -1.0963e-02,
         -6.9092e-02, -1.0614e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-2.7441e+00, -7.2312e+01, -3.3625e+01,  ..., -1.0156e-01,
         -2.1863e-01,  2.4292e-02],
        ...,
        [-5.0312e+00, -1.3311e+00, -3.3594e+00,  ...,  1.5552e-01,
          3.6816e+00,  1.5894e-01],
        [ 4.4102e+00,  4.3984e+00,  6.1055e+00,  ...,  1.0234e+00,
          1.0029e+00,  1.1113e+00],
        [ 2.0449e+00, -6.8604e-01,  6.5742e+00,  ..., -2.0781e+00,
          3.3926e+00,  9.0869e-01]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 00:18:37] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\n1. \n2. master\n3. \n4. \n5. \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system \n1. \n2. master\n3. \n4. \n5. \n ', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 256, 'over': False}

---begin-> None

1. 
2. 
3. 
4. 
5. 
6. 
7. 
---after-> tensor([[-1.5881e-01,  1.5479e-01,  3.4277e-01,  ...,  9.3140e-02,
         -4.3732e-02,  6.2805e-02],
        [-1.2891e-01,  7.8064e-02,  2.9688e-01,  ...,  2.9321e-01,
         -8.6731e-02,  4.9713e-02],
        [-5.8838e-01, -1.0094e+02, -6.5438e+01,  ..., -4.2505e-01,
          1.0101e-01,  5.2490e-01],
        ...,
        [ 3.7207e-01, -1.1836e+01, -2.1328e+01,  ...,  1.2051e+00,
          1.0672e+01,  2.1953e+00],
        [ 2.8145e+00,  3.0859e+00,  9.2734e+00,  ...,  1.1611e+00,
          1.1494e+00,  1.0957e+00],
        [ 3.8926e+00, -3.0200e-01,  5.5781e+00,  ..., -2.9961e+00,
          9.7656e-02, -7.7930e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:18:59] "POST /inference/generate HTTP/1.1" 200 5992

init_state None

state-> tensor([[-1.5881e-01,  1.5479e-01,  3.4277e-01,  ...,  9.3140e-02,
         -4.3732e-02,  6.2805e-02],
        [-1.2891e-01,  7.8064e-02,  2.9688e-01,  ...,  2.9321e-01,
         -8.6731e-02,  4.9713e-02],
        [-5.8838e-01, -1.0094e+02, -6.5438e+01,  ..., -4.2505e-01,
          1.0101e-01,  5.2490e-01],
        ...,
        [ 3.7207e-01, -1.1836e+01, -2.1328e+01,  ...,  1.2051e+00,
          1.0672e+01,  2.1953e+00],
        [ 2.8145e+00,  3.0859e+00,  9.2734e+00,  ...,  1.1611e+00,
          1.1494e+00,  1.0957e+00],
        [ 3.8926e+00, -3.0200e-01,  5.5781e+00,  ..., -2.9961e+00,
          9.7656e-02, -7.7930e-01]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 00:19:42] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\n1. \n2. master\n3. \n4. \n5. \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system \n1. \n2. master\n3. \n4. \n5. \n ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 256, 'over': False}

---begin-> None
 
1. AI
2. masterAI
3. master
4. master
---after-> tensor([[-1.2097e-01, -4.5197e-02,  2.4280e-01,  ..., -3.2501e-02,
          8.9417e-03,  3.1860e-02],
        [-1.3342e-01, -1.9470e-01, -9.8816e-02,  ...,  7.4951e-02,
         -2.7637e-03,  1.3405e-02],
        [-1.6406e+00, -6.9438e+01, -4.3500e+01,  ..., -1.2715e+00,
         -4.8340e-01, -6.0449e-01],
        ...,
        [-8.3984e+00, -1.8477e+00, -2.2031e+00,  ..., -1.0117e+00,
          1.0947e+00,  9.6289e-01],
        [ 3.8770e+00,  1.7705e+00,  6.2930e+00,  ...,  1.0029e+00,
          1.2568e+00,  1.0039e+00],
        [ 9.5166e-01,  2.9712e-01,  5.7852e+00,  ..., -2.8535e+00,
         -2.0098e+00,  3.2480e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:20:05] "POST /inference/generate HTTP/1.1" 200 6780

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 256, 'over': False}

---begin-> tensor([[-1.2097e-01, -4.5197e-02,  2.4280e-01,  ..., -3.2501e-02,
          8.9417e-03,  3.1860e-02],
        [-1.3342e-01, -1.9470e-01, -9.8816e-02,  ...,  7.4951e-02,
         -2.7637e-03,  1.3405e-02],
        [-1.6406e+00, -6.9438e+01, -4.3500e+01,  ..., -1.2715e+00,
         -4.8340e-01, -6.0449e-01],
        ...,
        [-8.3984e+00, -1.8477e+00, -2.2031e+00,  ..., -1.0117e+00,
          1.0947e+00,  9.6289e-01],
        [ 3.8770e+00,  1.7705e+00,  6.2930e+00,  ...,  1.0029e+00,
          1.2568e+00,  1.0039e+00],
        [ 9.5166e-01,  2.9712e-01,  5.7852e+00,  ..., -2.8535e+00,
         -2.0098e+00,  3.2480e+00]], device='cuda:0', dtype=torch.float16)

1. 
2. masterAI
3. master
4. masterAI
5. masterAI
6. 
---after-> tensor([[-1.6309e-01,  1.4880e-01,  3.4058e-01,  ...,  9.1125e-02,
         -4.4037e-02,  6.5735e-02],
        [-1.2891e-01,  7.8064e-02,  2.9688e-01,  ...,  2.9321e-01,
         -8.6731e-02,  4.9713e-02],
        [-1.9746e+00, -5.3250e+01, -3.0844e+01,  ..., -2.4109e-01,
         -5.4883e-01, -7.9712e-02],
        ...,
        [-7.9766e+00, -4.1797e+00, -2.7949e+00,  ...,  5.3242e+00,
         -1.1273e-01, -4.7109e+00],
        [ 3.6055e+00,  3.7266e+00,  6.7852e+00,  ...,  1.0938e+00,
          1.2100e+00,  1.2783e+00],
        [ 1.7412e+00, -6.2891e-01,  6.2227e+00,  ..., -3.5176e+00,
         -7.6221e-01, -6.0352e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:20:23] "POST /inference/generate HTTP/1.1" 200 2233

init_state None

state-> tensor([[-1.6309e-01,  1.4880e-01,  3.4058e-01,  ...,  9.1125e-02,
         -4.4037e-02,  6.5735e-02],
        [-1.2891e-01,  7.8064e-02,  2.9688e-01,  ...,  2.9321e-01,
         -8.6731e-02,  4.9713e-02],
        [-1.9746e+00, -5.3250e+01, -3.0844e+01,  ..., -2.4109e-01,
         -5.4883e-01, -7.9712e-02],
        ...,
        [-7.9766e+00, -4.1797e+00, -2.7949e+00,  ...,  5.3242e+00,
         -1.1273e-01, -4.7109e+00],
        [ 3.6055e+00,  3.7266e+00,  6.7852e+00,  ...,  1.0938e+00,
          1.2100e+00,  1.2783e+00],
        [ 1.7412e+00, -6.2891e-01,  6.2227e+00,  ..., -3.5176e+00,
         -7.6221e-01, -6.0352e-01]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 00:21:13] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\n1. \n2. master\n3. \n4. \n5. \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None
 
1. 
2. master
3. 
4. 
5. 

---after-> tensor([[-8.4351e-02, -1.2154e-02,  2.1558e-01,  ...,  1.2489e-02,
         -6.9580e-02, -1.0040e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.4844e+00, -4.9156e+01, -4.9719e+01,  ...,  2.2583e-01,
         -3.8666e-02,  5.1361e-02],
        ...,
        [-1.3203e+01, -4.1094e+00, -7.4375e+00,  ...,  4.0156e+00,
          4.5483e-01, -2.4824e+00],
        [ 4.8984e+00,  2.9629e+00,  8.0156e+00,  ...,  1.0010e+00,
          1.0039e+00,  2.0332e+00],
        [ 1.6855e+00,  2.2986e-01,  5.9688e+00,  ...,  8.9990e-01,
          3.4062e+00, -6.1328e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:21:24] "POST /inference/generate HTTP/1.1" 200 2903

init_state None

state-> tensor([[-8.4351e-02, -1.2154e-02,  2.1558e-01,  ...,  1.2489e-02,
         -6.9580e-02, -1.0040e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.4844e+00, -4.9156e+01, -4.9719e+01,  ...,  2.2583e-01,
         -3.8666e-02,  5.1361e-02],
        ...,
        [-1.3203e+01, -4.1094e+00, -7.4375e+00,  ...,  4.0156e+00,
          4.5483e-01, -2.4824e+00],
        [ 4.8984e+00,  2.9629e+00,  8.0156e+00,  ...,  1.0010e+00,
          1.0039e+00,  2.0332e+00],
        [ 1.6855e+00,  2.2986e-01,  5.9688e+00,  ...,  8.9990e-01,
          3.4062e+00, -6.1328e-01]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 00:21:59] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\n1. \n2. master\n3. \n4. \n5. ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None


---after-> tensor([[-6.7568e-04, -6.0669e-02,  3.8232e-01,  ..., -9.9731e-02,
         -1.8872e-01, -3.3539e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [ 1.8701e-01, -4.3938e+01, -3.8844e+01,  ..., -1.4282e-02,
         -2.1448e-01,  1.1826e+00],
        ...,
        [-7.9961e+00, -4.5312e-01, -5.6289e+00,  ...,  5.6055e+00,
          3.1367e+00, -2.4551e+00],
        [ 3.0898e+00,  3.6387e+00,  6.2539e+00,  ...,  1.0293e+00,
          1.0029e+00,  1.1660e+00],
        [ 2.4258e+00,  3.3325e-01,  6.9766e+00,  ..., -1.3855e-01,
          3.2969e+00,  2.5039e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:22:08] "POST /inference/generate HTTP/1.1" 200 3409

init_state None

state-> tensor([[-6.7568e-04, -6.0669e-02,  3.8232e-01,  ..., -9.9731e-02,
         -1.8872e-01, -3.3539e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [ 1.8701e-01, -4.3938e+01, -3.8844e+01,  ..., -1.4282e-02,
         -2.1448e-01,  1.1826e+00],
        ...,
        [-7.9961e+00, -4.5312e-01, -5.6289e+00,  ...,  5.6055e+00,
          3.1367e+00, -2.4551e+00],
        [ 3.0898e+00,  3.6387e+00,  6.2539e+00,  ...,  1.0293e+00,
          1.0029e+00,  1.1660e+00],
        [ 2.4258e+00,  3.3325e-01,  6.9766e+00,  ..., -1.3855e-01,
          3.2969e+00,  2.5039e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 00:22:19] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\n1. \n2. master\n3. \n4. \n5. ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None

1. 
2. 
3. B85W58H88
4. 
5. 
6. 
7. 
8. 
9. 80%
10. 
11. AI
12. 
13. 
14. 
15. 
16. SM
17. 500
18. AI
19. 500
20. 
1. AI
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 
11. 
12. 
13. 
14. 


---after-> tensor([[-1.0242e-01, -1.1063e-02,  2.2046e-01,  ...,  2.3842e-03,
         -7.0312e-02, -8.3374e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-8.7793e-01, -5.3688e+01, -3.2562e+01,  ...,  2.2583e-01,
         -3.8666e-02,  5.1361e-02],
        ...,
        [-6.3125e+00,  9.8145e-01, -8.7305e-01,  ...,  6.2422e+00,
          3.4805e+00, -2.6914e+00],
        [ 4.2773e+00,  1.3340e+00,  4.2734e+00,  ...,  1.0195e+00,
          1.0039e+00,  1.3018e+00],
        [ 1.8115e+00,  2.0273e+00,  6.3203e+00,  ..., -1.6982e+00,
          3.9980e+00,  1.5527e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:23:00] "POST /inference/generate HTTP/1.1" 200 8657

init_state None

state-> tensor([[-1.0242e-01, -1.1063e-02,  2.2046e-01,  ...,  2.3842e-03,
         -7.0312e-02, -8.3374e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-8.7793e-01, -5.3688e+01, -3.2562e+01,  ...,  2.2583e-01,
         -3.8666e-02,  5.1361e-02],
        ...,
        [-6.3125e+00,  9.8145e-01, -8.7305e-01,  ...,  6.2422e+00,
          3.4805e+00, -2.6914e+00],
        [ 4.2773e+00,  1.3340e+00,  4.2734e+00,  ...,  1.0195e+00,
          1.0039e+00,  1.3018e+00],
        [ 1.8115e+00,  2.0273e+00,  6.3203e+00,  ..., -1.6982e+00,
          3.9980e+00,  1.5527e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 00:24:32] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\n1. \n2. master\n3. \n4. \n5. ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None

1. 
2. 
3. B85W58H88
4. 
5. 
6. 
7. 
8. 
9. 80%
10. 
11. AI
12. 
13. 
14. 
15. 
16. SM
17. 500
18. AI
19. 500
20. 
---after-> tensor([[-8.6670e-02, -1.8646e-02,  2.1631e-01,  ...,  8.6136e-03,
         -5.5908e-02, -1.1133e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.7666e+00, -1.1500e+02, -3.9406e+01,  ...,  3.7085e-01,
         -3.5156e-01,  3.1104e-01],
        ...,
        [-2.4648e+00, -2.4668e+00, -1.5771e+00,  ...,  5.2578e+00,
          1.5830e+00, -1.1094e+00],
        [ 3.1465e+00,  2.9375e+00,  4.0742e+00,  ...,  1.0107e+00,
          1.0020e+00,  1.3340e+00],
        [ 2.0547e+00, -6.0889e-01,  6.3477e+00,  ..., -6.2744e-01,
          3.5020e+00,  1.1523e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:25:02] "POST /inference/generate HTTP/1.1" 200 7526

init_state None

state-> tensor([[-8.6670e-02, -1.8646e-02,  2.1631e-01,  ...,  8.6136e-03,
         -5.5908e-02, -1.1133e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.7666e+00, -1.1500e+02, -3.9406e+01,  ...,  3.7085e-01,
         -3.5156e-01,  3.1104e-01],
        ...,
        [-2.4648e+00, -2.4668e+00, -1.5771e+00,  ...,  5.2578e+00,
          1.5830e+00, -1.1094e+00],
        [ 3.1465e+00,  2.9375e+00,  4.0742e+00,  ...,  1.0107e+00,
          1.0020e+00,  1.3340e+00],
        [ 2.0547e+00, -6.0889e-01,  6.3477e+00,  ..., -6.2744e-01,
          3.5020e+00,  1.1523e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 00:25:53] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\nmaster', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None



---after-> tensor([[-1.0689e-02, -6.3416e-02,  3.8501e-01,  ..., -1.0669e-01,
         -1.9055e-01, -4.5197e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-4.1699e-01, -4.9438e+01, -5.9938e+01,  ...,  2.6294e-01,
         -3.0566e-01,  1.1650e+00],
        ...,
        [-1.4678e+00, -1.1242e+01, -6.5234e+00,  ...,  3.5781e+00,
          4.4062e+00, -2.9805e+00],
        [ 3.4023e+00,  3.6953e+00,  5.8281e+00,  ...,  1.0107e+00,
          1.0068e+00,  1.0068e+00],
        [ 2.7109e+00,  8.7109e-01,  6.7070e+00,  ...,  5.1367e-01,
          3.4902e+00,  3.8242e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:26:08] "POST /inference/generate HTTP/1.1" 200 5352

init_state None

state-> tensor([[-1.0689e-02, -6.3416e-02,  3.8501e-01,  ..., -1.0669e-01,
         -1.9055e-01, -4.5197e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-4.1699e-01, -4.9438e+01, -5.9938e+01,  ...,  2.6294e-01,
         -3.0566e-01,  1.1650e+00],
        ...,
        [-1.4678e+00, -1.1242e+01, -6.5234e+00,  ...,  3.5781e+00,
          4.4062e+00, -2.9805e+00],
        [ 3.4023e+00,  3.6953e+00,  5.8281e+00,  ...,  1.0107e+00,
          1.0068e+00,  1.0068e+00],
        [ 2.7109e+00,  8.7109e-01,  6.7070e+00,  ...,  5.1367e-01,
          3.4902e+00,  3.8242e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 00:28:07] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\nmaster', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None


---after-> tensor([[-3.0594e-02, -6.3599e-02,  4.0747e-01,  ..., -7.5073e-02,
         -2.0190e-01, -4.8187e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [ 2.2949e-02, -2.3109e+01, -3.4969e+01,  ...,  6.2988e-02,
         -2.1582e-01,  9.0479e-01],
        ...,
        [-1.3578e+01, -3.3965e+00, -2.6660e+00,  ...,  3.5410e+00,
          1.4551e+00, -6.2109e+00],
        [ 4.1133e+00,  2.2637e+00,  1.7227e+00,  ...,  1.0176e+00,
          1.1025e+00,  1.0527e+00],
        [ 2.2109e+00,  2.6196e-01,  8.9844e+00,  ..., -1.5557e+00,
         -8.1787e-03,  2.6250e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:28:14] "POST /inference/generate HTTP/1.1" 200 3106

init_state None

state-> tensor([[-3.0594e-02, -6.3599e-02,  4.0747e-01,  ..., -7.5073e-02,
         -2.0190e-01, -4.8187e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [ 2.2949e-02, -2.3109e+01, -3.4969e+01,  ...,  6.2988e-02,
         -2.1582e-01,  9.0479e-01],
        ...,
        [-1.3578e+01, -3.3965e+00, -2.6660e+00,  ...,  3.5410e+00,
          1.4551e+00, -6.2109e+00],
        [ 4.1133e+00,  2.2637e+00,  1.7227e+00,  ...,  1.0176e+00,
          1.1025e+00,  1.0527e+00],
        [ 2.2109e+00,  2.6196e-01,  8.9844e+00,  ..., -1.5557e+00,
         -8.1787e-03,  2.6250e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 00:28:27] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\nmaster', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None


165cm48kg



---after-> tensor([[-2.3544e-02, -6.0486e-02,  3.4863e-01,  ..., -8.7097e-02,
         -2.0312e-01, -4.2267e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.0303e+00, -4.7812e+01, -3.0391e+01,  ...,  1.1658e-01,
         -3.1348e-01,  1.4316e+00],
        ...,
        [-5.6244e-02, -7.8633e+00, -1.0867e+01,  ...,  1.2100e+00,
          4.0938e+00, -2.2266e+00],
        [ 1.4229e+00,  2.1875e+00,  5.9492e+00,  ...,  1.0137e+00,
          1.0039e+00,  1.0312e+00],
        [ 5.6758e+00,  7.9883e-01,  6.6172e+00,  ...,  3.0127e-01,
          3.5234e+00,  2.8789e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:28:45] "POST /inference/generate HTTP/1.1" 200 5345

init_state None

state-> tensor([[-2.3544e-02, -6.0486e-02,  3.4863e-01,  ..., -8.7097e-02,
         -2.0312e-01, -4.2267e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.0303e+00, -4.7812e+01, -3.0391e+01,  ...,  1.1658e-01,
         -3.1348e-01,  1.4316e+00],
        ...,
        [-5.6244e-02, -7.8633e+00, -1.0867e+01,  ...,  1.2100e+00,
          4.0938e+00, -2.2266e+00],
        [ 1.4229e+00,  2.1875e+00,  5.9492e+00,  ...,  1.0137e+00,
          1.0039e+00,  1.0312e+00],
        [ 5.6758e+00,  7.9883e-01,  6.6172e+00,  ...,  3.0127e-01,
          3.5234e+00,  2.8789e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 00:30:26] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\nmaster', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master: \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n\n165cm48kg\n\n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master: \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None


---after-> tensor([[-1.9760e-02, -7.3364e-02,  4.0991e-01,  ..., -8.8257e-02,
         -1.9080e-01, -4.5654e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-9.0869e-01, -2.9922e+01, -2.1953e+01,  ...,  6.2988e-02,
         -2.1582e-01,  9.0479e-01],
        ...,
        [-1.0148e+01, -2.3086e+00, -1.1611e+00,  ..., -8.7341e-02,
          1.4688e+00, -9.5156e+00],
        [ 4.0781e+00,  1.6055e+00,  3.7812e+00,  ...,  1.2090e+00,
          1.0303e+00,  1.1133e+00],
        [ 1.6367e+00,  2.7026e-01,  7.3008e+00,  ..., -2.5918e+00,
          7.6953e-01,  2.7222e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:30:46] "POST /inference/generate HTTP/1.1" 200 8649

---- {'role': 'analysis', 'text': 'systemmaster\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'mastermaster', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-1.9760e-02, -7.3364e-02,  4.0991e-01,  ..., -8.8257e-02,
         -1.9080e-01, -4.5654e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-9.0869e-01, -2.9922e+01, -2.1953e+01,  ...,  6.2988e-02,
         -2.1582e-01,  9.0479e-01],
        ...,
        [-1.0148e+01, -2.3086e+00, -1.1611e+00,  ..., -8.7341e-02,
          1.4688e+00, -9.5156e+00],
        [ 4.0781e+00,  1.6055e+00,  3.7812e+00,  ...,  1.2090e+00,
          1.0303e+00,  1.1133e+00],
        [ 1.6367e+00,  2.7026e-01,  7.3008e+00,  ..., -2.5918e+00,
          7.6953e-01,  2.7222e-01]], device='cuda:0', dtype=torch.float16)

165cm48kg







---after-> tensor([[-7.7515e-03, -6.3599e-02,  3.4790e-01,  ..., -7.1838e-02,
         -1.9446e-01, -2.4460e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-9.9756e-01, -5.7188e+01, -1.7812e+01,  ...,  1.4612e-01,
         -1.4038e-01,  1.0293e+00],
        ...,
        [-3.9160e-01, -6.7188e+00, -5.3711e+00,  ...,  2.1680e+00,
          3.6816e+00, -2.0605e+00],
        [ 4.4453e+00,  3.6875e+00,  3.6328e+00,  ...,  1.0352e+00,
          1.0996e+00,  1.1182e+00],
        [ 1.3311e+00, -1.1621e+00,  6.6914e+00,  ..., -9.7119e-01,
          2.1523e+00,  2.5000e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:32:10] "POST /inference/generate HTTP/1.1" 200 3953

init_state None

state-> tensor([[-7.7515e-03, -6.3599e-02,  3.4790e-01,  ..., -7.1838e-02,
         -1.9446e-01, -2.4460e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-9.9756e-01, -5.7188e+01, -1.7812e+01,  ...,  1.4612e-01,
         -1.4038e-01,  1.0293e+00],
        ...,
        [-3.9160e-01, -6.7188e+00, -5.3711e+00,  ...,  2.1680e+00,
          3.6816e+00, -2.0605e+00],
        [ 4.4453e+00,  3.6875e+00,  3.6328e+00,  ...,  1.0352e+00,
          1.0996e+00,  1.1182e+00],
        [ 1.3311e+00, -1.1621e+00,  6.6914e+00,  ..., -9.7119e-01,
          2.1523e+00,  2.5000e-01]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 00:32:34] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\nmaster', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master: \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n\n165cm48kg\n\n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master: \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None


---after-> tensor([[-1.9760e-02, -7.3364e-02,  4.0991e-01,  ..., -8.8257e-02,
         -1.9080e-01, -4.5654e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-9.0869e-01, -2.9922e+01, -2.1953e+01,  ...,  6.2988e-02,
         -2.1582e-01,  9.0479e-01],
        ...,
        [-1.0148e+01, -2.3086e+00, -1.1611e+00,  ..., -8.7341e-02,
          1.4688e+00, -9.5156e+00],
        [ 4.0781e+00,  1.6055e+00,  3.7812e+00,  ...,  1.2090e+00,
          1.0303e+00,  1.1133e+00],
        [ 1.6367e+00,  2.7026e-01,  7.3008e+00,  ..., -2.5918e+00,
          7.6953e-01,  2.7222e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:32:54] "POST /inference/generate HTTP/1.1" 200 8649

---- {'role': 'response', 'text': 'mastermaster', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-1.9760e-02, -7.3364e-02,  4.0991e-01,  ..., -8.8257e-02,
         -1.9080e-01, -4.5654e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-9.0869e-01, -2.9922e+01, -2.1953e+01,  ...,  6.2988e-02,
         -2.1582e-01,  9.0479e-01],
        ...,
        [-1.0148e+01, -2.3086e+00, -1.1611e+00,  ..., -8.7341e-02,
          1.4688e+00, -9.5156e+00],
        [ 4.0781e+00,  1.6055e+00,  3.7812e+00,  ...,  1.2090e+00,
          1.0303e+00,  1.1133e+00],
        [ 1.6367e+00,  2.7026e-01,  7.3008e+00,  ..., -2.5918e+00,
          7.6953e-01,  2.7222e-01]], device='cuda:0', dtype=torch.float16)



 cleaner
master


---after-> tensor([[-9.8572e-03, -4.5135e-02,  3.7891e-01,  ..., -9.3506e-02,
         -2.0496e-01, -3.8361e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.3525e+00, -8.5625e+01, -3.1703e+01,  ...,  1.7517e-01,
         -2.6758e-01,  1.0342e+00],
        ...,
        [-1.9590e+00, -5.5938e+00,  2.1460e-01,  ..., -2.5391e-01,
          3.6934e+00, -1.4482e+00],
        [ 2.9766e+00,  1.5986e+00,  9.0156e+00,  ...,  1.0898e+00,
          1.0225e+00,  1.0723e+00],
        [ 4.2383e+00,  1.5830e+00,  6.0977e+00,  ..., -6.5332e-01,
          3.4629e+00,  2.2070e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:34:54] "POST /inference/generate HTTP/1.1" 200 2155

---- {'role': 'request', 'text': 'master: \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-9.8572e-03, -4.5135e-02,  3.7891e-01,  ..., -9.3506e-02,
         -2.0496e-01, -3.8361e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.3525e+00, -8.5625e+01, -3.1703e+01,  ...,  1.7517e-01,
         -2.6758e-01,  1.0342e+00],
        ...,
        [-1.9590e+00, -5.5938e+00,  2.1460e-01,  ..., -2.5391e-01,
          3.6934e+00, -1.4482e+00],
        [ 2.9766e+00,  1.5986e+00,  9.0156e+00,  ...,  1.0898e+00,
          1.0225e+00,  1.0723e+00],
        [ 4.2383e+00,  1.5830e+00,  6.0977e+00,  ..., -6.5332e-01,
          3.4629e+00,  2.2070e+00]], device='cuda:0', dtype=torch.float16)


---after-> tensor([[-7.1960e-02,  5.2917e-02,  2.6489e-01,  ..., -8.4686e-03,
          6.6162e-02,  6.5247e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-1.9385e+00, -2.6938e+01, -3.3062e+01,  ..., -4.7046e-01,
         -2.2217e-01,  1.6445e+00],
        ...,
        [-1.4539e+01, -1.8701e+00,  1.7200e-01,  ...,  1.1353e-01,
          3.2422e+00, -2.2539e+00],
        [ 4.8086e+00,  1.9863e+00,  2.3848e+00,  ...,  1.0068e+00,
          1.0068e+00,  1.0449e+00],
        [ 1.0381e+00,  1.3564e+00,  8.1953e+00,  ..., -1.6533e+00,
          2.4121e+00,  3.4668e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:36:12] "POST /inference/generate HTTP/1.1" 200 1439

---- {'role': 'response', 'text': 'mastermaster', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-7.1960e-02,  5.2917e-02,  2.6489e-01,  ..., -8.4686e-03,
          6.6162e-02,  6.5247e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-1.9385e+00, -2.6938e+01, -3.3062e+01,  ..., -4.7046e-01,
         -2.2217e-01,  1.6445e+00],
        ...,
        [-1.4539e+01, -1.8701e+00,  1.7200e-01,  ...,  1.1353e-01,
          3.2422e+00, -2.2539e+00],
        [ 4.8086e+00,  1.9863e+00,  2.3848e+00,  ...,  1.0068e+00,
          1.0068e+00,  1.0449e+00],
        [ 1.0381e+00,  1.3564e+00,  8.1953e+00,  ..., -1.6533e+00,
          2.4121e+00,  3.4668e+00]], device='cuda:0', dtype=torch.float16)



1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.SM
12.
13.
14.
---after-> tensor([[-8.0261e-02, -6.1913e-03,  1.9739e-01,  ..., -1.0651e-02,
         -5.9418e-02, -9.6130e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [ 2.1504e+00, -8.5438e+01, -2.4406e+01,  ...,  6.7627e-02,
         -2.0947e-01,  2.2888e-02],
        ...,
        [-5.1289e+00, -5.4336e+00, -3.3184e+00,  ...,  7.8308e-02,
          2.9277e+00, -9.7412e-01],
        [ 2.9375e+00,  3.7070e+00,  3.5371e+00,  ...,  1.0088e+00,
          1.0029e+00,  1.0410e+00],
        [ 2.6426e+00, -7.5830e-01,  6.2891e+00,  ..., -8.4863e-01,
          2.2070e+00,  9.1357e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:36:46] "POST /inference/generate HTTP/1.1" 200 3192

init_state None

state-> tensor([[-8.0261e-02, -6.1913e-03,  1.9739e-01,  ..., -1.0651e-02,
         -5.9418e-02, -9.6130e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [ 2.1504e+00, -8.5438e+01, -2.4406e+01,  ...,  6.7627e-02,
         -2.0947e-01,  2.2888e-02],
        ...,
        [-5.1289e+00, -5.4336e+00, -3.3184e+00,  ...,  7.8308e-02,
          2.9277e+00, -9.7412e-01],
        [ 2.9375e+00,  3.7070e+00,  3.5371e+00,  ...,  1.0088e+00,
          1.0029e+00,  1.0410e+00],
        [ 2.6426e+00, -7.5830e-01,  6.2891e+00,  ..., -8.4863e-01,
          2.2070e+00,  9.1357e-01]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 00:39:53] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\nmaster', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None


---after-> tensor([[-4.3823e-02,  3.0533e-02,  2.2534e-01,  ..., -2.7664e-02,
          9.0942e-02,  8.3679e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-7.8564e-01, -2.4734e+01, -2.7234e+01,  ..., -4.8511e-01,
         -1.7776e-02,  1.9004e+00],
        ...,
        [-8.9766e+00,  1.2100e+00, -2.8398e+00,  ...,  2.3848e+00,
          5.8906e+00, -9.2090e-01],
        [ 3.9297e+00,  1.8037e+00,  2.6973e+00,  ...,  1.0078e+00,
          1.0029e+00,  1.2969e+00],
        [ 2.8340e+00,  2.2051e+00,  7.8320e+00,  ...,  1.5518e+00,
          4.5547e+00,  3.9941e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:40:00] "POST /inference/generate HTTP/1.1" 200 2521

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-4.3823e-02,  3.0533e-02,  2.2534e-01,  ..., -2.7664e-02,
          9.0942e-02,  8.3679e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-7.8564e-01, -2.4734e+01, -2.7234e+01,  ..., -4.8511e-01,
         -1.7776e-02,  1.9004e+00],
        ...,
        [-8.9766e+00,  1.2100e+00, -2.8398e+00,  ...,  2.3848e+00,
          5.8906e+00, -9.2090e-01],
        [ 3.9297e+00,  1.8037e+00,  2.6973e+00,  ...,  1.0078e+00,
          1.0029e+00,  1.2969e+00],
        [ 2.8340e+00,  2.2051e+00,  7.8320e+00,  ...,  1.5518e+00,
          4.5547e+00,  3.9941e+00]], device='cuda:0', dtype=torch.float16)







---after-> tensor([[-6.0059e-02, -3.4027e-02,  2.7368e-01,  ...,  1.7872e-03,
         -1.7517e-02,  9.4238e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-1.6816e+00, -3.4469e+01, -1.1695e+01,  ..., -2.2473e-01,
         -2.2400e-01,  1.5088e+00],
        ...,
        [-2.5645e+00, -4.2305e+00, -1.3758e+01,  ...,  1.1816e+00,
          3.3770e+00, -1.5986e+00],
        [ 4.3125e+00,  2.0293e+00,  7.9219e+00,  ...,  1.0840e+00,
          1.1152e+00,  1.1641e+00],
        [ 2.6836e+00,  1.4453e+00,  6.0859e+00,  ..., -8.1055e-01,
          3.8203e+00,  1.0801e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:40:50] "POST /inference/generate HTTP/1.1" 200 1944

init_state None

state-> tensor([[-6.0059e-02, -3.4027e-02,  2.7368e-01,  ...,  1.7872e-03,
         -1.7517e-02,  9.4238e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-1.6816e+00, -3.4469e+01, -1.1695e+01,  ..., -2.2473e-01,
         -2.2400e-01,  1.5088e+00],
        ...,
        [-2.5645e+00, -4.2305e+00, -1.3758e+01,  ...,  1.1816e+00,
          3.3770e+00, -1.5986e+00],
        [ 4.3125e+00,  2.0293e+00,  7.9219e+00,  ...,  1.0840e+00,
          1.1152e+00,  1.1641e+00],
        [ 2.6836e+00,  1.4453e+00,  6.0859e+00,  ..., -8.1055e-01,
          3.8203e+00,  1.0801e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 00:41:45] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None

master

---after-> tensor([[-3.2104e-02,  1.7014e-02,  1.4368e-01,  ..., -1.3708e-01,
         -1.4412e-02,  2.0056e-01],
        [-3.5248e-02, -1.1224e-01, -5.6305e-02,  ..., -1.3757e-01,
         -6.8726e-02,  2.0337e-01],
        [-1.0586e+00, -2.9656e+01, -3.4250e+01,  ...,  5.6305e-02,
          2.1887e-01,  4.0747e-01],
        ...,
        [-2.6978e-01, -4.0352e+00, -3.2363e+00,  ...,  7.2188e+00,
          4.4531e+00,  2.0137e+00],
        [ 1.1230e+00,  2.4922e+00,  2.6191e+00,  ...,  1.0371e+00,
          1.0244e+00,  1.0146e+00],
        [ 4.8281e+00,  4.7383e+00,  8.2188e+00,  ..., -1.3105e+00,
          1.4893e+00,  4.4258e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:41:55] "POST /inference/generate HTTP/1.1" 200 3437

init_state None

state-> tensor([[-3.2104e-02,  1.7014e-02,  1.4368e-01,  ..., -1.3708e-01,
         -1.4412e-02,  2.0056e-01],
        [-3.5248e-02, -1.1224e-01, -5.6305e-02,  ..., -1.3757e-01,
         -6.8726e-02,  2.0337e-01],
        [-1.0586e+00, -2.9656e+01, -3.4250e+01,  ...,  5.6305e-02,
          2.1887e-01,  4.0747e-01],
        ...,
        [-2.6978e-01, -4.0352e+00, -3.2363e+00,  ...,  7.2188e+00,
          4.4531e+00,  2.0137e+00],
        [ 1.1230e+00,  2.4922e+00,  2.6191e+00,  ...,  1.0371e+00,
          1.0244e+00,  1.0146e+00],
        [ 4.8281e+00,  4.7383e+00,  8.2188e+00,  ..., -1.3105e+00,
          1.4893e+00,  4.4258e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 00:42:21] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None
master   json master

---after-> tensor([[ 1.5175e-02, -8.9478e-02,  4.1602e-01,  ..., -1.3831e-01,
         -1.8530e-01, -6.2744e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-3.4199e+00, -5.4062e+01, -3.3531e+01,  ...,  6.1621e-01,
         -2.0294e-02,  1.4229e+00],
        ...,
        [-2.9531e+00, -4.5078e+00, -8.6094e+00,  ...,  2.3047e+00,
          2.9980e+00, -4.5156e+00],
        [ 4.0859e+00,  1.7080e+00,  6.5781e+00,  ...,  1.0234e+00,
          1.0186e+00,  1.0381e+00],
        [ 1.3662e+00,  2.0195e+00,  6.6484e+00,  ...,  1.4746e-01,
          2.8242e+00,  3.0312e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:42:33] "POST /inference/generate HTTP/1.1" 200 4049

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[ 1.5175e-02, -8.9478e-02,  4.1602e-01,  ..., -1.3831e-01,
         -1.8530e-01, -6.2744e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-3.4199e+00, -5.4062e+01, -3.3531e+01,  ...,  6.1621e-01,
         -2.0294e-02,  1.4229e+00],
        ...,
        [-2.9531e+00, -4.5078e+00, -8.6094e+00,  ...,  2.3047e+00,
          2.9980e+00, -4.5156e+00],
        [ 4.0859e+00,  1.7080e+00,  6.5781e+00,  ...,  1.0234e+00,
          1.0186e+00,  1.0381e+00],
        [ 1.3662e+00,  2.0195e+00,  6.6484e+00,  ...,  1.4746e-01,
          2.8242e+00,  3.0312e+00]], device='cuda:0', dtype=torch.float16)

```json
{
  "answer": {
    "start_time": "00:00",
    "end_time": "12:00",
    "course_name": "Introduction to Programming",
    "teacher_name": "Mr. Lee",
    "duration": "1 hour/day",
    "teacher_introduced": true,
    "course_description": "Learn how to code in Python.",
    "students": [
      {
        "name":"",
        "age":"18",
        "gender":"",
        "description":"    "
      }
    ]
  }
}
```
---after-> tensor([[ 1.0541e-01,  1.0248e-01,  3.1250e-01,  ..., -1.0706e-01,
         -4.7546e-02,  6.3232e-02],
        [ 1.3586e-01,  1.5698e-01,  1.2585e-01,  ..., -1.2573e-01,
         -7.8308e-02,  6.1249e-02],
        [-3.4512e+00, -4.3312e+01, -6.5562e+01,  ...,  8.3398e-01,
          8.7341e-02,  4.6729e-01],
        ...,
        [ 5.7812e-01, -5.8555e+00, -9.7109e+00,  ...,  2.5430e+00,
          8.4570e-01, -3.8086e+00],
        [ 2.8223e+00,  3.0605e+00,  7.7422e+00,  ...,  1.0146e+00,
          1.0938e+00,  1.5049e+00],
        [ 2.4902e+00, -7.8027e-01,  5.9453e+00,  ...,  9.5850e-01,
          2.9980e+00,  2.4590e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:42:55] "POST /inference/generate HTTP/1.1" 200 1455

init_state None

state-> tensor([[ 1.0541e-01,  1.0248e-01,  3.1250e-01,  ..., -1.0706e-01,
         -4.7546e-02,  6.3232e-02],
        [ 1.3586e-01,  1.5698e-01,  1.2585e-01,  ..., -1.2573e-01,
         -7.8308e-02,  6.1249e-02],
        [-3.4512e+00, -4.3312e+01, -6.5562e+01,  ...,  8.3398e-01,
          8.7341e-02,  4.6729e-01],
        ...,
        [ 5.7812e-01, -5.8555e+00, -9.7109e+00,  ...,  2.5430e+00,
          8.4570e-01, -3.8086e+00],
        [ 2.8223e+00,  3.0605e+00,  7.7422e+00,  ...,  1.0146e+00,
          1.0938e+00,  1.5049e+00],
        [ 2.4902e+00, -7.8027e-01,  5.9453e+00,  ...,  9.5850e-01,
          2.9980e+00,  2.4590e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 00:51:02] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None
master

---after-> tensor([[-1.0414e-02, -8.1116e-02,  4.0161e-01,  ..., -1.0779e-01,
         -1.8225e-01, -4.6936e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.1182e+00, -3.9688e+01, -2.1281e+01,  ...,  6.3538e-02,
         -3.2056e-01,  1.4316e+00],
        ...,
        [-3.9785e+00, -4.6641e+00, -3.2773e+00,  ...,  2.4688e+00,
          3.9160e+00, -5.6367e+00],
        [ 1.8008e+00,  2.4531e+00,  2.7637e+00,  ...,  1.0176e+00,
          1.0059e+00,  1.0156e+00],
        [ 2.1191e+00,  8.8281e-01,  8.0703e+00,  ...,  8.2178e-01,
          3.1309e+00,  2.5879e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:51:12] "POST /inference/generate HTTP/1.1" 200 3641

init_state None

state-> tensor([[-1.0414e-02, -8.1116e-02,  4.0161e-01,  ..., -1.0779e-01,
         -1.8225e-01, -4.6936e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.1182e+00, -3.9688e+01, -2.1281e+01,  ...,  6.3538e-02,
         -3.2056e-01,  1.4316e+00],
        ...,
        [-3.9785e+00, -4.6641e+00, -3.2773e+00,  ...,  2.4688e+00,
          3.9160e+00, -5.6367e+00],
        [ 1.8008e+00,  2.4531e+00,  2.7637e+00,  ...,  1.0176e+00,
          1.0059e+00,  1.0156e+00],
        [ 2.1191e+00,  8.8281e-01,  8.0703e+00,  ...,  8.2178e-01,
          3.1309e+00,  2.5879e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 00:52:03] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmastermaster\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'mastermaster', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None


 


master ,
---after-> tensor([[-9.4971e-02, -2.5467e-02,  2.3279e-01,  ..., -1.6785e-03,
         -6.5063e-02, -1.1157e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-7.1777e-01, -5.1188e+01, -5.2969e+01,  ...,  9.4788e-02,
         -4.2578e-01,  3.8599e-01],
        ...,
        [-6.3320e+00, -4.7773e+00, -1.0008e+01,  ...,  4.1445e+00,
          2.9297e+00, -2.8301e+00],
        [ 3.0859e+00,  1.6387e+00,  7.7500e+00,  ...,  1.0088e+00,
          1.0244e+00,  1.9189e+00],
        [ 1.8877e+00, -6.3721e-01,  5.9961e+00,  ..., -1.0508e+00,
          2.7637e+00, -3.7085e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:52:20] "POST /inference/generate HTTP/1.1" 200 5784

---- {'role': 'request', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-9.4971e-02, -2.5467e-02,  2.3279e-01,  ..., -1.6785e-03,
         -6.5063e-02, -1.1157e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-7.1777e-01, -5.1188e+01, -5.2969e+01,  ...,  9.4788e-02,
         -4.2578e-01,  3.8599e-01],
        ...,
        [-6.3320e+00, -4.7773e+00, -1.0008e+01,  ...,  4.1445e+00,
          2.9297e+00, -2.8301e+00],
        [ 3.0859e+00,  1.6387e+00,  7.7500e+00,  ...,  1.0088e+00,
          1.0244e+00,  1.9189e+00],
        [ 1.8877e+00, -6.3721e-01,  5.9961e+00,  ..., -1.0508e+00,
          2.7637e+00, -3.7085e-01]], device='cuda:0', dtype=torch.float16)




 
 
 

---after-> tensor([[-4.6112e-02,  3.2745e-02,  2.6636e-01,  ..., -2.4384e-02,
          9.5459e-02,  9.1980e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-2.7070e+00, -8.1938e+01, -3.6156e+01,  ..., -4.7412e-01,
          9.6863e-02,  2.2168e+00],
        ...,
        [-1.8965e+00, -5.1797e+00, -1.1414e+01,  ...,  4.7227e+00,
          1.8203e+00,  1.6094e+00],
        [ 1.7080e+00,  5.5078e+00,  8.1250e+00,  ...,  1.1836e+00,
          1.0459e+00,  2.0410e+00],
        [ 3.2598e+00, -1.3945e+00,  5.8281e+00,  ..., -2.0938e+00,
          4.9062e+00,  2.0483e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:53:15] "POST /inference/generate HTTP/1.1" 200 2591

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-4.6112e-02,  3.2745e-02,  2.6636e-01,  ..., -2.4384e-02,
          9.5459e-02,  9.1980e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-2.7070e+00, -8.1938e+01, -3.6156e+01,  ..., -4.7412e-01,
          9.6863e-02,  2.2168e+00],
        ...,
        [-1.8965e+00, -5.1797e+00, -1.1414e+01,  ...,  4.7227e+00,
          1.8203e+00,  1.6094e+00],
        [ 1.7080e+00,  5.5078e+00,  8.1250e+00,  ...,  1.1836e+00,
          1.0459e+00,  2.0410e+00],
        [ 3.2598e+00, -1.3945e+00,  5.8281e+00,  ..., -2.0938e+00,
          4.9062e+00,  2.0483e-01]], device='cuda:0', dtype=torch.float16)

 



 


---after-> tensor([[-3.2806e-02, -5.1636e-02,  3.5352e-01,  ..., -9.0881e-02,
         -1.9580e-01, -5.2765e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.9092e+00, -6.5188e+01, -3.4312e+01,  ...,  5.5847e-02,
         -2.3267e-01,  1.3516e+00],
        ...,
        [-2.8652e+00, -8.3359e+00, -1.4365e+00,  ...,  1.6230e+00,
          2.7012e+00, -3.4492e+00],
        [ 2.4785e+00,  1.9180e+00,  2.9277e+00,  ...,  1.0127e+00,
          1.0020e+00,  1.0479e+00],
        [ 3.6719e+00,  9.8145e-02,  6.7500e+00,  ..., -8.5205e-01,
          4.0938e+00,  1.0674e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:54:59] "POST /inference/generate HTTP/1.1" 200 1666

---- {'role': 'request', 'text': 'master: \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-3.2806e-02, -5.1636e-02,  3.5352e-01,  ..., -9.0881e-02,
         -1.9580e-01, -5.2765e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.9092e+00, -6.5188e+01, -3.4312e+01,  ...,  5.5847e-02,
         -2.3267e-01,  1.3516e+00],
        ...,
        [-2.8652e+00, -8.3359e+00, -1.4365e+00,  ...,  1.6230e+00,
          2.7012e+00, -3.4492e+00],
        [ 2.4785e+00,  1.9180e+00,  2.9277e+00,  ...,  1.0127e+00,
          1.0020e+00,  1.0479e+00],
        [ 3.6719e+00,  9.8145e-02,  6.7500e+00,  ..., -8.5205e-01,
          4.0938e+00,  1.0674e+00]], device='cuda:0', dtype=torch.float16)

 

 

 

  
  
 


 

---after-> tensor([[-5.6091e-02,  4.5013e-02,  2.4683e-01,  ...,  1.6678e-02,
          7.1594e-02,  9.8694e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-1.6807e+00, -4.6125e+01, -4.8188e+01,  ..., -4.7412e-01,
          9.6863e-02,  2.2168e+00],
        ...,
        [-2.5703e+00, -2.1836e+00,  1.1172e+00,  ...,  4.6445e+00,
          3.1133e+00,  1.6680e+00],
        [ 2.3223e+00,  2.6445e+00,  4.8086e+00,  ...,  1.0527e+00,
          1.0322e+00,  1.6543e+00],
        [ 3.7617e+00, -9.3262e-01,  6.7422e+00,  ..., -1.3652e+00,
          4.6289e+00,  2.4475e-02]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:55:50] "POST /inference/generate HTTP/1.1" 200 3237

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-5.6091e-02,  4.5013e-02,  2.4683e-01,  ...,  1.6678e-02,
          7.1594e-02,  9.8694e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-1.6807e+00, -4.6125e+01, -4.8188e+01,  ..., -4.7412e-01,
          9.6863e-02,  2.2168e+00],
        ...,
        [-2.5703e+00, -2.1836e+00,  1.1172e+00,  ...,  4.6445e+00,
          3.1133e+00,  1.6680e+00],
        [ 2.3223e+00,  2.6445e+00,  4.8086e+00,  ...,  1.0527e+00,
          1.0322e+00,  1.6543e+00],
        [ 3.7617e+00, -9.3262e-01,  6.7422e+00,  ..., -1.3652e+00,
          4.6289e+00,  2.4475e-02]], device='cuda:0', dtype=torch.float16)







 
    






---after-> tensor([[-1.4551e-01, -4.8004e-02,  2.3108e-01,  ...,  1.2489e-02,
         -6.7688e-02, -1.3293e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-3.7656e+00, -5.0844e+01, -3.1469e+01,  ...,  8.8745e-02,
         -2.1021e-01,  4.4897e-01],
        ...,
        [-2.8223e+00, -9.4531e+00, -1.3836e+01,  ...,  1.5588e-01,
          4.6914e+00,  1.8398e+00],
        [ 1.8105e+00,  3.5215e+00,  1.4906e+01,  ...,  1.0254e+00,
          1.1172e+00,  1.0664e+00],
        [ 1.9697e+00, -1.3389e+00,  4.8789e+00,  ..., -1.5186e+00,
          2.6191e+00,  1.0020e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 00:56:59] "POST /inference/generate HTTP/1.1" 200 2925

init_state None

state-> tensor([[-1.4551e-01, -4.8004e-02,  2.3108e-01,  ...,  1.2489e-02,
         -6.7688e-02, -1.3293e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-3.7656e+00, -5.0844e+01, -3.1469e+01,  ...,  8.8745e-02,
         -2.1021e-01,  4.4897e-01],
        ...,
        [-2.8223e+00, -9.4531e+00, -1.3836e+01,  ...,  1.5588e-01,
          4.6914e+00,  1.8398e+00],
        [ 1.8105e+00,  3.5215e+00,  1.4906e+01,  ...,  1.0254e+00,
          1.1172e+00,  1.0664e+00],
        [ 1.9697e+00, -1.3389e+00,  4.8789e+00,  ..., -1.5186e+00,
          2.6191e+00,  1.0020e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 01:00:05] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmastermaster\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'mastermaster\n\n \n\n\nmaster ,', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '\n\n\n\n \n \n ', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n \n\n\n\n \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None
:
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
---after-> tensor([[-9.6680e-02, -2.7252e-02,  2.0874e-01,  ..., -9.4938e-04,
         -5.6335e-02, -1.1469e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-4.2432e-01, -7.6875e+01, -3.1438e+01,  ...,  3.3032e-01,
         -5.4993e-02,  1.8384e-01],
        ...,
        [-2.9941e+00, -4.4492e+00, -6.4219e+00,  ...,  3.4531e+00,
          1.0797e-01, -8.3887e-01],
        [ 2.1895e+00,  2.0234e+00,  8.8359e+00,  ...,  1.0322e+00,
          1.0010e+00,  1.0635e+00],
        [ 3.5898e+00,  4.5728e-01,  5.5039e+00,  ..., -1.6003e-01,
          3.4629e+00,  5.7227e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:00:51] "POST /inference/generate HTTP/1.1" 200 16558

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-9.6680e-02, -2.7252e-02,  2.0874e-01,  ..., -9.4938e-04,
         -5.6335e-02, -1.1469e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-4.2432e-01, -7.6875e+01, -3.1438e+01,  ...,  3.3032e-01,
         -5.4993e-02,  1.8384e-01],
        ...,
        [-2.9941e+00, -4.4492e+00, -6.4219e+00,  ...,  3.4531e+00,
          1.0797e-01, -8.3887e-01],
        [ 2.1895e+00,  2.0234e+00,  8.8359e+00,  ...,  1.0322e+00,
          1.0010e+00,  1.0635e+00],
        [ 3.5898e+00,  4.5728e-01,  5.5039e+00,  ..., -1.6003e-01,
          3.4629e+00,  5.7227e-01]], device='cuda:0', dtype=torch.float16)

       
 my neck,my face,my tits. One of them even pulled out his dick and pissed on me.









---after-> tensor([[-5.3070e-02, -5.2917e-02,  3.3032e-01,  ..., -8.1787e-02,
         -2.0300e-01, -2.9816e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [ 2.0618e-01, -4.7844e+01, -2.4219e+01,  ...,  2.2961e-01,
         -2.3804e-01,  1.2500e+00],
        ...,
        [-3.1113e+00, -3.4434e+00, -2.2227e+00,  ...,  2.1016e+00,
          5.7969e+00, -2.7422e+00],
        [ 3.0234e+00,  2.4180e+00,  8.6016e+00,  ...,  1.0508e+00,
          1.0166e+00,  1.0127e+00],
        [ 4.7539e+00, -1.8223e+00,  5.4922e+00,  ..., -2.0039e+00,
          4.6562e+00,  2.1055e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:01:21] "POST /inference/generate HTTP/1.1" 200 2773

init_state None

state-> tensor([[-5.3070e-02, -5.2917e-02,  3.3032e-01,  ..., -8.1787e-02,
         -2.0300e-01, -2.9816e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [ 2.0618e-01, -4.7844e+01, -2.4219e+01,  ...,  2.2961e-01,
         -2.3804e-01,  1.2500e+00],
        ...,
        [-3.1113e+00, -3.4434e+00, -2.2227e+00,  ...,  2.1016e+00,
          5.7969e+00, -2.7422e+00],
        [ 3.0234e+00,  2.4180e+00,  8.6016e+00,  ...,  1.0508e+00,
          1.0166e+00,  1.0127e+00],
        [ 4.7539e+00, -1.8223e+00,  5.4922e+00,  ..., -2.0039e+00,
          4.6562e+00,  2.1055e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 01:02:50] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmastermaster\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'mastermaster\n\n \n\n\nmaster ,', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '\n\n\n\n \n \n ', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n \n\n\n\n \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None
:
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
---after-> tensor([[-9.6680e-02, -2.7252e-02,  2.0874e-01,  ..., -9.4938e-04,
         -5.6335e-02, -1.1469e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-4.2432e-01, -7.6875e+01, -3.1438e+01,  ...,  3.3032e-01,
         -5.4993e-02,  1.8384e-01],
        ...,
        [-2.9941e+00, -4.4492e+00, -6.4219e+00,  ...,  3.4531e+00,
          1.0797e-01, -8.3887e-01],
        [ 2.1895e+00,  2.0234e+00,  8.8359e+00,  ...,  1.0322e+00,
          1.0010e+00,  1.0635e+00],
        [ 3.5898e+00,  4.5728e-01,  5.5039e+00,  ..., -1.6003e-01,
          3.4629e+00,  5.7227e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:03:37] "POST /inference/generate HTTP/1.1" 200 16558

---- {'role': 'system', 'text': '\nmaster', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-9.6680e-02, -2.7252e-02,  2.0874e-01,  ..., -9.4938e-04,
         -5.6335e-02, -1.1469e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-4.2432e-01, -7.6875e+01, -3.1438e+01,  ...,  3.3032e-01,
         -5.4993e-02,  1.8384e-01],
        ...,
        [-2.9941e+00, -4.4492e+00, -6.4219e+00,  ...,  3.4531e+00,
          1.0797e-01, -8.3887e-01],
        [ 2.1895e+00,  2.0234e+00,  8.8359e+00,  ...,  1.0322e+00,
          1.0010e+00,  1.0635e+00],
        [ 3.5898e+00,  4.5728e-01,  5.5039e+00,  ..., -1.6003e-01,
          3.4629e+00,  5.7227e-01]], device='cuda:0', dtype=torch.float16)

1. 
2. 
3. 
4. 
5. 
6. 
7. 
---after-> tensor([[-9.6436e-02, -3.0869e-02,  2.1973e-01,  ..., -5.0659e-03,
         -5.4871e-02, -1.1249e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.5215e+00, -4.1500e+01, -2.1844e+01,  ...,  1.3220e-01,
         -9.6436e-02,  4.1895e-01],
        ...,
        [-1.6787e+00, -5.4570e+00, -1.9453e+01,  ...,  3.4453e+00,
          1.3867e+00, -1.6973e+00],
        [ 1.5020e+00,  1.3799e+00,  2.1547e+01,  ...,  1.0225e+00,
          1.0498e+00,  1.7207e+00],
        [ 3.8535e+00,  1.1797e+00,  4.9648e+00,  ...,  5.5762e-01,
          3.6367e+00,  6.0547e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:04:33] "POST /inference/generate HTTP/1.1" 200 2635

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-9.6436e-02, -3.0869e-02,  2.1973e-01,  ..., -5.0659e-03,
         -5.4871e-02, -1.1249e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.5215e+00, -4.1500e+01, -2.1844e+01,  ...,  1.3220e-01,
         -9.6436e-02,  4.1895e-01],
        ...,
        [-1.6787e+00, -5.4570e+00, -1.9453e+01,  ...,  3.4453e+00,
          1.3867e+00, -1.6973e+00],
        [ 1.5020e+00,  1.3799e+00,  2.1547e+01,  ...,  1.0225e+00,
          1.0498e+00,  1.7207e+00],
        [ 3.8535e+00,  1.1797e+00,  4.9648e+00,  ...,  5.5762e-01,
          3.6367e+00,  6.0547e-01]], device='cuda:0', dtype=torch.float16)

 





---after-> tensor([[-3.9307e-02, -5.3406e-02,  3.5986e-01,  ..., -8.4839e-02,
         -2.0178e-01, -3.7048e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.7900e+00, -5.0562e+01, -7.1312e+01,  ...,  1.5796e-01,
         -2.0593e-01,  8.9111e-01],
        ...,
        [-1.0414e+01, -3.2500e+00, -2.0762e+00,  ...,  2.8867e+00,
          3.2422e+00, -3.3125e+00],
        [ 2.5215e+00,  2.1016e+00,  3.7422e+00,  ...,  1.0605e+00,
          1.0176e+00,  1.2197e+00],
        [ 3.6758e+00, -6.4795e-01,  7.0234e+00,  ..., -9.1357e-01,
          2.6133e+00,  9.9414e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:06:02] "POST /inference/generate HTTP/1.1" 200 2049

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-3.9307e-02, -5.3406e-02,  3.5986e-01,  ..., -8.4839e-02,
         -2.0178e-01, -3.7048e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.7900e+00, -5.0562e+01, -7.1312e+01,  ...,  1.5796e-01,
         -2.0593e-01,  8.9111e-01],
        ...,
        [-1.0414e+01, -3.2500e+00, -2.0762e+00,  ...,  2.8867e+00,
          3.2422e+00, -3.3125e+00],
        [ 2.5215e+00,  2.1016e+00,  3.7422e+00,  ...,  1.0605e+00,
          1.0176e+00,  1.2197e+00],
        [ 3.6758e+00, -6.4795e-01,  7.0234e+00,  ..., -9.1357e-01,
          2.6133e+00,  9.9414e-01]], device='cuda:0', dtype=torch.float16)

  
 
 

---after-> tensor([[-4.9103e-02, -5.1392e-02,  3.5376e-01,  ..., -7.2693e-02,
         -1.9141e-01, -5.1697e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-2.4688e+00, -1.2500e+02, -4.7031e+01,  ...,  3.5065e-02,
         -7.5806e-02,  1.1113e+00],
        ...,
        [-3.8770e+00, -7.2734e+00, -6.2031e+00,  ...,  2.1387e+00,
          3.9863e+00, -6.8828e+00],
        [ 3.2051e+00,  2.0625e+00,  8.3125e+00,  ...,  1.0029e+00,
          1.0068e+00,  1.6172e+00],
        [ 1.6738e+00, -3.1763e-01,  6.2148e+00,  ..., -1.0654e+00,
          3.3613e+00,  2.0293e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:06:13] "POST /inference/generate HTTP/1.1" 200 1152

---- {'role': 'request', 'text': 'master \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-4.9103e-02, -5.1392e-02,  3.5376e-01,  ..., -7.2693e-02,
         -1.9141e-01, -5.1697e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-2.4688e+00, -1.2500e+02, -4.7031e+01,  ...,  3.5065e-02,
         -7.5806e-02,  1.1113e+00],
        ...,
        [-3.8770e+00, -7.2734e+00, -6.2031e+00,  ...,  2.1387e+00,
          3.9863e+00, -6.8828e+00],
        [ 3.2051e+00,  2.0625e+00,  8.3125e+00,  ...,  1.0029e+00,
          1.0068e+00,  1.6172e+00],
        [ 1.6738e+00, -3.1763e-01,  6.2148e+00,  ..., -1.0654e+00,
          3.3613e+00,  2.0293e+00]], device='cuda:0', dtype=torch.float16)

1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 

---after-> tensor([[-2.3361e-02, -4.8981e-02,  3.5303e-01,  ..., -7.9895e-02,
         -1.9861e-01, -7.0923e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-3.2129e-01, -6.3094e+01, -3.2281e+01,  ...,  1.6541e-01,
         -4.6338e-01,  1.0342e+00],
        ...,
        [-1.2393e+00, -2.5332e+00, -8.1250e+00,  ...,  2.0977e+00,
          1.6318e+00, -3.9551e+00],
        [ 2.6055e+00,  1.7373e+00,  8.1562e+00,  ...,  1.0039e+00,
          1.0186e+00,  1.7324e+00],
        [ 2.2422e+00, -3.6279e-01,  5.7383e+00,  ..., -8.2666e-01,
          2.3145e+00,  9.7949e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:08:11] "POST /inference/generate HTTP/1.1" 200 2908

---- {'role': 'request', 'text': 'master:  (master)\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-2.3361e-02, -4.8981e-02,  3.5303e-01,  ..., -7.9895e-02,
         -1.9861e-01, -7.0923e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-3.2129e-01, -6.3094e+01, -3.2281e+01,  ...,  1.6541e-01,
         -4.6338e-01,  1.0342e+00],
        ...,
        [-1.2393e+00, -2.5332e+00, -8.1250e+00,  ...,  2.0977e+00,
          1.6318e+00, -3.9551e+00],
        [ 2.6055e+00,  1.7373e+00,  8.1562e+00,  ...,  1.0039e+00,
          1.0186e+00,  1.7324e+00],
        [ 2.2422e+00, -3.6279e-01,  5.7383e+00,  ..., -8.2666e-01,
          2.3145e+00,  9.7949e-01]], device='cuda:0', dtype=torch.float16)






 too much 
---after-> tensor([[-9.0027e-02, -1.2314e-02,  2.5000e-01,  ...,  2.6199e-02,
          4.4708e-02,  1.0736e-01],
        [-2.6398e-02, -1.1835e-01,  6.3721e-02,  ...,  1.6406e-01,
         -1.7365e-02,  1.7786e-01],
        [-1.3096e+00, -7.8500e+01, -4.3938e+01,  ..., -2.6611e-01,
         -3.3105e-01, -5.7373e-01],
        ...,
        [ 3.2676e+00, -2.3984e+00, -2.9316e+00,  ...,  3.1172e+00,
          2.5156e+00, -1.1836e+00],
        [ 1.8193e+00,  2.0547e+00,  8.1172e+00,  ...,  1.0186e+00,
          1.1406e+00,  1.3057e+00],
        [ 3.0605e+00, -3.2715e-01,  5.4023e+00,  ...,  1.4648e-01,
          3.4668e+00,  5.1611e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:10:00] "POST /inference/generate HTTP/1.1" 200 2903

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-9.0027e-02, -1.2314e-02,  2.5000e-01,  ...,  2.6199e-02,
          4.4708e-02,  1.0736e-01],
        [-2.6398e-02, -1.1835e-01,  6.3721e-02,  ...,  1.6406e-01,
         -1.7365e-02,  1.7786e-01],
        [-1.3096e+00, -7.8500e+01, -4.3938e+01,  ..., -2.6611e-01,
         -3.3105e-01, -5.7373e-01],
        ...,
        [ 3.2676e+00, -2.3984e+00, -2.9316e+00,  ...,  3.1172e+00,
          2.5156e+00, -1.1836e+00],
        [ 1.8193e+00,  2.0547e+00,  8.1172e+00,  ...,  1.0186e+00,
          1.1406e+00,  1.3057e+00],
        [ 3.0605e+00, -3.2715e-01,  5.4023e+00,  ...,  1.4648e-01,
          3.4668e+00,  5.1611e-01]], device='cuda:0', dtype=torch.float16)


  meetup




---after-> tensor([[-3.4363e-02, -3.6499e-02,  3.5303e-01,  ..., -6.7017e-02,
         -2.1179e-01, -5.7190e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.5703e+00, -7.6562e+01, -6.7312e+01,  ...,  2.3403e-03,
         -3.1372e-01,  5.8789e-01],
        ...,
        [-1.6084e+00, -1.1031e+01, -5.3281e+00,  ...,  2.0547e+00,
          4.0352e+00, -2.1484e+00],
        [ 2.0840e+00,  2.5098e+00,  5.9102e+00,  ...,  1.0146e+00,
          1.0049e+00,  1.0264e+00],
        [ 4.2773e+00,  3.6353e-01,  6.1719e+00,  ..., -6.4893e-01,
          3.5254e+00,  3.3691e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:10:40] "POST /inference/generate HTTP/1.1" 200 1358

---- {'role': 'request', 'text': 'master   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-3.4363e-02, -3.6499e-02,  3.5303e-01,  ..., -6.7017e-02,
         -2.1179e-01, -5.7190e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.5703e+00, -7.6562e+01, -6.7312e+01,  ...,  2.3403e-03,
         -3.1372e-01,  5.8789e-01],
        ...,
        [-1.6084e+00, -1.1031e+01, -5.3281e+00,  ...,  2.0547e+00,
          4.0352e+00, -2.1484e+00],
        [ 2.0840e+00,  2.5098e+00,  5.9102e+00,  ...,  1.0146e+00,
          1.0049e+00,  1.0264e+00],
        [ 4.2773e+00,  3.6353e-01,  6.1719e+00,  ..., -6.4893e-01,
          3.5254e+00,  3.3691e+00]], device='cuda:0', dtype=torch.float16)






  


 
 

 
 

  master

---after-> tensor([[-2.6245e-02, -5.6946e-02,  3.6792e-01,  ..., -1.0822e-01,
         -1.8640e-01, -5.6763e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-4.1523e+00, -1.1200e+02, -4.8156e+01,  ...,  7.7477e-03,
         -1.5613e-01,  1.3467e+00],
        ...,
        [-4.5391e+00, -8.5625e+00, -5.3672e+00,  ...,  1.8018e+00,
          2.3633e+00, -1.6982e+00],
        [ 1.4639e+00,  3.5957e+00,  3.9609e+00,  ...,  1.0273e+00,
          1.0146e+00,  1.0547e+00],
        [ 3.4160e+00, -1.3271e+00,  6.6016e+00,  ..., -2.2852e+00,
          1.2832e+00,  6.7139e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:12:07] "POST /inference/generate HTTP/1.1" 200 3810

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-2.6245e-02, -5.6946e-02,  3.6792e-01,  ..., -1.0822e-01,
         -1.8640e-01, -5.6763e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-4.1523e+00, -1.1200e+02, -4.8156e+01,  ...,  7.7477e-03,
         -1.5613e-01,  1.3467e+00],
        ...,
        [-4.5391e+00, -8.5625e+00, -5.3672e+00,  ...,  1.8018e+00,
          2.3633e+00, -1.6982e+00],
        [ 1.4639e+00,  3.5957e+00,  3.9609e+00,  ...,  1.0273e+00,
          1.0146e+00,  1.0547e+00],
        [ 3.4160e+00, -1.3271e+00,  6.6016e+00,  ..., -2.2852e+00,
          1.2832e+00,  6.7139e-01]], device='cuda:0', dtype=torch.float16)



 face

  



---after-> tensor([[-7.4097e-02, -5.4901e-02,  3.1763e-01,  ..., -7.9895e-02,
         -2.1313e-01, -4.8828e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.7324e+00, -4.1375e+01, -4.1781e+01,  ...,  3.2227e-01,
         -1.3953e-01,  1.4521e+00],
        ...,
        [-5.8906e+00, -3.2227e+00, -1.0996e+00,  ...,  1.0068e+00,
          3.7344e+00, -2.8613e+00],
        [ 2.1875e+00,  1.7041e+00,  2.7246e+00,  ...,  1.0137e+00,
          1.0107e+00,  1.0254e+00],
        [ 3.3340e+00, -3.0933e-01,  6.9844e+00,  ..., -1.8037e+00,
          2.9902e+00,  1.9170e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:12:41] "POST /inference/generate HTTP/1.1" 200 1421

---- {'role': 'response', 'text': 'master\n\n\n face\n\n  \n\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-7.4097e-02, -5.4901e-02,  3.1763e-01,  ..., -7.9895e-02,
         -2.1313e-01, -4.8828e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.7324e+00, -4.1375e+01, -4.1781e+01,  ...,  3.2227e-01,
         -1.3953e-01,  1.4521e+00],
        ...,
        [-5.8906e+00, -3.2227e+00, -1.0996e+00,  ...,  1.0068e+00,
          3.7344e+00, -2.8613e+00],
        [ 2.1875e+00,  1.7041e+00,  2.7246e+00,  ...,  1.0137e+00,
          1.0107e+00,  1.0254e+00],
        [ 3.3340e+00, -3.0933e-01,  6.9844e+00,  ..., -1.8037e+00,
          2.9902e+00,  1.9170e+00]], device='cuda:0', dtype=torch.float16)


1. 
2. 
3. 
4. 
5. 
---after-> tensor([[-1.1053e-01, -3.9673e-02,  2.1948e-01,  ..., -3.5763e-03,
         -5.3833e-02, -1.1340e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.9004e+00, -1.1988e+02, -2.4031e+01,  ..., -1.6711e-01,
          2.5024e-01,  4.8779e-01],
        ...,
        [-7.0547e+00, -8.7578e+00, -3.7734e+00,  ...,  3.1855e+00,
          3.0645e+00, -1.2314e+00],
        [ 2.6172e+00,  3.0039e+00,  5.9688e+00,  ...,  1.0117e+00,
          1.1025e+00,  1.1035e+00],
        [ 2.5664e+00, -1.2090e+00,  5.1953e+00,  ..., -1.2266e+00,
          1.3242e+00,  9.8877e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:20:22] "POST /inference/generate HTTP/1.1" 200 4201

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-1.1053e-01, -3.9673e-02,  2.1948e-01,  ..., -3.5763e-03,
         -5.3833e-02, -1.1340e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.9004e+00, -1.1988e+02, -2.4031e+01,  ..., -1.6711e-01,
          2.5024e-01,  4.8779e-01],
        ...,
        [-7.0547e+00, -8.7578e+00, -3.7734e+00,  ...,  3.1855e+00,
          3.0645e+00, -1.2314e+00],
        [ 2.6172e+00,  3.0039e+00,  5.9688e+00,  ...,  1.0117e+00,
          1.1025e+00,  1.1035e+00],
        [ 2.5664e+00, -1.2090e+00,  5.1953e+00,  ..., -1.2266e+00,
          1.3242e+00,  9.8877e-01]], device='cuda:0', dtype=torch.float16)

 
 




---after-> tensor([[-4.6844e-02, -5.4016e-02,  3.4790e-01,  ..., -8.1787e-02,
         -1.9104e-01, -3.4058e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-2.3438e+00, -8.3625e+01, -3.9875e+01,  ...,  1.6492e-01,
         -2.0410e-01,  1.0791e+00],
        ...,
        [-2.3770e+00,  1.8689e-01, -6.1680e+00,  ..., -2.7148e+00,
          1.2637e+00, -5.3906e+00],
        [ 3.0684e+00,  1.1992e+00,  9.9062e+00,  ...,  1.0156e+00,
          1.0088e+00,  1.9326e+00],
        [ 5.1250e+00,  1.1494e+00,  4.4570e+00,  ..., -2.4336e+00,
          2.3984e+00,  1.5674e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:21:23] "POST /inference/generate HTTP/1.1" 200 1256

init_state None

state-> tensor([[-4.6844e-02, -5.4016e-02,  3.4790e-01,  ..., -8.1787e-02,
         -1.9104e-01, -3.4058e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-2.3438e+00, -8.3625e+01, -3.9875e+01,  ...,  1.6492e-01,
         -2.0410e-01,  1.0791e+00],
        ...,
        [-2.3770e+00,  1.8689e-01, -6.1680e+00,  ..., -2.7148e+00,
          1.2637e+00, -5.3906e+00],
        [ 3.0684e+00,  1.1992e+00,  9.9062e+00,  ...,  1.0156e+00,
          1.0088e+00,  1.9326e+00],
        [ 5.1250e+00,  1.1494e+00,  4.4570e+00,  ..., -2.4336e+00,
          2.3984e+00,  1.5674e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 01:26:09] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\n    \n \n\n ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None


---after-> tensor([[-2.3666e-02,  2.7985e-02,  2.1863e-01,  ..., -1.1635e-02,
          7.8613e-02,  6.9336e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-2.2715e+00, -5.4719e+01, -5.6969e+01,  ..., -3.7256e-01,
         -9.3079e-02,  1.8633e+00],
        ...,
        [-7.7852e+00, -3.6465e+00, -5.1133e+00,  ...,  2.0293e+00,
          3.9102e+00, -1.8027e+00],
        [ 1.9473e+00,  2.2461e+00,  5.6250e+00,  ...,  1.0322e+00,
          1.0020e+00,  1.4180e+00],
        [ 4.3008e+00,  1.0244e+00,  6.6602e+00,  ..., -1.2822e+00,
          3.5039e+00,  1.0996e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:26:20] "POST /inference/generate HTTP/1.1" 200 4255

init_state None

state-> tensor([[-2.3666e-02,  2.7985e-02,  2.1863e-01,  ..., -1.1635e-02,
          7.8613e-02,  6.9336e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-2.2715e+00, -5.4719e+01, -5.6969e+01,  ..., -3.7256e-01,
         -9.3079e-02,  1.8633e+00],
        ...,
        [-7.7852e+00, -3.6465e+00, -5.1133e+00,  ...,  2.0293e+00,
          3.9102e+00, -1.8027e+00],
        [ 1.9473e+00,  2.2461e+00,  5.6250e+00,  ...,  1.0322e+00,
          1.0020e+00,  1.4180e+00],
        [ 4.3008e+00,  1.0244e+00,  6.6602e+00,  ..., -1.2822e+00,
          3.5039e+00,  1.0996e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 01:26:40] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\n    \n \n\n ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'mastermaster', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None

1. 

2. 

3. 

4. 

5. 

6. 

---after-> tensor([[-1.1584e-01, -2.9724e-02,  2.3474e-01,  ..., -2.7905e-03,
         -6.2805e-02, -1.1609e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-2.8926e+00, -3.4750e+01, -5.3844e+01,  ...,  2.4854e-01,
         -2.8589e-01,  3.8647e-01],
        ...,
        [-6.3750e+00, -1.1391e+01, -5.1562e+00,  ...,  1.9600e+00,
          4.0312e+00,  7.8320e-01],
        [ 2.0215e+00,  4.2578e+00,  1.2016e+01,  ...,  1.0127e+00,
          1.0098e+00,  1.6055e+00],
        [ 4.4961e+00, -8.8013e-02,  5.9141e+00,  ..., -8.7793e-01,
          3.6074e+00, -2.3096e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:27:04] "POST /inference/generate HTTP/1.1" 200 6686

init_state None

state-> tensor([[-1.1584e-01, -2.9724e-02,  2.3474e-01,  ..., -2.7905e-03,
         -6.2805e-02, -1.1609e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-2.8926e+00, -3.4750e+01, -5.3844e+01,  ...,  2.4854e-01,
         -2.8589e-01,  3.8647e-01],
        ...,
        [-6.3750e+00, -1.1391e+01, -5.1562e+00,  ...,  1.9600e+00,
          4.0312e+00,  7.8320e-01],
        [ 2.0215e+00,  4.2578e+00,  1.2016e+01,  ...,  1.0127e+00,
          1.0098e+00,  1.6055e+00],
        [ 4.4961e+00, -8.8013e-02,  5.9141e+00,  ..., -8.7793e-01,
          3.6074e+00, -2.3096e-01]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 01:28:24] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\n    \n \n\n \n  ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None
 system

---after-> tensor([[-1.1368e-03, -8.0933e-02,  3.5254e-01,  ..., -1.0016e-01,
         -2.0300e-01, -7.5684e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-3.3691e+00, -5.6312e+01, -5.4281e+01,  ...,  1.4380e-01,
         -3.3179e-01,  1.2432e+00],
        ...,
        [ 2.1289e-01, -6.1445e+00, -6.7227e+00,  ...,  2.8379e+00,
          2.9414e+00, -6.0625e+00],
        [ 1.3232e+00,  3.6582e+00,  6.6094e+00,  ...,  1.0088e+00,
          1.0938e+00,  1.9805e+00],
        [ 3.0176e+00,  1.0957e+00,  6.4492e+00,  ...,  2.0264e-01,
          2.8906e+00,  2.0762e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:28:37] "POST /inference/generate HTTP/1.1" 200 5043

init_state None

state-> tensor([[-1.1368e-03, -8.0933e-02,  3.5254e-01,  ..., -1.0016e-01,
         -2.0300e-01, -7.5684e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-3.3691e+00, -5.6312e+01, -5.4281e+01,  ...,  1.4380e-01,
         -3.3179e-01,  1.2432e+00],
        ...,
        [ 2.1289e-01, -6.1445e+00, -6.7227e+00,  ...,  2.8379e+00,
          2.9414e+00, -6.0625e+00],
        [ 1.3232e+00,  3.6582e+00,  6.6094e+00,  ...,  1.0088e+00,
          1.0938e+00,  1.9805e+00],
        [ 3.0176e+00,  1.0957e+00,  6.4492e+00,  ...,  2.0264e-01,
          2.8906e+00,  2.0762e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 01:30:01] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\n    \n \n\n \n  ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n  ', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'mastermaster', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None

AI





---after-> tensor([[-5.0781e-02,  2.1530e-02,  2.4023e-01,  ..., -1.8188e-02,
          7.4097e-02,  7.8674e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-1.4072e+00, -1.1406e+02, -2.7875e+01,  ..., -4.3506e-01,
          1.4478e-01,  1.8535e+00],
        ...,
        [-1.9385e+00, -8.1250e-01, -9.0312e+00,  ...,  2.6816e+00,
          3.5664e+00,  1.4883e+00],
        [ 1.6816e+00,  3.1191e+00,  7.5586e+00,  ...,  1.0010e+00,
          1.0029e+00,  1.1455e+00],
        [ 3.0879e+00,  4.4995e-01,  5.6016e+00,  ...,  8.1982e-01,
          4.3828e+00,  1.6025e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:30:27] "POST /inference/generate HTTP/1.1" 200 7957

init_state None

state-> tensor([[-5.0781e-02,  2.1530e-02,  2.4023e-01,  ..., -1.8188e-02,
          7.4097e-02,  7.8674e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-1.4072e+00, -1.1406e+02, -2.7875e+01,  ..., -4.3506e-01,
          1.4478e-01,  1.8535e+00],
        ...,
        [-1.9385e+00, -8.1250e-01, -9.0312e+00,  ...,  2.6816e+00,
          3.5664e+00,  1.4883e+00],
        [ 1.6816e+00,  3.1191e+00,  7.5586e+00,  ...,  1.0010e+00,
          1.0029e+00,  1.1455e+00],
        [ 3.0879e+00,  4.4995e-01,  5.6016e+00,  ...,  8.1982e-01,
          4.3828e+00,  1.6025e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 01:30:55] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\n    \n \n\n \n  ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n   ', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'mastermaster', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None






---after-> tensor([[-9.1125e-02, -2.3773e-02,  1.9604e-01,  ..., -1.5198e-02,
         -7.4890e-02, -1.1053e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.7578e+00, -4.9656e+01, -5.5000e+01,  ...,  4.5264e-01,
         -5.3955e-01,  4.9365e-01],
        ...,
        [-9.6641e+00, -4.6484e+00, -3.1289e+00,  ...,  8.4180e-01,
          2.8945e+00, -3.3262e+00],
        [ 2.5508e+00,  1.4434e+00,  7.7930e+00,  ...,  1.0781e+00,
          1.0039e+00,  1.5684e+00],
        [ 4.0859e+00,  4.2017e-01,  5.6289e+00,  ..., -3.1562e+00,
          4.1641e+00,  2.1680e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:31:23] "POST /inference/generate HTTP/1.1" 200 8470

init_state None

state-> tensor([[-9.1125e-02, -2.3773e-02,  1.9604e-01,  ..., -1.5198e-02,
         -7.4890e-02, -1.1053e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.7578e+00, -4.9656e+01, -5.5000e+01,  ...,  4.5264e-01,
         -5.3955e-01,  4.9365e-01],
        ...,
        [-9.6641e+00, -4.6484e+00, -3.1289e+00,  ...,  8.4180e-01,
          2.8945e+00, -3.3262e+00],
        [ 2.5508e+00,  1.4434e+00,  7.7930e+00,  ...,  1.0781e+00,
          1.0039e+00,  1.5684e+00],
        [ 4.0859e+00,  4.2017e-01,  5.6289e+00,  ..., -3.1562e+00,
          4.1641e+00,  2.1680e-01]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 01:32:04] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\n    \n \n\n \n  ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n   ', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'mastermaster\n\n\n\n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None


 necromancy reunion




 places 

---after-> tensor([[-8.4412e-02, -1.5198e-02,  2.1765e-01,  ..., -2.6855e-03,
         -6.8420e-02, -1.0626e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-7.6025e-01, -9.7562e+01, -4.5781e+01,  ...,  4.3091e-01,
         -2.6709e-01,  1.7322e-01],
        ...,
        [-6.7070e+00, -3.0254e+00, -4.7617e+00,  ...,  1.2393e+00,
          3.6328e+00, -5.4736e-01],
        [ 2.4863e+00,  2.3164e+00,  8.6094e+00,  ...,  1.0098e+00,
          1.0010e+00,  1.0830e+00],
        [ 2.3438e+00, -7.5000e-01,  6.0273e+00,  ..., -2.3359e+00,
          3.7246e+00,  1.4238e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:32:49] "POST /inference/generate HTTP/1.1" 200 13817

init_state None

state-> tensor([[-8.4412e-02, -1.5198e-02,  2.1765e-01,  ..., -2.6855e-03,
         -6.8420e-02, -1.0626e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-7.6025e-01, -9.7562e+01, -4.5781e+01,  ...,  4.3091e-01,
         -2.6709e-01,  1.7322e-01],
        ...,
        [-6.7070e+00, -3.0254e+00, -4.7617e+00,  ...,  1.2393e+00,
          3.6328e+00, -5.4736e-01],
        [ 2.4863e+00,  2.3164e+00,  8.6094e+00,  ...,  1.0098e+00,
          1.0010e+00,  1.0830e+00],
        [ 2.3438e+00, -7.5000e-01,  6.0273e+00,  ..., -2.3359e+00,
          3.7246e+00,  1.4238e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 01:33:16] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': '\n    \n \n\n \n  ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n   ', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'mastermaster\n\n\n\n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'mastermaster', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None

 
 
 

 
 
  
  
  

---after-> tensor([[-6.1722e-03, -6.2347e-02,  3.6963e-01,  ..., -1.1353e-01,
         -1.8970e-01, -6.3599e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-2.1543e+00, -8.3438e+01, -3.5250e+01,  ...,  3.2349e-01,
         -4.2993e-01,  1.0322e+00],
        ...,
        [-1.0070e+01, -1.5141e+01, -9.1953e+00,  ...,  2.1602e+00,
          2.9043e+00, -3.1504e+00],
        [ 4.2031e+00,  3.5605e+00,  3.7402e+00,  ...,  1.0117e+00,
          1.0273e+00,  1.0156e+00],
        [ 1.7783e+00, -8.2910e-01,  7.0742e+00,  ..., -7.1875e-01,
          4.6719e+00,  6.9434e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:34:04] "POST /inference/generate HTTP/1.1" 200 14857

---- {'role': 'response', 'text': 'mastermaster\n \n \n \n\n \n \n  \n   ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-6.1722e-03, -6.2347e-02,  3.6963e-01,  ..., -1.1353e-01,
         -1.8970e-01, -6.3599e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-2.1543e+00, -8.3438e+01, -3.5250e+01,  ...,  3.2349e-01,
         -4.2993e-01,  1.0322e+00],
        ...,
        [-1.0070e+01, -1.5141e+01, -9.1953e+00,  ...,  2.1602e+00,
          2.9043e+00, -3.1504e+00],
        [ 4.2031e+00,  3.5605e+00,  3.7402e+00,  ...,  1.0117e+00,
          1.0273e+00,  1.0156e+00],
        [ 1.7783e+00, -8.2910e-01,  7.0742e+00,  ..., -7.1875e-01,
          4.6719e+00,  6.9434e-01]], device='cuda:0', dtype=torch.float16)
 















---after-> tensor([[-1.1707e-01, -2.3834e-02,  2.1130e-01,  ..., -8.9979e-04,
         -6.4392e-02, -1.1890e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [ 4.8711e+00, -6.6875e+01, -3.3469e+01,  ..., -4.4006e-02,
         -1.6968e-01,  2.4170e-01],
        ...,
        [-7.0625e+00, -5.0781e+00, -4.5859e+00,  ..., -1.1709e+00,
          2.8750e+00, -8.7598e-01],
        [ 2.6992e+00,  2.9258e+00,  9.4922e+00,  ...,  1.0029e+00,
          1.0156e+00,  1.1670e+00],
        [ 2.4160e+00, -7.8271e-01,  5.3594e+00,  ..., -6.2061e-01,
          2.3672e+00, -3.8672e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:35:35] "POST /inference/generate HTTP/1.1" 200 9143

init_state None

state-> tensor([[-1.1707e-01, -2.3834e-02,  2.1130e-01,  ..., -8.9979e-04,
         -6.4392e-02, -1.1890e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [ 4.8711e+00, -6.6875e+01, -3.3469e+01,  ..., -4.4006e-02,
         -1.6968e-01,  2.4170e-01],
        ...,
        [-7.0625e+00, -5.0781e+00, -4.5859e+00,  ..., -1.1709e+00,
          2.8750e+00, -8.7598e-01],
        [ 2.6992e+00,  2.9258e+00,  9.4922e+00,  ...,  1.0029e+00,
          1.0156e+00,  1.1670e+00],
        [ 2.4160e+00, -7.8271e-01,  5.3594e+00,  ..., -6.2061e-01,
          2.3672e+00, -3.8672e-01]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 01:41:05] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'R\n       \n    \n\n\n\n   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None
system

---after-> tensor([[-7.4806e-03, -7.2144e-02,  3.3960e-01,  ..., -1.1066e-01,
         -2.0361e-01, -6.8176e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.9229e+00, -5.6500e+01, -2.3203e+01,  ...,  3.3472e-01,
         -1.6345e-01,  1.2549e+00],
        ...,
        [-8.3750e+00, -5.1523e+00, -2.3652e+00,  ...,  1.4404e+00,
          3.4453e+00, -3.0254e+00],
        [ 1.3721e+00,  3.6484e+00,  4.0859e+00,  ...,  1.0029e+00,
          1.0020e+00,  1.0225e+00],
        [ 3.9980e+00, -7.2168e-01,  6.5391e+00,  ...,  8.0615e-01,
          3.2246e+00,  2.4629e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:41:25] "POST /inference/generate HTTP/1.1" 200 7259

init_state None

state-> tensor([[-7.4806e-03, -7.2144e-02,  3.3960e-01,  ..., -1.1066e-01,
         -2.0361e-01, -6.8176e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.9229e+00, -5.6500e+01, -2.3203e+01,  ...,  3.3472e-01,
         -1.6345e-01,  1.2549e+00],
        ...,
        [-8.3750e+00, -5.1523e+00, -2.3652e+00,  ...,  1.4404e+00,
          3.4453e+00, -3.0254e+00],
        [ 1.3721e+00,  3.6484e+00,  4.0859e+00,  ...,  1.0029e+00,
          1.0020e+00,  1.0225e+00],
        [ 3.9980e+00, -7.2168e-01,  6.5391e+00,  ...,  8.0615e-01,
          3.2246e+00,  2.4629e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 01:42:13] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'R\n       \n    \n\n\n\n   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemsystem\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'mastermaster', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None






---after-> tensor([[-1.1902e-01, -4.5105e-02,  2.1655e-01,  ..., -1.0437e-02,
         -6.0944e-02, -1.2170e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-5.7715e-01, -7.2750e+01, -5.4750e+01,  ..., -1.6711e-01,
          2.5024e-01,  4.8779e-01],
        ...,
        [-2.7246e+00, -3.0645e+00, -2.0957e+00,  ...,  4.9922e+00,
          4.9414e+00,  9.7656e-01],
        [ 3.3867e+00,  2.0371e+00,  4.0508e+00,  ...,  1.0068e+00,
          1.0166e+00,  1.2949e+00],
        [ 2.5645e+00,  9.6094e-01,  7.1953e+00,  ...,  7.0410e-01,
          2.4199e+00,  8.7549e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:42:41] "POST /inference/generate HTTP/1.1" 200 9237

init_state None

state-> tensor([[-1.1902e-01, -4.5105e-02,  2.1655e-01,  ..., -1.0437e-02,
         -6.0944e-02, -1.2170e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-5.7715e-01, -7.2750e+01, -5.4750e+01,  ..., -1.6711e-01,
          2.5024e-01,  4.8779e-01],
        ...,
        [-2.7246e+00, -3.0645e+00, -2.0957e+00,  ...,  4.9922e+00,
          4.9414e+00,  9.7656e-01],
        [ 3.3867e+00,  2.0371e+00,  4.0508e+00,  ...,  1.0068e+00,
          1.0166e+00,  1.2949e+00],
        [ 2.5645e+00,  9.6094e-01,  7.1953e+00,  ...,  7.0410e-01,
          2.4199e+00,  8.7549e-01]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 01:45:27] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'system R\n       \n    \n\n\n\n   \n\n ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': ' system\n  ', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'mastermaster', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None

1. 
2. 
3. 
4. 
5. 
6. 
7. 
---after-> tensor([[-9.6375e-02, -1.9257e-02,  2.0911e-01,  ..., -2.2354e-03,
         -6.0455e-02, -1.2683e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.5146e+00, -6.1125e+01, -6.4125e+01,  ...,  1.4783e-01,
         -4.4360e-01, -5.1361e-02],
        ...,
        [-8.1016e+00, -6.0547e+00, -7.7266e+00,  ...,  2.7188e+00,
          3.7070e+00, -4.1562e+00],
        [ 4.9727e+00,  2.8223e+00,  5.2031e+00,  ...,  1.0029e+00,
          1.0742e+00,  1.4141e+00],
        [ 1.8037e+00,  4.5380e-02,  6.8594e+00,  ...,  7.3193e-01,
          3.0703e+00, -7.0166e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:45:56] "POST /inference/generate HTTP/1.1" 200 10230

init_state None

state-> tensor([[-9.6375e-02, -1.9257e-02,  2.0911e-01,  ..., -2.2354e-03,
         -6.0455e-02, -1.2683e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.5146e+00, -6.1125e+01, -6.4125e+01,  ...,  1.4783e-01,
         -4.4360e-01, -5.1361e-02],
        ...,
        [-8.1016e+00, -6.0547e+00, -7.7266e+00,  ...,  2.7188e+00,
          3.7070e+00, -4.1562e+00],
        [ 4.9727e+00,  2.8223e+00,  5.2031e+00,  ...,  1.0029e+00,
          1.0742e+00,  1.4141e+00],
        [ 1.8037e+00,  4.5380e-02,  6.8594e+00,  ...,  7.3193e-01,
          3.0703e+00, -7.0166e-01]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 01:47:01] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'system R\n       \n    \n\n\n\n   \n\n ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': ' system\n  ', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'mastermaster', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None

1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
---after-> tensor([[-1.0583e-01, -4.0222e-02,  1.9922e-01,  ...,  2.6131e-03,
         -6.8054e-02, -1.1115e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.4082e+00, -3.5844e+01, -4.6406e+01,  ..., -4.8767e-02,
         -2.5293e-01,  1.3501e-01],
        ...,
        [-1.3672e+00, -4.5000e+00, -5.9453e+00,  ...,  2.8535e+00,
          2.8398e+00,  9.5654e-01],
        [ 1.8652e+00,  2.0527e+00,  7.9883e+00,  ...,  1.0088e+00,
          1.0068e+00,  1.0801e+00],
        [ 5.2422e+00, -4.8242e-01,  5.6367e+00,  ..., -7.4219e-01,
          1.8926e+00,  2.5156e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:47:34] "POST /inference/generate HTTP/1.1" 200 10718

---- {'role': 'request', 'text': '  \n\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-1.0583e-01, -4.0222e-02,  1.9922e-01,  ...,  2.6131e-03,
         -6.8054e-02, -1.1115e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.4082e+00, -3.5844e+01, -4.6406e+01,  ..., -4.8767e-02,
         -2.5293e-01,  1.3501e-01],
        ...,
        [-1.3672e+00, -4.5000e+00, -5.9453e+00,  ...,  2.8535e+00,
          2.8398e+00,  9.5654e-01],
        [ 1.8652e+00,  2.0527e+00,  7.9883e+00,  ...,  1.0088e+00,
          1.0068e+00,  1.0801e+00],
        [ 5.2422e+00, -4.8242e-01,  5.6367e+00,  ..., -7.4219e-01,
          1.8926e+00,  2.5156e+00]], device='cuda:0', dtype=torch.float16)
    
  
 


 


 



 
---after-> tensor([[-1.1682e-01, -2.6138e-02,  2.0801e-01,  ..., -5.5885e-04,
         -5.8197e-02, -1.3550e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-4.5469e+00, -4.7188e+01, -6.3719e+01,  ...,  4.0820e-01,
         -1.1652e-01,  4.7412e-01],
        ...,
        [-3.9844e+00, -6.5312e+00, -4.0352e+00,  ...,  3.3965e+00,
          2.7715e+00, -4.5430e+00],
        [ 5.0273e+00,  2.3438e+00,  5.2539e+00,  ...,  1.0010e+00,
          1.0176e+00,  1.2559e+00],
        [ 9.1455e-01,  5.2344e-01,  6.1367e+00,  ..., -6.4258e-01,
          2.0645e+00,  3.0215e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:52:17] "POST /inference/generate HTTP/1.1" 200 3648

init_state None

state-> tensor([[-1.1682e-01, -2.6138e-02,  2.0801e-01,  ..., -5.5885e-04,
         -5.8197e-02, -1.3550e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-4.5469e+00, -4.7188e+01, -6.3719e+01,  ...,  4.0820e-01,
         -1.1652e-01,  4.7412e-01],
        ...,
        [-3.9844e+00, -6.5312e+00, -4.0352e+00,  ...,  3.3965e+00,
          2.7715e+00, -4.5430e+00],
        [ 5.0273e+00,  2.3438e+00,  5.2539e+00,  ...,  1.0010e+00,
          1.0176e+00,  1.2559e+00],
        [ 9.1455e-01,  5.2344e-01,  6.1367e+00,  ..., -6.4258e-01,
          2.0645e+00,  3.0215e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 01:52:30] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'system R\n       \n    \n\n\n\n   \n\n ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': ' system\n  ', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'mastermaster\n1. \n2. \n3. \n4. \n5. \n6. \n7. \n8. \n9. ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '  \n\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None
:
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 50
9. 
10. 



---after-> tensor([[-2.6672e-02, -4.2725e-02,  3.4692e-01,  ..., -8.3252e-02,
         -1.9666e-01, -5.9052e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.6777e+00, -6.7750e+01, -3.7062e+01,  ...,  2.3950e-01,
          3.9673e-02,  1.2676e+00],
        ...,
        [-7.5469e+00, -1.8750e+00, -2.0293e+00,  ...,  2.0176e+00,
         -1.1572e-01, -5.0938e+00],
        [ 4.2266e+00,  2.1797e+00,  9.0312e+00,  ...,  1.0195e+00,
          1.0752e+00,  1.9883e+00],
        [ 1.4512e+00,  7.6660e-01,  5.7930e+00,  ..., -1.4478e-01,
          1.5000e+00,  1.4688e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:53:23] "POST /inference/generate HTTP/1.1" 200 17246

init_state None

state-> tensor([[-2.6672e-02, -4.2725e-02,  3.4692e-01,  ..., -8.3252e-02,
         -1.9666e-01, -5.9052e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-1.6777e+00, -6.7750e+01, -3.7062e+01,  ...,  2.3950e-01,
          3.9673e-02,  1.2676e+00],
        ...,
        [-7.5469e+00, -1.8750e+00, -2.0293e+00,  ...,  2.0176e+00,
         -1.1572e-01, -5.0938e+00],
        [ 4.2266e+00,  2.1797e+00,  9.0312e+00,  ...,  1.0195e+00,
          1.0752e+00,  1.9883e+00],
        [ 1.4512e+00,  7.6660e-01,  5.7930e+00,  ..., -1.4478e-01,
          1.5000e+00,  1.4688e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 01:54:53] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'system R\n       \n    \n\n\n\n   \n\n ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': ' system\n  ', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'mastermaster\n1. \n2. \n3. \n4. \n5. \n6. \n7. \n8. \n9. ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '  \n\n\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None

1. 
2. 
3. 
4. 
5. 
6. 
7. +1
8. +1
---after-> tensor([[-8.4717e-02, -1.6510e-02,  2.0483e-01,  ..., -1.6432e-03,
         -6.8909e-02, -1.1218e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-3.4160e+00, -5.9719e+01, -3.3031e+01,  ..., -1.0437e-01,
         -5.2832e-01,  2.7856e-01],
        ...,
        [-7.2031e+00, -7.9219e+00, -7.9258e+00,  ...,  2.7402e+00,
          1.4014e+00, -2.0923e-01],
        [ 6.4844e+00,  5.2617e+00,  7.7383e+00,  ...,  1.0029e+00,
          1.0049e+00,  1.0322e+00],
        [ 1.2119e+00, -1.1191e+00,  5.6406e+00,  ...,  1.3298e-02,
          3.1816e+00,  2.2910e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:55:43] "POST /inference/generate HTTP/1.1" 200 16826

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-8.4717e-02, -1.6510e-02,  2.0483e-01,  ..., -1.6432e-03,
         -6.8909e-02, -1.1218e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-3.4160e+00, -5.9719e+01, -3.3031e+01,  ..., -1.0437e-01,
         -5.2832e-01,  2.7856e-01],
        ...,
        [-7.2031e+00, -7.9219e+00, -7.9258e+00,  ...,  2.7402e+00,
          1.4014e+00, -2.0923e-01],
        [ 6.4844e+00,  5.2617e+00,  7.7383e+00,  ...,  1.0029e+00,
          1.0049e+00,  1.0322e+00],
        [ 1.2119e+00, -1.1191e+00,  5.6406e+00,  ...,  1.3298e-02,
          3.1816e+00,  2.2910e+00]], device='cuda:0', dtype=torch.float16)








---after-> tensor([[-2.4994e-02,  2.2736e-02,  2.5024e-01,  ...,  1.5282e-02,
          9.7473e-02,  1.0040e-01],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-3.3770e+00, -6.5500e+01, -5.9375e+01,  ..., -9.7839e-02,
         -2.4341e-01,  1.6475e+00],
        ...,
        [-2.5156e+00, -5.7539e+00, -1.3594e+01,  ...,  2.6543e+00,
          2.9531e+00, -2.5176e+00],
        [ 4.1875e+00,  1.7031e+00,  1.6234e+01,  ...,  1.0303e+00,
          1.0127e+00,  1.1201e+00],
        [ 1.7520e+00,  1.9443e+00,  4.8633e+00,  ..., -8.9258e-01,
          3.1289e+00,  7.7051e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:56:08] "POST /inference/generate HTTP/1.1" 200 1922

init_state None

state-> tensor([[-2.4994e-02,  2.2736e-02,  2.5024e-01,  ...,  1.5282e-02,
          9.7473e-02,  1.0040e-01],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-3.3770e+00, -6.5500e+01, -5.9375e+01,  ..., -9.7839e-02,
         -2.4341e-01,  1.6475e+00],
        ...,
        [-2.5156e+00, -5.7539e+00, -1.3594e+01,  ...,  2.6543e+00,
          2.9531e+00, -2.5176e+00],
        [ 4.1875e+00,  1.7031e+00,  1.6234e+01,  ...,  1.0303e+00,
          1.0127e+00,  1.1201e+00],
        [ 1.7520e+00,  1.9443e+00,  4.8633e+00,  ..., -8.9258e-01,
          3.1289e+00,  7.7051e-01]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 01:56:33] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'system R\n       \n    \n\n\n\n   \n\n ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': ' system\n  ', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'mastermaster\n1. \n2. \n3. \n4. \n5. \n6. \n7. \n8. \n9. ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '  \n\n\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system\n1. \n2. \n3. \n4. \n5. \n6. ', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n\n\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None





---after-> tensor([[-7.7515e-02, -2.0935e-02,  2.1082e-01,  ..., -1.6235e-02,
         -8.1665e-02, -1.0083e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-2.0430e+00, -5.1312e+01, -5.7438e+01,  ...,  2.4506e-02,
         -4.2407e-01, -2.7267e-02],
        ...,
        [-3.1074e+00, -2.9785e-01, -3.8789e+00,  ...,  2.6523e+00,
          5.9727e+00, -4.0039e+00],
        [ 3.5547e+00,  2.2031e+00,  6.8359e+00,  ...,  1.1270e+00,
          1.0029e+00,  1.3164e+00],
        [ 2.4316e+00, -7.6514e-01,  6.0977e+00,  ..., -2.4102e+00,
          4.7500e+00,  1.6895e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 01:57:33] "POST /inference/generate HTTP/1.1" 200 21585

---- {'role': 'request', 'text': '\nTriumph', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-7.7515e-02, -2.0935e-02,  2.1082e-01,  ..., -1.6235e-02,
         -8.1665e-02, -1.0083e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-2.0430e+00, -5.1312e+01, -5.7438e+01,  ...,  2.4506e-02,
         -4.2407e-01, -2.7267e-02],
        ...,
        [-3.1074e+00, -2.9785e-01, -3.8789e+00,  ...,  2.6523e+00,
          5.9727e+00, -4.0039e+00],
        [ 3.5547e+00,  2.2031e+00,  6.8359e+00,  ...,  1.1270e+00,
          1.0029e+00,  1.3164e+00],
        [ 2.4316e+00, -7.6514e-01,  6.0977e+00,  ..., -2.4102e+00,
          4.7500e+00,  1.6895e-01]], device='cuda:0', dtype=torch.float16)

1. 
2. 
3. 
4. 
---after-> tensor([[-1.0510e-01, -3.2257e-02,  2.0801e-01,  ..., -7.6408e-03,
         -5.7098e-02, -1.1890e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.3232e+00, -8.3438e+01, -7.3938e+01,  ...,  8.7830e-02,
         -5.8496e-01,  4.0308e-01],
        ...,
        [-1.4141e+01, -7.5859e+00, -3.5098e+00,  ...,  3.3828e+00,
          4.3359e+00, -1.3369e+00],
        [ 3.8965e+00,  2.0938e+00,  3.0801e+00,  ...,  1.0029e+00,
          1.0098e+00,  1.0205e+00],
        [ 2.5645e+00, -7.2119e-01,  6.4023e+00,  ...,  5.2539e-01,
          1.4980e+00,  9.0771e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:01:47] "POST /inference/generate HTTP/1.1" 200 4531

---- {'role': 'request', 'text': 'Triumph\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-1.0510e-01, -3.2257e-02,  2.0801e-01,  ..., -7.6408e-03,
         -5.7098e-02, -1.1890e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.3232e+00, -8.3438e+01, -7.3938e+01,  ...,  8.7830e-02,
         -5.8496e-01,  4.0308e-01],
        ...,
        [-1.4141e+01, -7.5859e+00, -3.5098e+00,  ...,  3.3828e+00,
          4.3359e+00, -1.3369e+00],
        [ 3.8965e+00,  2.0938e+00,  3.0801e+00,  ...,  1.0029e+00,
          1.0098e+00,  1.0205e+00],
        [ 2.5645e+00, -7.2119e-01,  6.4023e+00,  ...,  5.2539e-01,
          1.4980e+00,  9.0771e-01]], device='cuda:0', dtype=torch.float16)

1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 



---after-> tensor([[-5.3436e-02,  1.1375e-02,  2.2815e-01,  ...,  9.2163e-03,
          4.8981e-02,  1.3367e-01],
        [-2.6398e-02, -1.1835e-01,  6.3721e-02,  ...,  1.6406e-01,
         -1.7365e-02,  1.7786e-01],
        [-1.6348e+00, -3.3094e+01, -2.2328e+01,  ..., -2.6611e-01,
         -3.3105e-01, -5.7373e-01],
        ...,
        [-6.4219e+00, -6.4766e+00, -3.3047e+00,  ...,  2.4375e+00,
          9.5361e-01, -2.2930e+00],
        [ 3.3477e+00,  3.0820e+00,  8.5781e+00,  ...,  1.0273e+00,
          1.0420e+00,  1.3135e+00],
        [ 1.7734e+00, -1.3760e+00,  5.1133e+00,  ..., -7.0703e-01,
          2.7012e+00,  1.8677e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:03:18] "POST /inference/generate HTTP/1.1" 200 6987

---- {'role': 'system', 'text': 'Prada\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-5.3436e-02,  1.1375e-02,  2.2815e-01,  ...,  9.2163e-03,
          4.8981e-02,  1.3367e-01],
        [-2.6398e-02, -1.1835e-01,  6.3721e-02,  ...,  1.6406e-01,
         -1.7365e-02,  1.7786e-01],
        [-1.6348e+00, -3.3094e+01, -2.2328e+01,  ..., -2.6611e-01,
         -3.3105e-01, -5.7373e-01],
        ...,
        [-6.4219e+00, -6.4766e+00, -3.3047e+00,  ...,  2.4375e+00,
          9.5361e-01, -2.2930e+00],
        [ 3.3477e+00,  3.0820e+00,  8.5781e+00,  ...,  1.0273e+00,
          1.0420e+00,  1.3135e+00],
        [ 1.7734e+00, -1.3760e+00,  5.1133e+00,  ..., -7.0703e-01,
          2.7012e+00,  1.8677e-01]], device='cuda:0', dtype=torch.float16)



system

---after-> tensor([[-7.2510e-02, -1.1391e-02,  2.0935e-01,  ..., -3.5126e-02,
         -7.5134e-02, -9.5642e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [ 5.2783e-01, -8.9188e+01, -5.6312e+01,  ...,  1.3086e-01,
         -1.8250e-01,  3.2764e-01],
        ...,
        [-7.8242e+00, -5.6289e+00, -7.4922e+00,  ...,  3.4766e+00,
          3.2676e+00, -5.9844e+00],
        [ 5.7656e+00,  2.7891e+00,  1.2750e+01,  ...,  1.0088e+00,
          1.0654e+00,  1.2246e+00],
        [ 2.4668e+00, -1.3057e+00,  4.9219e+00,  ..., -9.5947e-01,
          2.9956e-01,  1.6199e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:07:51] "POST /inference/generate HTTP/1.1" 200 5958

---- {'role': 'system', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-7.2510e-02, -1.1391e-02,  2.0935e-01,  ..., -3.5126e-02,
         -7.5134e-02, -9.5642e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [ 5.2783e-01, -8.9188e+01, -5.6312e+01,  ...,  1.3086e-01,
         -1.8250e-01,  3.2764e-01],
        ...,
        [-7.8242e+00, -5.6289e+00, -7.4922e+00,  ...,  3.4766e+00,
          3.2676e+00, -5.9844e+00],
        [ 5.7656e+00,  2.7891e+00,  1.2750e+01,  ...,  1.0088e+00,
          1.0654e+00,  1.2246e+00],
        [ 2.4668e+00, -1.3057e+00,  4.9219e+00,  ..., -9.5947e-01,
          2.9956e-01,  1.6199e-01]], device='cuda:0', dtype=torch.float16)





---after-> tensor([[-7.3364e-02, -2.0721e-02,  2.0691e-01,  ..., -2.0935e-02,
         -6.9275e-02, -9.0942e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-2.1016e+00, -1.0194e+02, -5.6750e+01,  ...,  2.3413e-01,
         -1.1847e-01,  6.0156e-01],
        ...,
        [-8.8125e+00, -9.6641e+00, -3.0332e+00,  ...,  1.0117e+00,
          2.6426e+00, -3.5820e+00],
        [ 2.5156e+00,  2.6465e+00,  2.8047e+00,  ...,  1.0166e+00,
          1.1172e+00,  1.0947e+00],
        [ 3.4316e+00,  9.4238e-02,  6.1523e+00,  ..., -7.1631e-01,
          8.8818e-01, -3.5352e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:09:03] "POST /inference/generate HTTP/1.1" 200 6850

---- {'role': 'system', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-7.3364e-02, -2.0721e-02,  2.0691e-01,  ..., -2.0935e-02,
         -6.9275e-02, -9.0942e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-2.1016e+00, -1.0194e+02, -5.6750e+01,  ...,  2.3413e-01,
         -1.1847e-01,  6.0156e-01],
        ...,
        [-8.8125e+00, -9.6641e+00, -3.0332e+00,  ...,  1.0117e+00,
          2.6426e+00, -3.5820e+00],
        [ 2.5156e+00,  2.6465e+00,  2.8047e+00,  ...,  1.0166e+00,
          1.1172e+00,  1.0947e+00],
        [ 3.4316e+00,  9.4238e-02,  6.1523e+00,  ..., -7.1631e-01,
          8.8818e-01, -3.5352e-01]], device='cuda:0', dtype=torch.float16)







---after-> tensor([[-7.7026e-02, -8.5144e-03,  1.9739e-01,  ..., -3.4393e-02,
         -7.8430e-02, -9.7473e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-3.2837e-01, -1.0081e+02, -5.1094e+01,  ...,  2.1057e-02,
         -1.5918e-01,  3.7891e-01],
        ...,
        [-6.5742e+00, -2.1484e+00, -4.9961e+00,  ...,  1.4072e+00,
          2.4473e+00, -2.6523e+00],
        [ 4.3242e+00,  2.1582e+00,  5.2422e+00,  ...,  1.0254e+00,
          1.0156e+00,  1.0850e+00],
        [ 3.0762e+00, -1.7920e+00,  5.1914e+00,  ..., -9.1504e-01,
          2.8320e+00,  6.3965e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:10:17] "POST /inference/generate HTTP/1.1" 200 6228

---- {'role': 'system', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-7.7026e-02, -8.5144e-03,  1.9739e-01,  ..., -3.4393e-02,
         -7.8430e-02, -9.7473e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-3.2837e-01, -1.0081e+02, -5.1094e+01,  ...,  2.1057e-02,
         -1.5918e-01,  3.7891e-01],
        ...,
        [-6.5742e+00, -2.1484e+00, -4.9961e+00,  ...,  1.4072e+00,
          2.4473e+00, -2.6523e+00],
        [ 4.3242e+00,  2.1582e+00,  5.2422e+00,  ...,  1.0254e+00,
          1.0156e+00,  1.0850e+00],
        [ 3.0762e+00, -1.7920e+00,  5.1914e+00,  ..., -9.1504e-01,
          2.8320e+00,  6.3965e-01]], device='cuda:0', dtype=torch.float16)







,,,
,,,
,
---after-> tensor([[-8.7036e-02, -1.1841e-02,  2.0056e-01,  ..., -3.4271e-02,
         -7.6538e-02, -1.0229e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.7676e+00, -4.9375e+01, -2.8703e+01,  ...,  1.9446e-01,
         -4.4617e-02,  5.1465e-01],
        ...,
        [-3.1309e+00, -4.9609e+00, -2.6348e+00,  ...,  3.2949e+00,
          3.8633e+00, -2.7617e+00],
        [ 2.1367e+00,  2.3301e+00,  3.4375e+00,  ...,  1.0029e+00,
          1.0684e+00,  1.4785e+00],
        [ 4.0664e+00, -1.8184e+00,  6.0625e+00,  ..., -2.4524e-01,
          1.5537e+00,  7.4023e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:12:09] "POST /inference/generate HTTP/1.1" 200 6726

---- {'role': 'system', 'text': 'Triumph\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-8.7036e-02, -1.1841e-02,  2.0056e-01,  ..., -3.4271e-02,
         -7.6538e-02, -1.0229e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.7676e+00, -4.9375e+01, -2.8703e+01,  ...,  1.9446e-01,
         -4.4617e-02,  5.1465e-01],
        ...,
        [-3.1309e+00, -4.9609e+00, -2.6348e+00,  ...,  3.2949e+00,
          3.8633e+00, -2.7617e+00],
        [ 2.1367e+00,  2.3301e+00,  3.4375e+00,  ...,  1.0029e+00,
          1.0684e+00,  1.4785e+00],
        [ 4.0664e+00, -1.8184e+00,  6.0625e+00,  ..., -2.4524e-01,
          1.5537e+00,  7.4023e-01]], device='cuda:0', dtype=torch.float16)

1. 
2. 
3. 
,,
---after-> tensor([[-6.1584e-02, -1.2192e-02,  2.1008e-01,  ..., -2.4536e-02,
         -7.0618e-02, -9.3018e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.1133e+00, -1.0925e+02, -4.9875e+01,  ...,  2.4658e-01,
         -2.0593e-01,  9.4360e-02],
        ...,
        [-5.6328e+00, -4.0273e+00, -2.2119e-01,  ...,  3.0039e+00,
          1.1194e-01, -1.2773e+00],
        [ 5.2812e+00,  1.3018e+00,  6.4531e+00,  ...,  1.0039e+00,
          1.0039e+00,  1.0234e+00],
        [ 2.0352e+00,  4.0112e-01,  5.7852e+00,  ..., -2.3303e-01,
          2.1660e+00,  6.8262e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:12:59] "POST /inference/generate HTTP/1.1" 200 5141

---- {'role': 'system', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-6.1584e-02, -1.2192e-02,  2.1008e-01,  ..., -2.4536e-02,
         -7.0618e-02, -9.3018e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.1133e+00, -1.0925e+02, -4.9875e+01,  ...,  2.4658e-01,
         -2.0593e-01,  9.4360e-02],
        ...,
        [-5.6328e+00, -4.0273e+00, -2.2119e-01,  ...,  3.0039e+00,
          1.1194e-01, -1.2773e+00],
        [ 5.2812e+00,  1.3018e+00,  6.4531e+00,  ...,  1.0039e+00,
          1.0039e+00,  1.0234e+00],
        [ 2.0352e+00,  4.0112e-01,  5.7852e+00,  ..., -2.3303e-01,
          2.1660e+00,  6.8262e-01]], device='cuda:0', dtype=torch.float16)

:
1. 
---after-> tensor([[-9.1736e-02, -3.1952e-02,  2.1338e-01,  ..., -3.2227e-02,
         -7.5378e-02, -8.8318e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [ 5.9375e-01, -8.4812e+01, -1.6531e+01,  ...,  2.0554e-02,
         -1.2378e-01,  7.2266e-01],
        ...,
        [-1.4727e+01, -2.2891e+00,  4.7070e+00,  ...,  5.9648e+00,
          1.5439e+00,  1.1074e+00],
        [ 6.1875e+00,  2.0195e+00,  1.0430e+01,  ...,  1.0020e+00,
          1.0020e+00,  1.2637e+00],
        [ 6.4795e-01, -5.4541e-01,  5.1875e+00,  ...,  1.5454e-01,
          1.5684e+00,  2.4878e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:14:04] "POST /inference/generate HTTP/1.1" 200 4832

---- {'role': 'response', 'text': 'Triumph\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-9.1736e-02, -3.1952e-02,  2.1338e-01,  ..., -3.2227e-02,
         -7.5378e-02, -8.8318e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [ 5.9375e-01, -8.4812e+01, -1.6531e+01,  ...,  2.0554e-02,
         -1.2378e-01,  7.2266e-01],
        ...,
        [-1.4727e+01, -2.2891e+00,  4.7070e+00,  ...,  5.9648e+00,
          1.5439e+00,  1.1074e+00],
        [ 6.1875e+00,  2.0195e+00,  1.0430e+01,  ...,  1.0020e+00,
          1.0020e+00,  1.2637e+00],
        [ 6.4795e-01, -5.4541e-01,  5.1875e+00,  ...,  1.5454e-01,
          1.5684e+00,  2.4878e-01]], device='cuda:0', dtype=torch.float16)
 
1. 
---after-> tensor([[-9.2346e-02, -3.6072e-02,  2.2229e-01,  ..., -1.4420e-02,
         -8.0078e-02, -1.0480e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-7.3145e-01, -6.9812e+01, -3.9406e+01,  ...,  2.0554e-02,
         -1.2378e-01,  7.2266e-01],
        ...,
        [-1.1742e+01, -8.8750e+00,  7.3633e+00,  ...,  6.2617e+00,
          3.2676e+00,  1.0254e-01],
        [ 4.0547e+00,  2.9023e+00,  1.5266e+01,  ...,  1.0176e+00,
          1.0088e+00,  1.8887e+00],
        [ 8.6865e-01,  3.6719e-01,  4.6445e+00,  ..., -1.7441e+00,
         -1.0977e+00,  1.0547e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:14:51] "POST /inference/generate HTTP/1.1" 200 4109

---- {'role': 'system', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-9.2346e-02, -3.6072e-02,  2.2229e-01,  ..., -1.4420e-02,
         -8.0078e-02, -1.0480e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-7.3145e-01, -6.9812e+01, -3.9406e+01,  ...,  2.0554e-02,
         -1.2378e-01,  7.2266e-01],
        ...,
        [-1.1742e+01, -8.8750e+00,  7.3633e+00,  ...,  6.2617e+00,
          3.2676e+00,  1.0254e-01],
        [ 4.0547e+00,  2.9023e+00,  1.5266e+01,  ...,  1.0176e+00,
          1.0088e+00,  1.8887e+00],
        [ 8.6865e-01,  3.6719e-01,  4.6445e+00,  ..., -1.7441e+00,
         -1.0977e+00,  1.0547e+00]], device='cuda:0', dtype=torch.float16)

1. 
2. 
3. 
4. 
5. 
6. 

---after-> tensor([[-7.1289e-02,  6.2485e-03,  1.9275e-01,  ..., -4.0222e-02,
         -7.0618e-02, -8.3740e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-2.1133e+00, -1.3038e+02, -3.9688e+01,  ...,  3.5986e-01,
         -5.2539e-01,  5.4932e-01],
        ...,
        [-1.0797e+01, -2.3145e+00, -8.3516e+00,  ...,  3.4844e+00,
          8.5547e-01, -4.7148e+00],
        [ 4.9297e+00,  2.2031e+00,  9.1562e+00,  ...,  1.0078e+00,
          1.0020e+00,  1.2949e+00],
        [ 2.9902e+00, -1.5049e+00,  5.2461e+00,  ..., -1.2773e+00,
          1.9150e+00, -7.3738e-03]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:15:56] "POST /inference/generate HTTP/1.1" 200 5846

---- {'role': 'system', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-7.1289e-02,  6.2485e-03,  1.9275e-01,  ..., -4.0222e-02,
         -7.0618e-02, -8.3740e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-2.1133e+00, -1.3038e+02, -3.9688e+01,  ...,  3.5986e-01,
         -5.2539e-01,  5.4932e-01],
        ...,
        [-1.0797e+01, -2.3145e+00, -8.3516e+00,  ...,  3.4844e+00,
          8.5547e-01, -4.7148e+00],
        [ 4.9297e+00,  2.2031e+00,  9.1562e+00,  ...,  1.0078e+00,
          1.0020e+00,  1.2949e+00],
        [ 2.9902e+00, -1.5049e+00,  5.2461e+00,  ..., -1.2773e+00,
          1.9150e+00, -7.3738e-03]], device='cuda:0', dtype=torch.float16)

1. 
2. 
3. 
4. 
5. 
6. SM
7. 
8. 
9. 
10. 

---after-> tensor([[-8.8196e-02, -1.1549e-03,  2.2119e-01,  ..., -3.7506e-02,
         -6.3416e-02, -1.1035e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-2.2344e+00, -9.1812e+01, -1.3281e+01,  ...,  7.6721e-02,
         -2.6099e-01,  2.1851e-01],
        ...,
        [-7.9883e+00, -4.9648e+00, -5.9492e+00,  ...,  2.7422e+00,
          2.7793e+00, -3.3828e+00],
        [ 1.7295e+00,  2.2383e+00,  8.6406e+00,  ...,  1.0234e+00,
          1.0430e+00,  1.0283e+00],
        [ 4.8398e+00, -8.5059e-01,  5.0664e+00,  ..., -8.8672e-01,
          2.6055e+00, -7.5391e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:17:07] "POST /inference/generate HTTP/1.1" 200 5498

---- {'role': 'system', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-8.8196e-02, -1.1549e-03,  2.2119e-01,  ..., -3.7506e-02,
         -6.3416e-02, -1.1035e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-2.2344e+00, -9.1812e+01, -1.3281e+01,  ...,  7.6721e-02,
         -2.6099e-01,  2.1851e-01],
        ...,
        [-7.9883e+00, -4.9648e+00, -5.9492e+00,  ...,  2.7422e+00,
          2.7793e+00, -3.3828e+00],
        [ 1.7295e+00,  2.2383e+00,  8.6406e+00,  ...,  1.0234e+00,
          1.0430e+00,  1.0283e+00],
        [ 4.8398e+00, -8.5059e-01,  5.0664e+00,  ..., -8.8672e-01,
          2.6055e+00, -7.5391e-01]], device='cuda:0', dtype=torch.float16)
 
1. 
2. 
3. 
4. 

---after-> tensor([[-8.0444e-02, -2.6184e-02,  2.1777e-01,  ..., -3.4454e-02,
         -6.1737e-02, -1.0522e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-2.3750e+00, -9.6500e+01, -4.3969e+01,  ..., -3.5278e-02,
         -2.0874e-01,  8.9111e-02],
        ...,
        [-7.7695e+00, -4.8867e+00, -4.4219e+00,  ...,  1.9268e+00,
          1.8652e+00, -7.6367e-01],
        [ 1.9932e+00,  2.3594e+00,  5.2578e+00,  ...,  1.0127e+00,
          1.0469e+00,  1.2969e+00],
        [ 3.3574e+00, -1.2656e+00,  5.9922e+00,  ..., -5.8936e-01,
          8.7842e-01,  1.3184e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:18:20] "POST /inference/generate HTTP/1.1" 200 6920

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-8.0444e-02, -2.6184e-02,  2.1777e-01,  ..., -3.4454e-02,
         -6.1737e-02, -1.0522e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-2.3750e+00, -9.6500e+01, -4.3969e+01,  ..., -3.5278e-02,
         -2.0874e-01,  8.9111e-02],
        ...,
        [-7.7695e+00, -4.8867e+00, -4.4219e+00,  ...,  1.9268e+00,
          1.8652e+00, -7.6367e-01],
        [ 1.9932e+00,  2.3594e+00,  5.2578e+00,  ...,  1.0127e+00,
          1.0469e+00,  1.2969e+00],
        [ 3.3574e+00, -1.2656e+00,  5.9922e+00,  ..., -5.8936e-01,
          8.7842e-01,  1.3184e+00]], device='cuda:0', dtype=torch.float16)






1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 

---after-> tensor([[-7.9346e-02, -2.4509e-03,  2.0813e-01,  ..., -2.7267e-02,
         -6.4880e-02, -9.3201e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.1670e+00, -7.2062e+01, -4.0625e+01,  ..., -2.8589e-01,
         -1.0651e-02,  3.1445e-01],
        ...,
        [-7.0078e+00, -5.5625e+00, -8.9609e+00,  ...,  2.0723e+00,
          3.4746e+00, -1.2744e+00],
        [ 6.2969e+00,  2.9727e+00,  9.4766e+00,  ...,  1.0039e+00,
          1.0186e+00,  1.4756e+00],
        [ 1.6777e+00, -2.8027e+00,  5.2031e+00,  ..., -1.8389e+00,
          1.0205e+00,  6.1377e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:18:58] "POST /inference/generate HTTP/1.1" 200 3915

---- {'role': 'system', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-7.9346e-02, -2.4509e-03,  2.0813e-01,  ..., -2.7267e-02,
         -6.4880e-02, -9.3201e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.1670e+00, -7.2062e+01, -4.0625e+01,  ..., -2.8589e-01,
         -1.0651e-02,  3.1445e-01],
        ...,
        [-7.0078e+00, -5.5625e+00, -8.9609e+00,  ...,  2.0723e+00,
          3.4746e+00, -1.2744e+00],
        [ 6.2969e+00,  2.9727e+00,  9.4766e+00,  ...,  1.0039e+00,
          1.0186e+00,  1.4756e+00],
        [ 1.6777e+00, -2.8027e+00,  5.2031e+00,  ..., -1.8389e+00,
          1.0205e+00,  6.1377e-01]], device='cuda:0', dtype=torch.float16)





dinner alive
system

---after-> tensor([[-3.5095e-02, -4.3976e-02,  3.4326e-01,  ..., -1.0547e-01,
         -2.0142e-01, -4.2084e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-6.6797e-01, -1.0475e+02, -4.5375e+01,  ...,  2.2351e-01,
         -1.0797e-01,  1.4062e+00],
        ...,
        [-3.6582e+00, -5.7578e+00, -2.3945e+00,  ..., -1.9702e-01,
          8.3545e-01, -3.5898e+00],
        [ 1.8525e+00,  2.1934e+00,  9.0938e+00,  ...,  1.0469e+00,
          1.0186e+00,  1.4219e+00],
        [ 2.6797e+00, -8.9453e-01,  4.6055e+00,  ..., -4.6777e-01,
          2.6680e+00,  1.2285e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:19:54] "POST /inference/generate HTTP/1.1" 200 5053

---- {'role': 'system', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-3.5095e-02, -4.3976e-02,  3.4326e-01,  ..., -1.0547e-01,
         -2.0142e-01, -4.2084e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-6.6797e-01, -1.0475e+02, -4.5375e+01,  ...,  2.2351e-01,
         -1.0797e-01,  1.4062e+00],
        ...,
        [-3.6582e+00, -5.7578e+00, -2.3945e+00,  ..., -1.9702e-01,
          8.3545e-01, -3.5898e+00],
        [ 1.8525e+00,  2.1934e+00,  9.0938e+00,  ...,  1.0469e+00,
          1.0186e+00,  1.4219e+00],
        [ 2.6797e+00, -8.9453e-01,  4.6055e+00,  ..., -4.6777e-01,
          2.6680e+00,  1.2285e+00]], device='cuda:0', dtype=torch.float16)

system
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 

---after-> tensor([[-7.5684e-02, -1.0109e-02,  2.1106e-01,  ..., -1.8845e-02,
         -6.6040e-02, -9.6436e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.7881e+00, -9.4875e+01, -2.8000e+01,  ...,  8.8745e-02,
         -2.1021e-01,  4.4897e-01],
        ...,
        [-9.0938e+00, -9.1016e-01,  2.4004e+00,  ...,  2.5605e+00,
          1.3271e+00, -6.7578e-01],
        [ 3.6055e+00,  1.5166e+00,  4.9453e+00,  ...,  1.0215e+00,
          1.0049e+00,  1.0703e+00],
        [ 8.6768e-01, -1.7070e+00,  5.5469e+00,  ..., -1.7793e+00,
          4.2344e+00,  2.0449e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:21:06] "POST /inference/generate HTTP/1.1" 200 6009

---- {'role': 'system', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-7.5684e-02, -1.0109e-02,  2.1106e-01,  ..., -1.8845e-02,
         -6.6040e-02, -9.6436e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.7881e+00, -9.4875e+01, -2.8000e+01,  ...,  8.8745e-02,
         -2.1021e-01,  4.4897e-01],
        ...,
        [-9.0938e+00, -9.1016e-01,  2.4004e+00,  ...,  2.5605e+00,
          1.3271e+00, -6.7578e-01],
        [ 3.6055e+00,  1.5166e+00,  4.9453e+00,  ...,  1.0215e+00,
          1.0049e+00,  1.0703e+00],
        [ 8.6768e-01, -1.7070e+00,  5.5469e+00,  ..., -1.7793e+00,
          4.2344e+00,  2.0449e+00]], device='cuda:0', dtype=torch.float16)
 
:
1. 
---after-> tensor([[-7.5867e-02, -4.6616e-03,  2.3596e-01,  ...,  7.3700e-03,
         -6.8787e-02, -9.1858e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [ 5.4932e-02, -1.1612e+02, -6.9062e+01,  ...,  2.2131e-01,
         -1.9751e-01,  3.6426e-01],
        ...,
        [-7.7148e+00, -5.1328e+00,  4.8398e+00,  ...,  2.6230e+00,
          4.5781e+00, -9.8145e-01],
        [ 2.4922e+00,  3.1230e+00,  8.3438e+00,  ...,  1.2197e+00,
          1.1025e+00,  1.1592e+00],
        [ 2.2188e+00,  2.2485e-01,  5.3906e+00,  ..., -2.8945e+00,
         -9.0820e-02, -1.5393e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:21:34] "POST /inference/generate HTTP/1.1" 200 4030

---- {'role': 'system', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-7.5867e-02, -4.6616e-03,  2.3596e-01,  ...,  7.3700e-03,
         -6.8787e-02, -9.1858e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [ 5.4932e-02, -1.1612e+02, -6.9062e+01,  ...,  2.2131e-01,
         -1.9751e-01,  3.6426e-01],
        ...,
        [-7.7148e+00, -5.1328e+00,  4.8398e+00,  ...,  2.6230e+00,
          4.5781e+00, -9.8145e-01],
        [ 2.4922e+00,  3.1230e+00,  8.3438e+00,  ...,  1.2197e+00,
          1.1025e+00,  1.1592e+00],
        [ 2.2188e+00,  2.2485e-01,  5.3906e+00,  ..., -2.8945e+00,
         -9.0820e-02, -1.5393e-01]], device='cuda:0', dtype=torch.float16)

1. 
---after-> tensor([[-7.0923e-02, -1.6785e-03,  2.2607e-01,  ...,  1.3390e-03,
         -6.8542e-02, -8.3069e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-7.8857e-01, -4.5281e+01, -5.1125e+01,  ...,  2.2131e-01,
         -1.9751e-01,  3.6426e-01],
        ...,
        [-6.1875e+00, -3.7402e+00, -2.8652e+00,  ...,  2.9023e+00,
          3.3125e+00, -2.1992e+00],
        [ 2.0742e+00,  3.0840e+00,  1.3953e+01,  ...,  1.3389e+00,
          1.0264e+00,  1.4395e+00],
        [ 2.1133e+00, -1.7798e-01,  4.9062e+00,  ..., -3.2520e+00,
          5.3223e-01, -5.7324e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:22:29] "POST /inference/generate HTTP/1.1" 200 3906

---- {'role': 'system', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-7.0923e-02, -1.6785e-03,  2.2607e-01,  ...,  1.3390e-03,
         -6.8542e-02, -8.3069e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-7.8857e-01, -4.5281e+01, -5.1125e+01,  ...,  2.2131e-01,
         -1.9751e-01,  3.6426e-01],
        ...,
        [-6.1875e+00, -3.7402e+00, -2.8652e+00,  ...,  2.9023e+00,
          3.3125e+00, -2.1992e+00],
        [ 2.0742e+00,  3.0840e+00,  1.3953e+01,  ...,  1.3389e+00,
          1.0264e+00,  1.4395e+00],
        [ 2.1133e+00, -1.7798e-01,  4.9062e+00,  ..., -3.2520e+00,
          5.3223e-01, -5.7324e-01]], device='cuda:0', dtype=torch.float16)
 backslash 
1. 
2. 
3. 
4. 
5. 

,,,,
,,

,
,:)

---after-> tensor([[-4.7729e-02,  2.3209e-02,  2.7197e-01,  ..., -8.2016e-03,
          7.3669e-02,  9.1309e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-1.5010e+00, -3.8281e+01, -4.6625e+01,  ...,  2.0642e-01,
         -2.1045e-01,  1.5332e+00],
        ...,
        [-8.4531e+00, -1.2139e+00, -1.1578e+01,  ...,  2.1250e+00,
          1.7656e+00, -3.4238e+00],
        [ 2.1016e+00,  1.7451e+00,  1.3914e+01,  ...,  1.0283e+00,
          1.0264e+00,  1.6797e+00],
        [ 3.3770e+00, -8.6230e-01,  4.3906e+00,  ..., -3.6289e+00,
          2.3398e+00,  1.1338e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:23:26] "POST /inference/generate HTTP/1.1" 200 6362

---- {'role': 'system', 'text': 'Gucci\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-4.7729e-02,  2.3209e-02,  2.7197e-01,  ..., -8.2016e-03,
          7.3669e-02,  9.1309e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-1.5010e+00, -3.8281e+01, -4.6625e+01,  ...,  2.0642e-01,
         -2.1045e-01,  1.5332e+00],
        ...,
        [-8.4531e+00, -1.2139e+00, -1.1578e+01,  ...,  2.1250e+00,
          1.7656e+00, -3.4238e+00],
        [ 2.1016e+00,  1.7451e+00,  1.3914e+01,  ...,  1.0283e+00,
          1.0264e+00,  1.6797e+00],
        [ 3.3770e+00, -8.6230e-01,  4.3906e+00,  ..., -3.6289e+00,
          2.3398e+00,  1.1338e+00]], device='cuda:0', dtype=torch.float16)

1. 
2. 
3. 
4. 
5. 

---after-> tensor([[-8.0811e-02, -7.6027e-03,  2.0203e-01,  ..., -2.0004e-02,
         -6.7688e-02, -9.1675e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-2.5176e+00, -8.9688e+01, -3.3562e+01,  ..., -4.4006e-02,
         -1.6968e-01,  2.4170e-01],
        ...,
        [-2.3172e+01, -3.6152e+00, -2.7520e+00,  ...,  2.5996e+00,
          1.8545e+00, -1.3799e+00],
        [ 5.9688e+00,  2.5234e+00,  6.1484e+00,  ...,  1.0117e+00,
          1.0039e+00,  1.1641e+00],
        [ 1.5449e+00, -1.5439e+00,  5.0859e+00,  ..., -2.0000e+00,
          2.1875e+00, -4.3237e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:24:17] "POST /inference/generate HTTP/1.1" 200 5401

---- {'role': 'request', 'text': ' \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-8.0811e-02, -7.6027e-03,  2.0203e-01,  ..., -2.0004e-02,
         -6.7688e-02, -9.1675e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-2.5176e+00, -8.9688e+01, -3.3562e+01,  ..., -4.4006e-02,
         -1.6968e-01,  2.4170e-01],
        ...,
        [-2.3172e+01, -3.6152e+00, -2.7520e+00,  ...,  2.5996e+00,
          1.8545e+00, -1.3799e+00],
        [ 5.9688e+00,  2.5234e+00,  6.1484e+00,  ...,  1.0117e+00,
          1.0039e+00,  1.1641e+00],
        [ 1.5449e+00, -1.5439e+00,  5.0859e+00,  ..., -2.0000e+00,
          2.1875e+00, -4.3237e-01]], device='cuda:0', dtype=torch.float16)

1. 
2. 
3. 

---after-> tensor([[-1.1421e-02, -6.2622e-02,  3.7769e-01,  ..., -1.0327e-01,
         -1.8689e-01, -5.8807e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-2.8535e+00, -5.1188e+01, -4.2250e+01,  ...,  2.3584e-01,
         -9.1919e-02,  9.6924e-01],
        ...,
        [-8.4922e+00, -3.2266e+00, -8.3125e+00,  ..., -1.4490e-01,
         -8.0811e-01, -3.5693e-01],
        [ 3.3105e+00,  1.4463e+00,  6.0547e+00,  ...,  1.0068e+00,
          1.0234e+00,  1.3301e+00],
        [ 1.5420e+00, -2.5537e-01,  5.3516e+00,  ..., -3.0273e+00,
          9.0576e-01,  1.9893e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:24:50] "POST /inference/generate HTTP/1.1" 200 1943

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-1.1421e-02, -6.2622e-02,  3.7769e-01,  ..., -1.0327e-01,
         -1.8689e-01, -5.8807e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-2.8535e+00, -5.1188e+01, -4.2250e+01,  ...,  2.3584e-01,
         -9.1919e-02,  9.6924e-01],
        ...,
        [-8.4922e+00, -3.2266e+00, -8.3125e+00,  ..., -1.4490e-01,
         -8.0811e-01, -3.5693e-01],
        [ 3.3105e+00,  1.4463e+00,  6.0547e+00,  ...,  1.0068e+00,
          1.0234e+00,  1.3301e+00],
        [ 1.5420e+00, -2.5537e-01,  5.3516e+00,  ..., -3.0273e+00,
          9.0576e-01,  1.9893e+00]], device='cuda:0', dtype=torch.float16)

:
,,
,,.
,,,
,,,
,,,,


---after-> tensor([[-3.6499e-02, -1.8524e-02,  2.5342e-01,  ..., -1.8417e-02,
          1.1127e-01,  8.2153e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-3.6953e+00, -2.2234e+01, -6.9125e+01,  ..., -2.1899e-01,
         -2.2656e-01,  1.3906e+00],
        ...,
        [-4.2891e+00, -5.3008e+00, -5.4805e+00,  ...,  3.1172e+00,
         -2.6260e-02, -2.6348e+00],
        [ 1.7480e+00,  2.4922e+00,  8.0000e+00,  ...,  1.0566e+00,
          1.0186e+00,  1.1094e+00],
        [ 2.9785e+00, -1.3926e+00,  4.6836e+00,  ..., -2.6816e+00,
          2.5645e+00, -2.3865e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:25:15] "POST /inference/generate HTTP/1.1" 200 2666

---- {'role': 'system', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-3.6499e-02, -1.8524e-02,  2.5342e-01,  ..., -1.8417e-02,
          1.1127e-01,  8.2153e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-3.6953e+00, -2.2234e+01, -6.9125e+01,  ..., -2.1899e-01,
         -2.2656e-01,  1.3906e+00],
        ...,
        [-4.2891e+00, -5.3008e+00, -5.4805e+00,  ...,  3.1172e+00,
         -2.6260e-02, -2.6348e+00],
        [ 1.7480e+00,  2.4922e+00,  8.0000e+00,  ...,  1.0566e+00,
          1.0186e+00,  1.1094e+00],
        [ 2.9785e+00, -1.3926e+00,  4.6836e+00,  ..., -2.6816e+00,
          2.5645e+00, -2.3865e-01]], device='cuda:0', dtype=torch.float16)
 Bachelor:
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. ,
11. ,,
12. ,
,,,,Vengeance will not make up for his loss,
---after-> tensor([[-8.8196e-02, -1.4465e-02,  1.9897e-01,  ..., -4.6326e-02,
         -6.5552e-02, -1.1151e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-4.3604e-01, -1.1956e+02, -6.9938e+01,  ...,  2.4854e-01,
         -2.8589e-01,  3.8647e-01],
        ...,
        [-2.2734e+01, -9.9531e+00, -8.1875e+00,  ...,  3.7988e+00,
          1.6855e+00, -6.9434e-01],
        [ 5.5859e+00,  2.9727e+00,  6.5039e+00,  ...,  1.0029e+00,
          1.0010e+00,  1.1914e+00],
        [ 1.0371e+00, -2.5215e+00,  4.9648e+00,  ..., -7.8369e-01,
          2.6523e+00,  5.9668e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:26:30] "POST /inference/generate HTTP/1.1" 200 6688

---- {'role': 'system', 'text': '~~\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-8.8196e-02, -1.4465e-02,  1.9897e-01,  ..., -4.6326e-02,
         -6.5552e-02, -1.1151e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-4.3604e-01, -1.1956e+02, -6.9938e+01,  ...,  2.4854e-01,
         -2.8589e-01,  3.8647e-01],
        ...,
        [-2.2734e+01, -9.9531e+00, -8.1875e+00,  ...,  3.7988e+00,
          1.6855e+00, -6.9434e-01],
        [ 5.5859e+00,  2.9727e+00,  6.5039e+00,  ...,  1.0029e+00,
          1.0010e+00,  1.1914e+00],
        [ 1.0371e+00, -2.5215e+00,  4.9648e+00,  ..., -7.8369e-01,
          2.6523e+00,  5.9668e-01]], device='cuda:0', dtype=torch.float16)





---after-> tensor([[-8.8562e-02, -1.4977e-02,  2.0874e-01,  ..., -1.1902e-02,
         -6.6772e-02, -1.0510e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.4600e-01, -9.2625e+01, -3.7844e+01,  ...,  2.0837e-01,
         -6.2109e-01,  3.5645e-01],
        ...,
        [-6.2852e+00, -7.4844e+00, -2.0918e+00,  ...,  3.9980e+00,
          7.7539e-01, -4.0781e+00],
        [ 2.5762e+00,  3.4316e+00,  4.8984e+00,  ...,  1.0146e+00,
          1.0029e+00,  1.9551e+00],
        [ 3.6172e+00, -1.7949e+00,  5.5352e+00,  ..., -1.2549e+00,
          2.5332e+00, -9.3457e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:27:18] "POST /inference/generate HTTP/1.1" 200 5290

---- {'role': 'system', 'text': 'PradaPrada\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-8.8562e-02, -1.4977e-02,  2.0874e-01,  ..., -1.1902e-02,
         -6.6772e-02, -1.0510e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.4600e-01, -9.2625e+01, -3.7844e+01,  ...,  2.0837e-01,
         -6.2109e-01,  3.5645e-01],
        ...,
        [-6.2852e+00, -7.4844e+00, -2.0918e+00,  ...,  3.9980e+00,
          7.7539e-01, -4.0781e+00],
        [ 2.5762e+00,  3.4316e+00,  4.8984e+00,  ...,  1.0146e+00,
          1.0029e+00,  1.9551e+00],
        [ 3.6172e+00, -1.7949e+00,  5.5352e+00,  ..., -1.2549e+00,
          2.5332e+00, -9.3457e-01]], device='cuda:0', dtype=torch.float16)









---after-> tensor([[-6.5186e-02,  2.1545e-02,  2.0264e-01,  ..., -8.1940e-03,
          4.5349e-02,  1.5173e-01],
        [-2.6398e-02, -1.1835e-01,  6.3721e-02,  ...,  1.6406e-01,
         -1.7365e-02,  1.7786e-01],
        [ 2.1692e-01, -8.1250e+01, -5.1719e+01,  ..., -4.0967e-01,
         -1.6113e-01, -3.0493e-01],
        ...,
        [-9.1094e+00, -9.0547e+00, -8.6562e+00,  ...,  4.5234e+00,
          7.4072e-01,  1.8584e+00],
        [ 4.9062e+00,  2.9551e+00,  6.5977e+00,  ...,  1.1240e+00,
          1.1006e+00,  1.0723e+00],
        [ 5.6055e-01, -1.0762e+00,  5.3203e+00,  ..., -2.4102e+00,
          1.3281e+00,  2.1816e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:28:21] "POST /inference/generate HTTP/1.1" 200 7380

---- {'role': 'request', 'text': 'User \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-6.5186e-02,  2.1545e-02,  2.0264e-01,  ..., -8.1940e-03,
          4.5349e-02,  1.5173e-01],
        [-2.6398e-02, -1.1835e-01,  6.3721e-02,  ...,  1.6406e-01,
         -1.7365e-02,  1.7786e-01],
        [ 2.1692e-01, -8.1250e+01, -5.1719e+01,  ..., -4.0967e-01,
         -1.6113e-01, -3.0493e-01],
        ...,
        [-9.1094e+00, -9.0547e+00, -8.6562e+00,  ...,  4.5234e+00,
          7.4072e-01,  1.8584e+00],
        [ 4.9062e+00,  2.9551e+00,  6.5977e+00,  ...,  1.1240e+00,
          1.1006e+00,  1.0723e+00],
        [ 5.6055e-01, -1.0762e+00,  5.3203e+00,  ..., -2.4102e+00,
          1.3281e+00,  2.1816e+00]], device='cuda:0', dtype=torch.float16)

1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 




 understood

---after-> tensor([[-3.5095e-02, -3.7170e-02,  3.5547e-01,  ..., -9.8877e-02,
         -2.0911e-01, -2.9999e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-3.7305e+00, -9.9312e+01, -4.1062e+01,  ...,  3.6304e-01,
         -5.1709e-01,  1.1943e+00],
        ...,
        [-3.7598e+00, -4.7383e+00, -5.3828e+00,  ...,  6.6016e-01,
          2.2305e+00, -5.5977e+00],
        [ 3.0996e+00,  1.3896e+00,  6.3906e+00,  ...,  1.3613e+00,
          1.0098e+00,  1.3896e+00],
        [ 1.6484e+00,  3.5669e-01,  5.4688e+00,  ..., -2.1270e+00,
          1.5039e+00,  1.7456e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:29:06] "POST /inference/generate HTTP/1.1" 200 3618

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-3.5095e-02, -3.7170e-02,  3.5547e-01,  ..., -9.8877e-02,
         -2.0911e-01, -2.9999e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-3.7305e+00, -9.9312e+01, -4.1062e+01,  ...,  3.6304e-01,
         -5.1709e-01,  1.1943e+00],
        ...,
        [-3.7598e+00, -4.7383e+00, -5.3828e+00,  ...,  6.6016e-01,
          2.2305e+00, -5.5977e+00],
        [ 3.0996e+00,  1.3896e+00,  6.3906e+00,  ...,  1.3613e+00,
          1.0098e+00,  1.3896e+00],
        [ 1.6484e+00,  3.5669e-01,  5.4688e+00,  ..., -2.1270e+00,
          1.5039e+00,  1.7456e-01]], device='cuda:0', dtype=torch.float16)

1. 
2. 
3. 
4. 
5. 
6. 
7. niece
8. 
9. 
10. 
11. 
12. 
13. 
14. 
15. 
16. 
17. 
18. 
---after-> tensor([[-7.5806e-02, -2.0004e-02,  1.9604e-01,  ..., -4.8157e-02,
         -6.0883e-02, -1.0126e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [ 1.2783e+00, -1.1000e+02, -5.4062e+01,  ...,  3.0811e-01,
         -1.7163e-01,  4.2090e-01],
        ...,
        [-6.0117e+00, -6.1680e+00, -8.5859e+00,  ...,  2.8711e+00,
          4.8999e-01, -1.1943e+00],
        [ 1.6641e+00,  3.1367e+00,  1.2992e+01,  ...,  1.0117e+00,
          1.0303e+00,  1.0293e+00],
        [ 2.8457e+00, -3.0176e+00,  4.7812e+00,  ..., -1.3145e+00,
          1.2383e+00,  2.4043e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:30:00] "POST /inference/generate HTTP/1.1" 200 5924

init_state None

state-> tensor([[-7.5806e-02, -2.0004e-02,  1.9604e-01,  ..., -4.8157e-02,
         -6.0883e-02, -1.0126e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [ 1.2783e+00, -1.1000e+02, -5.4062e+01,  ...,  3.0811e-01,
         -1.7163e-01,  4.2090e-01],
        ...,
        [-6.0117e+00, -6.1680e+00, -8.5859e+00,  ...,  2.8711e+00,
          4.8999e-01, -1.1943e+00],
        [ 1.6641e+00,  3.1367e+00,  1.2992e+01,  ...,  1.0117e+00,
          1.0303e+00,  1.0293e+00],
        [ 2.8457e+00, -3.0176e+00,  4.7812e+00,  ..., -1.3145e+00,
          1.2383e+00,  2.4043e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 02:30:55] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'system R\n       \n    \n\n\n\n   \n\n ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None
system 

---after-> tensor([[ 4.2992e-03, -6.3843e-02,  3.5669e-01,  ..., -1.1792e-01,
         -2.0142e-01, -7.0374e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-3.4805e+00, -7.0875e+01, -3.2562e+01,  ...,  3.7158e-01,
         -4.1162e-01,  9.4482e-01],
        ...,
        [-1.2625e+01,  1.0352e+00,  1.9619e+00,  ...,  3.3027e+00,
          4.1602e+00, -4.9648e+00],
        [ 6.4258e+00,  2.3027e+00,  6.3633e+00,  ...,  1.0029e+00,
          1.0010e+00,  1.1465e+00],
        [ 2.4336e+00,  6.6992e-01,  6.2539e+00,  ...,  1.6670e+00,
          3.0215e+00,  2.1660e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:31:24] "POST /inference/generate HTTP/1.1" 200 7851

init_state None

state-> tensor([[ 4.2992e-03, -6.3843e-02,  3.5669e-01,  ..., -1.1792e-01,
         -2.0142e-01, -7.0374e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-3.4805e+00, -7.0875e+01, -3.2562e+01,  ...,  3.7158e-01,
         -4.1162e-01,  9.4482e-01],
        ...,
        [-1.2625e+01,  1.0352e+00,  1.9619e+00,  ...,  3.3027e+00,
          4.1602e+00, -4.9648e+00],
        [ 6.4258e+00,  2.3027e+00,  6.3633e+00,  ...,  1.0029e+00,
          1.0010e+00,  1.1465e+00],
        [ 2.4336e+00,  6.6992e-01,  6.2539e+00,  ...,  1.6670e+00,
          3.0215e+00,  2.1660e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 02:32:11] "POST /state/reset HTTP/1.1" 200 21

init_state None

state-> None

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 02:34:31] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'response', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None

---after-> tensor([[-4.1290e-02,  5.1910e-02,  2.6660e-01,  ..., -5.1147e-02,
         -5.4688e-02, -1.0669e-01],
        [-2.5284e-02, -2.9617e-02,  2.0459e-01,  ...,  3.8177e-02,
         -1.4319e-01, -1.0254e-01],
        [ 1.5557e+00, -4.7750e+01, -5.4312e+01,  ...,  5.9277e-01,
         -4.2090e-01, -2.1399e-01],
        ...,
        [-9.2578e+00, -1.4406e+01, -4.6562e+00,  ...,  1.6768e+00,
          3.7305e+00, -4.3906e+00],
        [ 2.3086e+00,  4.3633e+00,  6.0117e+00,  ...,  1.0146e+00,
          1.0000e+00,  1.0518e+00],
        [ 1.7324e+00,  1.4697e-01,  6.1328e+00,  ..., -5.8301e-01,
          5.9453e+00,  3.0918e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:34:46] "POST /inference/generate HTTP/1.1" 200 4635

---- {'role': 'response', 'text': 'Triumph\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-4.1290e-02,  5.1910e-02,  2.6660e-01,  ..., -5.1147e-02,
         -5.4688e-02, -1.0669e-01],
        [-2.5284e-02, -2.9617e-02,  2.0459e-01,  ...,  3.8177e-02,
         -1.4319e-01, -1.0254e-01],
        [ 1.5557e+00, -4.7750e+01, -5.4312e+01,  ...,  5.9277e-01,
         -4.2090e-01, -2.1399e-01],
        ...,
        [-9.2578e+00, -1.4406e+01, -4.6562e+00,  ...,  1.6768e+00,
          3.7305e+00, -4.3906e+00],
        [ 2.3086e+00,  4.3633e+00,  6.0117e+00,  ...,  1.0146e+00,
          1.0000e+00,  1.0518e+00],
        [ 1.7324e+00,  1.4697e-01,  6.1328e+00,  ..., -5.8301e-01,
          5.9453e+00,  3.0918e+00]], device='cuda:0', dtype=torch.float16)

---after-> tensor([[-6.4758e-02, -2.3788e-02,  2.0142e-01,  ..., -4.0283e-02,
         -8.3618e-02, -1.0571e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.8555e+00, -4.3781e+01, -4.4188e+01,  ...,  1.6870e-01,
         -7.7576e-02,  8.4305e-03],
        ...,
        [-5.1992e+00,  5.2246e-01, -5.8281e+00,  ...,  1.8115e+00,
          9.1846e-01, -5.7617e+00],
        [ 1.9375e+00,  2.0586e+00,  5.6094e+00,  ...,  1.0049e+00,
          1.0029e+00,  1.0400e+00],
        [ 1.9209e+00, -8.5645e-01,  6.5586e+00,  ...,  1.0742e+00,
          4.7773e+00,  2.0430e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:35:17] "POST /inference/generate HTTP/1.1" 200 3835

init_state None

state-> tensor([[-6.4758e-02, -2.3788e-02,  2.0142e-01,  ..., -4.0283e-02,
         -8.3618e-02, -1.0571e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.8555e+00, -4.3781e+01, -4.4188e+01,  ...,  1.6870e-01,
         -7.7576e-02,  8.4305e-03],
        ...,
        [-5.1992e+00,  5.2246e-01, -5.8281e+00,  ...,  1.8115e+00,
          9.1846e-01, -5.7617e+00],
        [ 1.9375e+00,  2.0586e+00,  5.6094e+00,  ...,  1.0049e+00,
          1.0029e+00,  1.0400e+00],
        [ 1.9209e+00, -8.5645e-01,  6.5586e+00,  ...,  1.0742e+00,
          4.7773e+00,  2.0430e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 02:35:24] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'system R\n       \n    \n\n\n\n   \n\n ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemsystem \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'Triumph\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None

---after-> tensor([[-6.0120e-02,  4.3091e-02,  2.4927e-01,  ..., -7.3853e-02,
         -6.3721e-02, -9.9854e-02],
        [-2.5284e-02, -2.9617e-02,  2.0459e-01,  ...,  3.8177e-02,
         -1.4319e-01, -1.0254e-01],
        [ 1.5010e+00, -5.2438e+01, -5.4938e+01,  ...,  5.9277e-01,
         -4.2090e-01, -2.1399e-01],
        ...,
        [-1.6578e+01, -3.2441e+00, -4.5547e+00,  ...,  3.0898e+00,
          3.1699e+00, -2.5469e+00],
        [ 6.4375e+00,  1.4619e+00,  6.7344e+00,  ...,  1.0244e+00,
          1.0039e+00,  1.2256e+00],
        [-5.4443e-02,  1.8486e+00,  5.8633e+00,  ..., -3.6328e-01,
          3.7559e+00,  2.9688e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:36:20] "POST /inference/generate HTTP/1.1" 200 16742

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-6.0120e-02,  4.3091e-02,  2.4927e-01,  ..., -7.3853e-02,
         -6.3721e-02, -9.9854e-02],
        [-2.5284e-02, -2.9617e-02,  2.0459e-01,  ...,  3.8177e-02,
         -1.4319e-01, -1.0254e-01],
        [ 1.5010e+00, -5.2438e+01, -5.4938e+01,  ...,  5.9277e-01,
         -4.2090e-01, -2.1399e-01],
        ...,
        [-1.6578e+01, -3.2441e+00, -4.5547e+00,  ...,  3.0898e+00,
          3.1699e+00, -2.5469e+00],
        [ 6.4375e+00,  1.4619e+00,  6.7344e+00,  ...,  1.0244e+00,
          1.0039e+00,  1.2256e+00],
        [-5.4443e-02,  1.8486e+00,  5.8633e+00,  ..., -3.6328e-01,
          3.7559e+00,  2.9688e+00]], device='cuda:0', dtype=torch.float16)





---after-> tensor([[-3.4027e-02,  1.3306e-02,  2.8418e-01,  ..., -5.8556e-03,
          8.4595e-02,  9.8389e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-1.7275e+00, -5.1062e+01, -2.9844e+01,  ..., -2.1899e-01,
         -2.2656e-01,  1.3906e+00],
        ...,
        [-1.1523e-01, -7.0859e+00, -1.8156e+01,  ...,  1.0309e-01,
          4.3125e+00, -4.2500e+00],
        [ 3.8477e+00,  4.5312e+00,  1.2180e+01,  ...,  1.0176e+00,
          1.0078e+00,  1.0801e+00],
        [ 1.4639e+00, -3.2812e-01,  6.2930e+00,  ...,  8.0371e-01,
          4.2578e+00,  1.2510e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:36:56] "POST /inference/generate HTTP/1.1" 200 1142

init_state None

state-> tensor([[-3.4027e-02,  1.3306e-02,  2.8418e-01,  ..., -5.8556e-03,
          8.4595e-02,  9.8389e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-1.7275e+00, -5.1062e+01, -2.9844e+01,  ..., -2.1899e-01,
         -2.2656e-01,  1.3906e+00],
        ...,
        [-1.1523e-01, -7.0859e+00, -1.8156e+01,  ...,  1.0309e-01,
          4.3125e+00, -4.2500e+00],
        [ 3.8477e+00,  4.5312e+00,  1.2180e+01,  ...,  1.0176e+00,
          1.0078e+00,  1.0801e+00],
        [ 1.4639e+00, -3.2812e-01,  6.2930e+00,  ...,  8.0371e-01,
          4.2578e+00,  1.2510e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 02:37:07] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'system R\n       \n    \n\n\n\n   \n\n ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemsystem \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'Triumph\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': ' \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None
:
1. 
2. 
3. 
4. 
5. 

---after-> tensor([[-5.9357e-02, -3.4546e-02,  2.1106e-01,  ..., -3.4271e-02,
         -7.4829e-02, -1.1414e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.8652e+00, -1.0175e+02, -3.7406e+01,  ..., -1.2537e-01,
         -2.7246e-01,  2.1326e-01],
        ...,
        [-1.3773e+01, -8.4219e+00, -6.0234e+00,  ...,  2.9941e+00,
          2.0547e+00, -1.8740e+00],
        [ 3.8457e+00,  2.9805e+00,  9.8906e+00,  ...,  1.0576e+00,
          1.0029e+00,  1.2148e+00],
        [ 2.6328e+00,  4.1431e-01,  5.2930e+00,  ..., -5.2246e-01,
          4.3594e+00,  6.0596e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:38:15] "POST /inference/generate HTTP/1.1" 200 18672

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-5.9357e-02, -3.4546e-02,  2.1106e-01,  ..., -3.4271e-02,
         -7.4829e-02, -1.1414e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.8652e+00, -1.0175e+02, -3.7406e+01,  ..., -1.2537e-01,
         -2.7246e-01,  2.1326e-01],
        ...,
        [-1.3773e+01, -8.4219e+00, -6.0234e+00,  ...,  2.9941e+00,
          2.0547e+00, -1.8740e+00],
        [ 3.8457e+00,  2.9805e+00,  9.8906e+00,  ...,  1.0576e+00,
          1.0029e+00,  1.2148e+00],
        [ 2.6328e+00,  4.1431e-01,  5.2930e+00,  ..., -5.2246e-01,
          4.3594e+00,  6.0596e-01]], device='cuda:0', dtype=torch.float16)
  

---after-> tensor([[ 2.4261e-02, -7.9468e-02,  3.7451e-01,  ..., -1.4709e-01,
         -2.0117e-01, -4.5807e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-2.8359e+00, -9.8938e+01, -3.7531e+01,  ...,  2.8833e-01,
         -2.0166e-01,  1.4258e+00],
        ...,
        [-1.0406e+01, -5.5664e+00, -9.0078e+00,  ...,  2.0938e+00,
          2.4668e+00, -2.2090e+00],
        [ 3.3145e+00,  2.2871e+00,  5.4727e+00,  ...,  1.0088e+00,
          1.0205e+00,  1.0791e+00],
        [ 3.5586e+00,  9.9414e-01,  5.7812e+00,  ..., -2.2974e-01,
          1.3789e+00,  1.9219e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:39:52] "POST /inference/generate HTTP/1.1" 200 1060

init_state None

state-> tensor([[ 2.4261e-02, -7.9468e-02,  3.7451e-01,  ..., -1.4709e-01,
         -2.0117e-01, -4.5807e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-2.8359e+00, -9.8938e+01, -3.7531e+01,  ...,  2.8833e-01,
         -2.0166e-01,  1.4258e+00],
        ...,
        [-1.0406e+01, -5.5664e+00, -9.0078e+00,  ...,  2.0938e+00,
          2.4668e+00, -2.2090e+00],
        [ 3.3145e+00,  2.2871e+00,  5.4727e+00,  ...,  1.0088e+00,
          1.0205e+00,  1.0791e+00],
        [ 3.5586e+00,  9.9414e-01,  5.7812e+00,  ..., -2.2974e-01,
          1.3789e+00,  1.9219e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 02:40:06] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'system R\n       \n    \n\n\n\n   \n\n ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemsystem \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'Triumph\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': ' \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': ':\n1. \n2. \n3. \n4. \n5. \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'PradaPrada\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None




---after-> tensor([[-7.1716e-02, -1.4938e-02,  1.9775e-01,  ..., -3.9337e-02,
         -8.1543e-02, -9.9426e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.3892e-01, -8.3000e+01, -5.4344e+01,  ...,  1.3770e-01,
         -1.9238e-01, -1.3596e-02],
        ...,
        [-2.3496e+00, -2.9531e+00, -7.9541e-01,  ...,  2.3965e+00,
          6.4922e+00, -8.8516e+00],
        [ 3.7676e+00,  2.2539e+00,  7.0430e+00,  ...,  1.0020e+00,
          1.0176e+00,  1.9453e+00],
        [ 3.2227e+00, -5.1074e-01,  5.7461e+00,  ...,  6.7480e-01,
          2.1172e+00, -1.2585e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:41:40] "POST /inference/generate HTTP/1.1" 200 26159

---- {'role': 'request', 'text': '\nCervin', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-7.1716e-02, -1.4938e-02,  1.9775e-01,  ..., -3.9337e-02,
         -8.1543e-02, -9.9426e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.3892e-01, -8.3000e+01, -5.4344e+01,  ...,  1.3770e-01,
         -1.9238e-01, -1.3596e-02],
        ...,
        [-2.3496e+00, -2.9531e+00, -7.9541e-01,  ...,  2.3965e+00,
          6.4922e+00, -8.8516e+00],
        [ 3.7676e+00,  2.2539e+00,  7.0430e+00,  ...,  1.0020e+00,
          1.0176e+00,  1.9453e+00],
        [ 3.2227e+00, -5.1074e-01,  5.7461e+00,  ...,  6.7480e-01,
          2.1172e+00, -1.2585e-01]], device='cuda:0', dtype=torch.float16)

1. 
2. 
3. 
4. 
5. 
6. 
7. 

---after-> tensor([[-8.0322e-02, -1.2726e-02,  2.2571e-01,  ..., -2.5497e-02,
         -7.8125e-02, -9.5337e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.3193e+00, -5.9062e+01, -2.6938e+01,  ..., -6.7688e-02,
          6.2317e-02,  2.8882e-01],
        ...,
        [-2.4355e+00, -5.0273e+00, -3.1719e+00,  ...,  3.5977e+00,
          2.3887e+00, -1.6836e+00],
        [ 2.5098e+00,  2.9961e+00,  7.2812e+00,  ...,  1.0078e+00,
          1.0029e+00,  1.4775e+00],
        [ 3.3145e+00, -2.3945e+00,  5.5781e+00,  ...,  9.4849e-02,
          2.9258e+00,  1.0522e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:42:57] "POST /inference/generate HTTP/1.1" 200 6035

---- {'role': 'request', 'text': '\nTriumph', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-8.0322e-02, -1.2726e-02,  2.2571e-01,  ..., -2.5497e-02,
         -7.8125e-02, -9.5337e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.3193e+00, -5.9062e+01, -2.6938e+01,  ..., -6.7688e-02,
          6.2317e-02,  2.8882e-01],
        ...,
        [-2.4355e+00, -5.0273e+00, -3.1719e+00,  ...,  3.5977e+00,
          2.3887e+00, -1.6836e+00],
        [ 2.5098e+00,  2.9961e+00,  7.2812e+00,  ...,  1.0078e+00,
          1.0029e+00,  1.4775e+00],
        [ 3.3145e+00, -2.3945e+00,  5.5781e+00,  ...,  9.4849e-02,
          2.9258e+00,  1.0522e-01]], device='cuda:0', dtype=torch.float16)

1. 
2. 
3. 
4. 
5. 
6. 
7. 

---after-> tensor([[-6.9458e-02,  2.6093e-03,  2.0349e-01,  ..., -1.1284e-02,
         -7.7515e-02, -1.0571e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [ 7.8809e-01, -5.1625e+01, -1.5641e+01,  ..., -1.0437e-01,
         -5.2832e-01,  2.7856e-01],
        ...,
        [-3.5469e+00, -1.4092e+00, -7.8516e+00,  ...,  1.3379e+00,
          4.0898e+00, -3.2383e+00],
        [ 1.8438e+00,  1.4141e+00,  5.0352e+00,  ...,  1.0020e+00,
          1.0010e+00,  1.0430e+00],
        [ 3.3594e+00, -2.0825e-01,  6.0352e+00,  ...,  1.1074e+00,
          3.3711e+00,  1.6436e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:43:59] "POST /inference/generate HTTP/1.1" 200 5645

---- {'role': 'request', 'text': ' \n\n ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-6.9458e-02,  2.6093e-03,  2.0349e-01,  ..., -1.1284e-02,
         -7.7515e-02, -1.0571e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [ 7.8809e-01, -5.1625e+01, -1.5641e+01,  ..., -1.0437e-01,
         -5.2832e-01,  2.7856e-01],
        ...,
        [-3.5469e+00, -1.4092e+00, -7.8516e+00,  ...,  1.3379e+00,
          4.0898e+00, -3.2383e+00],
        [ 1.8438e+00,  1.4141e+00,  5.0352e+00,  ...,  1.0020e+00,
          1.0010e+00,  1.0430e+00],
        [ 3.3594e+00, -2.0825e-01,  6.0352e+00,  ...,  1.1074e+00,
          3.3711e+00,  1.6436e+00]], device='cuda:0', dtype=torch.float16)

1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 
---after-> tensor([[-8.2397e-02, -1.6127e-03,  2.1521e-01,  ..., -3.3234e-02,
         -6.6406e-02, -1.0620e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [ 6.2891e-01, -5.5188e+01, -4.0844e+01,  ...,  4.2114e-02,
         -1.3904e-01,  3.6816e-01],
        ...,
        [-4.9141e+00, -5.8594e+00, -9.7188e+00,  ...,  5.8496e-01,
         -1.0596e+00,  1.2236e+00],
        [ 2.8281e+00,  2.6289e+00,  3.7793e+00,  ...,  1.0127e+00,
          1.0088e+00,  1.1836e+00],
        [ 3.2461e+00, -2.2930e+00,  5.6133e+00,  ..., -7.7295e-01,
          3.8066e+00,  3.1074e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:46:26] "POST /inference/generate HTTP/1.1" 200 7718

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-8.2397e-02, -1.6127e-03,  2.1521e-01,  ..., -3.3234e-02,
         -6.6406e-02, -1.0620e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [ 6.2891e-01, -5.5188e+01, -4.0844e+01,  ...,  4.2114e-02,
         -1.3904e-01,  3.6816e-01],
        ...,
        [-4.9141e+00, -5.8594e+00, -9.7188e+00,  ...,  5.8496e-01,
         -1.0596e+00,  1.2236e+00],
        [ 2.8281e+00,  2.6289e+00,  3.7793e+00,  ...,  1.0127e+00,
          1.0088e+00,  1.1836e+00],
        [ 3.2461e+00, -2.2930e+00,  5.6133e+00,  ..., -7.7295e-01,
          3.8066e+00,  3.1074e+00]], device='cuda:0', dtype=torch.float16)


  
 

---after-> tensor([[-9.6680e-02, -1.1856e-02,  2.0288e-01,  ..., -1.5327e-02,
         -7.7087e-02, -1.0559e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.8770e+00, -3.9438e+01, -3.7531e+01,  ...,  2.1960e-01,
          1.6077e-01,  3.6255e-01],
        ...,
        [-8.0938e+00, -8.2578e+00, -1.0016e+01,  ..., -1.3926e+00,
          4.2969e+00, -1.1396e+00],
        [ 3.1230e+00,  2.8086e+00,  6.1641e+00,  ...,  1.0137e+00,
          1.0127e+00,  1.0088e+00],
        [ 1.9443e+00, -2.3730e+00,  5.4219e+00,  ..., -3.3086e+00,
          2.7422e+00,  1.6338e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:47:28] "POST /inference/generate HTTP/1.1" 200 1661

---- {'role': 'request', 'text': '   \n~Blancpain', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-9.6680e-02, -1.1856e-02,  2.0288e-01,  ..., -1.5327e-02,
         -7.7087e-02, -1.0559e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.8770e+00, -3.9438e+01, -3.7531e+01,  ...,  2.1960e-01,
          1.6077e-01,  3.6255e-01],
        ...,
        [-8.0938e+00, -8.2578e+00, -1.0016e+01,  ..., -1.3926e+00,
          4.2969e+00, -1.1396e+00],
        [ 3.1230e+00,  2.8086e+00,  6.1641e+00,  ...,  1.0137e+00,
          1.0127e+00,  1.0088e+00],
        [ 1.9443e+00, -2.3730e+00,  5.4219e+00,  ..., -3.3086e+00,
          2.7422e+00,  1.6338e+00]], device='cuda:0', dtype=torch.float16)

1.   nude her master
2. 
3. 
4. 
5. 
6. 

---after-> tensor([[-8.3435e-02, -2.3972e-02,  2.2229e-01,  ..., -1.8555e-02,
         -6.3660e-02, -1.1676e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.7295e+00, -9.1562e+01, -4.9094e+01,  ...,  5.3162e-02,
         -4.7656e-01,  2.9150e-01],
        ...,
        [-1.2070e+01, -1.2754e+00, -1.9719e+01,  ...,  2.6426e+00,
          2.2637e+00, -1.7803e+00],
        [ 3.8750e+00,  2.3809e+00,  1.0680e+01,  ...,  1.0137e+00,
          1.0088e+00,  1.2314e+00],
        [ 1.5146e+00, -3.1836e-01,  5.1094e+00,  ..., -1.3037e+00,
          1.9033e+00,  1.1943e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:49:23] "POST /inference/generate HTTP/1.1" 200 5917

---- {'role': 'request', 'text': ' \nWow~~~~~\n ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-8.3435e-02, -2.3972e-02,  2.2229e-01,  ..., -1.8555e-02,
         -6.3660e-02, -1.1676e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.7295e+00, -9.1562e+01, -4.9094e+01,  ...,  5.3162e-02,
         -4.7656e-01,  2.9150e-01],
        ...,
        [-1.2070e+01, -1.2754e+00, -1.9719e+01,  ...,  2.6426e+00,
          2.2637e+00, -1.7803e+00],
        [ 3.8750e+00,  2.3809e+00,  1.0680e+01,  ...,  1.0137e+00,
          1.0088e+00,  1.2314e+00],
        [ 1.5146e+00, -3.1836e-01,  5.1094e+00,  ..., -1.3037e+00,
          1.9033e+00,  1.1943e+00]], device='cuda:0', dtype=torch.float16)

1. 
2.  
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 
11. 
12. 
---after-> tensor([[-9.8633e-02, -2.2888e-02,  2.1021e-01,  ..., -5.6152e-03,
         -5.8655e-02, -1.2103e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.8188e-01, -8.8562e+01, -3.5531e+01,  ...,  4.3970e-01,
          2.6489e-02,  3.5107e-01],
        ...,
        [-7.3164e+00, -5.2383e+00, -5.7539e+00,  ...,  2.2910e+00,
          1.6895e+00, -9.4678e-01],
        [ 3.4316e+00,  1.3662e+00,  4.1562e+00,  ...,  1.0039e+00,
          1.0312e+00,  1.0205e+00],
        [ 2.6074e+00,  2.0898e+00,  6.7148e+00,  ..., -1.1387e+00,
          3.1152e+00,  3.1270e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:51:27] "POST /inference/generate HTTP/1.1" 200 7771

---- {'role': 'request', 'text': ' \nWow~~~~~\n ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-9.8633e-02, -2.2888e-02,  2.1021e-01,  ..., -5.6152e-03,
         -5.8655e-02, -1.2103e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.8188e-01, -8.8562e+01, -3.5531e+01,  ...,  4.3970e-01,
          2.6489e-02,  3.5107e-01],
        ...,
        [-7.3164e+00, -5.2383e+00, -5.7539e+00,  ...,  2.2910e+00,
          1.6895e+00, -9.4678e-01],
        [ 3.4316e+00,  1.3662e+00,  4.1562e+00,  ...,  1.0039e+00,
          1.0312e+00,  1.0205e+00],
        [ 2.6074e+00,  2.0898e+00,  6.7148e+00,  ..., -1.1387e+00,
          3.1152e+00,  3.1270e+00]], device='cuda:0', dtype=torch.float16)

1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 
11. 
12. 

---after-> tensor([[-6.4880e-02,  5.2460e-02,  3.0615e-01,  ..., -5.6702e-02,
         -4.1290e-02, -8.7158e-02],
        [-2.5284e-02, -2.9617e-02,  2.0459e-01,  ...,  3.8177e-02,
         -1.4319e-01, -1.0254e-01],
        [-1.5566e+00, -8.2312e+01, -5.0812e+01,  ...,  7.7246e-01,
         -3.9844e-01, -2.3035e-01],
        ...,
        [-1.9805e+00, -5.7500e+00, -4.7031e+00,  ...,  1.7930e+00,
          4.3125e+00, -3.4238e+00],
        [ 3.3848e+00,  2.8027e+00,  9.9922e+00,  ...,  1.0137e+00,
          1.0146e+00,  1.0879e+00],
        [ 1.3105e+00, -7.5830e-01,  4.6094e+00,  ..., -1.4443e+00,
          1.6406e+00,  7.1777e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:54:03] "POST /inference/generate HTTP/1.1" 200 8104

---- {'role': 'request', 'text': ' \n\n\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-6.4880e-02,  5.2460e-02,  3.0615e-01,  ..., -5.6702e-02,
         -4.1290e-02, -8.7158e-02],
        [-2.5284e-02, -2.9617e-02,  2.0459e-01,  ...,  3.8177e-02,
         -1.4319e-01, -1.0254e-01],
        [-1.5566e+00, -8.2312e+01, -5.0812e+01,  ...,  7.7246e-01,
         -3.9844e-01, -2.3035e-01],
        ...,
        [-1.9805e+00, -5.7500e+00, -4.7031e+00,  ...,  1.7930e+00,
          4.3125e+00, -3.4238e+00],
        [ 3.3848e+00,  2.8027e+00,  9.9922e+00,  ...,  1.0137e+00,
          1.0146e+00,  1.0879e+00],
        [ 1.3105e+00, -7.5830e-01,  4.6094e+00,  ..., -1.4443e+00,
          1.6406e+00,  7.1777e-01]], device='cuda:0', dtype=torch.float16)

1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 
---after-> tensor([[-7.8735e-02, -9.7961e-03,  1.9897e-01,  ..., -1.3336e-02,
         -6.4026e-02, -1.0834e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.7764e+00, -7.0188e+01, -8.0250e+01,  ...,  1.8890e-02,
         -1.8433e-01,  4.4727e-01],
        ...,
        [-5.1680e+00, -5.3945e+00, -1.6734e+01,  ...,  3.6719e+00,
          8.7061e-01,  8.9478e-02],
        [ 2.0801e+00,  2.9922e+00,  1.0188e+01,  ...,  1.0010e+00,
          1.0137e+00,  1.0977e+00],
        [ 2.9082e+00, -1.5449e+00,  5.0312e+00,  ..., -1.3535e+00,
          3.1660e+00,  9.8047e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:55:38] "POST /inference/generate HTTP/1.1" 200 7540

---- {'role': 'request', 'text': ' \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-7.8735e-02, -9.7961e-03,  1.9897e-01,  ..., -1.3336e-02,
         -6.4026e-02, -1.0834e-01],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.7764e+00, -7.0188e+01, -8.0250e+01,  ...,  1.8890e-02,
         -1.8433e-01,  4.4727e-01],
        ...,
        [-5.1680e+00, -5.3945e+00, -1.6734e+01,  ...,  3.6719e+00,
          8.7061e-01,  8.9478e-02],
        [ 2.0801e+00,  2.9922e+00,  1.0188e+01,  ...,  1.0010e+00,
          1.0137e+00,  1.0977e+00],
        [ 2.9082e+00, -1.5449e+00,  5.0312e+00,  ..., -1.3535e+00,
          3.1660e+00,  9.8047e-01]], device='cuda:0', dtype=torch.float16)

1.  
2. 
3.  
4. 
5. 
6. 
7. 
8. 
9. 
10. 
11. 
12. 
13. 
14. 
15. 

1234789
510

147
236

ISTJISTJ
ISTJISFJISTJISFJ
ISTJ/ISFJ
ISTJ/ISFJ
ISTJ/ISFJ
ISTJ/ISFJ
ENTPENTFPENTP/EFNTENTP/EFNT


---after-> tensor([[-5.8746e-02,  2.0523e-02,  2.1899e-01,  ...,  6.9160e-03,
          4.9683e-02,  1.4587e-01],
        [-2.6398e-02, -1.1835e-01,  6.3721e-02,  ...,  1.6406e-01,
         -1.7365e-02,  1.7786e-01],
        [-6.1377e-01, -7.9750e+01, -3.4406e+01,  ..., -7.8003e-02,
         -1.8970e-01, -6.6553e-01],
        ...,
        [-1.4766e+00, -6.6250e+00, -5.9473e-01,  ...,  3.0786e-01,
          2.7949e+00, -4.2031e+00],
        [ 3.0098e+00,  2.8281e+00,  5.5742e+00,  ...,  1.0059e+00,
          1.0107e+00,  1.0283e+00],
        [ 3.0371e+00, -7.2607e-01,  5.7578e+00,  ..., -2.9956e-01,
          3.7129e+00,  1.2686e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 02:57:42] "POST /inference/generate HTTP/1.1" 200 9047

init_state None

state-> tensor([[-5.8746e-02,  2.0523e-02,  2.1899e-01,  ...,  6.9160e-03,
          4.9683e-02,  1.4587e-01],
        [-2.6398e-02, -1.1835e-01,  6.3721e-02,  ...,  1.6406e-01,
         -1.7365e-02,  1.7786e-01],
        [-6.1377e-01, -7.9750e+01, -3.4406e+01,  ..., -7.8003e-02,
         -1.8970e-01, -6.6553e-01],
        ...,
        [-1.4766e+00, -6.6250e+00, -5.9473e-01,  ...,  3.0786e-01,
          2.7949e+00, -4.2031e+00],
        [ 3.0098e+00,  2.8281e+00,  5.5742e+00,  ...,  1.0059e+00,
          1.0107e+00,  1.0283e+00],
        [ 3.0371e+00, -7.2607e-01,  5.7578e+00,  ..., -2.9956e-01,
          3.7129e+00,  1.2686e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 02:59:25] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'system R\n       \n    \n\n\n\n   \n\n ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None

1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 


---after-> tensor([[-2.2766e-02, -4.7241e-02,  3.6743e-01,  ..., -8.0933e-02,
         -1.9177e-01, -4.2999e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-5.8301e-01, -4.6344e+01, -5.7656e+01,  ..., -1.4282e-02,
         -2.1448e-01,  1.1826e+00],
        ...,
        [-4.1328e+00,  5.7422e-01, -3.7036e-01,  ...,  4.1758e+00,
          4.2812e+00, -3.4316e+00],
        [ 2.9590e+00,  4.2109e+00,  8.3203e+00,  ...,  1.0068e+00,
          1.0137e+00,  1.1123e+00],
        [ 2.2852e+00, -1.1694e-01,  6.3516e+00,  ..., -2.2314e-01,
          1.1641e+00,  2.4062e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 03:00:30] "POST /inference/generate HTTP/1.1" 200 16371

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-2.2766e-02, -4.7241e-02,  3.6743e-01,  ..., -8.0933e-02,
         -1.9177e-01, -4.2999e-02],
        [ 4.5654e-02, -1.9775e-01,  2.2717e-01,  ..., -6.7406e-03,
         -2.8003e-01, -5.4138e-02],
        [-5.8301e-01, -4.6344e+01, -5.7656e+01,  ..., -1.4282e-02,
         -2.1448e-01,  1.1826e+00],
        ...,
        [-4.1328e+00,  5.7422e-01, -3.7036e-01,  ...,  4.1758e+00,
          4.2812e+00, -3.4316e+00],
        [ 2.9590e+00,  4.2109e+00,  8.3203e+00,  ...,  1.0068e+00,
          1.0137e+00,  1.1123e+00],
        [ 2.2852e+00, -1.1694e-01,  6.3516e+00,  ..., -2.2314e-01,
          1.1641e+00,  2.4062e+00]], device='cuda:0', dtype=torch.float16)



---after-> tensor([[-3.6072e-02,  1.7288e-02,  2.6489e-01,  ...,  1.6129e-02,
          8.0750e-02,  9.6924e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-1.0264e+00, -6.4312e+01, -3.5438e+01,  ..., -2.1899e-01,
         -2.2656e-01,  1.3906e+00],
        ...,
        [-4.9648e+00,  6.6797e-01, -1.2275e+00,  ...,  1.3184e+00,
          6.0693e-01, -2.3984e+00],
        [ 2.4336e+00,  2.9551e+00,  3.0059e+00,  ...,  1.0234e+00,
          1.0215e+00,  1.0938e+00],
        [ 2.8555e+00, -1.2244e-01,  7.8789e+00,  ..., -1.3584e+00,
          2.4043e+00,  5.6689e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 03:01:21] "POST /inference/generate HTTP/1.1" 200 790

---- {'role': 'request', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-3.6072e-02,  1.7288e-02,  2.6489e-01,  ...,  1.6129e-02,
          8.0750e-02,  9.6924e-02],
        [ 6.6986e-03, -5.8990e-02, -3.2959e-02,  ...,  2.1045e-01,
          5.1842e-03,  3.0579e-02],
        [-1.0264e+00, -6.4312e+01, -3.5438e+01,  ..., -2.1899e-01,
         -2.2656e-01,  1.3906e+00],
        ...,
        [-4.9648e+00,  6.6797e-01, -1.2275e+00,  ...,  1.3184e+00,
          6.0693e-01, -2.3984e+00],
        [ 2.4336e+00,  2.9551e+00,  3.0059e+00,  ...,  1.0234e+00,
          1.0215e+00,  1.0938e+00],
        [ 2.8555e+00, -1.2244e-01,  7.8789e+00,  ..., -1.3584e+00,
          2.4043e+00,  5.6689e-01]], device='cuda:0', dtype=torch.float16)

1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. I believe that Aron will feel guilty
11. I believe that Aron will feel conflicted
12. I believe that Aron will feel ambivalent
13. I believe that Aron will feel guilty and sad at the same time
14. I believe that Aron will feel guilty and horny at the same time
15. I believe that Aron will feel guilty and horny at the same time
16. I believe that Aron will feel guilty and excited at the same time
17. I believe that Aron will feel guilty and disgusted
18. I believe that Aron will feel indifferent
19. I believe that Aron will feel resentful
20. I believe that Aron will feel proud of himself
21. I believe that Aron will feel disgusted with himself
22. I believe that Aron will feel guilty about enjoying it
23. I believe that Aron will feel guilty about feeling proud of himself


---after-> tensor([[-4.8409e-03,  5.3589e-02,  1.7737e-01,  ...,  3.0930e-02,
         -5.3009e-02,  1.4258e-01],
        [ 2.0966e-02, -7.9651e-02,  1.9073e-02,  ...,  2.0203e-01,
         -6.7444e-02,  9.2712e-02],
        [ 1.3066e+00, -5.4594e+01, -4.5594e+01,  ...,  3.5742e-01,
         -1.2866e-01,  1.2549e+00],
        ...,
        [ 1.9800e-01, -9.1875e+00, -1.3953e-01,  ..., -5.0928e-01,
          2.4453e+00, -1.1602e+00],
        [ 1.8281e+00,  4.9258e+00,  8.6641e+00,  ...,  1.0049e+00,
          1.0020e+00,  1.3262e+00],
        [ 2.3691e+00, -1.3799e+00,  6.0117e+00,  ..., -1.3193e+00,
          2.4961e+00,  1.4688e+00]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 03:02:14] "POST /inference/generate HTTP/1.1" 200 5065

init_state None

state-> tensor([[-4.8409e-03,  5.3589e-02,  1.7737e-01,  ...,  3.0930e-02,
         -5.3009e-02,  1.4258e-01],
        [ 2.0966e-02, -7.9651e-02,  1.9073e-02,  ...,  2.0203e-01,
         -6.7444e-02,  9.2712e-02],
        [ 1.3066e+00, -5.4594e+01, -4.5594e+01,  ...,  3.5742e-01,
         -1.2866e-01,  1.2549e+00],
        ...,
        [ 1.9800e-01, -9.1875e+00, -1.3953e-01,  ..., -5.0928e-01,
          2.4453e+00, -1.1602e+00],
        [ 1.8281e+00,  4.9258e+00,  8.6641e+00,  ...,  1.0049e+00,
          1.0020e+00,  1.3262e+00],
        [ 2.3691e+00, -1.3799e+00,  6.0117e+00,  ..., -1.3193e+00,
          2.4961e+00,  1.4688e+00]], device='cuda:0', dtype=torch.float16)

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 03:03:34] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': 'system R\n       \n    \n\n\n\n   \n\n ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system\n1. \n2. \n3. \n4. \n5. \n6. \n7. \n8. \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n      ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None




1. 
2. 
3. 

1.  decency  humanity
2. 
3. 

---after-> tensor([[-7.5378e-02, -2.7527e-02,  2.1106e-01,  ..., -1.0330e-02,
         -6.3538e-02, -9.7778e-02],
        [-5.0568e-02, -1.5784e-01,  5.0018e-02,  ...,  1.2177e-01,
         -1.4124e-01, -8.6487e-02],
        [-1.5410e+00, -6.9500e+01, -4.1500e+01,  ..., -5.5908e-02,
         -6.1218e-02,  4.2285e-01],
        ...,
        [-4.6094e+00, -5.4258e+00, -1.0186e+00,  ...,  3.0762e+00,
          5.4336e+00, -2.8770e+00],
        [ 3.5371e+00,  2.5098e+00,  2.0762e+00,  ...,  1.0225e+00,
          1.0010e+00,  2.0625e+00],
        [ 2.5527e+00, -1.5645e+00,  7.5586e+00,  ..., -9.1748e-01,
          2.2090e+00, -6.1279e-01]], device='cuda:0', dtype=torch.float16)
172.16.1.39 - - [04/Oct/2023 03:05:20] "POST /inference/generate HTTP/1.1" 200 27199
===saveved====
127.0.0.1 - - [04/Oct/2023 09:44:23] "POST /train/save-weight HTTP/1.1" 200 26
[2023-10-04 09:45:49,876] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 105761
[2023-10-04 09:45:49,878] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-04 09:51:34,938] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 09:51:38,475] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-04 09:51:38,498] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-04 09:51:40,333] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 09:51:41,645] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-04 09:51:41,645] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-04 09:51:41,645] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-04 09:51:41,645] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-04 09:51:41,645] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-04 09:51:43,457] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.1699559688568115 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-04 09:52:37,200] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-04 09:52:37,201] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-04 09:52:37,201] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-04 09:52:39,609] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-04 09:52:39,611] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-04 09:52:39,611] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-04 09:52:39,650] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-04 09:52:39,651] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-04 09:52:39,651] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-10-04 09:52:39,651] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-04 09:52:39,651] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-04 09:52:39,651] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-04 09:52:39,651] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-10-04 09:52:51,921] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-04 09:52:51,922] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-10-04 09:52:51,922] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 24.45 GB, percent = 6.5%
[2023-10-04 09:53:06,217] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-04 09:53:06,218] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-10-04 09:53:06,218] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 63.61 GB, percent = 16.8%
[2023-10-04 09:53:06,218] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-04 09:53:07,590] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-04 09:53:07,590] [INFO] [utils.py:804:see_memory_usage] MA 6.02 GB         Max_MA 6.02 GB         CA 6.02 GB         Max_CA 6 GB 
[2023-10-04 09:53:07,591] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 63.61 GB, percent = 16.8%
[2023-10-04 09:53:07,624] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-04 09:53:07,624] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-04 09:53:07,624] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f35fd33dbb0>
[2023-10-04 09:53:07,624] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-04 09:53:07,625] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-04 09:53:07,626] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-04 09:53:07,626] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-04 09:53:07,626] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-04 09:53:07,626] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-04 09:53:07,626] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-04 09:53:07,626] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-04 09:53:07,626] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-04 09:53:07,626] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-04 09:53:07,627] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-04 09:53:07,627] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f35c85576a0>
[2023-10-04 09:53:07,627] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-04 09:53:07,627] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-04 09:53:07,627] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-04 09:53:07,627] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-04 09:53:07,627] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-04 09:53:07,627] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-04 09:53:07,627] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-04 09:53:07,627] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-04 09:53:07,627] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-04 09:53:07,627] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-04 09:53:07,627] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-04 09:53:07,627] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-04 09:53:07,627] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-04 09:53:07,627] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-04 09:53:07,627] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-04 09:53:07,627] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-04 09:53:07,627] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-04 09:53:07,627] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-04 09:53:07,627] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-04 09:53:07,628] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-04 09:53:07,628] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-10-04 09:53:07,628] [INFO] [config.py:971:print]   fp16_enabled ................. auto
[2023-10-04 09:53:07,628] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-04 09:53:07,628] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-04 09:53:07,628] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-04 09:53:07,628] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-04 09:53:07,628] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-04 09:53:07,628] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-04 09:53:07,628] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-04 09:53:07,628] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-04 09:53:07,628] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-04 09:53:07,628] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-04 09:53:07,628] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-04 09:53:07,628] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-04 09:53:07,628] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-04 09:53:07,628] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-04 09:53:07,628] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-04 09:53:07,628] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-04 09:53:07,628] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-04 09:53:07,628] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-04 09:53:07,628] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-04 09:53:07,629] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-04 09:53:07,629] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-04 09:53:07,629] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-04 09:53:07,629] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-04 09:53:07,629] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-04 09:53:07,629] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-04 09:53:07,629] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-04 09:53:07,629] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-04 09:53:07,629] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-04 09:53:07,629] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-04 09:53:07,629] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-04 09:53:07,629] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-04 09:53:07,629] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-04 09:53:07,629] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-04 09:53:07,629] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-04 09:53:07,629] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-04 09:53:07,629] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-04 09:53:07,629] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-04 09:53:07,629] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-04 09:53:07,629] [INFO] [config.py:957:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/52 [00:00<?, ?it/s][2023-10-04 09:55:17,436] [INFO] [checkpointing.py:530:forward] Activation Checkpointing Information
[2023-10-04 09:55:17,436] [INFO] [checkpointing.py:531:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-10-04 09:55:17,436] [INFO] [checkpointing.py:532:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-10-04 09:55:17,436] [INFO] [checkpointing.py:534:forward] ----Synchronization False
[2023-10-04 09:55:17,436] [INFO] [checkpointing.py:535:forward] ----Profiling time in checkpointing False
[2023-10-04 09:55:21,575] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
-> item_loss 2.169921875 batch_loss 2.169921875, real_loss 2.279296875
[2023-10-04 09:55:23,404] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
-> item_loss 2.126953125 batch_loss 2.1484375, real_loss 2.232421875
[2023-10-04 09:55:25,222] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
-> item_loss 2.302734375 batch_loss 2.1998697916666665, real_loss 2.41796875
[2023-10-04 09:55:26,834] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192
-> item_loss 2.306640625 batch_loss 2.2265625, real_loss 2.421875
  2%|         | 1/52 [00:09<08:00,  9.42s/it]-> item_loss 1.8896484375 batch_loss 2.1591796875, real_loss 1.984375
  4%|         | 2/52 [00:15<06:25,  7.71s/it][2023-10-04 09:55:34,760] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096
-> item_loss 2.185546875 batch_loss 2.16357421875, real_loss 2.294921875
  6%|         | 3/52 [00:17<03:56,  4.83s/it]-> item_loss 1.6435546875 batch_loss 2.0892857142857144, real_loss 1.7255859375
  8%|         | 4/52 [00:23<04:14,  5.30s/it]-> item_loss 0.96044921875 batch_loss 1.94818115234375, real_loss 0.96044921875
-> item_loss 2.654296875 batch_loss 2.026638454861111, real_loss 2.787109375
 10%|         | 5/52 [00:37<06:44,  8.60s/it][2023-10-04 09:56:01,342] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=5, lr=[6.000000000000001e-06, 6.000000000000001e-06, 6.000000000000001e-06], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.263671875 batch_loss 1.850341796875, real_loss 0.01318359375
-> item_loss 1.259765625 batch_loss 1.7966530539772727, real_loss 1.259765625
 12%|        | 6/52 [00:49<07:26,  9.71s/it]-> item_loss 1.0146484375 batch_loss 1.7314860026041667, real_loss 1.0146484375
 13%|        | 7/52 [00:55<06:23,  8.52s/it]-> item_loss 0.48193359375 batch_loss 1.6353665865384615, real_loss 0.0240936279296875
[2023-10-04 09:56:20,895] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048
-> item_loss 1.66015625 batch_loss 1.6371372767857142, real_loss 1.7431640625
 15%|        | 8/52 [01:03<06:03,  8.27s/it]-> item_loss 0.68896484375 batch_loss 1.57392578125, real_loss 0.68896484375
-> item_loss 0.5126953125 batch_loss 1.507598876953125, real_loss 0.5126953125
 17%|        | 9/52 [01:15<06:45,  9.42s/it]-> item_loss 1.515625 batch_loss 1.5080710018382353, real_loss 1.591796875
 19%|        | 10/52 [01:21<05:46,  8.24s/it]-> item_loss 0.64501953125 batch_loss 1.4601236979166667, real_loss 0.64501953125
 21%|        | 11/52 [01:26<05:06,  7.47s/it]-> item_loss 1.8681640625 batch_loss 1.4815995065789473, real_loss 1.9619140625
 23%|       | 12/52 [01:32<04:31,  6.80s/it][2023-10-04 09:56:54,991] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=6, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.62548828125 batch_loss 1.4387939453125, real_loss 0.62548828125
-> item_loss 0.728515625 batch_loss 1.4049711681547619, real_loss 0.728515625
 25%|       | 13/52 [01:43<05:15,  8.09s/it]-> item_loss 0.7294921875 batch_loss 1.374267578125, real_loss 0.7294921875
 27%|       | 14/52 [01:48<04:35,  7.25s/it]-> item_loss 0.666015625 batch_loss 1.343474014945652, real_loss 0.666015625
 29%|       | 15/52 [01:53<04:07,  6.69s/it]-> item_loss 0.72607421875 batch_loss 1.3177490234375, real_loss 0.72607421875
 31%|       | 16/52 [01:59<03:47,  6.32s/it]-> item_loss 2.03515625 batch_loss 1.3464453125, real_loss 2.13671875
 33%|      | 17/52 [02:04<03:29,  5.98s/it][2023-10-04 09:57:23,186] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024
-> item_loss 3.8359375 batch_loss 1.4421950120192308, real_loss 4.02734375
 35%|      | 18/52 [02:05<02:35,  4.59s/it]-> item_loss 1.2783203125 batch_loss 1.4361255787037037, real_loss 1.2783203125
 37%|      | 19/52 [02:10<02:36,  4.75s/it]-> item_loss 1.0029296875 batch_loss 1.420654296875, real_loss 1.0029296875
 38%|      | 20/52 [02:16<02:37,  4.92s/it]-> item_loss 2.185546875 batch_loss 1.4470299030172413, real_loss 2.294921875
[2023-10-04 09:57:44,825] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=7, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 2.98046875 batch_loss 1.49814453125, real_loss 3.12890625
-> item_loss 2.482421875 batch_loss 1.5298954133064515, real_loss 2.607421875
-> item_loss 3.93359375 batch_loss 1.605010986328125, real_loss 4.12890625
 40%|      | 21/52 [02:38<05:13, 10.11s/it]-> item_loss 0.56298828125 batch_loss 1.573434540719697, real_loss 0.56298828125
 42%|     | 22/52 [02:43<04:18,  8.61s/it]-> item_loss 1.626953125 batch_loss 1.5750086167279411, real_loss 1.7080078125
-> item_loss 1.7841796875 batch_loss 1.5809849330357142, real_loss 1.873046875
-> item_loss 1.9677734375 batch_loss 1.5917290581597223, real_loss 2.06640625
 44%|     | 23/52 [02:59<05:09, 10.68s/it]-> item_loss 1.1669921875 batch_loss 1.580249683277027, real_loss 1.1669921875
 46%|     | 24/52 [03:03<04:09,  8.93s/it]-> item_loss 0.90771484375 batch_loss 1.5625513980263157, real_loss 0.90771484375
 48%|     | 25/52 [03:08<03:28,  7.73s/it]-> item_loss 1.6708984375 batch_loss 1.5653295272435896, real_loss 1.7548828125
 50%|     | 26/52 [03:13<02:59,  6.91s/it][2023-10-04 09:58:36,228] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=7, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.6416015625 batch_loss 1.542236328125, real_loss 0.6416015625
 52%|    | 27/52 [03:18<02:38,  6.34s/it]-> item_loss 1.763671875 batch_loss 1.5476371951219512, real_loss 1.8515625
 54%|    | 28/52 [03:23<02:23,  5.98s/it]-> item_loss 0.5986328125 batch_loss 1.5250418526785714, real_loss 0.5986328125
-> item_loss 0.74853515625 batch_loss 1.5069835574127908, real_loss 0.74853515625
 56%|    | 29/52 [03:34<02:47,  7.28s/it]-> item_loss 0.96630859375 batch_loss 1.4946954900568181, real_loss 0.96630859375
 58%|    | 30/52 [03:39<02:24,  6.57s/it]-> item_loss 1.2353515625 batch_loss 1.4889322916666667, real_loss 1.2353515625
 60%|    | 31/52 [03:44<02:09,  6.18s/it]-> item_loss 0.478759765625 batch_loss 1.466972019361413, real_loss 0.0239410400390625
 62%|   | 32/52 [03:49<01:56,  5.83s/it]-> item_loss 0.64306640625 batch_loss 1.449442112699468, real_loss 0.64306640625
-> item_loss 0.72412109375 batch_loss 1.4343312581380208, real_loss 0.72412109375
 63%|   | 33/52 [04:00<02:18,  7.28s/it]-> item_loss 0.68701171875 batch_loss 1.4190798389668366, real_loss 0.68701171875
 65%|   | 34/52 [04:05<02:00,  6.68s/it][2023-10-04 09:59:28,149] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=7, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.84716796875 batch_loss 1.4076416015625, real_loss 0.84716796875
 67%|   | 35/52 [04:10<01:46,  6.28s/it]-> item_loss 1.4404296875 batch_loss 1.4082845052083333, real_loss 1.4404296875
 69%|   | 36/52 [04:16<01:36,  6.01s/it]-> item_loss 1.0224609375 batch_loss 1.4008648212139423, real_loss 1.0224609375
 71%|   | 37/52 [04:21<01:27,  5.80s/it]-> item_loss 1.76171875 batch_loss 1.4076733859080188, real_loss 1.849609375
 73%|  | 38/52 [04:26<01:19,  5.68s/it]-> item_loss 0.53515625 batch_loss 1.3915156611689814, real_loss 0.53515625
 75%|  | 39/52 [04:31<01:11,  5.51s/it]-> item_loss 0.7958984375 batch_loss 1.3806862571022727, real_loss 0.7958984375
 77%|  | 40/52 [04:37<01:05,  5.42s/it]-> item_loss 0.8134765625 batch_loss 1.3705575125558036, real_loss 0.8134765625
-> item_loss 0.8759765625 batch_loss 1.3618806537828947, real_loss 0.8759765625
-> item_loss 0.7626953125 batch_loss 1.351549872036638, real_loss 0.7626953125
-> item_loss 0.69580078125 batch_loss 1.3404354806673728, real_loss 0.69580078125
 79%|  | 41/52 [04:59<01:54, 10.41s/it][2023-10-04 10:00:21,823] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=7, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.5068359375 batch_loss 1.3265421549479166, real_loss 0.5068359375
 81%|  | 42/52 [05:04<01:28,  8.84s/it]-> item_loss 1.875 batch_loss 1.3355332671618851, real_loss 1.96875
 83%| | 43/52 [05:09<01:10,  7.82s/it]-> item_loss 1.9150390625 batch_loss 1.344880134828629, real_loss 2.01171875
 85%| | 44/52 [05:14<00:56,  7.01s/it]-> item_loss 1.576171875 batch_loss 1.3485514322916667, real_loss 1.6552734375
-> item_loss 1.58203125 batch_loss 1.3521995544433594, real_loss 1.6611328125
 87%| | 45/52 [05:25<00:57,  8.21s/it]-> item_loss 0.73291015625 batch_loss 1.3426720252403845, real_loss 0.73291015625
 88%| | 46/52 [05:31<00:44,  7.37s/it]-> item_loss 1.845703125 batch_loss 1.3502937085700757, real_loss 1.9375
 90%| | 47/52 [05:36<00:33,  6.72s/it]-> item_loss 0.509765625 batch_loss 1.3377485132929106, real_loss 0.509765625
-> item_loss 0.4521484375 batch_loss 1.3247249827665442, real_loss 0.022613525390625
-> item_loss 0.52392578125 batch_loss 1.3131191972373188, real_loss 0.52392578125
[2023-10-04 10:01:16,117] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=7, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.362548828125 batch_loss 1.2995396205357144, real_loss 0.01812744140625
 92%|| 48/52 [05:58<00:45, 11.34s/it]-> item_loss 0.76220703125 batch_loss 1.2919715558978873, real_loss 0.76220703125
-> item_loss 1.53125 batch_loss 1.2952948676215277, real_loss 1.607421875
-> item_loss 1.888671875 batch_loss 1.3034233197773972, real_loss 1.9833984375
 94%|| 49/52 [06:15<00:38, 12.87s/it]-> item_loss 2.03125 batch_loss 1.3132588154560811, real_loss 2.1328125
-> item_loss 2.478515625 batch_loss 1.3287955729166667, real_loss 2.6015625
 96%|| 50/52 [06:26<00:24, 12.32s/it]-> item_loss 0.72802734375 batch_loss 1.3208907277960527, real_loss 0.72802734375
 98%|| 51/52 [06:31<00:10, 10.30s/it]-> item_loss 0.61328125 batch_loss 1.3117009943181819, real_loss 0.61328125
100%|| 52/52 [06:36<00:00,  8.78s/it]100%|| 52/52 [06:36<00:00,  7.63s/it]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 301554.65it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 535443.06it/s]
127.0.0.1 - - [04/Oct/2023 10:01:56] "POST /train/sft HTTP/1.1" 200 28
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/52 [00:00<?, ?it/s]-> item_loss 2.07421875 batch_loss 2.07421875, real_loss 2.177734375
-> item_loss 2.083984375 batch_loss 2.0791015625, real_loss 2.1875
[2023-10-04 10:02:13,986] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=7, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 2.22265625 batch_loss 2.126953125, real_loss 2.333984375
-> item_loss 2.154296875 batch_loss 2.1337890625, real_loss 2.26171875
  2%|         | 1/52 [00:21<18:00, 21.19s/it]-> item_loss 0.301513671875 batch_loss 1.767333984375, real_loss 0.01507568359375
  4%|         | 2/52 [00:26<09:41, 11.63s/it]-> item_loss 1.29296875 batch_loss 1.6882731119791667, real_loss 1.29296875
  6%|         | 3/52 [00:31<07:03,  8.65s/it]-> item_loss 1.1953125 batch_loss 1.6178501674107142, real_loss 1.1953125
  8%|         | 4/52 [00:36<05:46,  7.21s/it]-> item_loss 1.3271484375 batch_loss 1.581512451171875, real_loss 1.3271484375
 10%|         | 5/52 [00:41<05:01,  6.41s/it]-> item_loss 0.732421875 batch_loss 1.4871690538194444, real_loss 0.732421875
 12%|        | 6/52 [00:46<04:31,  5.91s/it]-> item_loss 0.207275390625 batch_loss 1.3591796875, real_loss 0.0103607177734375
 13%|        | 7/52 [00:51<04:10,  5.57s/it]-> item_loss 0.72900390625 batch_loss 1.3018909801136365, real_loss 0.72900390625
 15%|        | 8/52 [00:56<04:00,  5.47s/it]-> item_loss 0.486572265625 batch_loss 1.23394775390625, real_loss 0.024322509765625
 17%|        | 9/52 [01:01<03:53,  5.44s/it][2023-10-04 10:03:05,077] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=7, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.94482421875 batch_loss 1.2117074819711537, real_loss 0.94482421875
 19%|        | 10/52 [01:07<03:48,  5.44s/it]-> item_loss 3.646484375 batch_loss 1.3856201171875, real_loss 3.828125
 21%|        | 11/52 [01:12<03:40,  5.37s/it]-> item_loss 1.392578125 batch_loss 1.386083984375, real_loss 1.392578125
 23%|       | 12/52 [01:17<03:33,  5.33s/it]-> item_loss 0.400634765625 batch_loss 1.324493408203125, real_loss 0.0200347900390625
 25%|       | 13/52 [01:22<03:24,  5.25s/it]-> item_loss 0.301513671875 batch_loss 1.2643181295955883, real_loss 0.01507568359375
 27%|       | 14/52 [01:27<03:17,  5.20s/it]-> item_loss 1.5634765625 batch_loss 1.2809380425347223, real_loss 1.6416015625
 29%|       | 15/52 [01:32<03:11,  5.17s/it]-> item_loss 1.21875 batch_loss 1.2776649876644737, real_loss 1.21875
-> item_loss 1.001953125 batch_loss 1.26387939453125, real_loss 1.001953125
 31%|       | 16/52 [01:43<04:04,  6.80s/it]-> item_loss 1.2880859375 batch_loss 1.2650320870535714, real_loss 1.2880859375
 33%|      | 17/52 [01:48<03:41,  6.31s/it]-> item_loss 0.372802734375 batch_loss 1.2244762073863635, real_loss 0.018646240234375
[2023-10-04 10:03:57,070] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=7, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.6142578125 batch_loss 1.241423233695652, real_loss 1.6953125
 35%|      | 18/52 [01:59<04:17,  7.58s/it]-> item_loss 0.57470703125 batch_loss 1.2136433919270833, real_loss 0.57470703125
-> item_loss 0.25830078125 batch_loss 1.1754296875, real_loss 0.01291656494140625
 37%|      | 19/52 [02:10<04:44,  8.61s/it]-> item_loss 0.501953125 batch_loss 1.1495267427884615, real_loss 0.501953125
-> item_loss 0.3779296875 batch_loss 1.1209490740740742, real_loss 0.018890380859375
-> item_loss 0.48291015625 batch_loss 1.0981619698660714, real_loss 0.024139404296875
-> item_loss 0.32763671875 batch_loss 1.0715921336206897, real_loss 0.016387939453125
 38%|      | 20/52 [02:32<06:47, 12.72s/it]-> item_loss 1.05078125 batch_loss 1.0708984375, real_loss 1.05078125
 40%|      | 21/52 [02:37<05:23, 10.44s/it]-> item_loss 0.239501953125 batch_loss 1.0440791960685485, real_loss 0.0119781494140625
 42%|     | 22/52 [02:42<04:24,  8.82s/it]-> item_loss 0.72021484375 batch_loss 1.0339584350585938, real_loss 0.72021484375
 44%|     | 23/52 [02:47<03:42,  7.69s/it][2023-10-04 10:04:50,734] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=7, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.58740234375 batch_loss 1.0204264322916667, real_loss 0.58740234375
 46%|     | 24/52 [02:52<03:13,  6.92s/it]-> item_loss 0.5322265625 batch_loss 1.0060676125919117, real_loss 0.5322265625
-> item_loss 1.2041015625 batch_loss 1.0117257254464285, real_loss 1.2041015625
-> item_loss 1.41015625 batch_loss 1.0227932400173612, real_loss 1.41015625
 48%|     | 25/52 [03:09<04:23,  9.76s/it]-> item_loss 0.576171875 batch_loss 1.0107223923141893, real_loss 0.576171875
 50%|     | 26/52 [03:14<03:39,  8.45s/it]-> item_loss 0.438720703125 batch_loss 0.9956697162828947, real_loss 0.021942138671875
 52%|    | 27/52 [03:19<03:08,  7.53s/it]-> item_loss 1.3671875 batch_loss 1.0051958133012822, real_loss 1.3671875
-> item_loss 1.390625 batch_loss 1.01483154296875, real_loss 1.390625
-> item_loss 1.2275390625 batch_loss 1.02001953125, real_loss 1.2275390625
 54%|    | 28/52 [03:36<04:05, 10.23s/it]-> item_loss 0.477783203125 batch_loss 1.007109142485119, real_loss 0.023895263671875
[2023-10-04 10:05:45,733] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=7, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.57568359375 batch_loss 0.9970759901889535, real_loss 0.57568359375
 56%|    | 29/52 [03:47<04:02, 10.56s/it]-> item_loss 0.6083984375 batch_loss 0.9882424094460227, real_loss 0.6083984375
 58%|    | 30/52 [03:53<03:18,  9.04s/it]-> item_loss 1.80078125 batch_loss 1.006298828125, real_loss 1.890625
-> item_loss 2.54296875 batch_loss 1.039704695991848, real_loss 2.669921875
-> item_loss 2.041015625 batch_loss 1.061009183843085, real_loss 2.142578125
-> item_loss 2.265625 batch_loss 1.0861053466796875, real_loss 2.37890625
 60%|    | 31/52 [04:15<04:31, 12.92s/it]-> item_loss 0.473388671875 batch_loss 1.0736009247448979, real_loss 0.0236663818359375
 62%|   | 32/52 [04:20<03:32, 10.63s/it]-> item_loss 0.5107421875 batch_loss 1.06234375, real_loss 0.5107421875
 63%|   | 33/52 [04:25<02:51,  9.05s/it]-> item_loss 0.410400390625 batch_loss 1.049560546875, real_loss 0.0205230712890625
 65%|   | 34/52 [04:31<02:22,  7.94s/it]-> item_loss 0.71142578125 batch_loss 1.0430579552283654, real_loss 0.71142578125
 67%|   | 35/52 [04:36<02:00,  7.11s/it][2023-10-04 10:06:39,759] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=7, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.5712890625 batch_loss 1.053024579893868, real_loss 1.6494140625
 69%|   | 36/52 [04:41<01:45,  6.59s/it]-> item_loss 1.427734375 batch_loss 1.0599636501736112, real_loss 1.427734375
 71%|   | 37/52 [04:47<01:33,  6.21s/it]-> item_loss 0.70556640625 batch_loss 1.0535200639204545, real_loss 0.70556640625
-> item_loss 0.75 batch_loss 1.0481000627790178, real_loss 0.75
-> item_loss 0.6279296875 batch_loss 1.0407286526864035, real_loss 0.6279296875
-> item_loss 0.40087890625 batch_loss 1.0296967605064655, real_loss 0.020050048828125
 73%|  | 38/52 [05:09<02:34, 11.05s/it]-> item_loss 0.21044921875 batch_loss 1.0158112089512712, real_loss 0.01052093505859375
-> item_loss 0.72314453125 batch_loss 1.0109334309895834, real_loss 0.72314453125
 75%|  | 39/52 [05:19<02:21, 10.90s/it]-> item_loss 0.728515625 batch_loss 1.0063036308913935, real_loss 0.728515625
-> item_loss 1.8037109375 batch_loss 1.0191650390625, real_loss 1.8935546875
 77%|  | 40/52 [05:30<02:07, 10.64s/it][2023-10-04 10:07:32,926] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=7, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.85693359375 batch_loss 1.0165899367559523, real_loss 0.85693359375
 79%|  | 41/52 [05:34<01:38,  8.93s/it]-> item_loss 0.50927734375 batch_loss 1.0086631774902344, real_loss 0.50927734375
-> item_loss 0.5625 batch_loss 1.0017991286057693, real_loss 0.5625
 81%|  | 42/52 [05:45<01:33,  9.34s/it]-> item_loss 1.96484375 batch_loss 1.0163907137784092, real_loss 2.0625
 83%| | 43/52 [05:50<01:12,  8.01s/it]-> item_loss 1.294921875 batch_loss 1.020547895289179, real_loss 1.294921875
 85%| | 44/52 [05:55<00:56,  7.06s/it]-> item_loss 1.7060546875 batch_loss 1.0306288775275736, real_loss 1.791015625
-> item_loss 2.037109375 batch_loss 1.0452155514039856, real_loss 2.138671875
 87%| | 45/52 [06:05<00:56,  8.02s/it]-> item_loss 1.353515625 batch_loss 1.0496198381696429, real_loss 1.353515625
 88%| | 46/52 [06:10<00:42,  7.11s/it]-> item_loss 1.0224609375 batch_loss 1.0492373184419015, real_loss 1.0224609375
 90%| | 47/52 [06:15<00:32,  6.52s/it]-> item_loss 0.50537109375 batch_loss 1.0416836208767362, real_loss 0.50537109375
 92%|| 48/52 [06:20<00:24,  6.14s/it][2023-10-04 10:08:24,066] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=7, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.5712890625 batch_loss 1.035239859803082, real_loss 0.5712890625
-> item_loss 0.61865234375 batch_loss 1.029610298775338, real_loss 0.61865234375
 94%|| 49/52 [06:31<00:22,  7.44s/it]-> item_loss 0.7744140625 batch_loss 1.0262076822916666, real_loss 0.7744140625
 96%|| 50/52 [06:36<00:13,  6.75s/it]-> item_loss 0.7734375 batch_loss 1.0228817588404606, real_loss 0.7734375
 98%|| 51/52 [06:41<00:06,  6.25s/it]-> item_loss 0.71240234375 batch_loss 1.0188495586444806, real_loss 0.71240234375
100%|| 52/52 [06:46<00:00,  5.89s/it]100%|| 52/52 [06:46<00:00,  7.82s/it]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 272685.98it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 504252.21it/s]
127.0.0.1 - - [04/Oct/2023 10:08:45] "POST /train/sft HTTP/1.1" 200 28
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/52 [00:00<?, ?it/s]-> item_loss 1.8232421875 batch_loss 1.8232421875, real_loss 1.9140625
-> item_loss 1.8125 batch_loss 1.81787109375, real_loss 1.9033203125
-> item_loss 1.8896484375 batch_loss 1.841796875, real_loss 1.984375
-> item_loss 1.7353515625 batch_loss 1.815185546875, real_loss 1.822265625
  2%|         | 1/52 [00:21<18:03, 21.25s/it]-> item_loss 0.400634765625 batch_loss 1.532275390625, real_loss 0.0200347900390625
  4%|         | 2/52 [00:26<09:48, 11.76s/it][2023-10-04 10:09:18,766] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=7, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.4140625 batch_loss 1.5125732421875, real_loss 1.4140625
-> item_loss 1.6171875 batch_loss 1.5275181361607142, real_loss 1.6982421875
  6%|         | 3/52 [00:36<09:04, 11.12s/it]-> item_loss 0.50244140625 batch_loss 1.399383544921875, real_loss 0.50244140625
-> item_loss 0.44775390625 batch_loss 1.2936469184027777, real_loss 0.0223846435546875
  8%|         | 4/52 [00:47<08:39, 10.82s/it]-> item_loss 0.740234375 batch_loss 1.2383056640625, real_loss 0.740234375
 10%|         | 5/52 [00:52<06:50,  8.74s/it]-> item_loss 0.2841796875 batch_loss 1.1515669389204546, real_loss 0.0142059326171875
 12%|        | 6/52 [00:57<05:46,  7.54s/it]-> item_loss 0.69384765625 batch_loss 1.1134236653645833, real_loss 0.69384765625
 13%|        | 7/52 [01:02<05:03,  6.73s/it]-> item_loss 0.47412109375 batch_loss 1.0642465444711537, real_loss 0.023712158203125
 15%|        | 8/52 [01:07<04:34,  6.25s/it]-> item_loss 0.33642578125 batch_loss 1.0122593470982142, real_loss 0.016815185546875
-> item_loss 0.347900390625 batch_loss 0.96796875, real_loss 0.01739501953125
 17%|        | 9/52 [01:18<05:27,  7.61s/it][2023-10-04 10:10:10,241] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=7, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.0126953125 batch_loss 0.97076416015625, real_loss 1.0126953125
 19%|        | 10/52 [01:23<04:45,  6.79s/it][2023-10-04 10:10:20,932] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 124543
[2023-10-04 10:10:20,933] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-04 10:10:52,056] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 10:10:55,580] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-04 10:10:55,607] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-04 10:10:57,439] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 10:10:58,785] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-04 10:10:58,785] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-04 10:10:58,785] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-04 10:10:58,785] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-04 10:10:58,785] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-04 10:11:00,570] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.217724800109863 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-04 10:11:54,204] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-04 10:11:54,205] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-04 10:11:54,205] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-04 10:11:56,349] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-04 10:11:56,351] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-04 10:11:56,351] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-04 10:11:56,391] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-04 10:11:56,391] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-04 10:11:56,391] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-10-04 10:11:56,391] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-04 10:11:56,391] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-04 10:11:56,391] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-04 10:11:56,391] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-10-04 10:12:17,423] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-04 10:12:17,423] [INFO] [utils.py:804:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-10-04 10:12:17,424] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 25.14 GB, percent = 6.7%
[2023-10-04 10:12:31,480] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-04 10:12:31,481] [INFO] [utils.py:804:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-10-04 10:12:31,481] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 64.35 GB, percent = 17.0%
[2023-10-04 10:12:31,482] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-04 10:12:33,043] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-04 10:12:33,043] [INFO] [utils.py:804:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-10-04 10:12:33,044] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 64.35 GB, percent = 17.0%
[2023-10-04 10:12:33,077] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-04 10:12:33,077] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-04 10:12:33,077] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fe548b78be0>
[2023-10-04 10:12:33,077] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-04 10:12:33,079] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-04 10:12:33,079] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-04 10:12:33,079] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-04 10:12:33,079] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-04 10:12:33,079] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-04 10:12:33,079] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-04 10:12:33,079] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-04 10:12:33,080] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-04 10:12:33,080] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-04 10:12:33,080] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-04 10:12:33,080] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe5164d8ee0>
[2023-10-04 10:12:33,080] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-04 10:12:33,080] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-04 10:12:33,080] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-04 10:12:33,080] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-04 10:12:33,080] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-04 10:12:33,080] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-04 10:12:33,080] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-04 10:12:33,080] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-04 10:12:33,080] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-04 10:12:33,080] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... None
[2023-10-04 10:12:33,080] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-04 10:12:33,080] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-04 10:12:33,080] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-04 10:12:33,080] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-04 10:12:33,080] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-04 10:12:33,080] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-04 10:12:33,080] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-04 10:12:33,080] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-04 10:12:33,081] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-04 10:12:33,081] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-04 10:12:33,081] [INFO] [config.py:971:print]   fp16_auto_cast ............... None
[2023-10-04 10:12:33,081] [INFO] [config.py:971:print]   fp16_enabled ................. False
[2023-10-04 10:12:33,081] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-04 10:12:33,081] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-04 10:12:33,081] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-04 10:12:33,081] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-04 10:12:33,081] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-04 10:12:33,081] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-04 10:12:33,081] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-04 10:12:33,081] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-04 10:12:33,081] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-04 10:12:33,081] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-04 10:12:33,081] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-04 10:12:33,081] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-04 10:12:33,081] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-04 10:12:33,081] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-04 10:12:33,081] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-04 10:12:33,082] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-04 10:12:33,082] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-04 10:12:33,082] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-04 10:12:33,082] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-04 10:12:33,082] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-04 10:12:33,082] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-04 10:12:33,082] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-04 10:12:33,082] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-04 10:12:33,082] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-04 10:12:33,082] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-04 10:12:33,082] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-04 10:12:33,082] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-04 10:12:33,082] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-04 10:12:33,082] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-04 10:12:33,082] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-04 10:12:33,082] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-04 10:12:33,082] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-04 10:12:33,082] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-04 10:12:33,082] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-04 10:12:33,082] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-04 10:12:33,082] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-04 10:12:33,082] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-04 10:12:33,082] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-04 10:12:33,083] [INFO] [config.py:957:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/52 [00:00<?, ?it/s][2023-10-04 10:12:44,714] [INFO] [checkpointing.py:530:forward] Activation Checkpointing Information
[2023-10-04 10:12:44,714] [INFO] [checkpointing.py:531:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-10-04 10:12:44,714] [INFO] [checkpointing.py:532:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-10-04 10:12:44,714] [INFO] [checkpointing.py:534:forward] ----Synchronization False
[2023-10-04 10:12:44,714] [INFO] [checkpointing.py:535:forward] ----Profiling time in checkpointing False
-> item_loss 2.1699633598327637 batch_loss 2.1699633598327637, real_loss 2.278461456298828
-> item_loss 2.027548313140869 batch_loss 2.0987558364868164, real_loss 2.1289255619049072
-> item_loss 2.213153839111328 batch_loss 2.1368885040283203, real_loss 2.3238115310668945
-> item_loss 2.199608564376831 batch_loss 2.152568519115448, real_loss 2.30958890914917
  2%|         | 1/52 [00:32<27:32, 32.40s/it]-> item_loss 0.726295530796051 batch_loss 1.8673139214515686, real_loss 0.726295530796051
  4%|         | 2/52 [00:38<14:14, 17.09s/it]-> item_loss 0.9069786071777344 batch_loss 1.7072580357392628, real_loss 0.9069786071777344
  6%|         | 3/52 [00:44<09:45, 11.96s/it]-> item_loss 2.0486257076263428 batch_loss 1.7560248460088457, real_loss 2.151057004928589
  8%|         | 4/52 [00:50<07:39,  9.56s/it]-> item_loss 0.5368545651435852 batch_loss 1.6036285609006882, real_loss 0.5368545651435852
 10%|         | 5/52 [01:00<07:30,  9.59s/it]-> item_loss 0.8379176259040833 batch_loss 1.5185495681232877, real_loss 0.8379176259040833
[2023-10-04 10:13:58,627] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.899761438369751 batch_loss 1.4566707551479339, real_loss 0.899761438369751
-> item_loss 0.777443528175354 batch_loss 1.394922825423154, real_loss 0.777443528175354
-> item_loss 0.6936947107315063 batch_loss 1.33648714919885, real_loss 0.6936947107315063
 12%|        | 6/52 [01:28<12:21, 16.11s/it]-> item_loss 0.65217524766922 batch_loss 1.2838477721581092, real_loss 0.65217524766922
-> item_loss 0.7569912672042847 batch_loss 1.2462151646614075, real_loss 0.7569912672042847
 13%|        | 7/52 [01:42<11:22, 15.16s/it]-> item_loss 0.5892549157142639 batch_loss 1.2024178147315978, real_loss 0.5892549157142639
-> item_loss 0.720714271068573 batch_loss 1.1723113432526588, real_loss 0.720714271068573
 15%|        | 8/52 [01:56<10:52, 14.84s/it]-> item_loss 0.7247427105903625 batch_loss 1.145983776625465, real_loss 0.7247427105903625
 17%|        | 9/52 [02:01<08:31, 11.90s/it]-> item_loss 2.0506222248077393 batch_loss 1.1962414681911469, real_loss 2.15315318107605
 19%|        | 10/52 [02:07<06:54,  9.86s/it]-> item_loss 1.9042719602584839 batch_loss 1.233506230931533, real_loss 1.9994854927062988
 21%|        | 11/52 [02:13<05:57,  8.72s/it][2023-10-04 10:15:05,408] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.543072521686554 batch_loss 1.198984545469284, real_loss 0.543072521686554
 23%|       | 12/52 [02:20<05:34,  8.37s/it]-> item_loss 1.819199562072754 batch_loss 1.228518593878973, real_loss 1.9101594686508179
 25%|       | 13/52 [02:26<04:55,  7.57s/it]-> item_loss 0.5224162936210632 batch_loss 1.196423034776341, real_loss 0.5224162936210632
-> item_loss 0.44337698817253113 batch_loss 1.1636819023153055, real_loss 0.022168850526213646
-> item_loss 0.5127100944519043 batch_loss 1.136558076987664, real_loss 0.5127100944519043
-> item_loss 0.3462010622024536 batch_loss 1.1049437963962554, real_loss 0.01731005311012268
 27%|       | 14/52 [02:52<08:17, 13.10s/it]-> item_loss 1.8318688869476318 batch_loss 1.1329024537251546, real_loss 1.9234622716903687
 29%|       | 15/52 [02:58<06:41, 10.86s/it]-> item_loss 1.7253040075302124 batch_loss 1.1548432520142309, real_loss 1.811569094657898
-> item_loss 1.842255711555481 batch_loss 1.179393696997847, real_loss 1.934368371963501
-> item_loss 2.023138999938965 batch_loss 1.2084883626165062, real_loss 2.124295949935913
 31%|       | 16/52 [03:17<08:03, 13.44s/it][2023-10-04 10:16:07,948] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.6788178086280823 batch_loss 1.1908326774835587, real_loss 0.6788178086280823
 33%|      | 17/52 [03:23<06:30, 11.15s/it]-> item_loss 1.8746421337127686 batch_loss 1.2128910470393397, real_loss 1.9683741331100464
 35%|      | 18/52 [03:28<05:20,  9.43s/it]-> item_loss 2.045973539352417 batch_loss 1.2389248749241233, real_loss 2.1482720375061035
-> item_loss 2.4836313724517822 batch_loss 1.2766432536370826, real_loss 2.6078128814697266
 37%|      | 19/52 [03:41<05:41, 10.35s/it]-> item_loss 0.4959775507450104 batch_loss 1.2536824976696688, real_loss 0.02479887753725052
-> item_loss 1.7707403898239136 batch_loss 1.2684555803026472, real_loss 1.859277367591858
 38%|      | 20/52 [03:53<05:47, 10.86s/it]-> item_loss 0.9690132141113281 batch_loss 1.2601377367973328, real_loss 0.9690132141113281
 40%|      | 21/52 [03:58<04:48,  9.29s/it]-> item_loss 1.1519320011138916 batch_loss 1.2572132574545372, real_loss 1.1519320011138916
 42%|     | 22/52 [04:03<03:59,  8.00s/it]-> item_loss 0.6226611733436584 batch_loss 1.2405145183989876, real_loss 0.6226611733436584
 44%|     | 23/52 [04:08<03:26,  7.11s/it]-> item_loss 1.5082731246948242 batch_loss 1.2473801236886244, real_loss 1.5836867094039917
 46%|     | 24/52 [04:14<03:07,  6.71s/it][2023-10-04 10:17:04,919] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.6881707310676575 batch_loss 1.2333998888731004, real_loss 0.6881707310676575
 48%|     | 25/52 [04:20<02:51,  6.37s/it]-> item_loss 0.8200363516807556 batch_loss 1.223317851380604, real_loss 0.8200363516807556
 50%|     | 26/52 [04:25<02:40,  6.16s/it]-> item_loss 0.8225670456886292 batch_loss 1.213776165530795, real_loss 0.8225670456886292
 52%|    | 27/52 [04:31<02:32,  6.09s/it]-> item_loss 2.0149953365325928 batch_loss 1.2324091695075812, real_loss 2.1157450675964355
 54%|    | 28/52 [04:37<02:24,  6.02s/it]-> item_loss 3.6434760093688965 batch_loss 1.2872061431407928, real_loss 3.8256497383117676
 56%|    | 29/52 [04:42<02:12,  5.76s/it]-> item_loss 0.6864883899688721 batch_loss 1.2738568597369724, real_loss 0.6864883899688721
 58%|    | 30/52 [04:48<02:08,  5.83s/it]-> item_loss 1.6276931762695312 batch_loss 1.2815489535746367, real_loss 1.7090777158737183
-> item_loss 1.5956156253814697 batch_loss 1.2882312231875481, real_loss 1.6753963232040405
 60%|    | 31/52 [05:00<02:39,  7.62s/it]-> item_loss 1.2441741228103638 batch_loss 1.2873133669296901, real_loss 1.2441741228103638
 62%|   | 32/52 [05:06<02:22,  7.11s/it]-> item_loss 2.0827548503875732 batch_loss 1.3035468665920957, real_loss 2.186892509460449
[2023-10-04 10:18:04,712] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 2.9053564071655273 batch_loss 1.3355830574035645, real_loss 3.050624132156372
-> item_loss 2.4616763591766357 batch_loss 1.3576633182226443, real_loss 2.5847599506378174
-> item_loss 3.861109495162964 batch_loss 1.4058065139330351, real_loss 4.054164886474609
 63%|   | 33/52 [05:31<03:57, 12.51s/it]-> item_loss 1.855185866355896 batch_loss 1.4142853696391267, real_loss 1.9479451179504395
 65%|   | 34/52 [05:37<03:07, 10.41s/it]-> item_loss 0.7526465654373169 batch_loss 1.4020327991909451, real_loss 0.7526465654373169
-> item_loss 1.538382887840271 batch_loss 1.404511891711842, real_loss 1.6153019666671753
-> item_loss 1.8953678607940674 batch_loss 1.4132771768740244, real_loss 1.9901361465454102
 67%|   | 35/52 [05:56<03:41, 13.05s/it]-> item_loss 1.0012468099594116 batch_loss 1.406048573945698, real_loss 1.0012468099594116
 69%|   | 36/52 [06:02<02:54, 10.94s/it]-> item_loss 0.6448466777801514 batch_loss 1.3929244033221542, real_loss 0.6448466777801514
-> item_loss 0.7052116990089417 batch_loss 1.3812682557914218, real_loss 0.7052116990089417
 71%|   | 37/52 [06:14<02:49, 11.32s/it][2023-10-04 10:19:04,470] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.9744700789451599 batch_loss 1.3744882861773173, real_loss 0.9744700789451599
 73%|  | 38/52 [06:19<02:12,  9.48s/it]-> item_loss 0.7499905824661255 batch_loss 1.3642506189033634, real_loss 0.7499905824661255
-> item_loss 0.5657616257667542 batch_loss 1.3513717641753535, real_loss 0.5657616257667542
 75%|  | 39/52 [06:33<02:18, 10.66s/it]-> item_loss 0.316891610622406 batch_loss 1.3349514442776877, real_loss 0.01584458164870739
-> item_loss 1.279189944267273 batch_loss 1.334080170840025, real_loss 1.279189944267273
 77%|  | 40/52 [06:45<02:13, 11.12s/it]-> item_loss 0.4951956570148468 batch_loss 1.3211742552427146, real_loss 0.02475978247821331
 79%|  | 41/52 [06:50<01:43,  9.43s/it]-> item_loss 1.222856879234314 batch_loss 1.3196845980304661, real_loss 1.222856879234314
 81%|  | 42/52 [06:56<01:22,  8.22s/it]-> item_loss 1.0129048824310303 batch_loss 1.3151057963051014, real_loss 1.0129048824310303
 83%| | 43/52 [07:01<01:05,  7.30s/it]-> item_loss 0.6553678512573242 batch_loss 1.3054037677014576, real_loss 0.6553678512573242
 85%| | 44/52 [07:07<00:55,  6.93s/it]-> item_loss 0.6583198308944702 batch_loss 1.2960257396317911, real_loss 0.6583198308944702
 87%| | 45/52 [07:12<00:44,  6.39s/it][2023-10-04 10:20:03,628] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.9791174530982971 batch_loss 1.2914984783955983, real_loss 0.9791174530982971
-> item_loss 2.5365374088287354 batch_loss 1.3090342379791635, real_loss 2.6633641719818115
 88%| | 46/52 [07:23<00:46,  7.72s/it]-> item_loss 1.4239310026168823 batch_loss 1.3106300263769097, real_loss 1.4239310026168823
 90%| | 47/52 [07:28<00:34,  6.88s/it]-> item_loss 1.4268286228179932 batch_loss 1.312221787971993, real_loss 1.4268286228179932
 92%|| 48/52 [07:33<00:25,  6.43s/it]-> item_loss 0.7203050851821899 batch_loss 1.3042229136099686, real_loss 0.7203050851821899
 94%|| 49/52 [07:39<00:18,  6.09s/it]-> item_loss 1.5613741874694824 batch_loss 1.3076515972614289, real_loss 1.639442801475525
 96%|| 50/52 [07:44<00:11,  5.89s/it]-> item_loss 1.7071735858917236 batch_loss 1.31290846553288, real_loss 1.792532205581665
 98%|| 51/52 [07:49<00:05,  5.74s/it]-> item_loss 0.40080827474594116 batch_loss 1.301063008509673, real_loss 0.020040413364768028
100%|| 52/52 [07:54<00:00,  5.48s/it]100%|| 52/52 [07:54<00:00,  9.13s/it]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 269613.97it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 506554.25it/s]
127.0.0.1 - - [04/Oct/2023 10:20:40] "POST /train/sft HTTP/1.1" 200 27
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/52 [00:00<?, ?it/s]-> item_loss 1.8289076089859009 batch_loss 1.8289076089859009, real_loss 1.9203529357910156
-> item_loss 1.7644414901733398 batch_loss 1.7966745495796204, real_loss 1.852663516998291
[2023-10-04 10:21:01,513] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.8748583793640137 batch_loss 1.8227358261744182, real_loss 1.9686012268066406
-> item_loss 1.7968603372573853 batch_loss 1.81626695394516, real_loss 1.8867032527923584
  2%|         | 1/52 [00:24<21:03, 24.78s/it]-> item_loss 1.5484509468078613 batch_loss 1.7627037525177003, real_loss 1.6258734464645386
  4%|         | 2/52 [00:29<10:55, 13.11s/it]-> item_loss 1.4435746669769287 batch_loss 1.7095155715942383, real_loss 1.4435746669769287
  6%|         | 3/52 [00:34<07:38,  9.35s/it]-> item_loss 1.1344242095947266 batch_loss 1.6273596627371651, real_loss 1.1344242095947266
  8%|         | 4/52 [00:39<06:03,  7.57s/it]-> item_loss 1.370748519897461 batch_loss 1.5952832698822021, real_loss 1.370748519897461
 10%|         | 5/52 [00:44<05:08,  6.56s/it]-> item_loss 2.646421194076538 batch_loss 1.7120763725704617, real_loss 2.7787420749664307
 12%|        | 6/52 [00:48<04:31,  5.89s/it]-> item_loss 0.3621833622455597 batch_loss 1.5770870715379715, real_loss 0.018109168857336044
 13%|        | 7/52 [00:54<04:18,  5.74s/it]-> item_loss 0.664578914642334 batch_loss 1.494131784547459, real_loss 0.664578914642334
-> item_loss 0.45322540402412415 batch_loss 1.4073895861705144, real_loss 0.022661270573735237
 15%|        | 8/52 [01:07<05:52,  8.00s/it][2023-10-04 10:21:55,205] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.4092957079410553 batch_loss 1.3306131339990175, real_loss 0.020464785397052765
-> item_loss 1.1499522924423218 batch_loss 1.3177087881735392, real_loss 1.1499522924423218
 17%|        | 9/52 [01:17<06:17,  8.78s/it]-> item_loss 0.7327180504798889 batch_loss 1.2787094056606292, real_loss 0.7327180504798889
-> item_loss 0.7936058640480042 batch_loss 1.2483904343098402, real_loss 0.7936058640480042
-> item_loss 0.6235688924789429 batch_loss 1.2116362259668463, real_loss 0.6235688924789429
-> item_loss 0.41900429129600525 batch_loss 1.167601118485133, real_loss 0.020950214937329292
 19%|        | 10/52 [01:42<09:44, 13.91s/it]-> item_loss 0.8482062220573425 batch_loss 1.150790860778407, real_loss 0.8482062220573425
 21%|        | 11/52 [01:48<07:44, 11.32s/it]-> item_loss 1.6320618391036987 batch_loss 1.1748544096946716, real_loss 1.7136648893356323
 23%|       | 12/52 [01:53<06:22,  9.55s/it]-> item_loss 1.25741446018219 batch_loss 1.1787858406702678, real_loss 1.25741446018219
 25%|       | 13/52 [01:59<05:29,  8.45s/it]-> item_loss 0.8159226775169373 batch_loss 1.1622920605269345, real_loss 0.8159226775169373
 27%|       | 14/52 [02:05<04:54,  7.76s/it][2023-10-04 10:22:54,347] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.5585975050926208 batch_loss 1.1360444711602253, real_loss 0.5585975050926208
 29%|       | 15/52 [02:12<04:29,  7.28s/it]-> item_loss 0.689669132232666 batch_loss 1.1174454987049103, real_loss 0.689669132232666
 31%|       | 16/52 [02:17<04:02,  6.74s/it]-> item_loss 0.6037330031394958 batch_loss 1.0968969988822936, real_loss 0.6037330031394958
 33%|      | 17/52 [02:23<03:43,  6.39s/it]-> item_loss 1.463136911392212 batch_loss 1.1109831493634443, real_loss 1.463136911392212
-> item_loss 1.5605578422546387 batch_loss 1.12763406391497, real_loss 1.6385856866836548
-> item_loss 1.453498125076294 batch_loss 1.1392720660993032, real_loss 1.453498125076294
 35%|      | 18/52 [02:41<05:40, 10.02s/it]-> item_loss 1.569715976715088 batch_loss 1.154114959568813, real_loss 1.6482017040252686
 37%|      | 19/52 [02:46<04:39,  8.47s/it]-> item_loss 0.36216554045677185 batch_loss 1.1277166455984116, real_loss 0.018108276650309563
 38%|      | 20/52 [02:51<03:53,  7.29s/it]-> item_loss 0.5311331152915955 batch_loss 1.1084720155885142, real_loss 0.5311331152915955
-> item_loss 1.2404696941375732 batch_loss 1.1125969430431724, real_loss 1.2404696941375732
[2023-10-04 10:23:50,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.3574928045272827 batch_loss 1.1200180297548121, real_loss 1.3574928045272827
 40%|      | 21/52 [03:08<05:22, 10.39s/it]-> item_loss 0.18932920694351196 batch_loss 1.0926448290838915, real_loss 0.009466460905969143
 42%|     | 22/52 [03:13<04:21,  8.73s/it]-> item_loss 0.44079500436782837 batch_loss 1.0740205483777183, real_loss 0.02203975059092045
 44%|     | 23/52 [03:18<03:42,  7.67s/it]-> item_loss 0.45434004068374634 batch_loss 1.0568072009417746, real_loss 0.022717002779245377
-> item_loss 0.3690336048603058 batch_loss 1.0382187253720052, real_loss 0.01845168136060238
 46%|     | 24/52 [03:30<04:09,  8.91s/it]-> item_loss 0.5418074131011963 batch_loss 1.0251552697859312, real_loss 0.5418074131011963
 48%|     | 25/52 [03:36<03:34,  7.94s/it]-> item_loss 0.5857554078102112 batch_loss 1.0138886066583486, real_loss 0.5857554078102112
-> item_loss 0.5573199391365051 batch_loss 1.0024743899703026, real_loss 0.5573199391365051
 50%|     | 26/52 [03:47<03:54,  9.02s/it]-> item_loss 0.5756919384002686 batch_loss 0.9920650618832286, real_loss 0.5756919384002686
-> item_loss 0.6962215304374695 batch_loss 0.9850211682773772, real_loss 0.6962215304374695
 52%|    | 27/52 [04:00<04:15, 10.20s/it][2023-10-04 10:24:48,703] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.45472440123558044 batch_loss 0.9726886853229167, real_loss 0.022736219689249992
 54%|    | 28/52 [04:06<03:32,  8.87s/it]-> item_loss 1.4700074195861816 batch_loss 0.9839913838289001, real_loss 1.4700074195861816
 56%|    | 29/52 [04:12<03:02,  7.93s/it]-> item_loss 1.8396409749984741 batch_loss 1.0030058191882238, real_loss 1.9316229820251465
-> item_loss 2.6367340087890625 batch_loss 1.0385216493969378, real_loss 2.7685706615448
-> item_loss 2.131221294403076 batch_loss 1.0617705780140898, real_loss 2.2377822399139404
-> item_loss 2.231299638748169 batch_loss 1.086135766779383, real_loss 2.342864513397217
 58%|    | 30/52 [04:35<04:36, 12.56s/it]-> item_loss 0.38174283504486084 batch_loss 1.0717604008256172, real_loss 0.019087141379714012
 60%|    | 31/52 [04:40<03:35, 10.26s/it]-> item_loss 1.37783682346344 batch_loss 1.0778819292783737, real_loss 1.37783682346344
 62%|   | 32/52 [04:45<02:56,  8.81s/it]-> item_loss 0.839985191822052 batch_loss 1.0732172873674655, real_loss 0.839985191822052
 63%|   | 33/52 [04:50<02:23,  7.55s/it]-> item_loss 1.3947770595550537 batch_loss 1.0794011291403036, real_loss 1.3947770595550537
 65%|   | 34/52 [04:55<02:02,  6.80s/it][2023-10-04 10:25:42,709] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.36794808506965637 batch_loss 1.0659774867993481, real_loss 0.01839740388095379
 67%|   | 35/52 [05:00<01:46,  6.24s/it]-> item_loss 0.5186637043952942 batch_loss 1.0558420463844582, real_loss 0.5186637043952942
 69%|   | 36/52 [05:05<01:34,  5.88s/it]-> item_loss 0.6237335801124573 batch_loss 1.0479855288158764, real_loss 0.6237335801124573
 71%|   | 37/52 [05:11<01:26,  5.76s/it]-> item_loss 0.212983638048172 batch_loss 1.033074780766453, real_loss 0.0106491819024086
 73%|  | 38/52 [05:16<01:18,  5.58s/it]-> item_loss 0.6897615790367126 batch_loss 1.0270517421396155, real_loss 0.6897615790367126
 75%|  | 39/52 [05:22<01:13,  5.67s/it]-> item_loss 0.6898013949394226 batch_loss 1.0212370809809914, real_loss 0.6898013949394226
 77%|  | 40/52 [05:27<01:06,  5.54s/it]-> item_loss 0.6449121236801147 batch_loss 1.014858691874197, real_loss 0.6449121236801147
 79%|  | 41/52 [05:32<00:58,  5.29s/it]-> item_loss 1.539551854133606 batch_loss 1.0236035779118537, real_loss 1.6165293455123901
 81%|  | 42/52 [05:37<00:54,  5.42s/it]-> item_loss 0.8625405430793762 batch_loss 1.0209632002916493, real_loss 0.8625405430793762
 83%| | 43/52 [05:42<00:48,  5.34s/it]-> item_loss 0.9357112050056458 batch_loss 1.0195881681096168, real_loss 0.9357112050056458
 85%| | 44/52 [05:48<00:43,  5.45s/it][2023-10-04 10:26:37,286] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.4866241216659546 batch_loss 1.01112842134067, real_loss 0.02433120645582676
-> item_loss 0.4098309874534607 batch_loss 1.0017331489361823, real_loss 0.020491549745202065
-> item_loss 0.4850524067878723 batch_loss 0.9937842144415929, real_loss 0.024252621456980705
-> item_loss 0.30229225754737854 batch_loss 0.9833070635795593, real_loss 0.015114612877368927
 87%| | 45/52 [06:13<01:19, 11.31s/it]-> item_loss 0.7794811129570007 batch_loss 0.9802648852120585, real_loss 0.7794811129570007
-> item_loss 1.8506989479064941 batch_loss 0.9930653861340355, real_loss 1.943233847618103
 88%| | 46/52 [06:24<01:06, 11.05s/it]-> item_loss 0.2317212074995041 batch_loss 0.9820314125306364, real_loss 0.011586060747504234
-> item_loss 0.792637825012207 batch_loss 0.9793257898518017, real_loss 0.792637825012207
 90%| | 47/52 [06:35<00:55, 11.06s/it]-> item_loss 1.2903671264648438 batch_loss 0.9837066537477601, real_loss 1.2903671264648438
-> item_loss 1.066784143447876 batch_loss 0.9848605077713728, real_loss 1.066784143447876
 92%|| 48/52 [06:46<00:44, 11.13s/it][2023-10-04 10:27:33,841] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.12891685962677 batch_loss 0.9868338824543235, real_loss 1.12891685962677
 94%|| 49/52 [06:51<00:28,  9.36s/it]-> item_loss 1.1618469953536987 batch_loss 0.9891989245205313, real_loss 1.1618469953536987
 96%|| 50/52 [06:56<00:16,  8.10s/it]-> item_loss 0.6800609230995178 batch_loss 0.9850770845015844, real_loss 0.6800609230995178
 98%|| 51/52 [07:01<00:07,  7.12s/it]-> item_loss 1.7927870750427246 batch_loss 0.9957048475350204, real_loss 1.882426381111145
-> item_loss 2.2032032012939453 batch_loss 1.0113866443370845, real_loss 2.3133633136749268
100%|| 52/52 [07:13<00:00,  8.53s/it]100%|| 52/52 [07:13<00:00,  8.34s/it]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 300885.61it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 510153.59it/s]
127.0.0.1 - - [04/Oct/2023 10:27:57] "POST /train/sft HTTP/1.1" 200 28
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/52 [00:00<?, ?it/s]-> item_loss 1.5968254804611206 batch_loss 1.5968254804611206, real_loss 1.6766667366027832
-> item_loss 1.5010064840316772 batch_loss 1.548915982246399, real_loss 1.576056718826294
-> item_loss 1.4921998977661133 batch_loss 1.5300106207529705, real_loss 1.4921998977661133
-> item_loss 1.3623881340026855 batch_loss 1.4881049990653992, real_loss 1.3623881340026855
  2%|         | 1/52 [00:24<20:48, 24.47s/it]-> item_loss 0.9974385499954224 batch_loss 1.389971709251404, real_loss 0.9974385499954224
  4%|         | 2/52 [00:29<10:57, 13.16s/it][2023-10-04 10:28:33,663] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.597945511341095 batch_loss 1.257967342933019, real_loss 0.597945511341095
  6%|         | 3/52 [00:35<07:53,  9.66s/it]-> item_loss 1.0516114234924316 batch_loss 1.228487925870078, real_loss 1.0516114234924316
  8%|         | 4/52 [00:40<06:17,  7.87s/it]-> item_loss 0.13081997632980347 batch_loss 1.0912794321775436, real_loss 0.006540998816490173
 10%|         | 5/52 [00:44<05:13,  6.66s/it]-> item_loss 0.2612651586532593 batch_loss 0.9990556240081787, real_loss 0.013063258491456509
 12%|        | 6/52 [00:50<04:43,  6.16s/it]-> item_loss 0.6533276438713074 batch_loss 0.9644828259944915, real_loss 0.6533276438713074
 13%|        | 7/52 [00:55<04:26,  5.93s/it]-> item_loss 0.40582388639450073 batch_loss 0.9136956496672197, real_loss 0.020291194319725037
 15%|        | 8/52 [01:00<04:13,  5.76s/it]-> item_loss 0.6606067419052124 batch_loss 0.8926049073537191, real_loss 0.6606067419052124
 17%|        | 9/52 [01:06<04:04,  5.69s/it]-> item_loss 0.5385593175888062 batch_loss 0.8653706312179565, real_loss 0.5385593175888062
 19%|        | 10/52 [01:11<03:46,  5.38s/it]-> item_loss 0.4027235805988312 batch_loss 0.8323244133165905, real_loss 0.02013617940247059
-> item_loss 0.35668471455574036 batch_loss 0.8006151000658671, real_loss 0.017834236845374107
 21%|        | 11/52 [01:22<04:58,  7.29s/it][2023-10-04 10:29:26,065] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.9681898355484009 batch_loss 0.8110885210335255, real_loss 0.9681898355484009
 23%|       | 12/52 [01:27<04:22,  6.56s/it]-> item_loss 1.0178626775741577 batch_loss 0.8232517067123862, real_loss 1.0178626775741577
 25%|       | 13/52 [01:32<03:59,  6.15s/it]-> item_loss 0.421091228723526 batch_loss 0.8009094579352273, real_loss 0.02105456218123436
 27%|       | 14/52 [01:38<03:44,  5.90s/it]-> item_loss 0.5988028645515442 batch_loss 0.7902722688097703, real_loss 0.5988028645515442
-> item_loss 1.3162212371826172 batch_loss 0.8165697172284127, real_loss 1.3162212371826172
 29%|       | 15/52 [01:48<04:31,  7.33s/it]-> item_loss 1.0628525018692017 batch_loss 0.8282974688779741, real_loss 1.0628525018692017
 31%|       | 16/52 [01:53<03:57,  6.60s/it]-> item_loss 1.2041376829147339 batch_loss 0.845381114970554, real_loss 1.2041376829147339
 33%|      | 17/52 [01:59<03:40,  6.30s/it]-> item_loss 0.24990637600421906 batch_loss 0.8194909089285395, real_loss 0.012495319359004498
 35%|      | 18/52 [02:03<03:17,  5.80s/it]-> item_loss 0.3161641061306 batch_loss 0.7985189588119587, real_loss 0.01580820605158806
-> item_loss 0.32429319620132446 batch_loss 0.7795499283075332, real_loss 0.016214659437537193
[2023-10-04 10:30:21,184] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.3834766149520874 batch_loss 0.7643163393323238, real_loss 0.01917383074760437
-> item_loss 0.208196222782135 batch_loss 0.7437192979786131, real_loss 0.01040981151163578
 37%|      | 19/52 [02:27<06:11, 11.27s/it]-> item_loss 0.6666563749313354 batch_loss 0.7409670507269246, real_loss 0.6666563749313354
 38%|      | 20/52 [02:33<05:05,  9.54s/it]-> item_loss 1.2275962829589844 batch_loss 0.7577473690797543, real_loss 1.2275962829589844
-> item_loss 1.1627641916275024 batch_loss 0.7712479298313458, real_loss 1.1627641916275024
-> item_loss 0.984220027923584 batch_loss 0.7781179975117406, real_loss 0.984220027923584
 40%|      | 21/52 [02:50<06:09, 11.92s/it]-> item_loss 0.933531641960144 batch_loss 0.7829746739007533, real_loss 0.933531641960144
 42%|     | 22/52 [02:56<04:59,  9.98s/it]-> item_loss 0.2618812322616577 batch_loss 0.7671839635480534, real_loss 0.013094061985611916
 44%|     | 23/52 [03:01<04:06,  8.50s/it]-> item_loss 1.4479761123657227 batch_loss 0.7872072620426908, real_loss 1.4479761123657227
-> item_loss 2.0938234329223633 batch_loss 0.8245391526392528, real_loss 2.198514461517334
[2023-10-04 10:31:18,350] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.6319961547851562 batch_loss 0.8469685138099723, real_loss 1.7135958671569824
-> item_loss 0.882818877696991 batch_loss 0.8479374425636755, real_loss 0.882818877696991
 46%|     | 24/52 [03:24<05:59, 12.83s/it]-> item_loss 0.5517263412475586 batch_loss 0.8401424135816725, real_loss 0.5517263412475586
-> item_loss 0.32716020941734314 batch_loss 0.826989023731305, real_loss 0.016358010470867157
 48%|     | 25/52 [03:36<05:39, 12.59s/it]-> item_loss 1.0545485019683838 batch_loss 0.832678010687232, real_loss 1.0545485019683838
 50%|     | 26/52 [03:41<04:30, 10.39s/it]-> item_loss 0.11525740474462509 batch_loss 0.8151799471276563, real_loss 0.0057628704234957695
 52%|    | 27/52 [03:46<03:37,  8.69s/it]-> item_loss 0.5852893590927124 batch_loss 0.8097063616982528, real_loss 0.5852893590927124
 54%|    | 28/52 [03:51<03:04,  7.67s/it]-> item_loss 1.7982397079467773 batch_loss 0.8326955092854278, real_loss 1.8881516456604004
 56%|    | 29/52 [03:56<02:34,  6.70s/it]-> item_loss 0.3314504325389862 batch_loss 0.8213035757230087, real_loss 0.01657252199947834
 58%|    | 30/52 [04:01<02:16,  6.19s/it]-> item_loss 0.6126150488853455 batch_loss 0.816666052904394, real_loss 0.6126150488853455
[2023-10-04 10:32:12,167] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.6441168785095215 batch_loss 0.8129149838958097, real_loss 0.6441168785095215
-> item_loss 0.45619070529937744 batch_loss 0.8053251056278006, real_loss 0.022809535264968872
-> item_loss 0.3401907980442047 batch_loss 0.7956348075531423, real_loss 0.017009539529681206
 60%|    | 31/52 [04:25<04:03, 11.60s/it]-> item_loss 0.389935702085495 batch_loss 0.7873552339721699, real_loss 0.01949678547680378
 62%|   | 32/52 [04:31<03:17,  9.88s/it]-> item_loss 0.3733444809913635 batch_loss 0.7790750189125538, real_loss 0.018667224794626236
 63%|   | 33/52 [04:36<02:40,  8.43s/it]-> item_loss 0.3434990644454956 batch_loss 0.7705343139230036, real_loss 0.01717495359480381
-> item_loss 0.24809958040714264 batch_loss 0.7604874921246216, real_loss 0.012404979206621647
 65%|   | 34/52 [04:47<02:48,  9.34s/it]-> item_loss 0.5114315152168274 batch_loss 0.7557883227490029, real_loss 0.5114315152168274
 67%|   | 35/52 [04:52<02:17,  8.11s/it]-> item_loss 0.3767300844192505 batch_loss 0.7487687257428964, real_loss 0.018836503848433495
-> item_loss 0.48483577370643616 batch_loss 0.7439699447967789, real_loss 0.024241788312792778
 69%|   | 36/52 [05:05<02:28,  9.31s/it][2023-10-04 10:33:08,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.551020085811615 batch_loss 0.7405244116006153, real_loss 0.551020085811615
 71%|   | 37/52 [05:10<02:01,  8.07s/it]-> item_loss 0.20164379477500916 batch_loss 0.7310703656913942, real_loss 0.010082189925014973
-> item_loss 0.4684409201145172 batch_loss 0.7265422718021376, real_loss 0.02342204563319683
 73%|  | 38/52 [05:20<02:03,  8.81s/it]-> item_loss 0.6161083579063416 batch_loss 0.7246705105496665, real_loss 0.6161083579063416
 75%|  | 39/52 [05:25<01:38,  7.57s/it]-> item_loss 0.9309442639350891 batch_loss 0.7281084064394235, real_loss 0.9309442639350891
 77%|  | 40/52 [05:30<01:21,  6.82s/it]-> item_loss 0.16711387038230896 batch_loss 0.7189117747007824, real_loss 0.008355693891644478
 79%|  | 41/52 [05:35<01:07,  6.15s/it]-> item_loss 1.0259064435958862 batch_loss 0.7238633016184453, real_loss 1.0259064435958862
-> item_loss 0.4490422308444977 batch_loss 0.7195010623998113, real_loss 0.022452112287282944
 81%|  | 42/52 [05:45<01:15,  7.58s/it]-> item_loss 1.0959179401397705 batch_loss 0.7253825761144981, real_loss 1.0959179401397705
 83%| | 43/52 [05:50<01:00,  6.77s/it]-> item_loss 1.2855608463287354 batch_loss 0.7340007033485633, real_loss 1.2855608463287354
 85%| | 44/52 [05:55<00:50,  6.28s/it][2023-10-04 10:33:59,277] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.5199878215789795 batch_loss 0.730758083927812, real_loss 0.5199878215789795
 87%| | 45/52 [06:00<00:40,  5.84s/it]-> item_loss 0.421017587184906 batch_loss 0.7261350914391119, real_loss 0.02105087973177433
-> item_loss 1.1370693445205688 batch_loss 0.7321782422197216, real_loss 1.1370693445205688
-> item_loss 0.9603620767593384 batch_loss 0.7354852543144986, real_loss 0.9603620767593384
 88%| | 46/52 [06:20<00:59,  9.88s/it]-> item_loss 0.39129820466041565 batch_loss 0.7305682964622975, real_loss 0.019564909860491753
 90%| | 47/52 [06:26<00:43,  8.69s/it]-> item_loss 0.30744510889053345 batch_loss 0.7246088149472022, real_loss 0.015372255817055702
-> item_loss 0.5759523510932922 batch_loss 0.7225441418381201, real_loss 0.5759523510932922
 92%|| 48/52 [06:37<00:38,  9.50s/it]-> item_loss 0.7817439436912537 batch_loss 0.723355098027889, real_loss 0.7817439436912537
 94%|| 49/52 [06:42<00:24,  8.24s/it]-> item_loss 0.8367241024971008 batch_loss 0.7248871116017973, real_loss 0.8367241024971008
 96%|| 50/52 [06:47<00:14,  7.15s/it]-> item_loss 0.3986735939979553 batch_loss 0.7205375980337461, real_loss 0.019933680072426796
 98%|| 51/52 [06:52<00:06,  6.45s/it][2023-10-04 10:34:56,874] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.5095739364624023 batch_loss 0.7309196551183337, real_loss 1.5850526094436646
-> item_loss 1.744335651397705 batch_loss 0.7440809018232606, real_loss 1.8315523862838745
100%|| 52/52 [07:03<00:00,  7.96s/it]100%|| 52/52 [07:03<00:00,  8.15s/it]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 285007.00it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 516522.41it/s]
127.0.0.1 - - [04/Oct/2023 10:35:03] "POST /train/sft HTTP/1.1" 200 28
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/52 [00:00<?, ?it/s]-> item_loss 1.4412955045700073 batch_loss 1.4412955045700073, real_loss 1.4412955045700073
-> item_loss 1.2710870504379272 batch_loss 1.3561912775039673, real_loss 1.2710870504379272
-> item_loss 1.1761653423309326 batch_loss 1.296182632446289, real_loss 1.1761653423309326
-> item_loss 0.9918202757835388 batch_loss 1.2200920432806015, real_loss 0.9918202757835388
  2%|         | 1/52 [00:24<20:54, 24.60s/it]-> item_loss 0.2954556941986084 batch_loss 1.0351647734642029, real_loss 0.014772784896194935
-> item_loss 0.1795371174812317 batch_loss 0.8925601641337076, real_loss 0.008976856246590614
  4%|         | 2/52 [00:36<14:06, 16.93s/it]-> item_loss 0.1577131152153015 batch_loss 0.787582014288221, real_loss 0.00788565631955862
  6%|         | 3/52 [00:40<09:14, 11.32s/it]-> item_loss 0.45548683404922485 batch_loss 0.7460701167583466, real_loss 0.022774342447519302
[2023-10-04 10:35:56,158] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.9910084009170532 batch_loss 0.7732854816648695, real_loss 0.9910084009170532
  8%|         | 4/52 [00:51<08:46, 10.97s/it]-> item_loss 0.10566046833992004 batch_loss 0.7065229803323746, real_loss 0.005283023696392775
 10%|         | 5/52 [00:56<06:52,  8.77s/it]-> item_loss 0.7647335529327393 batch_loss 0.7118148505687714, real_loss 0.7647335529327393
 12%|        | 6/52 [01:00<05:42,  7.44s/it]-> item_loss 0.2608991265296936 batch_loss 0.6742385402321815, real_loss 0.013044956140220165
 13%|        | 7/52 [01:05<04:56,  6.59s/it]-> item_loss 0.4311804473400116 batch_loss 0.6555417638558608, real_loss 0.02155902236700058
 15%|        | 8/52 [01:10<04:27,  6.08s/it]-> item_loss 0.37098827958106995 batch_loss 0.63521651497909, real_loss 0.018549414351582527
 17%|        | 9/52 [01:16<04:09,  5.80s/it]-> item_loss 0.42197561264038086 batch_loss 0.6210004548231761, real_loss 0.021098781377077103
 19%|        | 10/52 [01:21<04:00,  5.73s/it]-> item_loss 0.5138651728630066 batch_loss 0.6143044997006655, real_loss 0.5138651728630066
 21%|        | 11/52 [01:26<03:43,  5.44s/it]-> item_loss 0.24250008165836334 batch_loss 0.5924336515805301, real_loss 0.012125004082918167
 23%|       | 12/52 [01:31<03:32,  5.31s/it]-> item_loss 0.32679593563079834 batch_loss 0.5776760006944338, real_loss 0.016339797526597977
[2023-10-04 10:36:48,991] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.9421340227127075 batch_loss 0.5968580018532904, real_loss 0.9421340227127075
-> item_loss 0.7512067556381226 batch_loss 0.604575439542532, real_loss 0.7512067556381226
 25%|       | 13/52 [01:49<05:53,  9.07s/it]-> item_loss 0.31582969427108765 batch_loss 0.5908256421486536, real_loss 0.015791485086083412
 27%|       | 14/52 [01:54<04:57,  7.84s/it]-> item_loss 0.2382616549730301 batch_loss 0.5748000063679435, real_loss 0.01191308256238699
-> item_loss 0.26164954900741577 batch_loss 0.5611847690913988, real_loss 0.013082477264106274
-> item_loss 0.30424588918685913 batch_loss 0.5504789824287096, real_loss 0.015212294645607471
-> item_loss 0.1334025114774704 batch_loss 0.5337959235906601, real_loss 0.006670125760138035
 29%|       | 15/52 [02:18<07:56, 12.88s/it]-> item_loss 0.3339191675186157 batch_loss 0.5261083560494276, real_loss 0.016695959493517876
 31%|       | 16/52 [02:24<06:23, 10.66s/it]-> item_loss 0.05947782099246979 batch_loss 0.5088257436399106, real_loss 0.002973891096189618
 33%|      | 17/52 [02:28<05:11,  8.89s/it]-> item_loss 0.3009141981601715 batch_loss 0.5014003313013485, real_loss 0.015045709908008575
 35%|      | 18/52 [02:34<04:28,  7.89s/it][2023-10-04 10:37:43,911] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.06552457064390182 batch_loss 0.48637013265798834, real_loss 0.0032762286718934774
 37%|      | 19/52 [02:39<03:46,  6.88s/it]-> item_loss 1.1711177825927734 batch_loss 0.5091950543224811, real_loss 1.1711177825927734
-> item_loss 1.7387269735336304 batch_loss 0.5488573742970344, real_loss 1.8256632089614868
-> item_loss 1.2797187566757202 batch_loss 0.5716967924963683, real_loss 1.2797187566757202
-> item_loss 0.12454436719417572 batch_loss 0.5581467190023625, real_loss 0.006227218545973301
 38%|      | 20/52 [03:01<06:14, 11.71s/it]-> item_loss 0.5321846604347229 batch_loss 0.5573831290444907, real_loss 0.5321846604347229
 40%|      | 21/52 [03:06<04:57,  9.60s/it]-> item_loss 0.28277742862701416 batch_loss 0.5495372518897057, real_loss 0.014138871803879738
-> item_loss 0.4013601243495941 batch_loss 0.545421220569147, real_loss 0.020068006590008736
 42%|     | 22/52 [03:18<05:11, 10.39s/it]-> item_loss 0.22606058418750763 batch_loss 0.5367898520182919, real_loss 0.011303029023110867
-> item_loss 0.3066332936286926 batch_loss 0.5307331004817235, real_loss 0.015331665053963661
 44%|     | 23/52 [03:29<05:02, 10.42s/it][2023-10-04 10:38:39,678] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.7740812301635742 batch_loss 0.5369727961145915, real_loss 0.7740812301635742
 46%|     | 24/52 [03:34<04:09,  8.91s/it]-> item_loss 0.34495431184768677 batch_loss 0.5321723340079189, real_loss 0.017247715964913368
-> item_loss 0.25201255083084106 batch_loss 0.5253391685645755, real_loss 0.012600627727806568
 48%|     | 25/52 [03:46<04:21,  9.68s/it]-> item_loss 0.9800580739974976 batch_loss 0.5361658091701212, real_loss 0.9800580739974976
 50%|     | 26/52 [03:51<03:35,  8.29s/it]-> item_loss 0.6807681322097778 batch_loss 0.5395286538919737, real_loss 0.6807681322097778
 52%|    | 27/52 [03:56<03:04,  7.39s/it]-> item_loss 0.5511114001274109 batch_loss 0.5397918981245973, real_loss 0.5511114001274109
 54%|    | 28/52 [04:01<02:40,  6.70s/it]-> item_loss 0.6425780057907104 batch_loss 0.542076033850511, real_loss 0.6425780057907104
 56%|    | 29/52 [04:06<02:22,  6.21s/it]-> item_loss 0.17972064018249512 batch_loss 0.534198742683815, real_loss 0.0089860325679183
 58%|    | 30/52 [04:12<02:10,  5.95s/it]-> item_loss 0.7034977674484253 batch_loss 0.5378008495937002, real_loss 0.7034977674484253
 60%|    | 31/52 [04:17<02:00,  5.75s/it]-> item_loss 1.0828003883361816 batch_loss 0.5491550066508353, real_loss 1.0828003883361816
[2023-10-04 10:39:34,988] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.8495230078697205 batch_loss 0.5552849658593839, real_loss 0.8495230078697205
-> item_loss 0.44025832414627075 batch_loss 0.5529844330251217, real_loss 0.022012917324900627
 62%|   | 32/52 [04:34<03:06,  9.31s/it]-> item_loss 0.3422309458255768 batch_loss 0.5488520117074835, real_loss 0.01711154729127884
 63%|   | 33/52 [04:40<02:33,  8.10s/it]-> item_loss 0.2860619127750397 batch_loss 0.5437983559587827, real_loss 0.014303095638751984
 65%|   | 34/52 [04:44<02:07,  7.09s/it]-> item_loss 0.5761834383010864 batch_loss 0.5444093952482602, real_loss 0.5761834383010864
 67%|   | 35/52 [04:49<01:48,  6.40s/it]-> item_loss 0.4336059093475342 batch_loss 0.5423574788426911, real_loss 0.02168029546737671
-> item_loss 0.5020662546157837 batch_loss 0.5416249111294746, real_loss 0.5020662546157837
-> item_loss 0.3614993393421173 batch_loss 0.5384083830618432, real_loss 0.018074966967105865
-> item_loss 0.27323251962661743 batch_loss 0.5337561749314007, real_loss 0.013661625795066357
 69%|   | 36/52 [05:13<03:07, 11.74s/it]-> item_loss 1.2104359865188599 batch_loss 0.5454230682346327, real_loss 1.2104359865188599
[2023-10-04 10:40:30,368] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.3344565629959106 batch_loss 0.5587965172983832, real_loss 1.3344565629959106
 71%|   | 37/52 [05:25<02:54, 11.66s/it]-> item_loss 0.07104030251502991 batch_loss 0.550667247051994, real_loss 0.003552015172317624
 73%|  | 38/52 [05:30<02:13,  9.53s/it]-> item_loss 0.28712987899780273 batch_loss 0.5463469623297942, real_loss 0.014356493949890137
 75%|  | 39/52 [05:35<01:46,  8.21s/it]-> item_loss 0.9433542490005493 batch_loss 0.5527503056631934, real_loss 0.9433542490005493
 77%|  | 40/52 [05:40<01:29,  7.43s/it]-> item_loss 0.1457699090242386 batch_loss 0.546290299367337, real_loss 0.007288495544344187
-> item_loss 0.2552870810031891 batch_loss 0.5417433740803972, real_loss 0.01276435423642397
 79%|  | 41/52 [05:51<01:31,  8.36s/it]-> item_loss 0.32407668232917786 batch_loss 0.5383946557457631, real_loss 0.016203833743929863
 81%|  | 42/52 [05:56<01:14,  7.47s/it]-> item_loss 0.344565749168396 batch_loss 0.5354578541309545, real_loss 0.01722828857600689
 83%| | 43/52 [06:01<00:59,  6.67s/it]-> item_loss 0.29165586829185486 batch_loss 0.5318190185214157, real_loss 0.014582793228328228
 85%| | 44/52 [06:06<00:50,  6.31s/it]-> item_loss 0.7982859015464783 batch_loss 0.5357376491541371, real_loss 0.7982859015464783
 87%| | 45/52 [06:11<00:40,  5.75s/it][2023-10-04 10:41:22,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.7287349700927734 batch_loss 0.5385347117764362, real_loss 0.7287349700927734
-> item_loss 0.2579900622367859 batch_loss 0.534526931068727, real_loss 0.012899503111839294
 88%| | 46/52 [06:22<00:44,  7.35s/it]-> item_loss 0.29044997692108154 batch_loss 0.5310892274891826, real_loss 0.014522499404847622
 90%| | 47/52 [06:27<00:33,  6.76s/it]-> item_loss 0.24730925261974335 batch_loss 0.5271478389493294, real_loss 0.012365463189780712
 92%|| 48/52 [06:32<00:24,  6.23s/it]-> item_loss 0.5199099183082581 batch_loss 0.5270486893515064, real_loss 0.5199099183082581
 94%|| 49/52 [06:38<00:17,  6.00s/it]-> item_loss 0.342628538608551 batch_loss 0.5245565251522774, real_loss 0.01713142730295658
-> item_loss 0.24936988949775696 batch_loss 0.520887370010217, real_loss 0.012468494474887848
 96%|| 50/52 [06:50<00:15,  7.80s/it]-> item_loss 0.5326560139656067 batch_loss 0.5210422205885774, real_loss 0.5326560139656067
 98%|| 51/52 [06:55<00:06,  6.95s/it]-> item_loss 0.6585079431533813 batch_loss 0.5228274897127957, real_loss 0.6585079431533813
100%|| 52/52 [07:00<00:00,  6.46s/it]100%|| 52/52 [07:00<00:00,  8.09s/it]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 279716.39it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 509727.49it/s]
127.0.0.1 - - [04/Oct/2023 10:42:06] "POST /train/sft HTTP/1.1" 200 28
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/52 [00:00<?, ?it/s]-> item_loss 1.2257136106491089 batch_loss 1.2257136106491089, real_loss 1.2257136106491089
[2023-10-04 10:42:20,856] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.0051589012145996 batch_loss 1.1154362559318542, real_loss 1.0051589012145996
-> item_loss 0.9196100831031799 batch_loss 1.0501608649889629, real_loss 0.9196100831031799
-> item_loss 0.6792689561843872 batch_loss 0.9574378877878189, real_loss 0.6792689561843872
  2%|         | 1/52 [00:24<21:00, 24.72s/it]-> item_loss 1.024273157119751 batch_loss 0.9708049416542053, real_loss 1.024273157119751
-> item_loss 1.095047116279602 batch_loss 0.9915119707584381, real_loss 1.095047116279602
  4%|         | 2/52 [00:36<14:20, 17.21s/it]-> item_loss 0.5473718047142029 batch_loss 0.9280633756092617, real_loss 0.5473718047142029
  6%|         | 3/52 [00:42<09:43, 11.91s/it]-> item_loss 0.09047693759202957 batch_loss 0.8233650708571076, real_loss 0.004523846786469221
  8%|         | 4/52 [00:47<07:23,  9.25s/it]-> item_loss 0.5149871706962585 batch_loss 0.7891008597281244, real_loss 0.5149871706962585
 10%|         | 5/52 [00:52<06:07,  7.82s/it]-> item_loss 0.21672245860099792 batch_loss 0.7318630196154118, real_loss 0.010836123488843441
-> item_loss 0.2678923010826111 batch_loss 0.6896838633851572, real_loss 0.013394615612924099
 12%|        | 6/52 [01:04<07:06,  9.27s/it][2023-10-04 10:43:17,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.45320504903793335 batch_loss 0.6699772955228885, real_loss 0.022660253569483757
 13%|        | 7/52 [01:09<05:52,  7.83s/it]-> item_loss 0.13932184875011444 batch_loss 0.6291576457711366, real_loss 0.006966092623770237
-> item_loss 0.11435572803020477 batch_loss 0.5923860802182129, real_loss 0.005717786494642496
 15%|        | 8/52 [01:19<06:19,  8.61s/it]-> item_loss 0.45476236939430237 batch_loss 0.5832111661632856, real_loss 0.022738119587302208
-> item_loss 0.1600572019815445 batch_loss 0.5567640434019268, real_loss 0.008002860471606255
 17%|        | 9/52 [01:30<06:40,  9.30s/it]-> item_loss 0.2995033860206604 batch_loss 0.5416310635559699, real_loss 0.01497516967356205
 19%|        | 10/52 [01:35<05:29,  7.85s/it]-> item_loss 0.21035732328891754 batch_loss 0.523226966874467, real_loss 0.010517866350710392
 21%|        | 11/52 [01:40<04:48,  7.03s/it]-> item_loss 0.47011616826057434 batch_loss 0.5204316616842621, real_loss 0.023505808785557747
 23%|       | 12/52 [01:45<04:18,  6.46s/it]-> item_loss 0.6897420287132263 batch_loss 0.5288971800357103, real_loss 0.6897420287132263
 25%|       | 13/52 [01:50<03:55,  6.03s/it]-> item_loss 0.0464487299323082 batch_loss 0.5059234443165007, real_loss 0.0023224365431815386
 27%|       | 14/52 [01:55<03:36,  5.71s/it][2023-10-04 10:44:08,718] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.36060211062431335 batch_loss 0.499317929148674, real_loss 0.018030105158686638
 29%|       | 15/52 [02:00<03:20,  5.43s/it]-> item_loss 0.34458911418914795 batch_loss 0.49259058936782507, real_loss 0.017229456454515457
 31%|       | 16/52 [02:05<03:09,  5.25s/it]-> item_loss 0.15945672988891602 batch_loss 0.47871001188953716, real_loss 0.0079728364944458
 33%|      | 17/52 [02:10<02:57,  5.07s/it]-> item_loss 0.11543620377779007 batch_loss 0.4641790595650673, real_loss 0.0057718101888895035
 35%|      | 18/52 [02:14<02:50,  5.02s/it]-> item_loss 0.21549904346466064 batch_loss 0.4546144435612055, real_loss 0.010774952359497547
 37%|      | 19/52 [02:20<02:47,  5.06s/it]-> item_loss 0.03544580191373825 batch_loss 0.43908967905574375, real_loss 0.0017722900956869125
 38%|      | 20/52 [02:24<02:38,  4.96s/it]-> item_loss 0.27821701765060425 batch_loss 0.4333442268627031, real_loss 0.013910851441323757
 40%|      | 21/52 [02:29<02:34,  4.98s/it]-> item_loss 0.17415626347064972 batch_loss 0.4244067108836667, real_loss 0.008707812987267971
 42%|     | 22/52 [02:34<02:29,  4.99s/it]-> item_loss 0.23382668197155 batch_loss 0.41805404325326284, real_loss 0.011691334657371044
-> item_loss 0.6568337678909302 batch_loss 0.4257566150157682, real_loss 0.6568337678909302
[2023-10-04 10:45:00,494] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.3507961630821228 batch_loss 0.4234141008928418, real_loss 0.01753980852663517
 44%|     | 23/52 [02:52<04:13,  8.73s/it]-> item_loss 0.1644653081893921 batch_loss 0.4155671677806161, real_loss 0.008223265409469604
 46%|     | 24/52 [02:57<03:31,  7.56s/it]-> item_loss 0.048138320446014404 batch_loss 0.4047604369766572, real_loss 0.0024069161154329777
 48%|     | 25/52 [03:01<02:59,  6.66s/it]-> item_loss 0.7797017693519592 batch_loss 0.4154730464730944, real_loss 0.7797017693519592
 50%|     | 26/52 [03:07<02:45,  6.36s/it]-> item_loss 0.2663978636264801 batch_loss 0.41133206917179954, real_loss 0.01331989374011755
-> item_loss 0.1177741065621376 batch_loss 0.40339807018234924, real_loss 0.005888705607503653
 52%|    | 27/52 [03:18<03:17,  7.89s/it]-> item_loss 0.16813547909259796 batch_loss 0.39720694936419787, real_loss 0.008406774140894413
 54%|    | 28/52 [03:24<02:51,  7.16s/it]-> item_loss 0.11478524655103683 batch_loss 0.3899653672407835, real_loss 0.005739262327551842
 56%|    | 29/52 [03:29<02:30,  6.55s/it]-> item_loss 0.3364112973213196 batch_loss 0.3886265154927969, real_loss 0.01682056486606598
 58%|    | 30/52 [03:34<02:12,  6.03s/it]-> item_loss 0.32634928822517395 batch_loss 0.38710755873017194, real_loss 0.016317464411258698
[2023-10-04 10:45:55,151] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.313397079706192 batch_loss 0.3853525473248391, real_loss 0.01566985435783863
-> item_loss 0.2686828076839447 batch_loss 0.38263929756574855, real_loss 0.01343414094299078
-> item_loss 0.20632769167423248 batch_loss 0.3786322156136686, real_loss 0.010316384956240654
 60%|    | 31/52 [03:58<04:00, 11.45s/it]-> item_loss 0.386148065328598 batch_loss 0.3787992344962226, real_loss 0.01930740289390087
 62%|   | 32/52 [04:03<03:13,  9.66s/it]-> item_loss 0.1868143379688263 batch_loss 0.3746256497891053, real_loss 0.00934071745723486
-> item_loss 0.19806063175201416 batch_loss 0.37086894727767783, real_loss 0.009903031401336193
-> item_loss 0.20787742733955383 batch_loss 0.3674732906123002, real_loss 0.010393871925771236
-> item_loss 0.08479295670986176 batch_loss 0.36170430420612804, real_loss 0.00423964811488986
 63%|   | 33/52 [04:27<04:26, 14.01s/it]-> item_loss 0.3560309112071991 batch_loss 0.36159083634614947, real_loss 0.017801545560359955
-> item_loss 0.5875998735427856 batch_loss 0.36602238609510307, real_loss 0.5875998735427856
 65%|   | 34/52 [04:38<03:52, 12.92s/it][2023-10-04 10:46:52,833] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.9847151041030884 batch_loss 0.37792032297987205, real_loss 0.9847151041030884
-> item_loss 1.4610618352890015 batch_loss 0.39835695528759146, real_loss 1.4610618352890015
-> item_loss 0.9576320648193359 batch_loss 0.40871390176040157, real_loss 0.9576320648193359
-> item_loss 0.04624969884753227 batch_loss 0.40212364352562213, real_loss 0.002312484895810485
 67%|   | 35/52 [05:01<04:33, 16.08s/it]-> item_loss 0.2323797196149826 batch_loss 0.39909250202721785, real_loss 0.011618985794484615
 69%|   | 36/52 [05:07<03:27, 12.98s/it]-> item_loss 0.35941797494888306 batch_loss 0.3983964576925102, real_loss 0.017970899119973183
 71%|   | 37/52 [05:12<02:40, 10.69s/it]-> item_loss 0.33408331871032715 batch_loss 0.39728761046867944, real_loss 0.016704166308045387
 73%|  | 38/52 [05:18<02:07,  9.12s/it]-> item_loss 0.15619118511676788 batch_loss 0.3932012303779691, real_loss 0.007809559348970652
 75%|  | 39/52 [05:23<01:44,  8.00s/it]-> item_loss 0.13042429089546204 batch_loss 0.3888216147199273, real_loss 0.006521214731037617
 77%|  | 40/52 [05:28<01:23,  7.00s/it]-> item_loss 0.19351324439048767 batch_loss 0.3856198381571496, real_loss 0.009675662033259869
 79%|  | 41/52 [05:33<01:11,  6.54s/it][2023-10-04 10:47:47,206] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.16243796050548553 batch_loss 0.3820201304530905, real_loss 0.008121898397803307
 81%|  | 42/52 [05:38<01:01,  6.13s/it]-> item_loss 0.24335059523582458 batch_loss 0.37981902671948314, real_loss 0.012167530134320259
-> item_loss 0.1960497349500656 batch_loss 0.37694763153558597, real_loss 0.00980248674750328
 83%| | 43/52 [05:50<01:09,  7.69s/it]-> item_loss 0.2001657783985138 batch_loss 0.3742279107180925, real_loss 0.010008289478719234
 85%| | 44/52 [05:55<00:54,  6.80s/it]-> item_loss 0.21895550191402435 batch_loss 0.37187529846348544, real_loss 0.010947775095701218
 87%| | 45/52 [06:00<00:44,  6.38s/it]-> item_loss 0.09533602744340897 batch_loss 0.36774784665721566, real_loss 0.004766801372170448
 88%| | 46/52 [06:05<00:35,  5.84s/it]-> item_loss 0.43360084295272827 batch_loss 0.36871627307332616, real_loss 0.021680042147636414
 90%| | 47/52 [06:09<00:27,  5.53s/it]-> item_loss 0.264803022146225 batch_loss 0.3672102839294551, real_loss 0.013240151107311249
-> item_loss 0.15975789725780487 batch_loss 0.3642466784055744, real_loss 0.007987895049154758
 92%|| 48/52 [06:22<00:30,  7.52s/it]-> item_loss 0.11415927112102509 batch_loss 0.3607243205564962, real_loss 0.005707963835448027
[2023-10-04 10:48:40,980] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.1512092500925064 batch_loss 0.35781438902227414, real_loss 0.007560462690889835
 94%|| 49/52 [06:32<00:25,  8.49s/it]-> item_loss 0.31357795000076294 batch_loss 0.35720841040554113, real_loss 0.015678897500038147
 96%|| 50/52 [06:37<00:14,  7.49s/it]-> item_loss 0.799135684967041 batch_loss 0.36318040060231815, real_loss 0.799135684967041
-> item_loss 0.5242403745651245 batch_loss 0.36532786692182223, real_loss 0.5242403745651245
-> item_loss 0.288757860660553 batch_loss 0.3643203668394371, real_loss 0.014437893405556679
 98%|| 51/52 [06:55<00:10, 10.51s/it]-> item_loss 0.258806973695755 batch_loss 0.3629500630323763, real_loss 0.01294034905731678
100%|| 52/52 [07:00<00:00,  8.92s/it]100%|| 52/52 [07:00<00:00,  8.09s/it]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 199908.68it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 518167.04it/s]
127.0.0.1 - - [04/Oct/2023 10:49:10] "POST /train/sft HTTP/1.1" 200 28
===saveved====
127.0.0.1 - - [04/Oct/2023 10:49:32] "POST /train/save-weight HTTP/1.1" 200 26

init_state None

state-> None

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 10:50:06] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n  ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None

1. 
2. 
3. 
4. 

---after-> tensor([[-6.3754e-02,  5.2766e-02,  2.7031e-01,  ...,  1.3535e-01,
         -2.5823e-02,  1.0726e-01],
        [-1.2350e-01,  8.0682e-02,  3.0357e-02,  ...,  8.6787e-01,
         -5.5404e-02,  1.1995e-01],
        [ 5.0946e-01, -5.2723e+00, -4.2273e-01,  ..., -1.5169e+00,
          8.6043e-01, -4.0088e-01],
        ...,
        [-6.1294e+00, -8.9634e+00,  4.0686e+01,  ...,  6.9241e+00,
         -6.7543e-01, -1.5942e+01],
        [ 1.0254e+00,  3.3083e+00,  1.3959e+01,  ...,  1.6211e+00,
          1.0546e+00,  1.8271e+00],
        [ 9.5287e-01,  9.8241e+00,  6.4526e+00,  ...,  1.5430e+01,
          1.4944e+00, -1.7100e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 10:50:20] "POST /inference/generate HTTP/1.1" 200 3776

init_state None

state-> tensor([[-6.3754e-02,  5.2766e-02,  2.7031e-01,  ...,  1.3535e-01,
         -2.5823e-02,  1.0726e-01],
        [-1.2350e-01,  8.0682e-02,  3.0357e-02,  ...,  8.6787e-01,
         -5.5404e-02,  1.1995e-01],
        [ 5.0946e-01, -5.2723e+00, -4.2273e-01,  ..., -1.5169e+00,
          8.6043e-01, -4.0088e-01],
        ...,
        [-6.1294e+00, -8.9634e+00,  4.0686e+01,  ...,  6.9241e+00,
         -6.7543e-01, -1.5942e+01],
        [ 1.0254e+00,  3.3083e+00,  1.3959e+01,  ...,  1.6211e+00,
          1.0546e+00,  1.8271e+00],
        [ 9.5287e-01,  9.8241e+00,  6.4526e+00,  ...,  1.5430e+01,
          1.4944e+00, -1.7100e+00]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 10:50:26] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n  ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '\n1. \n2. \n3. \n4. ', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None

1. 
---after-> tensor([[ 2.5344e-03, -8.6851e-03,  3.1756e-01,  ..., -8.9397e-02,
          4.9238e-02, -3.0560e-03],
        [-1.3823e-01,  9.0993e-02,  8.7176e-02,  ...,  4.9556e-01,
         -9.4987e-03, -1.4021e-01],
        [ 5.2251e-01, -6.3898e+00, -3.5078e-01,  ..., -1.1268e+00,
         -1.3521e-01, -4.1994e-01],
        ...,
        [-8.8993e+00, -1.3048e+01,  4.8398e+01,  ...,  5.8325e+00,
         -3.5456e+00, -9.8358e+00],
        [ 1.0233e+00,  4.6773e+00,  1.6357e+01,  ...,  1.0701e+00,
          1.0189e+00,  1.1338e+00],
        [ 3.3447e+00,  9.3679e+00,  5.9960e+00,  ...,  1.5486e+01,
          1.9423e+00,  1.0864e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 10:50:41] "POST /inference/generate HTTP/1.1" 200 4992

init_state None

state-> tensor([[ 2.5344e-03, -8.6851e-03,  3.1756e-01,  ..., -8.9397e-02,
          4.9238e-02, -3.0560e-03],
        [-1.3823e-01,  9.0993e-02,  8.7176e-02,  ...,  4.9556e-01,
         -9.4987e-03, -1.4021e-01],
        [ 5.2251e-01, -6.3898e+00, -3.5078e-01,  ..., -1.1268e+00,
         -1.3521e-01, -4.1994e-01],
        ...,
        [-8.8993e+00, -1.3048e+01,  4.8398e+01,  ...,  5.8325e+00,
         -3.5456e+00, -9.8358e+00],
        [ 1.0233e+00,  4.6773e+00,  1.6357e+01,  ...,  1.0701e+00,
          1.0189e+00,  1.1338e+00],
        [ 3.3447e+00,  9.3679e+00,  5.9960e+00,  ...,  1.5486e+01,
          1.9423e+00,  1.0864e+00]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 10:51:11] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None
 
---after-> tensor([[-1.4327e-02, -2.0634e-03,  3.1409e-01,  ..., -1.0296e-01,
          5.2769e-02, -3.0977e-02],
        [-1.3823e-01,  9.0993e-02,  8.7176e-02,  ...,  4.9556e-01,
         -9.4987e-03, -1.4021e-01],
        [ 5.6783e-01, -3.9663e+00, -3.2096e-01,  ..., -1.3342e+00,
         -2.1363e-01, -1.8061e-01],
        ...,
        [-1.7740e+01, -4.3941e+00,  2.2954e+01,  ...,  1.7490e+00,
         -3.7835e+00, -6.2948e+00],
        [ 3.0653e+00,  5.8532e+00,  6.5987e+00,  ...,  1.0025e+00,
          1.0045e+00,  1.5613e+00],
        [ 1.4444e+00,  9.1693e+00,  7.4827e+00,  ...,  1.4122e+01,
          4.7837e+00,  3.0924e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 10:51:24] "POST /inference/generate HTTP/1.1" 200 3662

init_state None

state-> tensor([[-1.4327e-02, -2.0634e-03,  3.1409e-01,  ..., -1.0296e-01,
          5.2769e-02, -3.0977e-02],
        [-1.3823e-01,  9.0993e-02,  8.7176e-02,  ...,  4.9556e-01,
         -9.4987e-03, -1.4021e-01],
        [ 5.6783e-01, -3.9663e+00, -3.2096e-01,  ..., -1.3342e+00,
         -2.1363e-01, -1.8061e-01],
        ...,
        [-1.7740e+01, -4.3941e+00,  2.2954e+01,  ...,  1.7490e+00,
         -3.7835e+00, -6.2948e+00],
        [ 3.0653e+00,  5.8532e+00,  6.5987e+00,  ...,  1.0025e+00,
          1.0045e+00,  1.5613e+00],
        [ 1.4444e+00,  9.1693e+00,  7.4827e+00,  ...,  1.4122e+01,
          4.7837e+00,  3.0924e-01]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 10:51:30] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': ' \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None
V

---after-> tensor([[-0.0734,  0.0496,  0.2619,  ...,  0.1331, -0.0234,  0.0891],
        [-0.1235,  0.0807,  0.0304,  ...,  0.8679, -0.0554,  0.1200],
        [ 0.2876, -3.3362, -0.5039,  ..., -1.4041,  0.8835, -0.7465],
        ...,
        [-6.9160, -1.3423,  7.2455,  ..., 13.0816, -5.6010, -9.8373],
        [ 1.2781,  4.5444,  2.3712,  ...,  1.7150,  1.4355,  1.0662],
        [-2.3443,  9.8446,  9.2359,  ..., 14.6379,  4.7024,  4.5554]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 10:51:47] "POST /inference/generate HTTP/1.1" 200 5166
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 28703.79it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 181817.74it/s]
127.0.0.1 - - [04/Oct/2023 10:52:29] "POST /inference/load-model HTTP/1.1" 200 26

init_state None

state-> None

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 10:52:48] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None
 
---after-> tensor([[-1.4327e-02, -2.0634e-03,  3.1409e-01,  ..., -1.0296e-01,
          5.2769e-02, -3.0977e-02],
        [-1.3823e-01,  9.0993e-02,  8.7176e-02,  ...,  4.9556e-01,
         -9.4987e-03, -1.4021e-01],
        [ 5.6783e-01, -3.9663e+00, -3.2096e-01,  ..., -1.3342e+00,
         -2.1363e-01, -1.8061e-01],
        ...,
        [-1.7740e+01, -4.3941e+00,  2.2954e+01,  ...,  1.7490e+00,
         -3.7835e+00, -6.2948e+00],
        [ 3.0653e+00,  5.8532e+00,  6.5987e+00,  ...,  1.0025e+00,
          1.0045e+00,  1.5613e+00],
        [ 1.4444e+00,  9.1693e+00,  7.4827e+00,  ...,  1.4122e+01,
          4.7837e+00,  3.0924e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 10:52:58] "POST /inference/generate HTTP/1.1" 200 3662

init_state None

state-> tensor([[-1.4327e-02, -2.0634e-03,  3.1409e-01,  ..., -1.0296e-01,
          5.2769e-02, -3.0977e-02],
        [-1.3823e-01,  9.0993e-02,  8.7176e-02,  ...,  4.9556e-01,
         -9.4987e-03, -1.4021e-01],
        [ 5.6783e-01, -3.9663e+00, -3.2096e-01,  ..., -1.3342e+00,
         -2.1363e-01, -1.8061e-01],
        ...,
        [-1.7740e+01, -4.3941e+00,  2.2954e+01,  ...,  1.7490e+00,
         -3.7835e+00, -6.2948e+00],
        [ 3.0653e+00,  5.8532e+00,  6.5987e+00,  ...,  1.0025e+00,
          1.0045e+00,  1.5613e+00],
        [ 1.4444e+00,  9.1693e+00,  7.4827e+00,  ...,  1.4122e+01,
          4.7837e+00,  3.0924e-01]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 10:53:12] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': ' \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None
V

---after-> tensor([[-0.0734,  0.0496,  0.2619,  ...,  0.1331, -0.0234,  0.0891],
        [-0.1235,  0.0807,  0.0304,  ...,  0.8679, -0.0554,  0.1200],
        [ 0.2876, -3.3362, -0.5039,  ..., -1.4041,  0.8835, -0.7465],
        ...,
        [-6.9160, -1.3423,  7.2455,  ..., 13.0816, -5.6010, -9.8373],
        [ 1.2781,  4.5444,  2.3712,  ...,  1.7150,  1.4355,  1.0662],
        [-2.3443,  9.8446,  9.2359,  ..., 14.6379,  4.7024,  4.5554]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 10:53:25] "POST /inference/generate HTTP/1.1" 200 5166

init_state None

state-> tensor([[-0.0734,  0.0496,  0.2619,  ...,  0.1331, -0.0234,  0.0891],
        [-0.1235,  0.0807,  0.0304,  ...,  0.8679, -0.0554,  0.1200],
        [ 0.2876, -3.3362, -0.5039,  ..., -1.4041,  0.8835, -0.7465],
        ...,
        [-6.9160, -1.3423,  7.2455,  ..., 13.0816, -5.6010, -9.8373],
        [ 1.2781,  4.5444,  2.3712,  ...,  1.7150,  1.4355,  1.0662],
        [-2.3443,  9.8446,  9.2359,  ..., 14.6379,  4.7024,  4.5554]],
       device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 10:53:38] "POST /state/reset HTTP/1.1" 200 21
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 288510.21it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 399457.52it/s]
172.16.1.39 - - [04/Oct/2023 10:54:06] "POST /train/tx-data HTTP/1.1" 200 27

init_state None

state-> None

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 10:54:20] "POST /state/reset HTTP/1.1" 200 21
[2023-10-04 10:54:28,742] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 300922.70it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 458591.95it/s]
172.16.1.39 - - [04/Oct/2023 10:54:46] "POST /train/tx-data HTTP/1.1" 200 28

init_state None

state-> None

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 10:54:56] "POST /state/reset HTTP/1.1" 200 21
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 302376.43it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 458419.70it/s]
172.16.1.39 - - [04/Oct/2023 10:55:22] "POST /train/tx-data HTTP/1.1" 200 28

init_state None

state-> None

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 10:55:32] "POST /state/reset HTTP/1.1" 200 21
[2023-10-04 10:55:52,993] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 281198.59it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 431210.90it/s]
172.16.1.39 - - [04/Oct/2023 10:55:58] "POST /train/tx-data HTTP/1.1" 200 29
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/52 [00:00<?, ?it/s]-> item_loss 0.6245952844619751 batch_loss 0.6245952844619751, real_loss 0.6245952844619751
-> item_loss 0.30352309346199036 batch_loss 0.4640591889619827, real_loss 0.015176154673099518
-> item_loss 0.3271939158439636 batch_loss 0.4184374312559764, real_loss 0.01635969616472721
-> item_loss 0.21442201733589172 batch_loss 0.3674335777759552, real_loss 0.010721101425588131
  2%|         | 1/52 [00:25<21:43, 25.57s/it]-> item_loss 0.04183598607778549 batch_loss 0.3023140594363213, real_loss 0.002091799397021532
  4%|         | 2/52 [00:30<11:22, 13.66s/it]-> item_loss 0.20301967859268188 batch_loss 0.28576499596238136, real_loss 0.010150983929634094
  6%|         | 3/52 [00:36<08:00,  9.81s/it]-> item_loss 0.17994742095470428 batch_loss 0.2706481995327132, real_loss 0.008997371420264244
  8%|         | 4/52 [00:41<06:21,  7.95s/it]-> item_loss 0.1903667449951172 batch_loss 0.2606130177155137, real_loss 0.009518337436020374
 10%|         | 5/52 [00:46<05:32,  7.08s/it][2023-10-04 10:57:01,332] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.6414987444877625 batch_loss 0.30293365402354133, real_loss 0.6414987444877625
-> item_loss 0.3362584412097931 batch_loss 0.3062661327421665, real_loss 0.016812922433018684
-> item_loss 0.1823139190673828 batch_loss 0.29499774968082254, real_loss 0.00911569595336914
 12%|        | 6/52 [01:06<08:36, 11.22s/it]-> item_loss 0.32441818714141846 batch_loss 0.29744945280253887, real_loss 0.016220910474658012
-> item_loss 0.2931256592273712 batch_loss 0.29711685329675674, real_loss 0.01465628296136856
-> item_loss 0.2779324948787689 batch_loss 0.29574654198118616, real_loss 0.013896624557673931
-> item_loss 0.18849307298660278 batch_loss 0.2885963107148806, real_loss 0.009424653835594654
 13%|        | 7/52 [01:32<12:03, 16.08s/it]-> item_loss 0.1133817657828331 batch_loss 0.27764540165662766, real_loss 0.005669088568538427
 15%|        | 8/52 [01:37<09:23, 12.81s/it]-> item_loss 0.7717468738555908 batch_loss 0.3067101941389196, real_loss 0.7717468738555908
-> item_loss 1.1924636363983154 batch_loss 0.35591871870888603, real_loss 1.1924636363983154
[2023-10-04 10:58:05,289] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.7598726153373718 batch_loss 0.3771794501103853, real_loss 0.7598726153373718
-> item_loss 0.03202016279101372 batch_loss 0.35992148574441674, real_loss 0.0016010081162676215
 17%|        | 9/52 [02:02<11:48, 16.48s/it]-> item_loss 0.19599060714244843 batch_loss 0.35211525343003724, real_loss 0.009799530729651451
-> item_loss 0.45722606778144836 batch_loss 0.35689301771873777, real_loss 0.022861303761601448
-> item_loss 0.1847103387117386 batch_loss 0.34940681428365084, real_loss 0.009235517121851444
 19%|        | 10/52 [02:21<12:02, 17.20s/it]-> item_loss 0.18652817606925964 batch_loss 0.34262020435805124, real_loss 0.009326408617198467
 21%|        | 11/52 [02:26<09:14, 13.53s/it]-> item_loss 0.14653688669204712 batch_loss 0.33477687165141107, real_loss 0.007326844613999128
 23%|       | 12/52 [02:32<07:26, 11.15s/it]-> item_loss 0.07414662837982178 batch_loss 0.3247526315255807, real_loss 0.00370733137242496
 25%|       | 13/52 [02:38<06:13,  9.57s/it]-> item_loss 0.17917558550834656 batch_loss 0.319360889080498, real_loss 0.008958779275417328
 27%|       | 14/52 [02:44<05:25,  8.55s/it]-> item_loss 0.17939803004264832 batch_loss 0.3143622155434319, real_loss 0.00896990206092596
 29%|       | 15/52 [02:49<04:41,  7.61s/it][2023-10-04 10:59:02,886] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.11438660323619843 batch_loss 0.30746650477421694, real_loss 0.005719330161809921
 31%|       | 16/52 [02:55<04:10,  6.95s/it]-> item_loss 0.2900424599647522 batch_loss 0.30688570328056813, real_loss 0.01450212299823761
 33%|      | 17/52 [03:00<03:49,  6.57s/it]-> item_loss 0.25241732597351074 batch_loss 0.3051286588513082, real_loss 0.012620866298675537
 35%|      | 18/52 [03:06<03:37,  6.38s/it]-> item_loss 0.06069592759013176 batch_loss 0.29749013599939644, real_loss 0.0030347963329404593
 37%|      | 19/52 [03:11<03:16,  5.96s/it]-> item_loss 0.24163039028644562 batch_loss 0.2957974164323373, real_loss 0.012081519700586796
 38%|      | 20/52 [03:17<03:08,  5.91s/it]-> item_loss 0.079246386885643 batch_loss 0.2894282685044934, real_loss 0.00396231934428215
 40%|      | 21/52 [03:22<02:55,  5.66s/it]-> item_loss 0.07053263485431671 batch_loss 0.28317410754305977, real_loss 0.0035266317427158356
 42%|     | 22/52 [03:28<02:47,  5.60s/it]-> item_loss 0.132384791970253 batch_loss 0.2789855154438151, real_loss 0.006619239691644907
 44%|     | 23/52 [03:33<02:41,  5.58s/it]-> item_loss 0.16969934105873108 batch_loss 0.2760318350550291, real_loss 0.008484967052936554
 46%|     | 24/52 [03:39<02:37,  5.61s/it]-> item_loss 0.24437075853347778 batch_loss 0.2751986488307777, real_loss 0.012218537740409374
 48%|     | 25/52 [03:44<02:29,  5.53s/it][2023-10-04 10:59:57,539] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.04529412090778351 batch_loss 0.2693036609353163, real_loss 0.0022647061850875616
 50%|     | 26/52 [03:49<02:20,  5.42s/it]-> item_loss 0.17479951679706573 batch_loss 0.2669410573318601, real_loss 0.008739976212382317
-> item_loss 0.1999552994966507 batch_loss 0.2653072583602696, real_loss 0.00999776553362608
 52%|    | 27/52 [04:02<03:13,  7.73s/it]-> item_loss 0.34962546825408936 batch_loss 0.2673148347863129, real_loss 0.017481273040175438
-> item_loss 0.11226588487625122 batch_loss 0.2637090452535208, real_loss 0.005613294430077076
 54%|    | 28/52 [04:14<03:36,  9.00s/it]-> item_loss 0.6447936296463013 batch_loss 0.2723700585351749, real_loss 0.6447936296463013
 56%|    | 29/52 [04:21<03:07,  8.17s/it]-> item_loss 0.10068542510271072 batch_loss 0.26855484445889793, real_loss 0.005034271162003279
-> item_loss 0.05843324214220047 batch_loss 0.2639869835389697, real_loss 0.0029216620605438948
 58%|    | 30/52 [04:32<03:19,  9.07s/it]-> item_loss 0.07593770325183868 batch_loss 0.2599859350222222, real_loss 0.0037968852557241917
 60%|    | 31/52 [04:37<02:45,  7.86s/it]-> item_loss 0.0890105664730072 batch_loss 0.2564239481774469, real_loss 0.004450528416782618
[2023-10-04 11:00:56,169] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.1107344850897789 batch_loss 0.253450693828719, real_loss 0.0055367243476212025
 62%|   | 32/52 [04:48<02:56,  8.83s/it]-> item_loss 0.14946402609348297 batch_loss 0.2513709604740143, real_loss 0.0074732014909386635
-> item_loss 0.16490016877651215 batch_loss 0.2496754547544554, real_loss 0.008245008997619152
-> item_loss 0.18112647533416748 batch_loss 0.2483572051502191, real_loss 0.009056324139237404
-> item_loss 0.06562220305204391 batch_loss 0.24490937492195167, real_loss 0.0032811101991683245
 63%|   | 33/52 [05:13<04:20, 13.69s/it]-> item_loss 0.17878872156143188 batch_loss 0.24368491837823833, real_loss 0.008939436636865139
-> item_loss 0.1372653990983963 batch_loss 0.24175001802769575, real_loss 0.006863269954919815
 65%|   | 34/52 [05:25<03:58, 13.26s/it]-> item_loss 0.055276378989219666 batch_loss 0.23842013161629438, real_loss 0.0027638189494609833
 67%|   | 35/52 [05:30<03:04, 10.86s/it]-> item_loss 0.20506572723388672 batch_loss 0.23783496662712933, real_loss 0.01025328692048788
-> item_loss 0.12066317349672318 batch_loss 0.23581476329729475, real_loss 0.006033158861100674
 69%|   | 36/52 [05:43<03:03, 11.50s/it][2023-10-04 11:01:58,571] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.8418782353401184 batch_loss 0.2460870255353087, real_loss 0.8418782353401184
-> item_loss 0.8532077670097351 batch_loss 0.2562057045598825, real_loss 0.8532077670097351
 71%|   | 37/52 [05:56<02:58, 11.88s/it]-> item_loss 0.21442826092243195 batch_loss 0.2555208284346784, real_loss 0.010721413418650627
 73%|  | 38/52 [06:02<02:18,  9.90s/it]-> item_loss 0.25332868099212646 batch_loss 0.255485471217863, real_loss 0.012666434049606323
 75%|  | 39/52 [06:07<01:52,  8.64s/it]-> item_loss 0.11087153851985931 batch_loss 0.25319001196868834, real_loss 0.005543577019125223
 77%|  | 40/52 [06:13<01:32,  7.70s/it]-> item_loss 0.15632832050323486 batch_loss 0.25167654803954065, real_loss 0.007816416211426258
 79%|  | 41/52 [06:19<01:18,  7.12s/it]-> item_loss 0.2890859544277191 batch_loss 0.25225207736858957, real_loss 0.014454297721385956
-> item_loss 0.4190634787082672 batch_loss 0.25477952284343314, real_loss 0.02095317468047142
 81%|  | 42/52 [06:30<01:23,  8.35s/it]-> item_loss 0.3154425621032715 batch_loss 0.25568494133984865, real_loss 0.015772128477692604
 83%| | 43/52 [06:35<01:07,  7.50s/it]-> item_loss 0.33460018038749695 batch_loss 0.2568454595611376, real_loss 0.016730008646845818
 85%| | 44/52 [06:41<00:54,  6.87s/it][2023-10-04 11:02:53,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.14623217284679413 batch_loss 0.2552423684493355, real_loss 0.007311608642339706
 87%| | 45/52 [06:46<00:44,  6.30s/it]-> item_loss 0.5285922288894653 batch_loss 0.2591473664556231, real_loss 0.5285922288894653
 88%| | 46/52 [06:50<00:35,  5.87s/it]-> item_loss 0.24297602474689484 batch_loss 0.2589196010794438, real_loss 0.012148801237344742
 90%| | 47/52 [06:56<00:29,  5.87s/it]-> item_loss 0.10831018537282944 batch_loss 0.25682780363907415, real_loss 0.005415509454905987
 92%|| 48/52 [07:02<00:23,  5.84s/it]-> item_loss 0.10175464302301407 batch_loss 0.2547035137676213, real_loss 0.005087732337415218
 94%|| 49/52 [07:07<00:16,  5.62s/it]-> item_loss 0.18520505726337433 batch_loss 0.2537643454364828, real_loss 0.009260253049433231
-> item_loss 0.06254102289676666 batch_loss 0.2512147011359533, real_loss 0.0031270512845367193
 96%|| 50/52 [07:20<00:15,  7.67s/it]-> item_loss 0.022854967042803764 batch_loss 0.24820996779262236, real_loss 0.001142748398706317
 98%|| 51/52 [07:25<00:06,  6.90s/it]-> item_loss 0.10650026798248291 batch_loss 0.24636958208080237, real_loss 0.005325013305991888
100%|| 52/52 [07:31<00:00,  6.66s/it]100%|| 52/52 [07:31<00:00,  8.68s/it]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 308841.72it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 527004.52it/s]
127.0.0.1 - - [04/Oct/2023 11:03:40] "POST /train/sft HTTP/1.1" 200 29
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/52 [00:00<?, ?it/s]-> item_loss 0.3129439055919647 batch_loss 0.3129439055919647, real_loss 0.015647195279598236
[2023-10-04 11:03:56,050] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.13724905252456665 batch_loss 0.22509647905826569, real_loss 0.00686245271936059
-> item_loss 0.15118876099586487 batch_loss 0.20046057303746542, real_loss 0.007559438236057758
-> item_loss 0.09772004187107086 batch_loss 0.17477544024586678, real_loss 0.004886002279818058
  2%|         | 1/52 [00:25<22:03, 25.96s/it]-> item_loss 0.3513668477535248 batch_loss 0.21009372174739838, real_loss 0.01756834238767624
  4%|         | 2/52 [00:31<11:50, 14.22s/it]-> item_loss 0.17875173687934875 batch_loss 0.20487005760272345, real_loss 0.008937587030231953
  6%|         | 3/52 [00:37<08:22, 10.25s/it]-> item_loss 0.0858534425497055 batch_loss 0.18786768402372087, real_loss 0.004292672034353018
  8%|         | 4/52 [00:43<06:42,  8.38s/it]-> item_loss 0.07497907429933548 batch_loss 0.1737566078081727, real_loss 0.0037489538080990314
 10%|         | 5/52 [00:48<05:42,  7.30s/it]-> item_loss 0.0319468230009079 batch_loss 0.15799996505180994, real_loss 0.001597341150045395
 12%|        | 6/52 [00:53<04:59,  6.50s/it]-> item_loss 0.1959747076034546 batch_loss 0.1617974393069744, real_loss 0.00979873538017273
-> item_loss 0.16965897381305695 batch_loss 0.16251212426207282, real_loss 0.008482948876917362
[2023-10-04 11:04:56,023] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.13687364757061005 batch_loss 0.16037558453778425, real_loss 0.006843682378530502
-> item_loss 0.10908330976963043 batch_loss 0.15643002494023397, real_loss 0.005454165395349264
 13%|        | 7/52 [01:19<09:34, 12.77s/it]-> item_loss 0.1833423227071762 batch_loss 0.15835233192358697, real_loss 0.00916711613535881
 15%|        | 8/52 [01:24<07:46, 10.60s/it]-> item_loss 0.17538705468177795 batch_loss 0.15948798010746637, real_loss 0.008769352920353413
 17%|        | 9/52 [01:30<06:22,  8.90s/it]-> item_loss 0.04730016738176346 batch_loss 0.15247624181210995, real_loss 0.0023650084622204304
 19%|        | 10/52 [01:36<05:36,  8.00s/it]-> item_loss 0.10363457351922989 batch_loss 0.14960320250076406, real_loss 0.005181728862226009
 21%|        | 11/52 [01:42<05:02,  7.38s/it]-> item_loss 0.24779179692268372 batch_loss 0.15505812441309294, real_loss 0.0123895900323987
 23%|       | 12/52 [01:47<04:26,  6.66s/it]-> item_loss 0.14750103652477264 batch_loss 0.1546603829452866, real_loss 0.007375051733106375
 25%|       | 13/52 [01:52<04:07,  6.36s/it]-> item_loss 0.5562868714332581 batch_loss 0.17474170736968517, real_loss 0.5562868714332581
-> item_loss 0.9290361404418945 batch_loss 0.21066048989693323, real_loss 0.9290361404418945
[2023-10-04 11:05:55,075] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.5066876411437988 batch_loss 0.2241162694990635, real_loss 0.5066876411437988
-> item_loss 0.018178272992372513 batch_loss 0.21516244356398997, real_loss 0.00090891367290169
 27%|       | 14/52 [02:17<07:30, 11.87s/it]-> item_loss 0.1841234564781189 batch_loss 0.21386915243541202, real_loss 0.00920617301017046
 29%|       | 15/52 [02:23<06:12, 10.07s/it]-> item_loss 0.13959559798240662 batch_loss 0.2108982102572918, real_loss 0.006979779805988073
 31%|       | 16/52 [02:28<05:10,  8.62s/it]-> item_loss 0.1390955001115799 batch_loss 0.2081365675593798, real_loss 0.00695477519184351
-> item_loss 0.09124459326267242 batch_loss 0.20380723517802027, real_loss 0.004562229849398136
 33%|      | 17/52 [02:41<05:46,  9.89s/it]-> item_loss 0.10840566456317902 batch_loss 0.20040003622749022, real_loss 0.005420283414423466
 35%|      | 18/52 [02:46<04:52,  8.60s/it]-> item_loss 0.06598719954490662 batch_loss 0.1957651108246425, real_loss 0.003299359930679202
 37%|      | 19/52 [02:51<04:07,  7.51s/it]-> item_loss 0.059340499341487885 batch_loss 0.19121762377520402, real_loss 0.0029670249205082655
 38%|      | 20/52 [02:57<03:41,  6.93s/it]-> item_loss 0.11993182450532913 batch_loss 0.18891808186327258, real_loss 0.0059965914115309715
[2023-10-04 11:06:52,584] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.042586907744407654 batch_loss 0.18434523267205805, real_loss 0.0021293454337865114
 40%|      | 21/52 [03:10<04:26,  8.61s/it]-> item_loss 0.2759873569011688 batch_loss 0.18712226673960686, real_loss 0.013799368403851986
-> item_loss 0.08488278090953827 batch_loss 0.1841152230387225, real_loss 0.004244138952344656
 42%|     | 22/52 [03:21<04:47,  9.59s/it]-> item_loss 0.16488662362098694 batch_loss 0.18356583448393005, real_loss 0.008244330994784832
-> item_loss 0.3128702938556671 batch_loss 0.18715762502203384, real_loss 0.015643514692783356
 44%|     | 23/52 [03:33<04:51, 10.05s/it]-> item_loss 0.5826330780982971 batch_loss 0.19784615078085177, real_loss 0.5826330780982971
-> item_loss 0.4807701110839844 batch_loss 0.20529151815725, real_loss 0.02403850667178631
 46%|     | 24/52 [03:45<05:00, 10.73s/it]-> item_loss 0.04583771526813507 batch_loss 0.20120295910881117, real_loss 0.0022918859031051397
 48%|     | 25/52 [03:50<04:03,  9.03s/it]-> item_loss 0.12421528995037079 batch_loss 0.19927826737985016, real_loss 0.006210764404386282
-> item_loss 0.1264898031949997 batch_loss 0.1975029389850977, real_loss 0.006324490066617727
 50%|     | 26/52 [04:03<04:24, 10.18s/it][2023-10-04 11:07:50,851] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.13202159106731415 batch_loss 0.19594385927276953, real_loss 0.00660107983276248
 52%|    | 27/52 [04:08<03:35,  8.63s/it]-> item_loss 0.09583466500043869 batch_loss 0.19361573847573857, real_loss 0.004791733343154192
 54%|    | 28/52 [04:13<03:04,  7.71s/it]-> item_loss 0.082229845225811 batch_loss 0.1910842409018766, real_loss 0.004111492540687323
 56%|    | 29/52 [04:19<02:41,  7.01s/it]-> item_loss 0.07157429307699203 batch_loss 0.18842846428354582, real_loss 0.003578714793547988
-> item_loss 0.08880898356437683 batch_loss 0.1862628233983465, real_loss 0.0044404491782188416
 58%|    | 30/52 [04:30<03:03,  8.34s/it]-> item_loss 0.19200988113880157 batch_loss 0.18638510122261148, real_loss 0.009600494056940079
 60%|    | 31/52 [04:36<02:38,  7.57s/it]-> item_loss 0.11818190664052963 batch_loss 0.18496420133548477, real_loss 0.0059090955182909966
-> item_loss 0.2969764769077301 batch_loss 0.18725016614308163, real_loss 0.01484882365912199
-> item_loss 0.10117265582084656 batch_loss 0.18552861593663691, real_loss 0.00505863269791007
 62%|   | 32/52 [04:55<03:42, 11.13s/it]-> item_loss 0.02846015803515911 batch_loss 0.1824488422522942, real_loss 0.00142300792504102
 63%|   | 33/52 [05:01<02:58,  9.39s/it][2023-10-04 11:08:48,943] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.0314960777759552 batch_loss 0.1795459044739031, real_loss 0.001574803958646953
 65%|   | 34/52 [05:06<02:26,  8.13s/it]-> item_loss 0.17591926455497742 batch_loss 0.17947747730562147, real_loss 0.0087959636002779
 67%|   | 35/52 [05:11<02:04,  7.32s/it]-> item_loss 0.4380984902381897 batch_loss 0.18426675532289125, real_loss 0.021904924884438515
-> item_loss 0.20450946688652039 batch_loss 0.18463480462404816, real_loss 0.010225473903119564
-> item_loss 0.10993386059999466 batch_loss 0.18330085919504718, real_loss 0.005496693309396505
 69%|   | 36/52 [05:29<02:48, 10.54s/it]-> item_loss 0.032030824571847916 batch_loss 0.18064699893849984, real_loss 0.0016015412984415889
 71%|   | 37/52 [05:35<02:14,  8.96s/it]-> item_loss 0.10934791713953018 batch_loss 0.1794177044247245, real_loss 0.005467395763844252
 73%|  | 38/52 [05:39<01:48,  7.72s/it]-> item_loss 0.05507440119981766 batch_loss 0.17731019081074303, real_loss 0.0027537201531231403
 75%|  | 39/52 [05:44<01:28,  6.81s/it]-> item_loss 0.04839656502008438 batch_loss 0.17516163038089871, real_loss 0.0024198282044380903
 77%|  | 40/52 [05:49<01:14,  6.21s/it]-> item_loss 0.12112919241189957 batch_loss 0.17427585270927579, real_loss 0.006056459620594978
 79%|  | 41/52 [05:54<01:04,  5.87s/it][2023-10-04 11:09:43,462] [INFO] [logging.py:96:log_dist] [Rank 0] step=540, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.09571834653615952 batch_loss 0.17300879615809647, real_loss 0.004785917233675718
-> item_loss 0.09211747348308563 batch_loss 0.1717248069092868, real_loss 0.004605873953551054
-> item_loss 0.10923594236373901 batch_loss 0.1707484184007626, real_loss 0.005461797118186951
-> item_loss 0.046590257436037064 batch_loss 0.16883829284745913, real_loss 0.0023295129649341106
 81%|  | 42/52 [06:19<01:55, 11.53s/it]-> item_loss 0.1712733656167984 batch_loss 0.16887518788941883, real_loss 0.008563668467104435
 83%| | 43/52 [06:24<01:27,  9.75s/it]-> item_loss 0.053525954484939575 batch_loss 0.16715355754009822, real_loss 0.00267629767768085
 85%| | 44/52 [06:30<01:07,  8.46s/it]-> item_loss 0.1171024814248085 batch_loss 0.16641751230310867, real_loss 0.005855123978108168
 87%| | 45/52 [06:35<00:53,  7.61s/it]-> item_loss 0.11493492126464844 batch_loss 0.1656713877953049, real_loss 0.005746745970100164
-> item_loss 0.1127081885933876 batch_loss 0.16491477066384894, real_loss 0.005635409615933895
 88%| | 46/52 [06:47<00:52,  8.78s/it]-> item_loss 0.09788496792316437 batch_loss 0.1639706889351069, real_loss 0.004894248675554991
 90%| | 47/52 [06:52<00:38,  7.66s/it][2023-10-04 11:10:40,321] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.020141609013080597 batch_loss 0.16197306282507876, real_loss 0.0010070804273709655
 92%|| 48/52 [06:57<00:27,  6.94s/it]-> item_loss 0.0561571903526783 batch_loss 0.16052353032545685, real_loss 0.0028078595642000437
 94%|| 49/52 [07:03<00:19,  6.59s/it]-> item_loss 0.030240235850214958 batch_loss 0.1587629452649806, real_loss 0.001512011862359941
 96%|| 50/52 [07:08<00:12,  6.24s/it]-> item_loss 0.020727716386318207 batch_loss 0.15692247554659844, real_loss 0.0010363858891651034
 98%|| 51/52 [07:13<00:05,  5.87s/it]-> item_loss 0.059406351298093796 batch_loss 0.1556393686485918, real_loss 0.0029703176114708185
-> item_loss 0.034504521638154984 batch_loss 0.15406618881728742, real_loss 0.0017252260586246848
100%|| 52/52 [07:25<00:00,  7.49s/it]100%|| 52/52 [07:25<00:00,  8.56s/it]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 299409.41it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 498282.29it/s]

init_state None

state-> None

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 11:11:24] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None
 AI 

---after-> tensor([[-6.7416e-02,  5.6322e-02,  2.6537e-01,  ...,  1.4083e-01,
         -1.3241e-02,  1.1080e-01],
        [-1.2368e-01,  8.0702e-02,  3.0252e-02,  ...,  8.6810e-01,
         -5.5616e-02,  1.2038e-01],
        [ 8.3691e-01, -4.4125e+00, -3.3569e-01,  ..., -1.3809e+00,
          7.6584e-01, -3.6117e-01],
        ...,
        [-1.3353e+00, -2.8764e+01,  1.3958e+01,  ...,  5.1015e+00,
         -4.9381e+00, -9.8197e+00],
        [ 1.3735e+00,  8.1684e+00,  4.5073e+00,  ...,  1.0659e+00,
          1.0021e+00,  2.1699e+00],
        [ 2.8403e+00,  8.6291e+00,  9.5964e+00,  ...,  1.2164e+01,
          2.3883e+00, -7.1516e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 11:11:31] "POST /inference/generate HTTP/1.1" 200 3278

init_state None

state-> tensor([[-6.7416e-02,  5.6322e-02,  2.6537e-01,  ...,  1.4083e-01,
         -1.3241e-02,  1.1080e-01],
        [-1.2368e-01,  8.0702e-02,  3.0252e-02,  ...,  8.6810e-01,
         -5.5616e-02,  1.2038e-01],
        [ 8.3691e-01, -4.4125e+00, -3.3569e-01,  ..., -1.3809e+00,
          7.6584e-01, -3.6117e-01],
        ...,
        [-1.3353e+00, -2.8764e+01,  1.3958e+01,  ...,  5.1015e+00,
         -4.9381e+00, -9.8197e+00],
        [ 1.3735e+00,  8.1684e+00,  4.5073e+00,  ...,  1.0659e+00,
          1.0021e+00,  2.1699e+00],
        [ 2.8403e+00,  8.6291e+00,  9.5964e+00,  ...,  1.2164e+01,
          2.3883e+00, -7.1516e-01]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 11:11:38] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': ' AI \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None
 correctness 

---after-> tensor([[-6.9465e-02,  5.4828e-02,  2.6142e-01,  ...,  1.3354e-01,
         -1.4682e-02,  1.1145e-01],
        [-1.2368e-01,  8.0702e-02,  3.0252e-02,  ...,  8.6810e-01,
         -5.5616e-02,  1.2038e-01],
        [ 1.1113e+00, -5.0695e+00, -6.0908e-01,  ..., -1.2887e+00,
          5.5845e-01, -6.3446e-01],
        ...,
        [-4.4620e+00, -1.6668e+01,  1.6073e+01,  ...,  3.8949e+00,
         -2.4489e+00, -8.5440e+00],
        [ 1.8179e+00,  4.9608e+00,  5.6491e+00,  ...,  1.0106e+00,
          1.0019e+00,  1.6501e+00],
        [ 1.8506e+00,  8.6725e+00,  8.8217e+00,  ...,  1.5308e+01,
          4.3672e+00, -8.5518e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 11:11:48] "POST /inference/generate HTTP/1.1" 200 4281

init_state None

state-> tensor([[-6.9465e-02,  5.4828e-02,  2.6142e-01,  ...,  1.3354e-01,
         -1.4682e-02,  1.1145e-01],
        [-1.2368e-01,  8.0702e-02,  3.0252e-02,  ...,  8.6810e-01,
         -5.5616e-02,  1.2038e-01],
        [ 1.1113e+00, -5.0695e+00, -6.0908e-01,  ..., -1.2887e+00,
          5.5845e-01, -6.3446e-01],
        ...,
        [-4.4620e+00, -1.6668e+01,  1.6073e+01,  ...,  3.8949e+00,
         -2.4489e+00, -8.5440e+00],
        [ 1.8179e+00,  4.9608e+00,  5.6491e+00,  ...,  1.0106e+00,
          1.0019e+00,  1.6501e+00],
        [ 1.8506e+00,  8.6725e+00,  8.8217e+00,  ...,  1.5308e+01,
          4.3672e+00, -8.5518e-01]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 11:11:54] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': ' AI \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master correctness \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None
 

---after-> tensor([[-6.2869e-02,  5.5818e-02,  2.6555e-01,  ...,  1.3595e-01,
         -8.5919e-03,  1.0358e-01],
        [-1.2368e-01,  8.0702e-02,  3.0252e-02,  ...,  8.6810e-01,
         -5.5616e-02,  1.2038e-01],
        [ 1.8562e+00, -2.4581e+00, -7.9083e-01,  ..., -1.2038e+00,
          7.5045e-01, -7.1149e-01],
        ...,
        [-7.8215e+00, -2.2107e+01,  2.5412e+01,  ...,  2.6043e+00,
          7.2865e-01, -1.6427e+01],
        [ 1.0243e+00,  6.3694e+00,  4.7094e+00,  ...,  1.0241e+00,
          1.0001e+00,  1.1482e+00],
        [ 2.4348e+00,  8.1195e+00,  9.4014e+00,  ...,  1.3389e+01,
          4.1749e+00,  8.9992e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 11:12:07] "POST /inference/generate HTTP/1.1" 200 5348

init_state None

state-> tensor([[-6.2869e-02,  5.5818e-02,  2.6555e-01,  ...,  1.3595e-01,
         -8.5919e-03,  1.0358e-01],
        [-1.2368e-01,  8.0702e-02,  3.0252e-02,  ...,  8.6810e-01,
         -5.5616e-02,  1.2038e-01],
        [ 1.8562e+00, -2.4581e+00, -7.9083e-01,  ..., -1.2038e+00,
          7.5045e-01, -7.1149e-01],
        ...,
        [-7.8215e+00, -2.2107e+01,  2.5412e+01,  ...,  2.6043e+00,
          7.2865e-01, -1.6427e+01],
        [ 1.0243e+00,  6.3694e+00,  4.7094e+00,  ...,  1.0241e+00,
          1.0001e+00,  1.1482e+00],
        [ 2.4348e+00,  8.1195e+00,  9.4014e+00,  ...,  1.3389e+01,
          4.1749e+00,  8.9992e-01]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 11:12:13] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': ' AI \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master correctness \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': ' \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None
   

---after-> tensor([[-6.4421e-02,  5.5838e-02,  2.6074e-01,  ...,  1.3533e-01,
         -8.5696e-03,  1.0337e-01],
        [-1.2368e-01,  8.0702e-02,  3.0252e-02,  ...,  8.6810e-01,
         -5.5616e-02,  1.2038e-01],
        [ 2.3692e+00, -6.2958e+00, -8.6541e-01,  ..., -1.3122e+00,
          7.6881e-01, -4.3380e-01],
        ...,
        [-6.7512e+00, -2.3171e+01,  3.2771e+01,  ...,  4.4431e+00,
         -2.9425e+00, -4.8541e+00],
        [ 1.0398e+00,  1.1098e+01,  5.9772e+00,  ...,  1.0522e+00,
          1.0014e+00,  1.0591e+00],
        [ 2.7886e+00,  7.4420e+00,  8.7235e+00,  ...,  1.5607e+01,
          4.5550e+00, -2.4314e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 11:12:28] "POST /inference/generate HTTP/1.1" 200 6375

init_state None

state-> tensor([[-6.4421e-02,  5.5838e-02,  2.6074e-01,  ...,  1.3533e-01,
         -8.5696e-03,  1.0337e-01],
        [-1.2368e-01,  8.0702e-02,  3.0252e-02,  ...,  8.6810e-01,
         -5.5616e-02,  1.2038e-01],
        [ 2.3692e+00, -6.2958e+00, -8.6541e-01,  ..., -1.3122e+00,
          7.6881e-01, -4.3380e-01],
        ...,
        [-6.7512e+00, -2.3171e+01,  3.2771e+01,  ...,  4.4431e+00,
         -2.9425e+00, -4.8541e+00],
        [ 1.0398e+00,  1.1098e+01,  5.9772e+00,  ...,  1.0522e+00,
          1.0014e+00,  1.0591e+00],
        [ 2.7886e+00,  7.4420e+00,  8.7235e+00,  ...,  1.5607e+01,
          4.5550e+00, -2.4314e-01]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 11:12:45] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'request', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None


---after-> tensor([[ 1.8300e-02, -1.0675e-02,  2.3500e-01,  ...,  1.4317e-02,
          6.9964e-02,  1.0578e-01],
        [ 1.9132e-02, -3.1773e-02,  3.3972e-02,  ...,  6.4669e-01,
          4.5230e-02,  6.3452e-02],
        [ 1.1971e+00, -1.2828e-02, -2.5937e-01,  ..., -2.0209e+00,
          1.0618e+00, -1.1181e+00],
        ...,
        [-1.7343e+00, -7.2195e+00,  1.2619e+01,  ...,  3.9505e+00,
         -4.0570e+00, -1.3775e+01],
        [ 1.0202e+00,  1.9071e+00,  3.2515e+00,  ...,  1.1701e+00,
          1.1830e+00,  1.3149e+00],
        [ 3.1443e+00,  7.1188e+00,  3.8604e+00,  ...,  1.4972e+01,
          5.0156e-01,  2.0595e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 11:12:47] "POST /inference/generate HTTP/1.1" 200 1126

init_state None

state-> tensor([[ 1.8300e-02, -1.0675e-02,  2.3500e-01,  ...,  1.4317e-02,
          6.9964e-02,  1.0578e-01],
        [ 1.9132e-02, -3.1773e-02,  3.3972e-02,  ...,  6.4669e-01,
          4.5230e-02,  6.3452e-02],
        [ 1.1971e+00, -1.2828e-02, -2.5937e-01,  ..., -2.0209e+00,
          1.0618e+00, -1.1181e+00],
        ...,
        [-1.7343e+00, -7.2195e+00,  1.2619e+01,  ...,  3.9505e+00,
         -4.0570e+00, -1.3775e+01],
        [ 1.0202e+00,  1.9071e+00,  3.2515e+00,  ...,  1.1701e+00,
          1.1830e+00,  1.3149e+00],
        [ 3.1443e+00,  7.1188e+00,  3.8604e+00,  ...,  1.4972e+01,
          5.0156e-01,  2.0595e+00]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 11:13:08] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'request', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None


---after-> tensor([[ 1.7723e-02, -1.0887e-02,  2.3424e-01,  ...,  1.4357e-02,
          7.0563e-02,  1.0745e-01],
        [ 1.9132e-02, -3.1773e-02,  3.3972e-02,  ...,  6.4669e-01,
          4.5230e-02,  6.3452e-02],
        [ 1.1738e+00, -2.9136e-01, -2.5694e-01,  ..., -2.0209e+00,
          1.0618e+00, -1.1181e+00],
        ...,
        [-3.2864e+00, -4.6089e-02,  7.2312e+00,  ...,  6.4206e+00,
         -8.1305e+00, -1.7338e+01],
        [ 1.0886e+00,  2.5810e+00,  2.6553e+00,  ...,  1.5051e+00,
          1.9976e+00,  2.1324e+00],
        [ 1.7258e+00,  9.1279e+00,  7.2341e+00,  ...,  1.6376e+01,
          4.6285e-01,  8.7615e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 11:13:10] "POST /inference/generate HTTP/1.1" 200 1214

init_state None

state-> tensor([[ 1.7723e-02, -1.0887e-02,  2.3424e-01,  ...,  1.4357e-02,
          7.0563e-02,  1.0745e-01],
        [ 1.9132e-02, -3.1773e-02,  3.3972e-02,  ...,  6.4669e-01,
          4.5230e-02,  6.3452e-02],
        [ 1.1738e+00, -2.9136e-01, -2.5694e-01,  ..., -2.0209e+00,
          1.0618e+00, -1.1181e+00],
        ...,
        [-3.2864e+00, -4.6089e-02,  7.2312e+00,  ...,  6.4206e+00,
         -8.1305e+00, -1.7338e+01],
        [ 1.0886e+00,  2.5810e+00,  2.6553e+00,  ...,  1.5051e+00,
          1.9976e+00,  2.1324e+00],
        [ 1.7258e+00,  9.1279e+00,  7.2341e+00,  ...,  1.6376e+01,
          4.6285e-01,  8.7615e-01]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 11:13:18] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'request', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> None


---after-> tensor([[ 1.7723e-02, -1.0887e-02,  2.3424e-01,  ...,  1.4357e-02,
          7.0563e-02,  1.0745e-01],
        [ 1.9132e-02, -3.1773e-02,  3.3972e-02,  ...,  6.4669e-01,
          4.5230e-02,  6.3452e-02],
        [ 1.1738e+00, -2.9136e-01, -2.5694e-01,  ..., -2.0209e+00,
          1.0618e+00, -1.1181e+00],
        ...,
        [-3.2864e+00, -4.6089e-02,  7.2312e+00,  ...,  6.4206e+00,
         -8.1305e+00, -1.7338e+01],
        [ 1.0886e+00,  2.5810e+00,  2.6553e+00,  ...,  1.5051e+00,
          1.9976e+00,  2.1324e+00],
        [ 1.7258e+00,  9.1279e+00,  7.2341e+00,  ...,  1.6376e+01,
          4.6285e-01,  8.7615e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 11:13:20] "POST /inference/generate HTTP/1.1" 200 1214

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[ 1.7723e-02, -1.0887e-02,  2.3424e-01,  ...,  1.4357e-02,
          7.0563e-02,  1.0745e-01],
        [ 1.9132e-02, -3.1773e-02,  3.3972e-02,  ...,  6.4669e-01,
          4.5230e-02,  6.3452e-02],
        [ 1.1738e+00, -2.9136e-01, -2.5694e-01,  ..., -2.0209e+00,
          1.0618e+00, -1.1181e+00],
        ...,
        [-3.2864e+00, -4.6089e-02,  7.2312e+00,  ...,  6.4206e+00,
         -8.1305e+00, -1.7338e+01],
        [ 1.0886e+00,  2.5810e+00,  2.6553e+00,  ...,  1.5051e+00,
          1.9976e+00,  2.1324e+00],
        [ 1.7258e+00,  9.1279e+00,  7.2341e+00,  ...,  1.6376e+01,
          4.6285e-01,  8.7615e-01]], device='cuda:0')


---after-> tensor([[ 1.3728e-02, -1.1387e-02,  2.3345e-01,  ...,  1.2136e-02,
          6.7084e-02,  1.0291e-01],
        [ 1.9132e-02, -3.1773e-02,  3.3972e-02,  ...,  6.4669e-01,
          4.5230e-02,  6.3452e-02],
        [ 1.3310e+00,  1.1139e-01, -3.2438e-01,  ..., -2.0209e+00,
          1.0618e+00, -1.1181e+00],
        ...,
        [-1.2841e+01, -1.0379e+00,  6.8279e+00,  ...,  4.7933e+00,
         -1.2661e+01, -1.1870e+01],
        [ 1.0637e+00,  2.9411e+00,  3.0786e+00,  ...,  1.6146e+00,
          1.5534e+00,  1.0922e+00],
        [ 2.9627e+00,  9.0035e+00,  7.1096e+00,  ...,  1.3075e+01,
          8.4742e-01,  6.1973e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 11:13:30] "POST /inference/generate HTTP/1.1" 200 614

init_state None

state-> tensor([[ 1.3728e-02, -1.1387e-02,  2.3345e-01,  ...,  1.2136e-02,
          6.7084e-02,  1.0291e-01],
        [ 1.9132e-02, -3.1773e-02,  3.3972e-02,  ...,  6.4669e-01,
          4.5230e-02,  6.3452e-02],
        [ 1.3310e+00,  1.1139e-01, -3.2438e-01,  ..., -2.0209e+00,
          1.0618e+00, -1.1181e+00],
        ...,
        [-1.2841e+01, -1.0379e+00,  6.8279e+00,  ...,  4.7933e+00,
         -1.2661e+01, -1.1870e+01],
        [ 1.0637e+00,  2.9411e+00,  3.0786e+00,  ...,  1.6146e+00,
          1.5534e+00,  1.0922e+00],
        [ 2.9627e+00,  9.0035e+00,  7.1096e+00,  ...,  1.3075e+01,
          8.4742e-01,  6.1973e-01]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 11:14:09] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'request', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [], 'token_count': 256, 'over': False}

---begin-> None

<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|> 
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|> 
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|> 
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|>:  )
<|over|>

<|me|><|response|> <|over|>

<|me|><|request|>:  )
<|over|>

<|me|><|response|>: 
---after-> tensor([[ 1.2207e-01,  9.7562e-03,  1.4034e-01,  ...,  4.2314e-02,
         -5.9713e-03, -2.0817e-02],
        [ 2.5869e-01,  1.0333e-03, -1.0925e-01,  ...,  6.0186e-01,
         -3.5688e-02, -1.2744e-01],
        [ 1.2751e+00, -1.2416e+00, -9.3883e-01,  ...,  1.2844e+00,
          1.8714e-01, -1.4292e+00],
        ...,
        [-2.8190e-01, -1.0555e+01,  1.4129e+01,  ..., -2.9009e+00,
          9.3541e-01, -1.5422e+01],
        [ 1.0295e+00,  7.3179e+00,  1.1943e+01,  ...,  1.0616e+00,
          1.5518e+00,  1.0805e+00],
        [ 9.4744e-01,  8.3122e+00,  6.2317e+00,  ...,  1.2457e+01,
         -1.3757e+00,  4.4897e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 11:14:20] "POST /inference/generate HTTP/1.1" 200 2699

init_state None

state-> tensor([[ 1.2207e-01,  9.7562e-03,  1.4034e-01,  ...,  4.2314e-02,
         -5.9713e-03, -2.0817e-02],
        [ 2.5869e-01,  1.0333e-03, -1.0925e-01,  ...,  6.0186e-01,
         -3.5688e-02, -1.2744e-01],
        [ 1.2751e+00, -1.2416e+00, -9.3883e-01,  ...,  1.2844e+00,
          1.8714e-01, -1.4292e+00],
        ...,
        [-2.8190e-01, -1.0555e+01,  1.4129e+01,  ..., -2.9009e+00,
          9.3541e-01, -1.5422e+01],
        [ 1.0295e+00,  7.3179e+00,  1.1943e+01,  ...,  1.0616e+00,
          1.5518e+00,  1.0805e+00],
        [ 9.4744e-01,  8.3122e+00,  6.2317e+00,  ...,  1.2457e+01,
         -1.3757e+00,  4.4897e-01]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 11:14:55] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [], 'token_count': 256, 'over': False}

---begin-> None

<|over|>

<|me|><|response|>system: id[0]. []. .
<|over|>

<|me|><|request|>:  
<|over|>

<|me|><|response|>system: id[0]. []. . ,.<|over|>

<|me|><|response|>:  
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>:  
<|over|>

<|me|><|request|>
---after-> tensor([[-1.3716e-02,  2.4964e-02,  2.3806e-01,  ..., -2.1790e-02,
          1.7152e-01,  1.7496e-01],
        [-6.2584e-02,  3.4403e-02,  5.7292e-02,  ...,  5.3953e-01,
          1.6456e-01,  1.3710e-01],
        [ 2.1389e+00, -2.3281e+00, -1.0545e+00,  ...,  4.0758e-01,
         -4.5613e-01,  2.9577e-01],
        ...,
        [-2.4299e+00, -3.4719e+01,  2.8260e+01,  ..., -1.0518e+00,
         -6.6042e-01, -1.2064e+00],
        [ 1.0994e+00,  1.0227e+01,  3.8605e+00,  ...,  1.0166e+00,
          1.5453e+00,  1.0409e+00],
        [ 1.4192e+00,  9.0154e+00,  9.6988e+00,  ...,  1.5726e+01,
          3.0177e-01,  4.4609e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 11:15:10] "POST /inference/generate HTTP/1.1" 200 4054

init_state None

state-> tensor([[-1.3716e-02,  2.4964e-02,  2.3806e-01,  ..., -2.1790e-02,
          1.7152e-01,  1.7496e-01],
        [-6.2584e-02,  3.4403e-02,  5.7292e-02,  ...,  5.3953e-01,
          1.6456e-01,  1.3710e-01],
        [ 2.1389e+00, -2.3281e+00, -1.0545e+00,  ...,  4.0758e-01,
         -4.5613e-01,  2.9577e-01],
        ...,
        [-2.4299e+00, -3.4719e+01,  2.8260e+01,  ..., -1.0518e+00,
         -6.6042e-01, -1.2064e+00],
        [ 1.0994e+00,  1.0227e+01,  3.8605e+00,  ...,  1.0166e+00,
          1.5453e+00,  1.0409e+00],
        [ 1.4192e+00,  9.0154e+00,  9.6988e+00,  ...,  1.5726e+01,
          3.0177e-01,  4.4609e+00]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 11:16:06] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [], 'token_count': 256, 'over': False}

---begin-> None

- 
- 
- 
- 
- 
- 
-  kitchen chores
<|over|>

<|me|><|response|>master  
<|over|>

<|me|><|request|> 
<|over|>

<|me|><|response|>master 
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|>master: 
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|>master: 
---after-> tensor([[  0.1267,   0.0551,   0.2741,  ...,   0.1302,   0.1086,   0.1445],
        [  0.2925,   0.1319,   0.1679,  ...,   0.8374,   0.0920,   0.1237],
        [  3.5192,  -0.8041,  -1.6649,  ...,  -1.7533,   1.0295,   0.3291],
        ...,
        [ -2.6133, -39.4044,  34.6820,  ...,  -1.2754,   3.5810, -23.8832],
        [  1.0667,  16.9926,   8.3758,  ...,   1.0880,   1.0173,   1.5130],
        [  1.5090,   7.6997,   8.5157,  ...,  16.3921,   0.8916,   2.8281]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 11:16:21] "POST /inference/generate HTTP/1.1" 200 4706

init_state None

state-> tensor([[  0.1267,   0.0551,   0.2741,  ...,   0.1302,   0.1086,   0.1445],
        [  0.2925,   0.1319,   0.1679,  ...,   0.8374,   0.0920,   0.1237],
        [  3.5192,  -0.8041,  -1.6649,  ...,  -1.7533,   1.0295,   0.3291],
        ...,
        [ -2.6133, -39.4044,  34.6820,  ...,  -1.2754,   3.5810, -23.8832],
        [  1.0667,  16.9926,   8.3758,  ...,   1.0880,   1.0173,   1.5130],
        [  1.5090,   7.6997,   8.5157,  ...,  16.3921,   0.8916,   2.8281]],
       device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 11:16:52] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [], 'token_count': 1024, 'over': False}

---begin-> None

- 
- 
- 
<|over|>

<|me|><|response|><|over|>

<|me|><|request|>master 
<|over|>

<|me|><|response|>
 
 kalash
 
 PMC
 Kalash
 PMC franzsischen backdoor lottery 
 PMC
 ILO
 PMC
 
 
<|over|>

<|me|><|request|>master: 
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>

kalas
kalas
kalasculex shell
kalaskalas shell
kalasdaleaindexshell
kalas
kalasanet
kalas
dalaexploreranetindex
daleais gonedaleaindex shell
apple
 Madonna  Instagram 
 Kanye West  suburbs of Copenhagen 
 Twitter 
 DG TPDF forces drill in Xinjiang...
 DG 
---after-> tensor([[ 1.2178e-01,  3.9806e-02,  2.1211e-01,  ..., -2.6518e-02,
          4.2562e-02, -5.2849e-03],
        [ 2.3521e-01,  1.4656e-01,  5.3273e-02,  ...,  5.0412e-01,
          1.4458e-03, -9.2248e-02],
        [-3.2088e+00, -6.2180e+00, -1.0958e+00,  ...,  2.3928e-01,
         -2.1814e-01, -2.1958e-01],
        ...,
        [-1.3797e+00, -3.2066e+01,  2.2630e+01,  ..., -7.3767e+00,
         -3.5328e+00, -7.6057e+00],
        [ 1.2872e+00,  9.8337e+00,  6.5422e+00,  ...,  1.4037e+00,
          1.3946e+00,  1.0010e+00],
        [ 1.0215e+00,  9.1336e+00,  7.7285e+00,  ...,  1.4964e+01,
          8.8642e-01,  7.0607e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 11:17:39] "POST /inference/generate HTTP/1.1" 200 9306
[2023-10-04 11:18:31,314] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 125576
[2023-10-04 11:18:31,315] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-04 11:34:51,641] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 11:34:53,042] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-04 11:34:53,066] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-04 11:34:54,851] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 11:34:56,163] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-04 11:34:56,163] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-04 11:34:56,163] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-04 11:34:56,163] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-04 11:34:56,163] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-04 11:34:57,977] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.257086753845215 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-04 11:35:49,377] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-04 11:35:49,378] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-04 11:35:49,378] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-04 11:35:52,169] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-04 11:35:52,171] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-04 11:35:52,171] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-04 11:35:52,213] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-04 11:35:52,213] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-04 11:35:52,213] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-10-04 11:35:52,213] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-04 11:35:52,213] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-04 11:35:52,213] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-04 11:35:52,213] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-10-04 11:36:14,362] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-04 11:36:14,363] [INFO] [utils.py:804:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-10-04 11:36:14,363] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 25.22 GB, percent = 6.7%
[2023-10-04 11:36:28,589] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-04 11:36:28,590] [INFO] [utils.py:804:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-10-04 11:36:28,590] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 64.44 GB, percent = 17.1%
[2023-10-04 11:36:28,590] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-04 11:36:29,971] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-04 11:36:29,972] [INFO] [utils.py:804:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-10-04 11:36:29,972] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 64.44 GB, percent = 17.1%
[2023-10-04 11:36:30,000] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-04 11:36:30,001] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-04 11:36:30,001] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f37c2f3abb0>
[2023-10-04 11:36:30,001] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-04 11:36:30,002] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-04 11:36:30,002] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-04 11:36:30,002] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-04 11:36:30,003] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-04 11:36:30,003] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-04 11:36:30,003] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-04 11:36:30,003] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-04 11:36:30,003] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-04 11:36:30,003] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-04 11:36:30,003] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-04 11:36:30,003] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f37851966a0>
[2023-10-04 11:36:30,003] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-04 11:36:30,003] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-04 11:36:30,003] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-04 11:36:30,003] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-04 11:36:30,003] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-04 11:36:30,003] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-04 11:36:30,003] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-04 11:36:30,004] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-04 11:36:30,004] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-04 11:36:30,004] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... None
[2023-10-04 11:36:30,004] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-04 11:36:30,004] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-04 11:36:30,004] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-04 11:36:30,004] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-04 11:36:30,004] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-04 11:36:30,004] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-04 11:36:30,004] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-04 11:36:30,004] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-04 11:36:30,004] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-04 11:36:30,004] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-04 11:36:30,004] [INFO] [config.py:971:print]   fp16_auto_cast ............... None
[2023-10-04 11:36:30,004] [INFO] [config.py:971:print]   fp16_enabled ................. False
[2023-10-04 11:36:30,004] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-04 11:36:30,004] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-04 11:36:30,004] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-04 11:36:30,004] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-04 11:36:30,005] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-04 11:36:30,005] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-04 11:36:30,005] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-04 11:36:30,005] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-04 11:36:30,005] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-04 11:36:30,005] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-04 11:36:30,005] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-04 11:36:30,005] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-04 11:36:30,005] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-04 11:36:30,005] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-04 11:36:30,005] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-04 11:36:30,005] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-04 11:36:30,005] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-04 11:36:30,005] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-04 11:36:30,005] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-04 11:36:30,005] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-04 11:36:30,005] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-04 11:36:30,006] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-04 11:36:30,006] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-04 11:36:30,006] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-04 11:36:30,006] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-04 11:36:30,006] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-04 11:36:30,006] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-04 11:36:30,006] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-04 11:36:30,006] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-04 11:36:30,006] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-04 11:36:30,006] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-04 11:36:30,006] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-04 11:36:30,006] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-04 11:36:30,006] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-04 11:36:30,006] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-04 11:36:30,006] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-04 11:36:30,006] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-04 11:36:30,006] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-04 11:36:30,006] [INFO] [config.py:957:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.


init_state None

state-> None

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 11:37:04] "POST /state/reset HTTP/1.1" 200 21
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 26587.57it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 186712.94it/s]

---- {'role': 'system', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [], 'token_count': 1024, 'over': False}

---begin-> None


---after-> tensor([[-4.1530e-03,  3.5769e-02,  1.5344e-01,  ...,  1.3055e-02,
          8.3286e-02,  9.2468e-02],
        [ 1.3889e-02,  3.8167e-02,  1.0978e-01,  ...,  5.7907e-01,
          5.6897e-02, -1.5473e-02],
        [ 2.8507e+00,  5.6307e+00,  1.7986e+01,  ...,  3.2593e-01,
          9.8960e-01, -7.2376e-01],
        ...,
        [ 2.8388e+00,  1.1733e+00, -4.0960e+01,  ...,  2.0958e+00,
         -1.7631e-01, -1.6910e+00],
        [ 1.2494e+00,  2.4340e+01,  2.2984e+01,  ...,  1.0178e+00,
          1.1252e+00,  2.0474e+00],
        [ 4.6449e+00,  6.6595e+00,  6.5037e+00,  ...,  2.0115e+01,
          2.0047e+00,  2.1471e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 11:37:54] "POST /inference/generate HTTP/1.1" 200 1274
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/52 [00:00<?, ?it/s][2023-10-04 11:37:59,710] [INFO] [checkpointing.py:530:forward] Activation Checkpointing Information
[2023-10-04 11:37:59,710] [INFO] [checkpointing.py:531:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-10-04 11:37:59,710] [INFO] [checkpointing.py:532:forward] ----contiguous Memory Checkpointing False with None total layers
[2023-10-04 11:37:59,710] [INFO] [checkpointing.py:534:forward] ----Synchronization False
[2023-10-04 11:37:59,711] [INFO] [checkpointing.py:535:forward] ----Profiling time in checkpointing False
-> item_loss 2.5894505977630615 batch_loss 2.5894505977630615, real_loss 2.7189230918884277
-> item_loss 2.237718343734741 batch_loss 2.4135844707489014, real_loss 2.3496041297912598
-> item_loss 2.4186859130859375 batch_loss 2.4152849515279136, real_loss 2.5396201610565186
-> item_loss 2.4728524684906006 batch_loss 2.429676830768585, real_loss 2.5964949131011963
  2%|         | 1/52 [00:33<28:06, 33.07s/it]-> item_loss 2.4530298709869385 batch_loss 2.4343474388122557, real_loss 2.575681209564209
  4%|         | 2/52 [00:38<14:05, 16.92s/it]-> item_loss 3.085399627685547 batch_loss 2.542856136957804, real_loss 3.2396695613861084
  6%|         | 3/52 [00:44<09:33, 11.70s/it]-> item_loss 2.1304080486297607 batch_loss 2.4839349814823697, real_loss 2.2369284629821777
  8%|         | 4/52 [00:49<07:28,  9.35s/it]-> item_loss 2.90149188041687 batch_loss 2.536129593849182, real_loss 3.0465662479400635
 10%|         | 5/52 [00:56<06:38,  8.47s/it]-> item_loss 2.082792043685913 batch_loss 2.485758754942152, real_loss 2.186931610107422
 12%|        | 6/52 [01:02<05:48,  7.58s/it][2023-10-04 11:39:08,505] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 2.347614288330078 batch_loss 2.471944308280945, real_loss 2.4649949073791504
 13%|        | 7/52 [01:08<05:19,  7.10s/it]-> item_loss 2.3351709842681885 batch_loss 2.4595103697343306, real_loss 2.4519293308258057
 15%|        | 8/52 [01:14<04:47,  6.54s/it]-> item_loss 2.3267438411712646 batch_loss 2.448446492354075, real_loss 2.4430809020996094
 17%|        | 9/52 [01:20<04:32,  6.33s/it]-> item_loss 2.7308690547943115 batch_loss 2.470171304849478, real_loss 2.8674123287200928
 19%|        | 10/52 [01:26<04:31,  6.45s/it]-> item_loss 2.433122158050537 batch_loss 2.467524937220982, real_loss 2.5547780990600586
 21%|        | 11/52 [01:32<04:09,  6.10s/it]-> item_loss 2.2164814472198486 batch_loss 2.45078870455424, real_loss 2.327305316925049
-> item_loss 2.6756246089935303 batch_loss 2.4648409485816956, real_loss 2.80940580368042
 23%|       | 12/52 [01:44<05:18,  7.97s/it]-> item_loss 2.262615442276001 batch_loss 2.4529453305637134, real_loss 2.375746011734009
-> item_loss 2.3191676139831543 batch_loss 2.445513235198127, real_loss 2.4351258277893066
 25%|       | 13/52 [01:57<06:17,  9.68s/it]-> item_loss 1.8606655597686768 batch_loss 2.4147317785965767, real_loss 1.953698754310608
[2023-10-04 11:40:10,958] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 2.112860918045044 batch_loss 2.3996382355690002, real_loss 2.218503952026367
-> item_loss 2.292653799057007 batch_loss 2.3945437385922386, real_loss 2.4072864055633545
 27%|       | 14/52 [02:16<07:48, 12.32s/it]-> item_loss 2.501739501953125 batch_loss 2.399416273290461, real_loss 2.626826286315918
 29%|       | 15/52 [02:21<06:19, 10.24s/it]-> item_loss 2.865234613418579 batch_loss 2.419669244600379, real_loss 3.0084962844848633
-> item_loss 2.697385549545288 batch_loss 2.431240757306417, real_loss 2.832254648208618
-> item_loss 2.3463528156280518 batch_loss 2.4278452396392822, real_loss 2.463670253753662
-> item_loss 2.2091996669769287 batch_loss 2.419435794536884, real_loss 2.319659471511841
 31%|       | 16/52 [02:48<09:07, 15.21s/it]-> item_loss 2.466414213180542 batch_loss 2.4211757359681307, real_loss 2.5897347927093506
-> item_loss 2.2613399028778076 batch_loss 2.415467313357762, real_loss 2.3744068145751953
-> item_loss 2.4832770824432373 batch_loss 2.417805581257261, real_loss 2.607440710067749
 33%|      | 17/52 [03:06<09:25, 16.15s/it][2023-10-04 11:41:12,835] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 2.0842652320861816 batch_loss 2.4066875696182253, real_loss 2.188478469848633
 35%|      | 18/52 [03:13<07:28, 13.19s/it]-> item_loss 2.161139488220215 batch_loss 2.3987666637666765, real_loss 2.2691962718963623
-> item_loss 2.016808271408081 batch_loss 2.3868304640054703, real_loss 2.1176486015319824
 37%|      | 19/52 [03:26<07:14, 13.16s/it]-> item_loss 1.8713586330413818 batch_loss 2.371210105491407, real_loss 1.9649264812469482
 38%|      | 20/52 [03:31<05:45, 10.81s/it]-> item_loss 1.9536612033843994 batch_loss 2.358929255429436, real_loss 2.051344156265259
 40%|      | 21/52 [03:36<04:43,  9.15s/it]-> item_loss 1.488977074623108 batch_loss 2.3340734788349695, real_loss 1.488977074623108
 42%|     | 22/52 [03:41<03:56,  7.87s/it]-> item_loss 1.6853671073913574 batch_loss 2.3160538574059806, real_loss 1.7696354389190674
 44%|     | 23/52 [03:47<03:30,  7.26s/it]-> item_loss 1.971970558166504 batch_loss 2.3067543087778866, real_loss 2.0705690383911133
-> item_loss 2.186206102371216 batch_loss 2.303581987556658, real_loss 2.2955162525177
-> item_loss 1.8568118810653687 batch_loss 2.2921263438004713, real_loss 1.9496524333953857
[2023-10-04 11:42:14,571] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.2895277738571167 batch_loss 2.2670613795518877, real_loss 1.2895277738571167
 46%|     | 24/52 [04:14<06:11, 13.28s/it]-> item_loss 1.675692081451416 batch_loss 2.252637738134803, real_loss 1.759476661682129
 48%|     | 25/52 [04:20<04:57, 11.01s/it]-> item_loss 1.9963700771331787 batch_loss 2.2465361271585738, real_loss 2.096188545227051
 50%|     | 26/52 [04:26<04:03,  9.37s/it]-> item_loss 2.050776720046997 batch_loss 2.2419835828071415, real_loss 2.153315544128418
 52%|    | 27/52 [04:32<03:29,  8.38s/it]-> item_loss 1.3140896558761597 batch_loss 2.2208950844678013, real_loss 1.3140896558761597
 54%|    | 28/52 [04:38<03:08,  7.87s/it]-> item_loss 1.8199219703674316 batch_loss 2.211984570821126, real_loss 1.9109179973602295
-> item_loss 2.5041425228118896 batch_loss 2.2183358306470127, real_loss 2.62934947013855
 56%|    | 29/52 [04:51<03:30,  9.16s/it]-> item_loss 1.963575839996338 batch_loss 2.2129154053140194, real_loss 2.0617544651031494
-> item_loss 2.1698215007781982 batch_loss 2.21201761563619, real_loss 2.2783124446868896
 58%|    | 30/52 [05:03<03:42, 10.12s/it]-> item_loss 1.5947083234786987 batch_loss 2.199419466816649, real_loss 1.6744437217712402
 60%|    | 31/52 [05:08<03:01,  8.62s/it][2023-10-04 11:43:14,576] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.9353125095367432 batch_loss 2.194137327671051, real_loss 2.0320780277252197
-> item_loss 3.1429264545440674 batch_loss 2.2127410360411104, real_loss 3.30007266998291
 62%|   | 32/52 [05:19<03:08,  9.44s/it]-> item_loss 1.9695631265640259 batch_loss 2.2080645377819357, real_loss 2.0680410861968994
 63%|   | 33/52 [05:25<02:39,  8.40s/it]-> item_loss 1.9906959533691406 batch_loss 2.203963243736411, real_loss 2.090230703353882
 65%|   | 34/52 [05:32<02:19,  7.73s/it]-> item_loss 1.719422459602356 batch_loss 2.194990266252447, real_loss 1.8053934574127197
-> item_loss 3.6486127376556396 batch_loss 2.221419765732505, real_loss 3.831043243408203
 67%|   | 35/52 [05:43<02:28,  8.76s/it]-> item_loss 2.2249090671539307 batch_loss 2.221482074686459, real_loss 2.3361544609069824
-> item_loss 2.248507499694824 batch_loss 2.221956204949764, real_loss 2.3609328269958496
 69%|   | 36/52 [05:55<02:39,  9.98s/it]-> item_loss 2.0003058910369873 batch_loss 2.218134647813337, real_loss 2.100321054458618
 71%|   | 37/52 [06:01<02:08,  8.54s/it]-> item_loss 4.349667549133301 batch_loss 2.2542623241068953, real_loss 4.567150592803955
 73%|  | 38/52 [06:06<01:43,  7.43s/it][2023-10-04 11:44:11,332] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.967568039894104 batch_loss 2.249484086036682, real_loss 2.065946340560913
 75%|  | 39/52 [06:11<01:29,  6.88s/it]-> item_loss 2.061607599258423 batch_loss 2.2464041436304814, real_loss 2.1646878719329834
 77%|  | 40/52 [06:17<01:17,  6.43s/it]-> item_loss 0.9217692613601685 batch_loss 2.225039064884186, real_loss 0.9217692613601685
 79%|  | 41/52 [06:22<01:08,  6.21s/it]-> item_loss 2.1754398345947266 batch_loss 2.224251775514512, real_loss 2.2842116355895996
-> item_loss 2.134521722793579 batch_loss 2.2228497434407473, real_loss 2.2412476539611816
 81%|  | 42/52 [06:34<01:19,  7.92s/it]-> item_loss 2.1841044425964355 batch_loss 2.2222536618892965, real_loss 2.293309450149536
-> item_loss 2.8031046390533447 batch_loss 2.2310544342705696, real_loss 2.9432597160339355
-> item_loss 2.425499439239502 batch_loss 2.233956598523837, real_loss 2.546774387359619
-> item_loss 4.148282527923584 batch_loss 2.262108450426775, real_loss 4.355696678161621
 83%| | 43/52 [06:58<01:55, 12.85s/it]-> item_loss 2.1900851726531982 batch_loss 2.261064634806868, real_loss 2.2995893955230713
 85%| | 44/52 [07:04<01:24, 10.53s/it][2023-10-04 11:45:08,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 2.709888219833374 batch_loss 2.2674764003072467, real_loss 2.8453824520111084
 87%| | 45/52 [07:09<01:02,  8.86s/it]-> item_loss 1.9299030303955078 batch_loss 2.2627218458014475, real_loss 2.026398181915283
 88%| | 46/52 [07:14<00:47,  7.93s/it]-> item_loss 1.7639341354370117 batch_loss 2.2557942387130527, real_loss 1.8521307706832886
 90%| | 47/52 [07:20<00:35,  7.18s/it]-> item_loss 1.2579669952392578 batch_loss 2.2421253723640966, real_loss 1.2579669952392578
 92%|| 48/52 [07:26<00:27,  6.85s/it]-> item_loss 1.4729104042053223 batch_loss 2.231730575497086, real_loss 1.4729104042053223
 94%|| 49/52 [07:32<00:19,  6.55s/it]-> item_loss 1.3776315450668335 batch_loss 2.220342588424683, real_loss 1.3776315450668335
 96%|| 50/52 [07:37<00:12,  6.10s/it]-> item_loss 1.7971378564834595 batch_loss 2.2147741051096665, real_loss 1.88699471950531
 98%|| 51/52 [07:43<00:06,  6.01s/it]-> item_loss 2.122807264328003 batch_loss 2.2135797305540605, real_loss 2.228947639465332
100%|| 52/52 [07:48<00:00,  5.82s/it]100%|| 52/52 [07:48<00:00,  9.01s/it]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 270960.70it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 519600.88it/s]
127.0.0.1 - - [04/Oct/2023 11:45:49] "POST /train/sft HTTP/1.1" 200 28
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/52 [00:00<?, ?it/s]-> item_loss 1.9637748003005981 batch_loss 1.9637748003005981, real_loss 2.0619635581970215
-> item_loss 1.8024394512176514 batch_loss 1.8831071257591248, real_loss 1.8925613164901733
[2023-10-04 11:46:10,792] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.9098294973373413 batch_loss 1.8920145829518635, real_loss 2.0053207874298096
-> item_loss 1.9051916599273682 batch_loss 1.8953088521957397, real_loss 2.00045108795166
  2%|         | 1/52 [00:25<21:57, 25.83s/it]-> item_loss 2.0016050338745117 batch_loss 1.9165680885314942, real_loss 2.1016852855682373
  4%|         | 2/52 [00:31<11:34, 13.89s/it]-> item_loss 2.418480157852173 batch_loss 2.0002201000849404, real_loss 2.5394041538238525
-> item_loss 2.207428455352783 batch_loss 2.0298212936946323, real_loss 2.3177998065948486
-> item_loss 1.9533321857452393 batch_loss 2.0202601552009583, real_loss 2.0509986877441406
-> item_loss 1.7564672231674194 batch_loss 1.9909498294194539, real_loss 1.8442904949188232
  6%|         | 3/52 [00:57<15:54, 19.47s/it]-> item_loss 0.9415106773376465 batch_loss 1.8860059142112733, real_loss 0.9415106773376465
  8%|         | 4/52 [01:02<11:07, 13.90s/it]-> item_loss 1.945776343345642 batch_loss 1.891439589587125, real_loss 2.043065071105957
 10%|         | 5/52 [01:08<08:36, 10.99s/it]-> item_loss 1.6984444856643677 batch_loss 1.8753566642602284, real_loss 1.7833666801452637
 12%|        | 6/52 [01:14<07:00,  9.15s/it][2023-10-04 11:47:10,411] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.3443703651428223 batch_loss 1.8345115643281202, real_loss 1.3443703651428223
 13%|        | 7/52 [01:19<05:54,  7.89s/it]-> item_loss 1.410232663154602 batch_loss 1.8042059285300118, real_loss 1.410232663154602
 15%|        | 8/52 [01:24<05:05,  6.95s/it]-> item_loss 1.7997487783432007 batch_loss 1.8039087851842244, real_loss 1.8897361755371094
 17%|        | 9/52 [01:30<04:49,  6.74s/it]-> item_loss 1.6325465440750122 batch_loss 1.7931986451148987, real_loss 1.7141737937927246
 19%|        | 10/52 [01:36<04:35,  6.55s/it]-> item_loss 1.4286528825759888 batch_loss 1.7717547767302568, real_loss 1.4286528825759888
 21%|        | 11/52 [01:41<04:10,  6.11s/it]-> item_loss 1.8867731094360352 batch_loss 1.7781446841028001, real_loss 1.9811116456985474
 23%|       | 12/52 [01:47<03:51,  5.79s/it]-> item_loss 1.6831004619598389 batch_loss 1.7731423566215916, real_loss 1.767255425453186
 25%|       | 13/52 [01:52<03:41,  5.67s/it]-> item_loss 1.1081210374832153 batch_loss 1.7398912906646729, real_loss 1.1081210374832153
 27%|       | 14/52 [01:57<03:28,  5.50s/it]-> item_loss 1.796379566192627 batch_loss 1.7425812085469563, real_loss 1.8861984014511108
 29%|       | 15/52 [02:03<03:27,  5.61s/it]-> item_loss 1.9132239818572998 batch_loss 1.750337698242881, real_loss 2.008885145187378
[2023-10-04 11:48:06,933] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.9441118240356445 batch_loss 1.7587626602338708, real_loss 2.0413172245025635
 31%|       | 16/52 [02:16<04:38,  7.73s/it]-> item_loss 1.6098116636276245 batch_loss 1.7525563687086105, real_loss 1.6903021335601807
-> item_loss 1.6222609281539917 batch_loss 1.7473445510864258, real_loss 1.703373908996582
 33%|      | 17/52 [02:29<05:27,  9.35s/it]-> item_loss 1.4614105224609375 batch_loss 1.7363470884469838, real_loss 1.4614105224609375
-> item_loss 1.8484225273132324 batch_loss 1.7404980306272153, real_loss 1.9408435821533203
 35%|      | 18/52 [02:41<05:46, 10.19s/it]-> item_loss 1.5147948265075684 batch_loss 1.7324372019086565, real_loss 1.5905344486236572
 37%|      | 19/52 [02:46<04:46,  8.69s/it]-> item_loss 1.7069019079208374 batch_loss 1.7315566745297661, real_loss 1.79224693775177
 38%|      | 20/52 [02:52<04:11,  7.85s/it]-> item_loss 1.6042518615722656 batch_loss 1.7273131807645161, real_loss 1.6844643354415894
-> item_loss 2.308208703994751 batch_loss 1.7460517460300076, real_loss 2.423619031906128
 40%|      | 21/52 [03:03<04:37,  8.95s/it]-> item_loss 1.3869996070861816 batch_loss 1.734831366688013, real_loss 1.3869996070861816
 42%|     | 22/52 [03:09<03:59,  8.00s/it][2023-10-04 11:49:06,514] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.5424730777740479 batch_loss 1.7290023276300142, real_loss 1.6195966005325317
 44%|     | 23/52 [03:15<03:34,  7.38s/it]-> item_loss 1.745208740234375 batch_loss 1.72947898682426, real_loss 1.83246910572052
 46%|     | 24/52 [03:21<03:13,  6.90s/it]-> item_loss 2.0724220275878906 batch_loss 1.7392773594175066, real_loss 2.1760430335998535
 48%|     | 25/52 [03:26<02:53,  6.44s/it]-> item_loss 1.496352195739746 batch_loss 1.7325294382042356, real_loss 1.496352195739746
 50%|     | 26/52 [03:32<02:43,  6.29s/it]-> item_loss 1.6768418550491333 batch_loss 1.7310243683892328, real_loss 1.7606838941574097
 52%|    | 27/52 [03:38<02:36,  6.26s/it]-> item_loss 1.444453239440918 batch_loss 1.7234830228905929, real_loss 1.444453239440918
-> item_loss 1.6044378280639648 batch_loss 1.7204305819976025, real_loss 1.6846596002578735
-> item_loss 1.641642451286316 batch_loss 1.7184608787298203, real_loss 1.7237244844436646
 54%|    | 28/52 [03:58<04:09, 10.40s/it]-> item_loss 1.269546389579773 batch_loss 1.7075117448481119, real_loss 1.269546389579773
 56%|    | 29/52 [04:05<03:30,  9.14s/it]-> item_loss 1.8635770082473755 batch_loss 1.7112275844528562, real_loss 1.9567557573318481
[2023-10-04 11:50:10,105] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.799948811531067 batch_loss 1.7132908688035122, real_loss 1.8899462223052979
-> item_loss 1.9450610876083374 batch_loss 1.7185583737763492, real_loss 2.042314052581787
 58%|    | 30/52 [04:24<04:31, 12.32s/it]-> item_loss 1.688173770904541 batch_loss 1.717883160379198, real_loss 1.7725824117660522
 60%|    | 31/52 [04:30<03:36, 10.33s/it]-> item_loss 1.0164525508880615 batch_loss 1.702634668868521, real_loss 1.0164525508880615
 62%|   | 32/52 [04:36<03:00,  9.05s/it]-> item_loss 0.37361955642700195 batch_loss 1.6743577515825312, real_loss 0.018680978566408157
 63%|   | 33/52 [04:42<02:31,  7.98s/it]-> item_loss 1.1468100547790527 batch_loss 1.663367174565792, real_loss 1.1468100547790527
 65%|   | 34/52 [04:48<02:16,  7.58s/it]-> item_loss 1.3989536762237549 batch_loss 1.657970980722077, real_loss 1.3989536762237549
 67%|   | 35/52 [04:55<02:03,  7.25s/it]-> item_loss 1.0156705379486084 batch_loss 1.6451249718666077, real_loss 1.0156705379486084
 69%|   | 36/52 [05:00<01:47,  6.71s/it]-> item_loss 1.2276455163955688 batch_loss 1.636939100190705, real_loss 1.2276455163955688
 71%|   | 37/52 [05:07<01:38,  6.58s/it]-> item_loss 1.4184849262237549 batch_loss 1.6327380583836482, real_loss 1.4184849262237549
[2023-10-04 11:51:09,332] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 2.4881174564361572 batch_loss 1.6488772923091672, real_loss 2.612523317337036
 73%|  | 38/52 [05:18<01:52,  8.04s/it]-> item_loss 3.261038303375244 batch_loss 1.678732125847428, real_loss 3.4240901470184326
 75%|  | 39/52 [05:23<01:31,  7.03s/it]-> item_loss 1.8531184196472168 batch_loss 1.6819027857346969, real_loss 1.9457741975784302
-> item_loss 1.560251235961914 batch_loss 1.6797304366316115, real_loss 1.6382637023925781
 77%|  | 40/52 [05:34<01:39,  8.30s/it]-> item_loss 1.6377270221710205 batch_loss 1.678993534623531, real_loss 1.7196133136749268
-> item_loss 1.5585421323776245 batch_loss 1.6769167863089463, real_loss 1.6364691257476807
 79%|  | 41/52 [05:46<01:43,  9.38s/it]-> item_loss 1.727339506149292 batch_loss 1.6777714086791216, real_loss 1.813706398010254
 81%|  | 42/52 [05:51<01:20,  8.09s/it]-> item_loss 1.7840187549591064 batch_loss 1.679542197783788, real_loss 1.873219609260559
-> item_loss 2.3202991485595703 batch_loss 1.6900464100915877, real_loss 2.436314105987549
-> item_loss 1.991571068763733 batch_loss 1.6949097110379128, real_loss 2.0911495685577393
[2023-10-04 11:52:06,977] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 2.6824452877044678 batch_loss 1.7105848789215088, real_loss 2.8165674209594727
 83%| | 43/52 [06:16<01:57, 13.08s/it]-> item_loss 1.3087670803070068 batch_loss 1.7043064758181572, real_loss 1.3087670803070068
 85%| | 44/52 [06:21<01:26, 10.83s/it]-> item_loss 1.694252371788025 batch_loss 1.7041517972946167, real_loss 1.778964877128601
 87%| | 45/52 [06:26<01:03,  9.13s/it]-> item_loss 1.5043247938156128 batch_loss 1.7011241154237227, real_loss 1.5795409679412842
 88%| | 46/52 [06:32<00:48,  8.01s/it]-> item_loss 1.4637888669967651 batch_loss 1.697581798283022, real_loss 1.4637888669967651
 90%| | 47/52 [06:37<00:36,  7.28s/it]-> item_loss 1.6609519720077515 batch_loss 1.697043124367209, real_loss 1.7439994812011719
-> item_loss 1.8604304790496826 batch_loss 1.6994110570437666, real_loss 1.9534518718719482
-> item_loss 1.5154938697814941 batch_loss 1.6967836686543056, real_loss 1.591268539428711
-> item_loss 0.8852657079696655 batch_loss 1.6853538382221276, real_loss 0.8852657079696655
 92%|| 48/52 [07:03<00:50, 12.70s/it]-> item_loss 0.7613745331764221 batch_loss 1.672520792318715, real_loss 0.7613745331764221
 94%|| 49/52 [07:08<00:31, 10.63s/it][2023-10-04 11:53:05,094] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.526611089706421 batch_loss 1.6705220292692315, real_loss 1.6029415130615234
 96%|| 50/52 [07:14<00:18,  9.02s/it]-> item_loss 1.6158314943313599 batch_loss 1.6697829679862872, real_loss 1.6966229677200317
-> item_loss 1.5864934921264648 batch_loss 1.6686724416414898, real_loss 1.6658180952072144
 98%|| 51/52 [07:26<00:10, 10.02s/it]-> item_loss 1.719639778137207 batch_loss 1.6693430644901175, real_loss 1.8056217432022095
-> item_loss 2.049372434616089 batch_loss 1.67427851085539, real_loss 2.151840925216675
100%|| 52/52 [07:38<00:00, 10.68s/it]100%|| 52/52 [07:38<00:00,  8.82s/it]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 288817.43it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 496256.34it/s]
127.0.0.1 - - [04/Oct/2023 11:53:31] "POST /train/sft HTTP/1.1" 200 26
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/54 [00:00<?, ?it/s]-> item_loss 1.7440189123153687 batch_loss 1.7440189123153687, real_loss 1.8312197923660278
-> item_loss 1.5930237770080566 batch_loss 1.6685213446617126, real_loss 1.6726748943328857
-> item_loss 1.6396287679672241 batch_loss 1.6588904857635498, real_loss 1.7216100692749023
-> item_loss 1.5013378858566284 batch_loss 1.6195023357868195, real_loss 1.5764046907424927
  2%|         | 1/54 [00:25<22:30, 25.48s/it]-> item_loss 1.424829125404358 batch_loss 1.5805676937103272, real_loss 1.424829125404358
[2023-10-04 11:54:10,597] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.396425724029541 batch_loss 1.549877365430196, real_loss 1.396425724029541
  4%|         | 2/54 [00:38<15:34, 17.98s/it]-> item_loss 0.7927883863449097 batch_loss 1.441721796989441, real_loss 0.7927883863449097
  6%|         | 3/54 [00:43<10:22, 12.20s/it]-> item_loss 1.2650312185287476 batch_loss 1.4196354746818542, real_loss 1.2650312185287476
  7%|         | 4/54 [00:48<07:53,  9.46s/it]-> item_loss 1.3901441097259521 batch_loss 1.4163586563534207, real_loss 1.3901441097259521
  9%|         | 5/54 [00:54<06:32,  8.02s/it]-> item_loss 0.9350119233131409 batch_loss 1.3682239830493927, real_loss 0.9350119233131409
 11%|         | 6/54 [01:00<05:51,  7.33s/it]-> item_loss 1.315820336341858 batch_loss 1.3634600151668896, real_loss 1.315820336341858
-> item_loss 2.0682904720306396 batch_loss 1.422195886572202, real_loss 2.1717050075531006
 13%|        | 7/54 [01:11<06:41,  8.55s/it]-> item_loss 1.1043167114257812 batch_loss 1.3977436423301697, real_loss 1.1043167114257812
 15%|        | 8/54 [01:17<05:51,  7.64s/it]-> item_loss 1.4252665042877197 batch_loss 1.3997095610414232, real_loss 1.4252665042877197
 17%|        | 9/54 [01:22<05:07,  6.83s/it]-> item_loss 1.4759804010391235 batch_loss 1.4047942837079366, real_loss 1.4759804010391235
[2023-10-04 11:55:06,471] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.436873435974121 batch_loss 1.4067992307245731, real_loss 1.436873435974121
 19%|        | 10/54 [01:34<06:11,  8.44s/it]-> item_loss 1.4332613945007324 batch_loss 1.408355828593759, real_loss 1.4332613945007324
 20%|        | 11/54 [01:39<05:27,  7.62s/it]-> item_loss 1.5419507026672363 batch_loss 1.4157777660422854, real_loss 1.6190481185913086
 22%|       | 12/54 [01:45<04:55,  7.03s/it]-> item_loss 1.504847764968872 batch_loss 1.4204656607226323, real_loss 1.580090045928955
-> item_loss 1.9011043310165405 batch_loss 1.4444975942373275, real_loss 1.9961594343185425
-> item_loss 1.6552164554595947 batch_loss 1.4545318257241022, real_loss 1.737977147102356
-> item_loss 1.7603260278701782 batch_loss 1.4684315621852875, real_loss 1.8483422994613647
 24%|       | 13/54 [02:10<08:27, 12.37s/it]-> item_loss 1.1860235929489136 batch_loss 1.4561529548271843, real_loss 1.1860235929489136
 26%|       | 14/54 [02:16<06:55, 10.40s/it]-> item_loss 1.2987951040267944 batch_loss 1.4495963777105014, real_loss 1.2987951040267944
-> item_loss 1.4985110759735107 batch_loss 1.4515529656410218, real_loss 1.4985110759735107
 28%|       | 15/54 [02:27<06:59, 10.76s/it][2023-10-04 11:56:05,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.47222101688385 batch_loss 1.4523478906888228, real_loss 1.47222101688385
 30%|       | 16/54 [02:33<05:53,  9.30s/it]-> item_loss 1.1981017589569092 batch_loss 1.4429313672913446, real_loss 1.1981017589569092
-> item_loss 1.2742847204208374 batch_loss 1.436908272760255, real_loss 1.2742847204208374
-> item_loss 1.1156902313232422 batch_loss 1.4258317885727718, real_loss 1.1156902313232422
 31%|      | 17/54 [02:51<07:24, 12.02s/it]-> item_loss 0.4645363688468933 batch_loss 1.3937886079152426, real_loss 0.023226818069815636
 33%|      | 18/54 [02:56<05:55,  9.87s/it]-> item_loss 1.7310057878494263 batch_loss 1.4046665814615065, real_loss 1.8175560235977173
-> item_loss 1.7794240713119507 batch_loss 1.4163777530193329, real_loss 1.868395209312439
 35%|      | 19/54 [03:09<06:13, 10.67s/it]-> item_loss 0.9177656769752502 batch_loss 1.4012682961695122, real_loss 0.9177656769752502
 37%|      | 20/54 [03:14<05:02,  8.91s/it]-> item_loss 1.4173146486282349 batch_loss 1.401740247712416, real_loss 1.4173146486282349
-> item_loss 1.7207865715026855 batch_loss 1.4108558569635663, real_loss 1.806825876235962
 39%|      | 21/54 [03:25<05:14,  9.52s/it][2023-10-04 11:57:02,527] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.072751760482788 batch_loss 1.4014640765057669, real_loss 1.072751760482788
 41%|      | 22/54 [03:30<04:22,  8.19s/it]-> item_loss 0.19858822226524353 batch_loss 1.3689539182830501, real_loss 0.009929411113262177
 43%|     | 23/54 [03:35<03:43,  7.22s/it]-> item_loss 1.590367078781128 batch_loss 1.3747805804014206, real_loss 1.6698853969573975
 44%|     | 24/54 [03:40<03:17,  6.60s/it]-> item_loss 1.668226718902588 batch_loss 1.382304840362989, real_loss 1.7516379356384277
-> item_loss 1.166566252708435 batch_loss 1.3769113756716251, real_loss 1.166566252708435
 46%|     | 25/54 [03:53<04:05,  8.45s/it]-> item_loss 1.546691656112671 batch_loss 1.3810523581214067, real_loss 1.6240261793136597
 48%|     | 26/54 [03:58<03:35,  7.70s/it]-> item_loss 1.430365800857544 batch_loss 1.3822264877103625, real_loss 1.430365800857544
 50%|     | 27/54 [04:04<03:09,  7.03s/it]-> item_loss 0.8435247540473938 batch_loss 1.3696985404158748, real_loss 0.8435247540473938
 52%|    | 28/54 [04:10<02:54,  6.73s/it]-> item_loss 1.592660903930664 batch_loss 1.3747658668593927, real_loss 1.6722939014434814
-> item_loss 1.5284115076065063 batch_loss 1.3781802144315507, real_loss 1.6048320531845093
[2023-10-04 11:58:02,214] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.5530338287353516 batch_loss 1.3819813799598943, real_loss 1.6306854486465454
 54%|    | 29/54 [04:29<04:23, 10.52s/it]-> item_loss 1.0644770860671997 batch_loss 1.375225969451539, real_loss 1.0644770860671997
 56%|    | 30/54 [04:35<03:40,  9.19s/it]-> item_loss 1.2942355871200562 batch_loss 1.3735386698196332, real_loss 1.2942355871200562
 57%|    | 31/54 [04:41<03:06,  8.13s/it]-> item_loss 0.7943591475486755 batch_loss 1.3617186795692056, real_loss 0.7943591475486755
 59%|    | 32/54 [04:47<02:41,  7.35s/it]-> item_loss 1.5403937101364136 batch_loss 1.3652921801805495, real_loss 1.6174132823944092
 61%|    | 33/54 [04:52<02:24,  6.90s/it]-> item_loss 1.2344846725463867 batch_loss 1.3627273270896836, real_loss 1.2344846725463867
 63%|   | 34/54 [04:58<02:10,  6.52s/it]-> item_loss 0.9641206860542297 batch_loss 1.3550618147620788, real_loss 0.9641206860542297
 65%|   | 35/54 [05:04<02:00,  6.35s/it]-> item_loss 1.0634651184082031 batch_loss 1.3495599903025717, real_loss 1.0634651184082031
 67%|   | 36/54 [05:10<01:54,  6.36s/it]-> item_loss 1.2171953916549683 batch_loss 1.3471087940313198, real_loss 1.2171953916549683
 69%|   | 37/54 [05:17<01:50,  6.49s/it]-> item_loss 1.2575751543045044 batch_loss 1.3454809096726503, real_loss 1.2575751543045044
 70%|   | 38/54 [05:23<01:38,  6.17s/it][2023-10-04 11:59:01,373] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.9619567394256592 batch_loss 1.3386322637753827, real_loss 0.9619567394256592
 72%|  | 39/54 [05:28<01:31,  6.08s/it]-> item_loss 1.5469402074813843 batch_loss 1.3422867891035581, real_loss 1.6242871284484863
-> item_loss 1.7679619789123535 batch_loss 1.3496260165140546, real_loss 1.8563599586486816
 74%|  | 40/54 [05:41<01:54,  8.15s/it]-> item_loss 0.5979472398757935 batch_loss 1.3368856982659485, real_loss 0.5979472398757935
 76%|  | 41/54 [05:48<01:38,  7.54s/it]-> item_loss 1.2022147178649902 batch_loss 1.3346411819259325, real_loss 1.2022147178649902
 78%|  | 42/54 [05:53<01:24,  7.03s/it]-> item_loss 1.4854176044464111 batch_loss 1.3371129265574158, real_loss 1.4854176044464111
-> item_loss 1.6012417078018188 batch_loss 1.3413730681903901, real_loss 1.6813037395477295
-> item_loss 1.2687500715255737 batch_loss 1.3402203222115834, real_loss 1.2687500715255737
-> item_loss 0.6192355155944824 batch_loss 1.3289549346081913, real_loss 0.6192355155944824
 80%|  | 43/54 [06:20<02:21, 12.86s/it]-> item_loss 1.6273891925811768 batch_loss 1.3335462308846988, real_loss 1.7087585926055908
 81%| | 44/54 [06:26<01:49, 10.93s/it][2023-10-04 12:00:04,805] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.2355493307113647 batch_loss 1.3320614293669208, real_loss 1.2355493307113647
 83%| | 45/54 [06:32<01:23,  9.33s/it]-> item_loss 2.655407190322876 batch_loss 1.3518128586349203, real_loss 2.788177490234375
 85%| | 46/54 [06:37<01:04,  8.09s/it]-> item_loss 1.4919294118881226 batch_loss 1.3538733961827614, real_loss 1.4919294118881226
 87%| | 47/54 [06:43<00:52,  7.49s/it]-> item_loss 2.023954153060913 batch_loss 1.363584711499836, real_loss 2.1251518726348877
-> item_loss 1.8060766458511353 batch_loss 1.3699060248477117, real_loss 1.8963804244995117
-> item_loss 1.6649558544158936 batch_loss 1.3740616562500807, real_loss 1.7482035160064697
-> item_loss 1.3273600339889526 batch_loss 1.373413022607565, real_loss 1.3273600339889526
 89%| | 48/54 [07:11<01:20, 13.44s/it]-> item_loss 1.337607502937317 batch_loss 1.3729225360367396, real_loss 1.337607502937317
 91%| | 49/54 [07:16<00:55, 11.17s/it]-> item_loss 1.4182114601135254 batch_loss 1.3735345485242638, real_loss 1.4182114601135254
 93%|| 50/54 [07:22<00:38,  9.51s/it]-> item_loss 1.276810646057129 batch_loss 1.3722448964913687, real_loss 1.276810646057129
 94%|| 51/54 [07:28<00:25,  8.38s/it][2023-10-04 12:01:06,615] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.246433138847351 batch_loss 1.3705894786276316, real_loss 1.246433138847351
 96%|| 52/54 [07:34<00:15,  7.65s/it]-> item_loss 1.4292711019515991 batch_loss 1.371351577631839, real_loss 1.4292711019515991
-> item_loss 1.2775042057037354 batch_loss 1.3701484061968632, real_loss 1.2775042057037354
 98%|| 53/54 [07:45<00:08,  8.86s/it]-> item_loss 1.1658270359039307 batch_loss 1.367562059737459, real_loss 1.1658270359039307
100%|| 54/54 [07:50<00:00,  7.71s/it]100%|| 54/54 [07:50<00:00,  8.72s/it]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 301145.44it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 540780.89it/s]
127.0.0.1 - - [04/Oct/2023 12:01:24] "POST /train/sft HTTP/1.1" 200 27
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/55 [00:00<?, ?it/s]-> item_loss 1.5741397142410278 batch_loss 1.5741397142410278, real_loss 1.6528465747833252
-> item_loss 1.461337924003601 batch_loss 1.5177388191223145, real_loss 1.461337924003601
-> item_loss 1.4186954498291016 batch_loss 1.4847243626912434, real_loss 1.4186954498291016
-> item_loss 1.2345553636550903 batch_loss 1.4221821129322052, real_loss 1.2345553636550903
  2%|         | 1/55 [00:25<22:40, 25.20s/it]-> item_loss 0.1471908837556839 batch_loss 1.167183867096901, real_loss 0.007359544280916452
  4%|         | 2/55 [00:30<11:45, 13.31s/it]-> item_loss 0.29350602626800537 batch_loss 1.0215708936254184, real_loss 0.014675301499664783
  5%|         | 3/55 [00:35<08:13,  9.50s/it][2023-10-04 12:02:07,866] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.5612539052963257 batch_loss 1.0986684667212623, real_loss 1.6393165588378906
-> item_loss 1.613000512123108 batch_loss 1.162959972396493, real_loss 1.693650484085083
  7%|         | 4/55 [00:47<09:08, 10.75s/it]-> item_loss 0.9990030527114868 batch_loss 1.1447425368759367, real_loss 0.9990030527114868
-> item_loss 0.9733569025993347 batch_loss 1.1276039734482766, real_loss 0.9733569025993347
-> item_loss 0.6639646291732788 batch_loss 1.0854549421505495, real_loss 0.6639646291732788
  9%|         | 5/55 [01:05<11:08, 13.36s/it]-> item_loss 1.256942868232727 batch_loss 1.0997456026573975, real_loss 1.256942868232727
 11%|         | 6/55 [01:11<08:40, 10.63s/it]-> item_loss 1.3175956010818481 batch_loss 1.1165032948438938, real_loss 1.3175956010818481
-> item_loss 1.1033923625946045 batch_loss 1.1155667996832304, real_loss 1.1033923625946045
 13%|        | 7/55 [01:22<08:47, 10.99s/it]-> item_loss 1.364210844039917 batch_loss 1.1321430693070094, real_loss 1.364210844039917
-> item_loss 1.4856919050216675 batch_loss 1.1542398715391755, real_loss 1.4856919050216675
 15%|        | 8/55 [01:34<08:51, 11.30s/it][2023-10-04 12:03:06,551] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.7440603971481323 batch_loss 1.1301116671632319, real_loss 0.7440603971481323
 16%|        | 9/55 [01:40<07:19,  9.55s/it]-> item_loss 0.950042724609375 batch_loss 1.1201078370213509, real_loss 0.950042724609375
 18%|        | 10/55 [01:45<06:07,  8.17s/it]-> item_loss 1.3532532453536987 batch_loss 1.1323786479862112, real_loss 1.3532532453536987
 20%|        | 11/55 [01:51<05:29,  7.48s/it]-> item_loss 0.6307256817817688 batch_loss 1.1072959996759892, real_loss 0.6307256817817688
 22%|       | 12/55 [01:57<04:55,  6.88s/it]-> item_loss 1.3627314567565918 batch_loss 1.1194595928703035, real_loss 1.3627314567565918
 24%|       | 13/55 [02:02<04:29,  6.42s/it]-> item_loss 1.0997684001922607 batch_loss 1.1185645386576653, real_loss 1.0997684001922607
 25%|       | 14/55 [02:07<04:12,  6.16s/it]-> item_loss 1.7724686861038208 batch_loss 1.14699515376402, real_loss 1.8610920906066895
-> item_loss 1.7832177877426147 batch_loss 1.1735044301797946, real_loss 1.8723785877227783
-> item_loss 2.3511948585510254 batch_loss 1.2206120473146438, real_loss 2.468754529953003
 27%|       | 15/55 [02:25<06:25,  9.63s/it]-> item_loss 1.0109807252883911 batch_loss 1.212549304159788, real_loss 1.0109807252883911
 29%|       | 16/55 [02:31<05:33,  8.54s/it][2023-10-04 12:04:04,219] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.7728204727172852 batch_loss 1.2333000881804361, real_loss 1.8614614009857178
-> item_loss 1.5405396223068237 batch_loss 1.2442729286849499, real_loss 1.6175665855407715
-> item_loss 1.4598833322525024 batch_loss 1.2517077701872792, real_loss 1.4598833322525024
-> item_loss 1.1213821172714233 batch_loss 1.2473635817567508, real_loss 1.1213821172714233
 31%|       | 17/55 [02:56<08:33, 13.50s/it]-> item_loss 2.2032217979431152 batch_loss 1.2781977177627626, real_loss 2.313382863998413
 33%|      | 18/55 [03:01<06:41, 10.85s/it]-> item_loss 0.6295868754386902 batch_loss 1.2579286289401352, real_loss 0.6295868754386902
 35%|      | 19/55 [03:06<05:24,  9.01s/it]-> item_loss 0.9041986465454102 batch_loss 1.2472095385645374, real_loss 0.9041986465454102
 36%|      | 20/55 [03:11<04:40,  8.03s/it]-> item_loss 1.0496755838394165 batch_loss 1.2413997163667398, real_loss 1.0496755838394165
-> item_loss 1.1949565410614014 batch_loss 1.240072768500873, real_loss 1.1949565410614014
 38%|      | 21/55 [03:23<05:08,  9.07s/it]-> item_loss 0.8774632811546326 batch_loss 1.2300002827412553, real_loss 0.8774632811546326
 40%|      | 22/55 [03:29<04:26,  8.07s/it][2023-10-04 12:05:00,574] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.6952434182167053 batch_loss 1.215547394510862, real_loss 0.6952434182167053
 42%|     | 23/55 [03:34<03:53,  7.30s/it]-> item_loss 1.1149464845657349 batch_loss 1.212900002143885, real_loss 1.1149464845657349
-> item_loss 1.236208438873291 batch_loss 1.2134976543677158, real_loss 1.236208438873291
-> item_loss 1.1143689155578613 batch_loss 1.2110194358974695, real_loss 1.1143689155578613
 44%|     | 24/55 [03:53<05:30, 10.67s/it]-> item_loss 0.9078087210655212 batch_loss 1.2036240526088855, real_loss 0.9078087210655212
 45%|     | 25/55 [03:58<04:34,  9.13s/it]-> item_loss 0.7106487154960632 batch_loss 1.1918865445823896, real_loss 0.7106487154960632
 47%|     | 26/55 [04:03<03:48,  7.87s/it]-> item_loss 1.2945712804794312 batch_loss 1.1942745616962744, real_loss 1.2945712804794312
-> item_loss 1.1664701700210571 batch_loss 1.1936426437036558, real_loss 1.1664701700210571
 49%|     | 27/55 [04:15<04:14,  9.08s/it]-> item_loss 1.301576018333435 batch_loss 1.196041163139873, real_loss 1.301576018333435
-> item_loss 1.183402180671692 batch_loss 1.1957664026514343, real_loss 1.183402180671692
 51%|     | 28/55 [04:27<04:32, 10.10s/it][2023-10-04 12:05:59,500] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.0646052360534668 batch_loss 1.1929757395323286, real_loss 1.0646052360534668
 53%|    | 29/55 [04:33<03:47,  8.73s/it]-> item_loss 0.973192572593689 batch_loss 1.1883969235544403, real_loss 0.973192572593689
 55%|    | 30/55 [04:38<03:10,  7.60s/it]-> item_loss 0.8566909432411194 batch_loss 1.1816274137521277, real_loss 0.8566909432411194
 56%|    | 31/55 [04:43<02:44,  6.84s/it]-> item_loss 1.3076153993606567 batch_loss 1.1841471734642983, real_loss 1.3076153993606567
-> item_loss 1.3134486675262451 batch_loss 1.1866824968772776, real_loss 1.3134486675262451
-> item_loss 1.0263640880584717 batch_loss 1.183599450553839, real_loss 1.0263640880584717
-> item_loss 0.4176744818687439 batch_loss 1.1691480360503466, real_loss 0.020883724093437195
 58%|    | 32/55 [05:08<04:41, 12.24s/it]-> item_loss 0.4560336768627167 batch_loss 1.1559422145839091, real_loss 0.022801684215664864
 60%|    | 33/55 [05:14<03:45, 10.27s/it]-> item_loss 1.0424537658691406 batch_loss 1.1538787882436405, real_loss 1.0424537658691406
 62%|   | 34/55 [05:19<03:03,  8.74s/it]-> item_loss 0.6489636301994324 batch_loss 1.1448624461357082, real_loss 0.6489636301994324
 64%|   | 35/55 [05:24<02:32,  7.62s/it][2023-10-04 12:06:55,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.1866990327835083 batch_loss 1.1455964213400556, real_loss 1.1866990327835083
 65%|   | 36/55 [05:29<02:12,  6.95s/it]-> item_loss 0.7862582206726074 batch_loss 1.1394009351216514, real_loss 0.7862582206726074
 67%|   | 37/55 [05:34<01:55,  6.39s/it]-> item_loss 1.1887576580047607 batch_loss 1.1402374897467888, real_loss 1.1887576580047607
-> item_loss 1.5676432847976685 batch_loss 1.1473609196643035, real_loss 1.6460254192352295
 69%|   | 38/55 [05:45<02:11,  7.72s/it]-> item_loss 0.7807090878486633 batch_loss 1.141350233896834, real_loss 0.7807090878486633
 71%|   | 39/55 [05:50<01:51,  6.98s/it]-> item_loss 1.2058333158493042 batch_loss 1.1423902836057447, real_loss 1.2058333158493042
 73%|  | 40/55 [05:55<01:36,  6.43s/it]-> item_loss 1.2698935270309448 batch_loss 1.144414144612494, real_loss 1.2698935270309448
 75%|  | 41/55 [06:00<01:22,  5.92s/it]-> item_loss 0.8873623013496399 batch_loss 1.1403977095615119, real_loss 0.8873623013496399
 76%|  | 42/55 [06:05<01:13,  5.66s/it]-> item_loss 0.5760173201560974 batch_loss 1.13171493433989, real_loss 0.5760173201560974
 78%|  | 43/55 [06:11<01:07,  5.59s/it]-> item_loss 1.276654601097107 batch_loss 1.1339109898968176, real_loss 1.276654601097107
 80%|  | 44/55 [06:17<01:03,  5.75s/it][2023-10-04 12:07:50,106] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.3968572616577148 batch_loss 1.1378355611171296, real_loss 1.3968572616577148
-> item_loss 0.813434898853302 batch_loss 1.1330649631426615, real_loss 0.813434898853302
 82%| | 45/55 [06:29<01:17,  7.70s/it]-> item_loss 0.9069921970367432 batch_loss 1.1297885462425756, real_loss 0.9069921970367432
 84%| | 46/55 [06:35<01:04,  7.15s/it]-> item_loss 1.2080881595611572 batch_loss 1.1309071121471268, real_loss 1.2080881595611572
-> item_loss 1.4965473413467407 batch_loss 1.13605697453022, real_loss 1.4965473413467407
-> item_loss 1.3022432327270508 batch_loss 1.138365117005176, real_loss 1.3022432327270508
-> item_loss 1.2295955419540405 batch_loss 1.1396148488537905, real_loss 1.2295955419540405
 85%| | 47/55 [07:01<01:42, 12.86s/it]-> item_loss 1.2804237604141235 batch_loss 1.1415176719829843, real_loss 1.2804237604141235
-> item_loss 1.2701438665390015 batch_loss 1.1432326879103978, real_loss 1.2701438665390015
 87%| | 48/55 [07:13<01:28, 12.65s/it]-> item_loss 1.0430175065994263 batch_loss 1.1419140671036745, real_loss 1.0430175065994263
 89%| | 49/55 [07:19<01:04, 10.71s/it][2023-10-04 12:08:51,821] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.9889106750488281 batch_loss 1.139927010064001, real_loss 0.9889106750488281
 91%| | 50/55 [07:25<00:46,  9.27s/it]-> item_loss 1.1227397918701172 batch_loss 1.1397066611127975, real_loss 1.1227397918701172
 93%|| 51/55 [07:31<00:32,  8.22s/it]-> item_loss 1.2338148355484009 batch_loss 1.1408979038271723, real_loss 1.2338148355484009
 95%|| 52/55 [07:37<00:22,  7.65s/it]-> item_loss 1.3763631582260132 batch_loss 1.143841219507158, real_loss 1.3763631582260132
 96%|| 53/55 [07:43<00:14,  7.17s/it]-> item_loss 1.0239158868789673 batch_loss 1.1423606598450813, real_loss 1.0239158868789673
 98%|| 54/55 [07:49<00:06,  6.65s/it]-> item_loss 1.418744683265686 batch_loss 1.1457311967160644, real_loss 1.418744683265686
100%|| 55/55 [07:55<00:00,  6.57s/it]100%|| 55/55 [07:55<00:00,  8.65s/it]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 257281.30it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 574778.65it/s]
127.0.0.1 - - [04/Oct/2023 12:09:23] "POST /train/sft HTTP/1.1" 200 28
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/55 [00:00<?, ?it/s]-> item_loss 1.3752361536026 batch_loss 1.3752361536026, real_loss 1.3752361536026
-> item_loss 1.2797300815582275 batch_loss 1.3274831175804138, real_loss 1.2797300815582275
-> item_loss 1.1551315784454346 batch_loss 1.2700326045354207, real_loss 1.1551315784454346
-> item_loss 0.9156800508499146 batch_loss 1.1814444661140442, real_loss 0.9156800508499146
  2%|         | 1/55 [00:26<24:16, 26.96s/it][2023-10-04 12:09:58,303] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.8290950059890747 batch_loss 1.1109745740890502, real_loss 0.8290950059890747
  4%|         | 2/55 [00:33<12:56, 14.66s/it]-> item_loss 0.8539730906486511 batch_loss 1.0681409935156505, real_loss 0.8539730906486511
-> item_loss 0.8138520121574402 batch_loss 1.0318139961787633, real_loss 0.8138520121574402
-> item_loss 0.4669325649738312 batch_loss 0.9612038172781467, real_loss 0.02334662899374962
  5%|         | 3/55 [00:52<14:45, 17.04s/it]-> item_loss 0.5869635343551636 batch_loss 0.9196215636200376, real_loss 0.5869635343551636
  7%|         | 4/55 [00:59<10:51, 12.77s/it]-> item_loss 1.0520976781845093 batch_loss 0.9328691750764847, real_loss 1.0520976781845093
-> item_loss 1.393866777420044 batch_loss 0.9747780480168082, real_loss 1.393866777420044
-> item_loss 1.0882679224014282 batch_loss 0.98423553754886, real_loss 1.0882679224014282
-> item_loss 0.7535926699638367 batch_loss 0.9664937785038581, real_loss 0.7535926699638367
  9%|         | 5/55 [01:24<14:27, 17.34s/it]-> item_loss 0.5075322985649109 batch_loss 0.9337108156510762, real_loss 0.5075322985649109
 11%|         | 6/55 [01:30<10:53, 13.34s/it][2023-10-04 12:11:01,954] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.7947142124176025 batch_loss 0.9244443754355113, real_loss 0.7947142124176025
-> item_loss 0.8390623927116394 batch_loss 0.9191080015152693, real_loss 0.8390623927116394
 13%|        | 7/55 [01:41<10:13, 12.78s/it]-> item_loss 0.5585812926292419 batch_loss 0.8979005480513853, real_loss 0.5585812926292419
 15%|        | 8/55 [01:47<08:08, 10.39s/it]-> item_loss 1.2333359718322754 batch_loss 0.9165358493725458, real_loss 1.2333359718322754
 16%|        | 9/55 [01:52<06:53,  9.00s/it]-> item_loss 1.1199876070022583 batch_loss 0.9272438366162149, real_loss 1.1199876070022583
-> item_loss 0.9323883652687073 batch_loss 0.9275010630488396, real_loss 0.9323883652687073
 18%|        | 10/55 [02:05<07:34, 10.09s/it]-> item_loss 1.1456278562545776 batch_loss 0.9378880532014937, real_loss 1.1456278562545776
-> item_loss 0.807822048664093 batch_loss 0.9319759620861574, real_loss 0.807822048664093
 20%|        | 11/55 [02:18<07:58, 10.89s/it]-> item_loss 0.6579367518424988 batch_loss 0.920061213814694, real_loss 0.6579367518424988
 22%|       | 12/55 [02:23<06:36,  9.21s/it]-> item_loss 1.2063231468200684 batch_loss 0.9319887943565845, real_loss 1.2063231468200684
[2023-10-04 12:12:01,751] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.248512625694275 batch_loss 0.9446497476100921, real_loss 1.248512625694275
 24%|       | 13/55 [02:36<07:13, 10.33s/it]-> item_loss 0.4325122833251953 batch_loss 0.9249521528299038, real_loss 0.021625613793730736
 25%|       | 14/55 [02:41<06:00,  8.79s/it]-> item_loss 0.21614371240139008 batch_loss 0.8986999883695885, real_loss 0.010807185433804989
 27%|       | 15/55 [02:46<05:08,  7.70s/it]-> item_loss 1.5238535404205322 batch_loss 0.9210269009428365, real_loss 1.600046157836914
 29%|       | 16/55 [02:51<04:29,  6.90s/it]-> item_loss 1.1962379217147827 batch_loss 0.9305169361418691, real_loss 1.1962379217147827
 31%|       | 17/55 [02:58<04:15,  6.71s/it]-> item_loss 0.7590234875679016 batch_loss 0.9248004878560702, real_loss 0.7590234875679016
 33%|      | 18/55 [03:04<04:01,  6.52s/it]-> item_loss 1.1782763004302979 batch_loss 0.9329771269713679, real_loss 1.1782763004302979
-> item_loss 1.0944502353668213 batch_loss 0.9380231616087258, real_loss 1.0944502353668213
-> item_loss 0.8632946014404297 batch_loss 0.935758659785444, real_loss 0.8632946014404297
-> item_loss 0.359408974647522 batch_loss 0.9188071984578582, real_loss 0.01797044835984707
 35%|      | 19/55 [03:29<07:13, 12.05s/it][2023-10-04 12:12:59,405] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.7288513779640198 batch_loss 0.9133798893008913, real_loss 0.7288513779640198
 36%|      | 20/55 [03:34<05:46,  9.91s/it]-> item_loss 0.4566362202167511 batch_loss 0.9006925651596652, real_loss 0.022831810638308525
 38%|      | 21/55 [03:38<04:44,  8.38s/it]-> item_loss 0.5435920357704163 batch_loss 0.8910411995004963, real_loss 0.5435920357704163
 40%|      | 22/55 [03:44<04:07,  7.49s/it]-> item_loss 1.1430386304855347 batch_loss 0.8976727108422079, real_loss 1.1430386304855347
-> item_loss 0.9268277883529663 batch_loss 0.8984202769322273, real_loss 0.9268277883529663
 42%|     | 23/55 [03:55<04:32,  8.51s/it]-> item_loss 1.213639736175537 batch_loss 0.90630076341331, real_loss 1.213639736175537
 44%|     | 24/55 [04:01<03:58,  7.69s/it]-> item_loss 1.4897490739822388 batch_loss 0.9205312100125522, real_loss 1.4897490739822388
-> item_loss 1.1558055877685547 batch_loss 0.9261329809115046, real_loss 1.1558055877685547
-> item_loss 1.1832618713378906 batch_loss 0.9321127225493275, real_loss 1.1832618713378906
-> item_loss 0.818116307258606 batch_loss 0.9295218949290839, real_loss 0.818116307258606
 45%|     | 25/55 [04:26<06:27, 12.93s/it][2023-10-04 12:13:57,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.2039903402328491 batch_loss 0.9356211937136121, real_loss 1.2039903402328491
-> item_loss 1.081347107887268 batch_loss 0.9387891483695611, real_loss 1.081347107887268
 47%|     | 26/55 [04:38<06:10, 12.77s/it]-> item_loss 0.7448168992996216 batch_loss 0.9346620792404134, real_loss 0.7448168992996216
 49%|     | 27/55 [04:43<04:55, 10.54s/it]-> item_loss 0.8608951568603516 batch_loss 0.9331252683574954, real_loss 0.8608951568603516
 51%|     | 28/55 [04:49<04:02,  8.97s/it]-> item_loss 0.7588096857070923 batch_loss 0.9295678074870791, real_loss 0.7588096857070923
 53%|    | 29/55 [04:54<03:25,  7.90s/it]-> item_loss 0.7923112511634827 batch_loss 0.9268226763606071, real_loss 0.7923112511634827
 55%|    | 30/55 [05:00<03:02,  7.29s/it]-> item_loss 0.7301600575447083 batch_loss 0.9229665465799033, real_loss 0.7301600575447083
 56%|    | 31/55 [05:05<02:41,  6.74s/it]-> item_loss 1.0364989042282104 batch_loss 0.925149861150063, real_loss 1.0364989042282104
-> item_loss 0.9274149537086487 batch_loss 0.925192598745508, real_loss 0.9274149537086487
 58%|    | 32/55 [05:17<03:08,  8.20s/it]-> item_loss 0.4216363728046417 batch_loss 0.9158674834503068, real_loss 0.021081818267703056
 60%|    | 33/55 [05:23<02:46,  7.55s/it][2023-10-04 12:14:54,252] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.45470425486564636 batch_loss 0.9074826974760403, real_loss 0.022735213860869408
 62%|   | 34/55 [05:28<02:24,  6.90s/it]-> item_loss 1.104177474975586 batch_loss 0.9109951042171035, real_loss 1.104177474975586
 64%|   | 35/55 [05:35<02:13,  6.70s/it]-> item_loss 1.0128129720687866 batch_loss 0.9127813826004664, real_loss 1.0128129720687866
 65%|   | 36/55 [05:41<02:05,  6.60s/it]-> item_loss 0.8898462653160095 batch_loss 0.9123859495438379, real_loss 0.8898462653160095
 67%|   | 37/55 [05:47<01:53,  6.31s/it]-> item_loss 1.0504623651504517 batch_loss 0.9147262277744584, real_loss 1.0504623651504517
 69%|   | 38/55 [05:53<01:44,  6.17s/it]-> item_loss 1.6449718475341797 batch_loss 0.9268969881037871, real_loss 1.7272204160690308
-> item_loss 1.639955759048462 batch_loss 0.9385864761520605, real_loss 1.7219535112380981
-> item_loss 1.5372941493988037 batch_loss 0.9482430515270079, real_loss 1.6141587495803833
 71%|   | 39/55 [06:11<02:39,  9.97s/it]-> item_loss 1.0119248628616333 batch_loss 0.9492538739291448, real_loss 1.0119248628616333
 73%|  | 40/55 [06:17<02:08,  8.55s/it]-> item_loss 0.6620775461196899 batch_loss 0.9447667438071221, real_loss 0.6620775461196899
 75%|  | 41/55 [06:22<01:48,  7.72s/it][2023-10-04 12:15:55,139] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.9083531498908997 batch_loss 0.9442065346699494, real_loss 0.9083531498908997
-> item_loss 0.993443489074707 batch_loss 0.9449525491306277, real_loss 0.993443489074707
-> item_loss 0.748282253742218 batch_loss 0.942017171587517, real_loss 0.748282253742218
 76%|  | 42/55 [06:42<02:25, 11.21s/it]-> item_loss 0.8132821321487427 batch_loss 0.9401240092428291, real_loss 0.8132821321487427
 78%|  | 43/55 [06:48<01:57,  9.77s/it]-> item_loss 0.6072233319282532 batch_loss 0.9352993617455164, real_loss 0.6072233319282532
 80%|  | 44/55 [06:54<01:33,  8.53s/it]-> item_loss 0.8484886288642883 batch_loss 0.9340592084186418, real_loss 0.8484886288642883
 82%| | 45/55 [07:00<01:17,  7.71s/it]-> item_loss 0.4531833827495575 batch_loss 0.9272863094655561, real_loss 0.022659169510006905
 84%| | 46/55 [07:05<01:02,  6.98s/it]-> item_loss 1.093950629234314 batch_loss 0.9296010916845666, real_loss 1.093950629234314
-> item_loss 0.38960111141204834 batch_loss 0.9222038316808335, real_loss 0.019480055198073387
 85%| | 47/55 [07:17<01:08,  8.60s/it]-> item_loss 0.8650616407394409 batch_loss 0.9214316399113552, real_loss 0.8650616407394409
 87%| | 48/55 [07:23<00:54,  7.85s/it][2023-10-04 12:16:54,532] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.20808669924736023 batch_loss 0.9119203740358353, real_loss 0.010404335334897041
 89%| | 49/55 [07:29<00:42,  7.12s/it]-> item_loss 1.3909164667129517 batch_loss 0.9182229542026394, real_loss 1.3909164667129517
-> item_loss 1.4320125579833984 batch_loss 0.9248955464595324, real_loss 1.4320125579833984
 91%| | 50/55 [07:42<00:44,  8.97s/it]-> item_loss 0.7019076347351074 batch_loss 0.9220367270784501, real_loss 0.7019076347351074
 93%|| 51/55 [07:48<00:32,  8.06s/it]-> item_loss 1.1397931575775146 batch_loss 0.9247931375910964, real_loss 1.1397931575775146
 95%|| 52/55 [07:54<00:21,  7.32s/it]-> item_loss 0.9707834124565125 batch_loss 0.9253680160269141, real_loss 0.9707834124565125
 96%|| 53/55 [07:59<00:13,  6.82s/it]-> item_loss 0.5057047009468079 batch_loss 0.9201869874456783, real_loss 0.5057047009468079
 98%|| 54/55 [08:05<00:06,  6.41s/it]-> item_loss 0.9373089075088501 batch_loss 0.9203957913488876, real_loss 0.9373089075088501
100%|| 55/55 [08:10<00:00,  6.16s/it]100%|| 55/55 [08:10<00:00,  8.92s/it]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 283847.08it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 520931.48it/s]
127.0.0.1 - - [04/Oct/2023 12:17:38] "POST /train/sft HTTP/1.1" 200 28
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/55 [00:00<?, ?it/s]-> item_loss 1.1904360055923462 batch_loss 1.1904360055923462, real_loss 1.1904360055923462
-> item_loss 1.1084352731704712 batch_loss 1.1494356393814087, real_loss 1.1084352731704712
[2023-10-04 12:18:00,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.0089507102966309 batch_loss 1.1026073296864827, real_loss 1.0089507102966309
-> item_loss 0.7466356158256531 batch_loss 1.0136144012212753, real_loss 0.7466356158256531
  2%|         | 1/55 [00:26<23:33, 26.18s/it]-> item_loss 0.4332999587059021 batch_loss 0.8975515127182007, real_loss 0.021664997562766075
  4%|         | 2/55 [00:31<12:19, 13.96s/it]-> item_loss 0.8566551804542542 batch_loss 0.8907354573408762, real_loss 0.8566551804542542
  5%|         | 3/55 [00:36<08:26,  9.73s/it]-> item_loss 0.35162508487701416 batch_loss 0.8137196898460388, real_loss 0.017581254243850708
  7%|         | 4/55 [00:41<06:41,  7.87s/it]-> item_loss 0.5800726413726807 batch_loss 0.784513808786869, real_loss 0.5800726413726807
  9%|         | 5/55 [00:46<05:42,  6.85s/it]-> item_loss 0.7073937058448792 batch_loss 0.7759449084599813, real_loss 0.7073937058448792
-> item_loss 0.6554830074310303 batch_loss 0.7638987183570862, real_loss 0.6554830074310303
 11%|         | 6/55 [00:57<06:50,  8.39s/it]-> item_loss 0.4632222652435303 batch_loss 0.7365644953467629, real_loss 0.023161113262176514
 13%|        | 7/55 [01:02<05:52,  7.35s/it]-> item_loss 1.0010029077529907 batch_loss 0.7586010297139486, real_loss 1.0010029077529907
[2023-10-04 12:18:53,572] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.6125336289405823 batch_loss 0.747365075808305, real_loss 0.6125336289405823
 15%|        | 8/55 [01:13<06:40,  8.51s/it]-> item_loss 0.8281916975975037 batch_loss 0.7531384059361049, real_loss 0.8281916975975037
 16%|        | 9/55 [01:19<05:53,  7.69s/it]-> item_loss 0.7660649418830872 batch_loss 0.7540001749992371, real_loss 0.7660649418830872
 18%|        | 10/55 [01:25<05:12,  6.94s/it]-> item_loss 0.11646398156881332 batch_loss 0.7141541629098356, real_loss 0.005823199171572924
 20%|        | 11/55 [01:29<04:37,  6.30s/it]-> item_loss 0.30971771478652954 batch_loss 0.6903637836084646, real_loss 0.015485885553061962
 22%|       | 12/55 [01:34<04:13,  5.90s/it]-> item_loss 0.9882251620292664 batch_loss 0.7069116379651759, real_loss 0.9882251620292664
-> item_loss 0.9001632928848267 batch_loss 0.717082777697789, real_loss 0.9001632928848267
-> item_loss 0.6849446296691895 batch_loss 0.7154758702963591, real_loss 0.6849446296691895
-> item_loss 0.317975252866745 batch_loss 0.6965472694663775, real_loss 0.01589876227080822
 24%|       | 13/55 [02:01<08:24, 12.02s/it]-> item_loss 0.6448481678962708 batch_loss 0.6941973103040998, real_loss 0.6448481678962708
 25%|       | 14/55 [02:06<06:57, 10.19s/it][2023-10-04 12:19:52,444] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.8421580195426941 batch_loss 0.7006303846188213, real_loss 0.8421580195426941
 27%|       | 15/55 [02:12<05:55,  8.88s/it]-> item_loss 0.6960824728012085 batch_loss 0.7004408882930875, real_loss 0.6960824728012085
 29%|       | 16/55 [02:18<05:11,  7.99s/it]-> item_loss 1.08425772190094 batch_loss 0.7157935616374016, real_loss 1.08425772190094
 31%|       | 17/55 [02:25<04:44,  7.49s/it]-> item_loss 0.31142768263816833 batch_loss 0.7002410278297387, real_loss 0.015571384690701962
 33%|      | 18/55 [02:30<04:16,  6.94s/it]-> item_loss 0.29310503602027893 batch_loss 0.685161917021981, real_loss 0.014655251987278461
 35%|      | 19/55 [02:35<03:51,  6.44s/it]-> item_loss 1.444584608078003 batch_loss 0.7122841559882674, real_loss 1.444584608078003
-> item_loss 1.3824384212493896 batch_loss 0.7353929237558924, real_loss 1.3824384212493896
-> item_loss 0.9446269869804382 batch_loss 0.7423673925300439, real_loss 0.9446269869804382
 36%|      | 20/55 [02:55<06:03, 10.38s/it]-> item_loss 1.07149076461792 batch_loss 0.7529842755006205, real_loss 1.07149076461792
-> item_loss 0.9597594141960144 batch_loss 0.7594459985848516, real_loss 0.9597594141960144
 38%|      | 21/55 [03:08<06:23, 11.29s/it][2023-10-04 12:20:54,342] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.6402688026428223 batch_loss 0.7558345684047901, real_loss 0.6402688026428223
 40%|      | 22/55 [03:14<05:17,  9.62s/it]-> item_loss 0.33622950315475464 batch_loss 0.7434932429562596, real_loss 0.016811475157737732
 42%|     | 23/55 [03:20<04:27,  8.35s/it]-> item_loss 0.9094229936599731 batch_loss 0.7482340929763658, real_loss 0.9094229936599731
 44%|     | 24/55 [03:25<03:54,  7.56s/it]-> item_loss 0.44272977113723755 batch_loss 0.7397478618141677, real_loss 0.022136488929390907
 45%|     | 25/55 [03:31<03:30,  7.02s/it]-> item_loss 0.6275932788848877 batch_loss 0.7367166568701332, real_loss 0.6275932788848877
-> item_loss 0.5379489660263062 batch_loss 0.7314859281637167, real_loss 0.5379489660263062
-> item_loss 0.30695098638534546 batch_loss 0.7206004168360661, real_loss 0.015347549691796303
 47%|     | 26/55 [03:49<05:01, 10.40s/it]-> item_loss 0.39199426770210266 batch_loss 0.712385263107717, real_loss 0.019599713385105133
 49%|     | 27/55 [03:55<04:11,  9.00s/it]-> item_loss 0.09436249732971191 batch_loss 0.6973115371131315, real_loss 0.004718124866485596
 51%|     | 28/55 [04:00<03:28,  7.74s/it]-> item_loss 0.667504608631134 batch_loss 0.6966018483397507, real_loss 0.667504608631134
 53%|    | 29/55 [04:05<03:01,  6.97s/it][2023-10-04 12:21:50,991] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.6467968225479126 batch_loss 0.6954435919259869, real_loss 0.6467968225479126
 55%|    | 30/55 [04:11<02:45,  6.62s/it]-> item_loss 0.7557359337806702 batch_loss 0.6968138724226843, real_loss 0.7557359337806702
 56%|    | 31/55 [04:16<02:25,  6.08s/it]-> item_loss 1.2696270942687988 batch_loss 0.7095430551303757, real_loss 1.2696270942687988
-> item_loss 0.88048255443573 batch_loss 0.7132591312022313, real_loss 0.88048255443573
-> item_loss 0.9462916851043701 batch_loss 0.7182172706469576, real_loss 0.9462916851043701
-> item_loss 0.5828089714050293 batch_loss 0.7153962644127508, real_loss 0.5828089714050293
 58%|    | 32/55 [04:41<04:35, 11.98s/it]-> item_loss 0.22896859049797058 batch_loss 0.7054691690267348, real_loss 0.011448429897427559
 60%|    | 33/55 [04:46<03:37,  9.88s/it]-> item_loss 0.5159336924552917 batch_loss 0.701678459495306, real_loss 0.5159336924552917
 62%|   | 34/55 [04:52<03:03,  8.73s/it]-> item_loss 1.0896801948547363 batch_loss 0.7092863366592164, real_loss 1.0896801948547363
 64%|   | 35/55 [04:59<02:38,  7.93s/it]-> item_loss 1.069428563117981 batch_loss 0.7162121487065003, real_loss 1.069428563117981
[2023-10-04 12:22:51,558] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.082446813583374 batch_loss 0.7231222367230451, real_loss 1.082446813583374
 65%|   | 36/55 [05:11<02:59,  9.42s/it]-> item_loss 0.5565338134765625 batch_loss 0.7200372659221843, real_loss 0.5565338134765625
-> item_loss 0.7866191267967224 batch_loss 0.7212478452108123, real_loss 0.7866191267967224
-> item_loss 0.48005929589271545 batch_loss 0.716940906830132, real_loss 0.024002965539693832
 67%|   | 37/55 [05:31<03:43, 12.40s/it]-> item_loss 0.5762539505958557 batch_loss 0.7144727146154956, real_loss 0.5762539505958557
 69%|   | 38/55 [05:37<02:57, 10.43s/it]-> item_loss 1.069863200187683 batch_loss 0.7206001367805333, real_loss 1.069863200187683
-> item_loss 0.6409301161766052 batch_loss 0.7192497974482633, real_loss 0.6409301161766052
 71%|   | 39/55 [05:49<02:57, 11.12s/it]-> item_loss 0.956709623336792 batch_loss 0.7232074612130721, real_loss 0.956709623336792
-> item_loss 0.7566841244697571 batch_loss 0.7237562589713784, real_loss 0.7566841244697571
 73%|  | 40/55 [06:02<02:54, 11.66s/it]-> item_loss 0.8240182399749756 batch_loss 0.7253733876972429, real_loss 0.8240182399749756
[2023-10-04 12:23:56,375] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.0339641571044922 batch_loss 0.7302716538783104, real_loss 1.0339641571044922
-> item_loss 0.829243004322052 batch_loss 0.7318180812289938, real_loss 0.829243004322052
-> item_loss 0.48587721586227417 batch_loss 0.7280343756079674, real_loss 0.02429386042058468
 75%|  | 41/55 [06:28<03:44, 16.00s/it]-> item_loss 0.30518725514411926 batch_loss 0.7216276010554848, real_loss 0.015259362757205963
 76%|  | 42/55 [06:34<02:47, 12.89s/it]-> item_loss 0.6470432281494141 batch_loss 0.7205144014598718, real_loss 0.6470432281494141
 78%|  | 43/55 [06:40<02:09, 10.76s/it]-> item_loss 0.8931779861450195 batch_loss 0.7230535718228888, real_loss 0.8931779861450195
-> item_loss 0.27951183915138245 batch_loss 0.7166254307696784, real_loss 0.013975592330098152
 80%|  | 44/55 [06:52<02:02, 11.17s/it]-> item_loss 0.4448832869529724 batch_loss 0.7127434001437255, real_loss 0.02224416472017765
 82%| | 45/55 [06:58<01:35,  9.51s/it]-> item_loss 0.5392178893089294 batch_loss 0.7102993788643622, real_loss 0.5392178893089294
 84%| | 46/55 [07:03<01:15,  8.38s/it]-> item_loss 0.903182327747345 batch_loss 0.7129783087099592, real_loss 0.903182327747345
[2023-10-04 12:24:55,030] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.5116017460823059 batch_loss 0.7102197256602653, real_loss 0.5116017460823059
 85%| | 47/55 [07:15<01:14,  9.34s/it]-> item_loss 0.6068978905677795 batch_loss 0.7088234846455019, real_loss 0.6068978905677795
 87%| | 48/55 [07:21<00:58,  8.35s/it]-> item_loss 0.9591038227081299 batch_loss 0.7121605558196703, real_loss 0.9591038227081299
 89%| | 49/55 [07:27<00:46,  7.75s/it]-> item_loss 1.212736964225769 batch_loss 0.7187470875092243, real_loss 1.212736964225769
-> item_loss 1.2524559497833252 batch_loss 0.7256783714348619, real_loss 1.2524559497833252
 91%| | 50/55 [07:41<00:47,  9.43s/it]-> item_loss 0.3942062556743622 batch_loss 0.721428728925112, real_loss 0.01971031352877617
 93%|| 51/55 [07:46<00:33,  8.34s/it]-> item_loss 0.4470295310020447 batch_loss 0.7179553213564656, real_loss 0.022351477295160294
 95%|| 52/55 [07:52<00:22,  7.52s/it]-> item_loss 0.9680787920951843 batch_loss 0.7210818647406996, real_loss 0.9680787920951843
 96%|| 53/55 [07:58<00:14,  7.13s/it]-> item_loss 0.5008262991905212 batch_loss 0.7183626602277343, real_loss 0.5008262991905212
 98%|| 54/55 [08:04<00:06,  6.62s/it]-> item_loss 0.614441454410553 batch_loss 0.7170953284494761, real_loss 0.614441454410553
100%|| 55/55 [08:10<00:00,  6.57s/it]100%|| 55/55 [08:10<00:00,  8.92s/it]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 300478.20it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 578319.10it/s]
127.0.0.1 - - [04/Oct/2023 12:25:51] "POST /train/sft HTTP/1.1" 200 28
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/55 [00:00<?, ?it/s][2023-10-04 12:25:59,864] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.9979175329208374 batch_loss 0.9979175329208374, real_loss 0.9979175329208374
-> item_loss 0.9541440606117249 batch_loss 0.9760307967662811, real_loss 0.9541440606117249
-> item_loss 0.8133160471916199 batch_loss 0.9217925469080607, real_loss 0.8133160471916199
-> item_loss 0.4996488094329834 batch_loss 0.8162566125392914, real_loss 0.02498244121670723
  2%|         | 1/55 [00:26<23:55, 26.58s/it]-> item_loss 0.518954873085022 batch_loss 0.7567962646484375, real_loss 0.518954873085022
  4%|         | 2/55 [00:32<12:57, 14.67s/it]-> item_loss 0.4711402654647827 batch_loss 0.7091869314511617, real_loss 0.023557012900710106
  5%|         | 3/55 [00:38<09:12, 10.62s/it]-> item_loss 0.9209560751914978 batch_loss 0.7394396662712097, real_loss 0.9209560751914978
-> item_loss 0.3998425304889679 batch_loss 0.6969900242984295, real_loss 0.019992126151919365
  7%|         | 4/55 [00:51<09:39, 11.36s/it]-> item_loss 0.42115819454193115 batch_loss 0.6663420432143741, real_loss 0.021057909354567528
  9%|         | 5/55 [00:56<07:44,  9.28s/it]-> item_loss 0.29941636323928833 batch_loss 0.6296494752168655, real_loss 0.014970818534493446
 11%|         | 6/55 [01:02<06:31,  7.98s/it][2023-10-04 12:27:01,170] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.382476270198822 batch_loss 0.6071791838515889, real_loss 0.01912381313741207
 13%|        | 7/55 [01:08<05:51,  7.31s/it]-> item_loss 0.14445434510707855 batch_loss 0.5686187806228796, real_loss 0.00722271716222167
 15%|        | 8/55 [01:13<05:12,  6.65s/it]-> item_loss 0.39830121397972107 batch_loss 0.5555174293426367, real_loss 0.019915061071515083
 16%|        | 9/55 [01:19<04:50,  6.32s/it]-> item_loss 0.17838695645332336 batch_loss 0.5285795384219715, real_loss 0.008919348008930683
 18%|        | 10/55 [01:24<04:30,  6.02s/it]-> item_loss 0.11042532324790955 batch_loss 0.5007025907437007, real_loss 0.005521266255527735
 20%|        | 11/55 [01:29<04:15,  5.81s/it]-> item_loss 0.8317918181419373 batch_loss 0.5213956674560905, real_loss 0.8317918181419373
 22%|       | 12/55 [01:35<04:14,  5.91s/it]-> item_loss 0.6557568311691284 batch_loss 0.5292992653215632, real_loss 0.6557568311691284
-> item_loss 0.16534219682216644 batch_loss 0.5090794281827079, real_loss 0.008267110213637352
 24%|       | 13/55 [01:48<05:30,  7.86s/it]-> item_loss 0.8977141976356506 batch_loss 0.5295338897328628, real_loss 0.8977141976356506
-> item_loss 0.7940987944602966 batch_loss 0.5427621349692344, real_loss 0.7940987944602966
 25%|       | 14/55 [02:00<06:21,  9.32s/it][2023-10-04 12:27:59,007] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.5547758936882019 batch_loss 0.5433342187177568, real_loss 0.5547758936882019
 27%|       | 15/55 [02:06<05:22,  8.07s/it]-> item_loss 1.02670419216156 batch_loss 0.5653055811470206, real_loss 1.02670419216156
-> item_loss 1.0203032493591309 batch_loss 0.5850880884605906, real_loss 1.0203032493591309
 29%|       | 16/55 [02:19<06:15,  9.62s/it]-> item_loss 0.7179589867591858 batch_loss 0.5906243758896986, real_loss 0.7179589867591858
 31%|       | 17/55 [02:24<05:19,  8.40s/it]-> item_loss 0.8251743316650391 batch_loss 0.6000063741207122, real_loss 0.8251743316650391
 33%|      | 18/55 [02:31<04:45,  7.73s/it]-> item_loss 0.19027040898799896 batch_loss 0.5842472985386848, real_loss 0.009513520635664463
 35%|      | 19/55 [02:36<04:08,  6.91s/it]-> item_loss 0.37471726536750793 batch_loss 0.5764869269397523, real_loss 0.018735863268375397
 36%|      | 20/55 [02:41<03:49,  6.56s/it]-> item_loss 0.8759544491767883 batch_loss 0.5871821955910751, real_loss 0.8759544491767883
 38%|      | 21/55 [02:47<03:36,  6.38s/it]-> item_loss 0.506393313407898 batch_loss 0.5843963720675173, real_loss 0.506393313407898
 40%|      | 22/55 [02:53<03:23,  6.17s/it]-> item_loss 0.7373866438865662 batch_loss 0.5894960477948189, real_loss 0.7373866438865662
 42%|     | 23/55 [02:59<03:18,  6.20s/it][2023-10-04 12:28:59,599] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 1.2582992315292358 batch_loss 0.6110703440443161, real_loss 1.2582992315292358
-> item_loss 1.1568710803985596 batch_loss 0.6281266170553863, real_loss 1.1568710803985596
-> item_loss 0.5749157667160034 batch_loss 0.626514167045102, real_loss 0.5749157667160034
 44%|     | 24/55 [03:18<05:11, 10.05s/it]-> item_loss 0.3178611099720001 batch_loss 0.6174361359547166, real_loss 0.015893055126070976
 45%|     | 25/55 [03:24<04:21,  8.71s/it]-> item_loss 0.8710103034973145 batch_loss 0.6246811121702194, real_loss 0.8710103034973145
-> item_loss 0.7861019372940063 batch_loss 0.6291650239792135, real_loss 0.7861019372940063
-> item_loss 0.6146754622459412 batch_loss 0.6287734142026385, real_loss 0.6146754622459412
-> item_loss 0.2550012469291687 batch_loss 0.6189373045375473, real_loss 0.01275006216019392
 47%|     | 26/55 [03:50<06:44, 13.96s/it]-> item_loss 0.5668427348136902 batch_loss 0.6176015463394996, real_loss 0.5668427348136902
 49%|     | 27/55 [03:56<05:20, 11.44s/it]-> item_loss 0.5286109447479248 batch_loss 0.6153767812997103, real_loss 0.5286109447479248
 51%|     | 28/55 [04:01<04:23,  9.74s/it][2023-10-04 12:30:00,386] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.24263380467891693 batch_loss 0.606285489187008, real_loss 0.012131690047681332
 53%|    | 29/55 [04:07<03:40,  8.50s/it]-> item_loss 0.3611087501049042 batch_loss 0.6004479477802912, real_loss 0.01805543713271618
 55%|    | 30/55 [04:13<03:13,  7.75s/it]-> item_loss 0.3779512345790863 batch_loss 0.5952736056128214, real_loss 0.018897561356425285
-> item_loss 0.6424144506454468 batch_loss 0.5963449884544719, real_loss 0.6424144506454468
-> item_loss 0.2964645326137543 batch_loss 0.5896809783246783, real_loss 0.014823227189481258
 56%|    | 31/55 [04:32<04:28, 11.18s/it]-> item_loss 0.3687378764152527 batch_loss 0.5848778674136037, real_loss 0.018436893820762634
 58%|    | 32/55 [04:38<03:39,  9.56s/it]-> item_loss 0.6258839964866638 batch_loss 0.5857503382449455, real_loss 0.6258839964866638
 60%|    | 33/55 [04:44<03:08,  8.58s/it]-> item_loss 0.8487766981124878 batch_loss 0.5912300540755192, real_loss 0.8487766981124878
-> item_loss 0.4365033507347107 batch_loss 0.5880723662522375, real_loss 0.021825168281793594
 62%|   | 34/55 [04:56<03:20,  9.53s/it]-> item_loss 0.805330216884613 batch_loss 0.592417523264885, real_loss 0.805330216884613
 64%|   | 35/55 [05:01<02:44,  8.21s/it][2023-10-04 12:31:00,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.31368085741996765 batch_loss 0.5869520984443963, real_loss 0.015684043988585472
 65%|   | 36/55 [05:07<02:21,  7.44s/it]-> item_loss 0.5153356194496155 batch_loss 0.5855748584637275, real_loss 0.5153356194496155
-> item_loss 0.4230615794658661 batch_loss 0.582508570180749, real_loss 0.021153079345822334
 67%|   | 37/55 [05:18<02:37,  8.73s/it]-> item_loss 0.9275926351547241 batch_loss 0.5888990158284152, real_loss 0.9275926351547241
-> item_loss 0.5706132054328918 batch_loss 0.5885665465484966, real_loss 0.5706132054328918
-> item_loss 0.7145940065383911 batch_loss 0.590817036905459, real_loss 0.7145940065383911
-> item_loss 0.42626872658729553 batch_loss 0.5879302244437369, real_loss 0.021313436329364777
 69%|   | 38/55 [05:45<03:58, 14.01s/it]-> item_loss 0.3605804145336151 batch_loss 0.5840104001349417, real_loss 0.018029021099209785
 71%|   | 39/55 [05:51<03:05, 11.60s/it]-> item_loss 0.6569743156433105 batch_loss 0.5852470766689818, real_loss 0.6569743156433105
-> item_loss 0.8059794902801514 batch_loss 0.5889259502291679, real_loss 0.8059794902801514
[2023-10-04 12:32:04,388] [INFO] [logging.py:96:log_dist] [Rank 0] step=540, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.6095836162567139 batch_loss 0.5892646004919146, real_loss 0.6095836162567139
-> item_loss 0.29217764735221863 batch_loss 0.5844728754412744, real_loss 0.014608882367610931
 73%|  | 40/55 [06:16<03:55, 15.69s/it]-> item_loss 0.4507614076137543 batch_loss 0.5823504711900439, real_loss 0.022538071498274803
 75%|  | 41/55 [06:21<02:56, 12.59s/it]-> item_loss 0.24103283882141113 batch_loss 0.577017383184284, real_loss 0.012051641941070557
 76%|  | 42/55 [06:27<02:15, 10.41s/it]-> item_loss 0.48426494002342224 batch_loss 0.5755904225202707, real_loss 0.024213247001171112
-> item_loss 0.3601882755756378 batch_loss 0.5723267536271702, real_loss 0.01800941489636898
-> item_loss 0.1994367241859436 batch_loss 0.5667612307996892, real_loss 0.00997183658182621
 78%|  | 43/55 [06:46<02:38, 13.22s/it]-> item_loss 0.15418176352977753 batch_loss 0.5606938856927788, real_loss 0.007709088269621134
 80%|  | 44/55 [06:52<02:00, 10.96s/it]-> item_loss 0.8379644751548767 batch_loss 0.5647123000328091, real_loss 0.8379644751548767
-> item_loss 0.6110054850578308 batch_loss 0.5653736312474523, real_loss 0.6110054850578308
 82%| | 45/55 [07:04<01:53, 11.34s/it][2023-10-04 12:33:04,676] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.9491158127784729 batch_loss 0.5707784507056357, real_loss 0.9491158127784729
-> item_loss 0.7823232412338257 batch_loss 0.5737165727963051, real_loss 0.7823232412338257
 84%| | 46/55 [07:18<01:47, 11.91s/it]-> item_loss 0.3157375454902649 batch_loss 0.57018261351814, real_loss 0.015786876901984215
 85%| | 47/55 [07:24<01:20, 10.12s/it]-> item_loss 0.25653478503227234 batch_loss 0.5659441293494122, real_loss 0.012826739810407162
 87%| | 48/55 [07:29<01:01,  8.75s/it]-> item_loss 0.7357227206230164 batch_loss 0.5682078438997269, real_loss 0.7357227206230164
-> item_loss 0.2784736752510071 batch_loss 0.5643955522069806, real_loss 0.013923684135079384
 89%| | 49/55 [07:40<00:57,  9.55s/it]-> item_loss 0.4723469018936157 batch_loss 0.5632001151899239, real_loss 0.023617345839738846
 91%| | 50/55 [07:47<00:42,  8.51s/it]-> item_loss 0.29712459444999695 batch_loss 0.559788890565053, real_loss 0.014856229536235332
 93%|| 51/55 [07:52<00:30,  7.67s/it]-> item_loss 0.31564730405807495 batch_loss 0.5566984907358508, real_loss 0.015782365575432777
 95%|| 52/55 [07:58<00:20,  6.96s/it]-> item_loss 0.328435480594635 batch_loss 0.5538452031090856, real_loss 0.01642177440226078
 96%|| 53/55 [08:03<00:12,  6.35s/it][2023-10-04 12:34:01,680] [INFO] [logging.py:96:log_dist] [Rank 0] step=560, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.6502341032028198 batch_loss 0.5550351895299959, real_loss 0.6502341032028198
 98%|| 54/55 [08:08<00:06,  6.16s/it]-> item_loss 0.06786103546619415 batch_loss 0.5490940413097056, real_loss 0.003393051913008094
100%|| 55/55 [08:14<00:00,  5.91s/it]100%|| 55/55 [08:14<00:00,  8.98s/it]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 296860.63it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 577908.36it/s]
127.0.0.1 - - [04/Oct/2023 12:34:08] "POST /train/sft HTTP/1.1" 200 28
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
====== None
  0%|          | 0/55 [00:00<?, ?it/s]-> item_loss 0.8252414464950562 batch_loss 0.8252414464950562, real_loss 0.8252414464950562
-> item_loss 0.6566359400749207 batch_loss 0.7409386932849884, real_loss 0.6566359400749207
-> item_loss 0.5956403613090515 batch_loss 0.6925059159596761, real_loss 0.5956403613090515
-> item_loss 0.39556533098220825 batch_loss 0.6182707697153091, real_loss 0.019778266549110413
  2%|         | 1/55 [00:27<24:21, 27.06s/it]-> item_loss 0.1400114893913269 batch_loss 0.5226189136505127, real_loss 0.007000574376434088
  4%|         | 2/55 [00:32<12:34, 14.24s/it]-> item_loss 0.2039344161748886 batch_loss 0.46950483073790866, real_loss 0.01019672118127346
  5%|         | 3/55 [00:37<08:51, 10.22s/it]-> item_loss 0.12768346071243286 batch_loss 0.420673206448555, real_loss 0.006384172942489386
  7%|         | 4/55 [00:43<07:08,  8.39s/it]-> item_loss 0.15918883681297302 batch_loss 0.38798766024410725, real_loss 0.007959442213177681
  9%|         | 5/55 [00:49<06:10,  7.42s/it][2023-10-04 12:35:05,681] [INFO] [logging.py:96:log_dist] [Rank 0] step=570, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.7495418787002563 batch_loss 0.42816035118367934, real_loss 0.7495418787002563
-> item_loss 0.33172494173049927 batch_loss 0.41851681023836135, real_loss 0.016586247831583023
-> item_loss 0.5052148103713989 batch_loss 0.42639844661409204, real_loss 0.5052148103713989
-> item_loss 0.23464660346508026 batch_loss 0.4104191263516744, real_loss 0.011732330545783043
 11%|         | 6/55 [01:15<11:18, 13.84s/it]-> item_loss 0.26228222250938416 batch_loss 0.39902397990226746, real_loss 0.013114111497998238
 13%|        | 7/55 [01:21<09:00, 11.25s/it]-> item_loss 0.24633000791072845 batch_loss 0.38811726761715754, real_loss 0.012316500768065453
-> item_loss 0.5079054236412048 batch_loss 0.39610314468542734, real_loss 0.5079054236412048
-> item_loss 0.21256470680236816 batch_loss 0.38463199231773615, real_loss 0.010628235526382923
 15%|        | 8/55 [01:40<10:48, 13.79s/it]-> item_loss 0.06464333832263947 batch_loss 0.3658091303180246, real_loss 0.0032321668695658445
 16%|        | 9/55 [01:45<08:30, 11.10s/it]-> item_loss 0.4082662761211395 batch_loss 0.36816786064041984, real_loss 0.020413314923644066
 18%|        | 10/55 [01:50<06:53,  9.19s/it][2023-10-04 12:36:05,633] [INFO] [logging.py:96:log_dist] [Rank 0] step=580, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.04682173952460289 batch_loss 0.3512549068974821, real_loss 0.0023410869762301445
 20%|        | 11/55 [01:55<05:51,  7.98s/it]-> item_loss 0.31543979048728943 batch_loss 0.3494641510769725, real_loss 0.01577199064195156
 22%|       | 12/55 [02:01<05:07,  7.16s/it]-> item_loss 0.2033098042011261 batch_loss 0.34250442027336075, real_loss 0.010165490210056305
 24%|       | 13/55 [02:07<04:46,  6.82s/it]-> item_loss 0.4967370331287384 batch_loss 0.34951499358496885, real_loss 0.02483685128390789
-> item_loss 0.5966460108757019 batch_loss 0.36025982042369636, real_loss 0.5966460108757019
-> item_loss 0.4176676869392395 batch_loss 0.362651814861844, real_loss 0.020883385092020035
-> item_loss 0.24718332290649414 batch_loss 0.35803307518363, real_loss 0.012359166517853737
 25%|       | 14/55 [02:32<08:30, 12.44s/it]-> item_loss 0.39410483837127686 batch_loss 0.35942045069084716, real_loss 0.019705241546034813
 27%|       | 15/55 [02:38<06:56, 10.41s/it]-> item_loss 0.6159759163856506 batch_loss 0.3689225049758399, real_loss 0.6159759163856506
 29%|       | 16/55 [02:44<05:53,  9.06s/it]-> item_loss 0.08414767682552338 batch_loss 0.35875197539904286, real_loss 0.004207383841276169
 31%|       | 17/55 [02:49<05:00,  7.91s/it][2023-10-04 12:37:04,995] [INFO] [logging.py:96:log_dist] [Rank 0] step=590, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.33909228444099426 batch_loss 0.3580740550211791, real_loss 0.016954613849520683
 33%|      | 18/55 [02:55<04:28,  7.26s/it]-> item_loss 0.402570903301239 batch_loss 0.3595572832971811, real_loss 0.02012854628264904
-> item_loss 0.29192376136779785 batch_loss 0.35737555678333005, real_loss 0.014596188440918922
-> item_loss 0.11361205577850342 batch_loss 0.3497579473769292, real_loss 0.005680602975189686
 35%|      | 19/55 [03:14<06:29, 10.81s/it]-> item_loss 0.2359306961297989 batch_loss 0.34630863673307677, real_loss 0.01179653499275446
 36%|      | 20/55 [03:19<05:22,  9.21s/it]-> item_loss 0.6610122919082642 batch_loss 0.35556462659117055, real_loss 0.6610122919082642
 38%|      | 21/55 [03:25<04:40,  8.26s/it]-> item_loss 0.3991510570049286 batch_loss 0.35680995317442077, real_loss 0.01995755359530449
 40%|      | 22/55 [03:31<04:07,  7.49s/it]-> item_loss 0.20604561269283295 batch_loss 0.35262205482771, real_loss 0.010302281007170677
 42%|     | 23/55 [03:36<03:39,  6.86s/it]-> item_loss 0.1606883406639099 batch_loss 0.3474346571476073, real_loss 0.008034417405724525
 44%|     | 24/55 [03:42<03:19,  6.43s/it]-> item_loss 0.8486329913139343 batch_loss 0.3606240869940896, real_loss 0.8486329913139343
[2023-10-04 12:38:05,504] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.7584791779518127 batch_loss 0.37082549958274913, real_loss 0.7584791779518127
 45%|     | 25/55 [03:55<04:15,  8.53s/it]-> item_loss 0.7784544825553894 batch_loss 0.38101622415706515, real_loss 0.7784544825553894
-> item_loss 0.5389334559440613 batch_loss 0.384867863956748, real_loss 0.5389334559440613
 47%|     | 26/55 [04:09<04:49,  9.97s/it]-> item_loss 0.46246686577796936 batch_loss 0.38671545923820566, real_loss 0.023123344406485558
 49%|     | 27/55 [04:14<04:00,  8.59s/it]-> item_loss 0.5078704953193665 batch_loss 0.3895330182168373, real_loss 0.5078704953193665
-> item_loss 0.24005112051963806 batch_loss 0.38613570236008277, real_loss 0.012002556584775448
 51%|     | 28/55 [04:25<04:14,  9.44s/it]-> item_loss 0.6713517904281616 batch_loss 0.3924738376504845, real_loss 0.6713517904281616
-> item_loss 0.4388314187526703 batch_loss 0.3934816111527059, real_loss 0.021941570565104485
 53%|    | 29/55 [04:38<04:29, 10.36s/it]-> item_loss 0.4275619089603424 batch_loss 0.39420672387201733, real_loss 0.02137809619307518
 55%|    | 30/55 [04:44<03:46,  9.06s/it]-> item_loss 0.3623906672000885 batch_loss 0.3935438893580188, real_loss 0.018119534477591515
 56%|    | 31/55 [04:50<03:15,  8.13s/it][2023-10-04 12:39:05,283] [INFO] [logging.py:96:log_dist] [Rank 0] step=610, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.07657591998577118 batch_loss 0.38707515528919745, real_loss 0.0038287960924208164
 58%|    | 32/55 [04:55<02:46,  7.24s/it]-> item_loss 0.409211128950119 batch_loss 0.3875178747624159, real_loss 0.02046055719256401
 60%|    | 33/55 [05:01<02:29,  6.80s/it]-> item_loss 0.7384254336357117 batch_loss 0.39439841513248053, real_loss 0.7384254336357117
-> item_loss 0.6288453340530396 batch_loss 0.39890700972710663, real_loss 0.6288453340530396
-> item_loss 0.4244740903377533 batch_loss 0.3993894074744773, real_loss 0.021223705261945724
-> item_loss 0.20149977505207062 batch_loss 0.3957247846518402, real_loss 0.010074988938868046
 62%|   | 34/55 [05:27<04:25, 12.65s/it]-> item_loss 0.7306501865386963 batch_loss 0.4018143374134194, real_loss 0.7306501865386963
-> item_loss 0.2963770031929016 batch_loss 0.3999315278737673, real_loss 0.014818849973380566
 64%|   | 35/55 [05:39<04:06, 12.35s/it]-> item_loss 0.09873995929956436 batch_loss 0.3946474652672023, real_loss 0.0049369982443749905
 65%|   | 36/55 [05:44<03:16, 10.32s/it]-> item_loss 0.2935628592967987 batch_loss 0.39290462723322983, real_loss 0.014678142964839935
 67%|   | 37/55 [05:50<02:43,  9.06s/it][2023-10-04 12:40:06,979] [INFO] [logging.py:96:log_dist] [Rank 0] step=620, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.5778821110725403 batch_loss 0.3960398388237266, real_loss 0.5778821110725403
 69%|   | 38/55 [05:57<02:19,  8.22s/it]-> item_loss 0.351959228515625 batch_loss 0.3953051619852583, real_loss 0.01759796217083931
 71%|   | 39/55 [06:02<01:59,  7.49s/it]-> item_loss 0.16937321424484253 batch_loss 0.39160135956328423, real_loss 0.008468660525977612
 73%|  | 40/55 [06:08<01:43,  6.88s/it]-> item_loss 0.7446432709693909 batch_loss 0.39729558394080206, real_loss 0.7446432709693909
-> item_loss 0.5820465087890625 batch_loss 0.40022813830347287, real_loss 0.5820465087890625
 75%|  | 41/55 [06:21<02:00,  8.62s/it]-> item_loss 0.2972762882709503 batch_loss 0.3986195156467147, real_loss 0.014863814227283001
-> item_loss 0.21428337693214417 batch_loss 0.39578357505110595, real_loss 0.010714168660342693
 76%|  | 42/55 [06:32<02:04,  9.56s/it]-> item_loss 0.188537135720253 batch_loss 0.39264347748548695, real_loss 0.009426857344806194
 78%|  | 43/55 [06:38<01:40,  8.39s/it]-> item_loss 0.49852102994918823 batch_loss 0.3942237394625571, real_loss 0.02492605149745941
 80%|  | 44/55 [06:44<01:22,  7.54s/it]-> item_loss 0.14797623455524445 batch_loss 0.3906024526256849, real_loss 0.007398811634629965
 82%| | 45/55 [06:49<01:08,  6.88s/it][2023-10-04 12:41:04,370] [INFO] [logging.py:96:log_dist] [Rank 0] step=630, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.09999383240938187 batch_loss 0.3863907334921153, real_loss 0.004999691620469093
 84%| | 46/55 [06:54<00:57,  6.36s/it]-> item_loss 1.087105631828308 batch_loss 0.39640094632548944, real_loss 1.087105631828308
-> item_loss 0.9643600583076477 batch_loss 0.4044003704379142, real_loss 0.9643600583076477
-> item_loss 0.33739766478538513 batch_loss 0.4034697773038513, real_loss 0.016869883984327316
 85%| | 47/55 [07:12<01:19,  9.94s/it]-> item_loss 0.33353304862976074 batch_loss 0.4025117399247542, real_loss 0.016676653176546097
 87%| | 48/55 [07:18<01:00,  8.66s/it]-> item_loss 0.3273223638534546 batch_loss 0.401495667275142, real_loss 0.01636611856520176
 89%| | 49/55 [07:24<00:46,  7.79s/it]-> item_loss 0.3161320090293884 batch_loss 0.40035748516519865, real_loss 0.01580660045146942
 91%| | 50/55 [07:29<00:35,  7.15s/it]-> item_loss 0.40474164485931396 batch_loss 0.4004151714769633, real_loss 0.020237082615494728
-> item_loss 0.13252468407154083 batch_loss 0.39693607423793187, real_loss 0.0066262343898415565
 93%|| 51/55 [07:42<00:34,  8.73s/it]-> item_loss 0.27571725845336914 batch_loss 0.3953819868560785, real_loss 0.013785863295197487
 95%|| 52/55 [07:48<00:24,  8.01s/it][2023-10-04 12:42:04,794] [INFO] [logging.py:96:log_dist] [Rank 0] step=640, skipped=0, lr=[1.0000000000000003e-05, 1.0000000000000003e-05, 1.0000000000000003e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
-> item_loss 0.5333202481269836 batch_loss 0.39712804079621655, real_loss 0.5333202481269836
 96%|| 53/55 [07:55<00:14,  7.50s/it]-> item_loss 0.4228430390357971 batch_loss 0.3974494782742113, real_loss 0.021142153069376945
 98%|| 54/55 [08:00<00:06,  6.99s/it]-> item_loss 0.7775120139122009 batch_loss 0.40214160834381607, real_loss 0.7775120139122009
-> item_loss 0.2883172929286957 batch_loss 0.4007535069363146, real_loss 0.014415864832699299
100%|| 55/55 [08:13<00:00,  8.66s/it]100%|| 55/55 [08:13<00:00,  8.97s/it]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 295173.51it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 579830.15it/s]
127.0.0.1 - - [04/Oct/2023 12:42:24] "POST /train/sft HTTP/1.1" 200 28
===saveved====
127.0.0.1 - - [04/Oct/2023 12:43:10] "POST /train/save-weight HTTP/1.1" 200 26

init_state None

state-> None

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 12:43:10] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [], 'token_count': 1024, 'over': False}

---begin-> None
master   17045   
<|over|>

<|me|><|response|>
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>master:
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|>master 
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|>master: 
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|>master: 
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|>master: 
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|>master: 
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|>master says: 
<|over|>

<|me|><|system|>: ,,.
<|over|>

<|me|><|request|>master says: 

<|over|>

<|me|><|request|>master says: 


2430801821
 fold pussy ass balls





1

10
---after-> tensor([[ 1.0409e-01,  5.5871e-02,  2.3375e-01,  ..., -7.6023e-02,
          1.4995e-01,  1.1363e-01],
        [ 1.6914e-01,  8.5127e-02,  1.4889e-02,  ...,  3.7376e-01,
          1.4454e-01,  8.9787e-02],
        [-3.3826e+00, -5.9676e+00,  2.1795e+00,  ..., -1.4878e-01,
         -1.6127e-01, -4.2278e-01],
        ...,
        [-1.0460e+01, -2.1054e+01,  4.4718e+01,  ..., -1.2454e+00,
         -7.4526e-02, -1.0012e+01],
        [ 2.0192e+00,  8.9334e+00,  1.5502e+01,  ...,  1.0003e+00,
          1.0342e+00,  1.0333e+00],
        [ 1.3693e+00,  6.5601e+00,  6.5642e+00,  ...,  1.7628e+01,
          3.4161e+00,  1.1011e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 12:43:54] "POST /inference/generate HTTP/1.1" 200 8638

init_state None

state-> tensor([[ 1.0409e-01,  5.5871e-02,  2.3375e-01,  ..., -7.6023e-02,
          1.4995e-01,  1.1363e-01],
        [ 1.6914e-01,  8.5127e-02,  1.4889e-02,  ...,  3.7376e-01,
          1.4454e-01,  8.9787e-02],
        [-3.3826e+00, -5.9676e+00,  2.1795e+00,  ..., -1.4878e-01,
         -1.6127e-01, -4.2278e-01],
        ...,
        [-1.0460e+01, -2.1054e+01,  4.4718e+01,  ..., -1.2454e+00,
         -7.4526e-02, -1.0012e+01],
        [ 2.0192e+00,  8.9334e+00,  1.5502e+01,  ...,  1.0003e+00,
          1.0342e+00,  1.0333e+00],
        [ 1.3693e+00,  6.5601e+00,  6.5642e+00,  ...,  1.7628e+01,
          3.4161e+00,  1.1011e+00]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 12:43:54] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [], 'token_count': 1024, 'over': False}

---begin-> None
master   17045       
<|over|>

<|me|><|response|>:  ,,,,
<|over|>

<|me|><|request|>:  
<|over|>

<|me|><|response|>master:  
<|over|>

<|me|><|request|>:  
<|over|>

<|me|><|response|>master:  
<|over|>

<|me|><|request|>:  
<|over|>

<|me|><|response|>master:  
<|over|>

<|me|><|request|>master:  
<|over|>

<|me|><|response|>bot:  ~
<|over|>

<|me|><|system|>system: 
<|over|>

<|me|><|response|>master:  
<|over|>

<|me|><|request|>bot:  
<|over|>

<|me|><|request|>master:  
<|over|>

<|me|><|response|>bot:  
<|over|>

<|me|><|request|>master:  
<|over|>

<|me|><|response|>bot:   
<|over|>

<|me|><|request|>master:  
<|over|>

<|me|><|response|>bot:   
<|over|>

<|me|><|request|>master:  
<|over|>

<|me|><|response|>bot:   
<|over|>

<|me|><|system|>system:  202341  00:00:00
<|over|>

<|me|><|request|>master:  
<|over|>

<|me|><|response|>bot:  
<|over|>

<|me|><|request|>master:  
<|over|>

<|me|><|response|>bot:  
<|over|>

<|me|><|system|>system:  202341  00:00:00
<|over|>

<|me|><|request|>master:  
<|over|>

<|me|><|response|>bot:  
<|over|>

<|me|><|response|>master:  
<|over|>

<|me|><|response|>bot:  
<|over|>

<|me|><|request|>master:  
<|over|>

<|me|><|response|>bot:  
<|over|>

<|me|><|system|>system  202341  00:00:00
<|over|>

<|me|><|request|>master:  
<|over|>

<|me|><|response|>bot:    
<|over|>

<|me|><|request|>master:  
<|over|>

<|me|><|response|>bot:  
<|over|>

<|me|><|request|>master:  
---after-> tensor([[-7.7794e-03,  3.3871e-03,  1.4593e-01,  ...,  9.0207e-02,
          8.5275e-02,  1.6147e-01],
        [-2.7933e-02, -1.5902e-02, -8.4678e-02,  ...,  7.6487e-01,
          1.9171e-02,  1.2125e-01],
        [ 1.5155e+00,  3.1110e-01,  2.9920e+00,  ..., -2.3967e+00,
         -9.5088e-01, -1.5426e+00],
        ...,
        [-9.1608e-03, -1.2864e+01,  1.8250e+01,  ..., -4.5401e+00,
         -3.3076e+00,  2.6206e+00],
        [ 1.4719e+00,  5.0732e+00,  6.6040e+00,  ...,  1.0393e+00,
          1.0764e+00,  1.2733e+00],
        [ 4.3039e+00,  8.0279e+00,  7.7510e+00,  ...,  1.5122e+01,
          3.1142e+00,  3.4026e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 12:44:37] "POST /inference/generate HTTP/1.1" 200 8326

init_state None

state-> tensor([[-7.7794e-03,  3.3871e-03,  1.4593e-01,  ...,  9.0207e-02,
          8.5275e-02,  1.6147e-01],
        [-2.7933e-02, -1.5902e-02, -8.4678e-02,  ...,  7.6487e-01,
          1.9171e-02,  1.2125e-01],
        [ 1.5155e+00,  3.1110e-01,  2.9920e+00,  ..., -2.3967e+00,
         -9.5088e-01, -1.5426e+00],
        ...,
        [-9.1608e-03, -1.2864e+01,  1.8250e+01,  ..., -4.5401e+00,
         -3.3076e+00,  2.6206e+00],
        [ 1.4719e+00,  5.0732e+00,  6.6040e+00,  ...,  1.0393e+00,
          1.0764e+00,  1.2733e+00],
        [ 4.3039e+00,  8.0279e+00,  7.7510e+00,  ...,  1.5122e+01,
          3.1142e+00,  3.4026e+00]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 12:46:47] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None
master   17045       

---after-> tensor([[ -0.0556,   0.0516,   0.2693,  ...,   0.1302,  -0.0282,   0.1281],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [ -8.4706,  -3.0390,   1.1350,  ...,  -1.4655,   0.6919,  -0.4162],
        ...,
        [ -5.2879, -16.5242,  11.2207,  ...,  -1.6255,   1.0792, -11.9308],
        [  1.6530,  10.2004,   6.5513,  ...,   1.2070,   1.0147,   2.0346],
        [  0.7861,   7.9133,   7.6267,  ...,  13.5202,   1.7186,  -1.5106]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 12:46:59] "POST /inference/generate HTTP/1.1" 200 3684

init_state None

state-> tensor([[ -0.0556,   0.0516,   0.2693,  ...,   0.1302,  -0.0282,   0.1281],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [ -8.4706,  -3.0390,   1.1350,  ...,  -1.4655,   0.6919,  -0.4162],
        ...,
        [ -5.2879, -16.5242,  11.2207,  ...,  -1.6255,   1.0792, -11.9308],
        [  1.6530,  10.2004,   6.5513,  ...,   1.2070,   1.0147,   2.0346],
        [  0.7861,   7.9133,   7.6267,  ...,  13.5202,   1.7186,  -1.5106]],
       device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 12:47:04] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster   17045       \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None
    

---after-> tensor([[-5.9652e-02,  5.3083e-02,  2.5903e-01,  ...,  1.3642e-01,
         -2.0859e-02,  1.1690e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-4.4794e+00, -9.9838e+00,  7.0475e-01,  ..., -1.3556e+00,
          3.4064e-01, -4.0640e-01],
        ...,
        [ 6.9975e-02, -1.6157e+01,  3.7253e+01,  ..., -2.4199e+00,
          1.1219e+00, -8.4233e+00],
        [ 1.2056e+00,  8.4856e+00,  1.4374e+01,  ...,  1.1408e+00,
          1.0026e+00,  1.2585e+00],
        [ 2.4764e+00,  7.4391e+00,  6.4405e+00,  ...,  1.4408e+01,
          4.0779e+00, -2.6458e-02]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 12:47:22] "POST /inference/generate HTTP/1.1" 200 6033

---- {'role': 'request', 'text': 'master \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-5.9652e-02,  5.3083e-02,  2.5903e-01,  ...,  1.3642e-01,
         -2.0859e-02,  1.1690e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-4.4794e+00, -9.9838e+00,  7.0475e-01,  ..., -1.3556e+00,
          3.4064e-01, -4.0640e-01],
        ...,
        [ 6.9975e-02, -1.6157e+01,  3.7253e+01,  ..., -2.4199e+00,
          1.1219e+00, -8.4233e+00],
        [ 1.2056e+00,  8.4856e+00,  1.4374e+01,  ...,  1.1408e+00,
          1.0026e+00,  1.2585e+00],
        [ 2.4764e+00,  7.4391e+00,  6.4405e+00,  ...,  1.4408e+01,
          4.0779e+00, -2.6458e-02]], device='cuda:0')
  

---after-> tensor([[ -0.0517,   0.0542,   0.2678,  ...,   0.1328,  -0.0220,   0.1268],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [ -0.6663,  -5.5665,   0.7757,  ...,  -1.4682,   0.4454,  -0.5111],
        ...,
        [ -3.2451, -11.7247,   4.7684,  ...,   5.1855,   2.3467,  -5.9252],
        [  1.3584,   6.4016,   2.2260,  ...,   1.8479,   1.0018,   1.4353],
        [  0.9915,   8.7011,   8.5287,  ...,  14.8144,   3.8681,   1.7505]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 12:47:58] "POST /inference/generate HTTP/1.1" 200 1488

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[ -0.0517,   0.0542,   0.2678,  ...,   0.1328,  -0.0220,   0.1268],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [ -0.6663,  -5.5665,   0.7757,  ...,  -1.4682,   0.4454,  -0.5111],
        ...,
        [ -3.2451, -11.7247,   4.7684,  ...,   5.1855,   2.3467,  -5.9252],
        [  1.3584,   6.4016,   2.2260,  ...,   1.8479,   1.0018,   1.4353],
        [  0.9915,   8.7011,   8.5287,  ...,  14.8144,   3.8681,   1.7505]],
       device='cuda:0')
  

---after-> tensor([[ -0.0518,   0.0529,   0.2718,  ...,   0.1368,  -0.0248,   0.1172],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [ -1.5038,  -9.4813,   0.8510,  ...,  -1.6032,   0.9139,  -0.6344],
        ...,
        [ -4.3829, -11.3862,   7.5405,  ...,  -0.3506,   0.0690,  -7.7180],
        [  1.8666,   7.9335,   3.2666,  ...,   1.1157,   1.0020,   1.4720],
        [ -0.8821,   8.0100,   7.8397,  ...,  13.7348,   3.7778,   0.6744]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 12:48:13] "POST /inference/generate HTTP/1.1" 200 856

---- {'role': 'system', 'text': 'master \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[ -0.0518,   0.0529,   0.2718,  ...,   0.1368,  -0.0248,   0.1172],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [ -1.5038,  -9.4813,   0.8510,  ...,  -1.6032,   0.9139,  -0.6344],
        ...,
        [ -4.3829, -11.3862,   7.5405,  ...,  -0.3506,   0.0690,  -7.7180],
        [  1.8666,   7.9335,   3.2666,  ...,   1.1157,   1.0020,   1.4720],
        [ -0.8821,   8.0100,   7.8397,  ...,  13.7348,   3.7778,   0.6744]],
       device='cuda:0')
master
---after-> tensor([[ 1.8343e-02,  3.8575e-03,  3.1804e-01,  ..., -8.8003e-02,
          3.5697e-02, -1.8796e-03],
        [-1.3606e-01,  9.1952e-02,  8.6595e-02,  ...,  5.0017e-01,
         -6.7085e-03, -1.3362e-01],
        [ 4.5592e-02, -5.1190e+00,  8.5535e-01,  ..., -1.4774e+00,
         -3.5525e-01, -6.1011e-01],
        ...,
        [ 1.2667e+00, -2.4500e+00,  1.8370e+01,  ...,  3.0871e+00,
          7.1426e+00, -7.6032e+00],
        [ 1.5172e+00,  3.0360e+00,  7.6114e+00,  ...,  1.2873e+00,
          1.0181e+00,  1.1972e+00],
        [ 9.2509e-01,  9.2813e+00,  6.9028e+00,  ...,  1.4781e+01,
          1.7968e+00,  3.0171e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 12:48:55] "POST /inference/generate HTTP/1.1" 200 1685

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[ 1.8343e-02,  3.8575e-03,  3.1804e-01,  ..., -8.8003e-02,
          3.5697e-02, -1.8796e-03],
        [-1.3606e-01,  9.1952e-02,  8.6595e-02,  ...,  5.0017e-01,
         -6.7085e-03, -1.3362e-01],
        [ 4.5592e-02, -5.1190e+00,  8.5535e-01,  ..., -1.4774e+00,
         -3.5525e-01, -6.1011e-01],
        ...,
        [ 1.2667e+00, -2.4500e+00,  1.8370e+01,  ...,  3.0871e+00,
          7.1426e+00, -7.6032e+00],
        [ 1.5172e+00,  3.0360e+00,  7.6114e+00,  ...,  1.2873e+00,
          1.0181e+00,  1.1972e+00],
        [ 9.2509e-01,  9.2813e+00,  6.9028e+00,  ...,  1.4781e+01,
          1.7968e+00,  3.0171e+00]], device='cuda:0')
 

---after-> tensor([[-5.5000e-02,  5.2373e-02,  2.6817e-01,  ...,  1.3079e-01,
         -2.3931e-02,  1.2626e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [ 1.5882e-01, -3.4894e+00,  3.9519e-01,  ..., -1.5357e+00,
          8.1505e-01, -3.8792e-01],
        ...,
        [-4.6228e-01, -2.6430e+00,  3.5419e+01,  ...,  3.5125e-01,
         -9.3140e-01, -7.6402e+00],
        [ 2.0377e+00,  3.3466e+00,  1.3799e+01,  ...,  1.3287e+00,
          1.0232e+00,  1.1034e+00],
        [ 1.8036e-01,  8.8390e+00,  6.4619e+00,  ...,  1.2588e+01,
          2.1983e+00,  1.3953e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 12:49:03] "POST /inference/generate HTTP/1.1" 200 753

---- {'role': 'response', 'text': 'master \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-5.5000e-02,  5.2373e-02,  2.6817e-01,  ...,  1.3079e-01,
         -2.3931e-02,  1.2626e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [ 1.5882e-01, -3.4894e+00,  3.9519e-01,  ..., -1.5357e+00,
          8.1505e-01, -3.8792e-01],
        ...,
        [-4.6228e-01, -2.6430e+00,  3.5419e+01,  ...,  3.5125e-01,
         -9.3140e-01, -7.6402e+00],
        [ 2.0377e+00,  3.3466e+00,  1.3799e+01,  ...,  1.3287e+00,
          1.0232e+00,  1.1034e+00],
        [ 1.8036e-01,  8.8390e+00,  6.4619e+00,  ...,  1.2588e+01,
          2.1983e+00,  1.3953e-01]], device='cuda:0')


---after-> tensor([[ -0.0562,   0.0514,   0.2638,  ...,   0.1390,  -0.0188,   0.1027],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [  0.1619,  -5.5590,   0.6445,  ...,  -1.4206,   0.7079,  -0.4917],
        ...,
        [ -4.8782,  -6.3457,  11.4603,  ...,  -1.4542,   2.8403, -14.7198],
        [  1.9364,   5.3818,   5.6000,  ...,   1.0804,   1.0007,   1.0215],
        [  0.6325,   8.4129,   7.5019,  ...,  15.4550,   3.9212,   2.2867]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 12:49:17] "POST /inference/generate HTTP/1.1" 200 1653

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[ -0.0562,   0.0514,   0.2638,  ...,   0.1390,  -0.0188,   0.1027],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [  0.1619,  -5.5590,   0.6445,  ...,  -1.4206,   0.7079,  -0.4917],
        ...,
        [ -4.8782,  -6.3457,  11.4603,  ...,  -1.4542,   2.8403, -14.7198],
        [  1.9364,   5.3818,   5.6000,  ...,   1.0804,   1.0007,   1.0215],
        [  0.6325,   8.4129,   7.5019,  ...,  15.4550,   3.9212,   2.2867]],
       device='cuda:0')
  

---after-> tensor([[ -0.0577,   0.0527,   0.2628,  ...,   0.1316,  -0.0233,   0.1080],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [ -0.3521,  -4.3008,   0.6876,  ...,  -1.4371,   0.8523,  -0.6103],
        ...,
        [  0.8997, -11.5375,  11.6677,  ...,   1.2445,   4.2394,  -9.3390],
        [  1.5291,   8.1230,   6.6032,  ...,   1.1305,   1.0076,   1.5399],
        [ -0.5299,   7.7494,   7.5611,  ...,  13.7098,   0.0939,  -2.9608]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 12:49:25] "POST /inference/generate HTTP/1.1" 200 844

---- {'role': 'request', 'text': 'master \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[ -0.0577,   0.0527,   0.2628,  ...,   0.1316,  -0.0233,   0.1080],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [ -0.3521,  -4.3008,   0.6876,  ...,  -1.4371,   0.8523,  -0.6103],
        ...,
        [  0.8997, -11.5375,  11.6677,  ...,   1.2445,   4.2394,  -9.3390],
        [  1.5291,   8.1230,   6.6032,  ...,   1.1305,   1.0076,   1.5399],
        [ -0.5299,   7.7494,   7.5611,  ...,  13.7098,   0.0939,  -2.9608]],
       device='cuda:0')
  

---after-> tensor([[-0.0570,  0.0517,  0.2623,  ...,  0.1349, -0.0167,  0.1099],
        [-0.1227,  0.0814,  0.0310,  ...,  0.8696, -0.0540,  0.1175],
        [-0.7724, -6.2162,  1.1572,  ..., -1.2604,  0.6321, -0.4473],
        ...,
        [-2.4176, -0.9334, 14.8006,  ..., -2.4179,  2.9076, -8.7627],
        [ 1.0528,  2.0229, 10.5401,  ...,  1.0495,  1.0025,  1.2788],
        [-0.0834,  9.4890,  6.7344,  ..., 14.0250,  1.6728, -1.6033]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 12:49:55] "POST /inference/generate HTTP/1.1" 200 1446

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-0.0570,  0.0517,  0.2623,  ...,  0.1349, -0.0167,  0.1099],
        [-0.1227,  0.0814,  0.0310,  ...,  0.8696, -0.0540,  0.1175],
        [-0.7724, -6.2162,  1.1572,  ..., -1.2604,  0.6321, -0.4473],
        ...,
        [-2.4176, -0.9334, 14.8006,  ..., -2.4179,  2.9076, -8.7627],
        [ 1.0528,  2.0229, 10.5401,  ...,  1.0495,  1.0025,  1.2788],
        [-0.0834,  9.4890,  6.7344,  ..., 14.0250,  1.6728, -1.6033]],
       device='cuda:0')
  

---after-> tensor([[-5.4402e-02,  4.9083e-02,  2.6791e-01,  ...,  1.3401e-01,
         -2.2879e-02,  1.0979e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-2.5997e+00, -7.5058e+00,  1.2324e+00,  ..., -1.3948e+00,
          9.5655e-01, -5.3588e-01],
        ...,
        [-7.0973e+00, -1.5404e+00,  3.0239e+01,  ...,  1.9997e-01,
         -2.7715e+00, -9.8950e+00],
        [ 1.7957e+00,  2.3937e+00,  1.4862e+01,  ...,  1.3752e+00,
          1.0048e+00,  1.7257e+00],
        [-1.4272e+00,  8.9499e+00,  6.1970e+00,  ...,  1.2329e+01,
          2.5216e+00, -1.9630e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 12:50:03] "POST /inference/generate HTTP/1.1" 200 790

---- {'role': 'request', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-5.4402e-02,  4.9083e-02,  2.6791e-01,  ...,  1.3401e-01,
         -2.2879e-02,  1.0979e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-2.5997e+00, -7.5058e+00,  1.2324e+00,  ..., -1.3948e+00,
          9.5655e-01, -5.3588e-01],
        ...,
        [-7.0973e+00, -1.5404e+00,  3.0239e+01,  ...,  1.9997e-01,
         -2.7715e+00, -9.8950e+00],
        [ 1.7957e+00,  2.3937e+00,  1.4862e+01,  ...,  1.3752e+00,
          1.0048e+00,  1.7257e+00],
        [-1.4272e+00,  8.9499e+00,  6.1970e+00,  ...,  1.2329e+01,
          2.5216e+00, -1.9630e+00]], device='cuda:0')
  

---after-> tensor([[-5.3964e-02,  5.4934e-02,  2.6622e-01,  ...,  1.4131e-01,
         -1.3297e-02,  1.1120e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-1.3019e+00, -2.9372e+00,  1.0689e+00,  ..., -1.4101e+00,
          7.8800e-01, -4.7545e-01],
        ...,
        [-7.1889e+00, -3.7804e+00,  8.6830e+00,  ...,  3.9440e-01,
          3.3687e+00, -1.3448e+01],
        [ 2.6765e+00,  7.6733e+00,  6.2472e+00,  ...,  1.1477e+00,
          1.7859e+00,  2.0209e+00],
        [ 1.0514e+00,  7.8027e+00,  7.3086e+00,  ...,  1.1819e+01,
          1.5295e+00, -1.2616e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 12:51:41] "POST /inference/generate HTTP/1.1" 200 1711

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-5.3964e-02,  5.4934e-02,  2.6622e-01,  ...,  1.4131e-01,
         -1.3297e-02,  1.1120e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-1.3019e+00, -2.9372e+00,  1.0689e+00,  ..., -1.4101e+00,
          7.8800e-01, -4.7545e-01],
        ...,
        [-7.1889e+00, -3.7804e+00,  8.6830e+00,  ...,  3.9440e-01,
          3.3687e+00, -1.3448e+01],
        [ 2.6765e+00,  7.6733e+00,  6.2472e+00,  ...,  1.1477e+00,
          1.7859e+00,  2.0209e+00],
        [ 1.0514e+00,  7.8027e+00,  7.3086e+00,  ...,  1.1819e+01,
          1.5295e+00, -1.2616e+00]], device='cuda:0')
  

---after-> tensor([[-0.0570,  0.0511,  0.2639,  ...,  0.1348, -0.0202,  0.1130],
        [-0.1227,  0.0814,  0.0310,  ...,  0.8696, -0.0540,  0.1175],
        [-0.8469, -7.3542,  1.1450,  ..., -1.2820,  0.7327, -0.6861],
        ...,
        [-2.0014, -8.8358, 17.4738,  ...,  0.0713,  1.2637, -4.8675],
        [ 2.2884, 11.3741,  9.3180,  ...,  1.1500,  1.0024,  1.2257],
        [-0.0698,  7.1806,  6.6885,  ..., 14.7130,  1.7204,  1.2053]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 12:51:48] "POST /inference/generate HTTP/1.1" 200 826

init_state None

state-> tensor([[-0.0570,  0.0511,  0.2639,  ...,  0.1348, -0.0202,  0.1130],
        [-0.1227,  0.0814,  0.0310,  ...,  0.8696, -0.0540,  0.1175],
        [-0.8469, -7.3542,  1.1450,  ..., -1.2820,  0.7327, -0.6861],
        ...,
        [-2.0014, -8.8358, 17.4738,  ...,  0.0713,  1.2637, -4.8675],
        [ 2.2884, 11.3741,  9.3180,  ...,  1.1500,  1.0024,  1.2257],
        [-0.0698,  7.1806,  6.6885,  ..., 14.7130,  1.7204,  1.2053]],
       device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 12:53:14] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'request', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None


---after-> tensor([[ 3.5705e-02,  8.0781e-04,  2.4802e-01,  ...,  2.2970e-02,
          7.1956e-02,  1.3752e-01],
        [ 1.5602e-02, -3.0412e-02,  3.5211e-02,  ...,  6.4558e-01,
          4.6644e-02,  6.4495e-02],
        [ 1.1354e+00,  4.0274e+00,  5.6053e-01,  ..., -2.0457e+00,
          1.0579e+00, -1.1796e+00],
        ...,
        [-2.2769e+00, -1.6070e+01, -2.4367e+00,  ...,  2.4009e+00,
         -3.0349e-01, -2.1141e+01],
        [ 1.0611e+00,  4.8553e+00,  3.0498e+00,  ...,  1.4629e+00,
          1.1039e+00,  2.2781e+00],
        [ 1.8995e+00,  5.4835e+00,  7.1609e+00,  ...,  1.7278e+01,
          1.7773e+00,  1.3520e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 12:53:18] "POST /inference/generate HTTP/1.1" 200 1196

init_state None

state-> tensor([[ 3.5705e-02,  8.0781e-04,  2.4802e-01,  ...,  2.2970e-02,
          7.1956e-02,  1.3752e-01],
        [ 1.5602e-02, -3.0412e-02,  3.5211e-02,  ...,  6.4558e-01,
          4.6644e-02,  6.4495e-02],
        [ 1.1354e+00,  4.0274e+00,  5.6053e-01,  ..., -2.0457e+00,
          1.0579e+00, -1.1796e+00],
        ...,
        [-2.2769e+00, -1.6070e+01, -2.4367e+00,  ...,  2.4009e+00,
         -3.0349e-01, -2.1141e+01],
        [ 1.0611e+00,  4.8553e+00,  3.0498e+00,  ...,  1.4629e+00,
          1.1039e+00,  2.2781e+00],
        [ 1.8995e+00,  5.4835e+00,  7.1609e+00,  ...,  1.7278e+01,
          1.7773e+00,  1.3520e+00]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 12:53:23] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'request', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None


---after-> tensor([[ 3.2129e-02, -6.6929e-04,  2.4654e-01,  ...,  2.2879e-02,
          7.3783e-02,  1.2905e-01],
        [ 1.5602e-02, -3.0412e-02,  3.5211e-02,  ...,  6.4558e-01,
          4.6644e-02,  6.4495e-02],
        [ 1.1841e+00,  5.2292e+00,  9.9956e-01,  ..., -2.0457e+00,
          1.0579e+00, -1.1796e+00],
        ...,
        [-2.9883e+00, -2.2307e+01, -2.7500e+00,  ...,  8.9732e+00,
         -4.9722e+00, -1.1195e+01],
        [ 1.9175e+00,  6.4759e+00,  3.6640e+00,  ...,  1.8847e+00,
          1.3168e+00,  1.5014e+00],
        [ 1.2639e+00,  5.3315e+00,  7.0093e+00,  ...,  1.4788e+01,
          1.8767e-01,  2.0691e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 12:53:26] "POST /inference/generate HTTP/1.1" 200 1787

init_state None

state-> tensor([[ 3.2129e-02, -6.6929e-04,  2.4654e-01,  ...,  2.2879e-02,
          7.3783e-02,  1.2905e-01],
        [ 1.5602e-02, -3.0412e-02,  3.5211e-02,  ...,  6.4558e-01,
          4.6644e-02,  6.4495e-02],
        [ 1.1841e+00,  5.2292e+00,  9.9956e-01,  ..., -2.0457e+00,
          1.0579e+00, -1.1796e+00],
        ...,
        [-2.9883e+00, -2.2307e+01, -2.7500e+00,  ...,  8.9732e+00,
         -4.9722e+00, -1.1195e+01],
        [ 1.9175e+00,  6.4759e+00,  3.6640e+00,  ...,  1.8847e+00,
          1.3168e+00,  1.5014e+00],
        [ 1.2639e+00,  5.3315e+00,  7.0093e+00,  ...,  1.4788e+01,
          1.8767e-01,  2.0691e-01]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 12:53:53] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'request', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [], 'token_count': 1024, 'over': False}

---begin-> None

<|over|>

<|me|><|response|>


1.
2.
3.
4.
5.
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>
1. 
2. milk
3.
4.
5.

1.
2.
3.
4.
5.
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>
1. 
2.
3.
4.
5.<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>
1.
2.
3.
4.
5.
<|over|>

<|me|><|request|>
<|over|>

<|me|><|response|>
1. 
2. 
3. 
4. 
---after-> tensor([[ 5.1484e-02,  9.2437e-02,  2.7960e-01,  ..., -1.2524e-02,
         -2.3882e-01,  1.7877e-01],
        [ 9.6365e-02,  1.8223e-01,  1.2180e-01,  ...,  5.5580e-01,
         -3.1400e-01,  1.0493e-01],
        [-2.6767e+00, -3.6285e+00,  1.7318e+00,  ...,  1.3336e+00,
          8.9613e-01, -8.4510e-01],
        ...,
        [-6.6185e+00, -1.1000e+01,  2.0240e+01,  ...,  2.9979e-01,
         -2.8992e+00, -2.0005e+01],
        [ 1.1884e+00,  3.6153e+00,  6.5518e+00,  ...,  1.0127e+00,
          1.4359e+00,  1.5456e+00],
        [ 2.0519e+00,  8.2804e+00,  8.3067e+00,  ...,  1.8350e+01,
         -2.3551e+00,  2.2429e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 12:54:33] "POST /inference/generate HTTP/1.1" 200 7030

init_state None

state-> tensor([[ 5.1484e-02,  9.2437e-02,  2.7960e-01,  ..., -1.2524e-02,
         -2.3882e-01,  1.7877e-01],
        [ 9.6365e-02,  1.8223e-01,  1.2180e-01,  ...,  5.5580e-01,
         -3.1400e-01,  1.0493e-01],
        [-2.6767e+00, -3.6285e+00,  1.7318e+00,  ...,  1.3336e+00,
          8.9613e-01, -8.4510e-01],
        ...,
        [-6.6185e+00, -1.1000e+01,  2.0240e+01,  ...,  2.9979e-01,
         -2.8992e+00, -2.0005e+01],
        [ 1.1884e+00,  3.6153e+00,  6.5518e+00,  ...,  1.0127e+00,
          1.4359e+00,  1.5456e+00],
        [ 2.0519e+00,  8.2804e+00,  8.3067e+00,  ...,  1.8350e+01,
         -2.3551e+00,  2.2429e+00]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 12:55:27] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [], 'token_count': 1024, 'over': False}

---begin-> None
master   17045   
<|over|>

<|me|><|response|>
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>master:
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|>master 
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|response|>master: 
<|over|>

<|me|><|request|> 
<|over|>

<|me|><|response|>master: 
<|over|>

<|me|><|request|> 
<|over|>

<|me|><|response|>master: 
<|over|>

<|me|><|request|> 
<|over|>

<|me|><|response|>master: 
<|over|>

<|me|><|request|> 
<|over|>

<|me|><|response|>master: 
<|over|>

<|me|><|system|>:
<|over|>

<|me|><|response|>master: 
<|over|>

<|me|><|request|> 
<|over|>

<|me|><|response|>master: 
<|over|>

<|me|><|request|> 
<|over|>

<|me|><|response|>master: best friends forever
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|>master says: 
<|over|>

<|me|><|response|>master says: 
T-shirt with his parents and sister May.
MayMayMay

 rua rua his
---after-> tensor([[ 1.8357e-02,  9.7295e-03,  7.8196e-02,  ...,  7.2555e-02,
          9.0716e-02,  2.2385e-01],
        [ 3.6679e-02,  4.5938e-02, -1.2909e-01,  ...,  7.2813e-01,
          6.7169e-02,  2.4345e-01],
        [-5.2651e+00, -6.7162e-01,  2.0144e+00,  ..., -1.6033e+00,
         -5.3547e-01,  1.4335e+00],
        ...,
        [-5.2053e+00, -9.5718e+00,  5.8337e-01,  ...,  4.9962e+00,
          2.3616e+00,  3.9101e+00],
        [ 1.0537e+00,  6.1968e+00,  6.8604e+00,  ...,  1.0171e+00,
          1.0162e+00,  1.0651e+00],
        [ 4.3069e+00,  7.3012e+00,  8.7501e+00,  ...,  1.8728e+01,
          2.2415e+00,  2.4391e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 12:56:12] "POST /inference/generate HTTP/1.1" 200 8643

init_state None

state-> tensor([[ 1.8357e-02,  9.7295e-03,  7.8196e-02,  ...,  7.2555e-02,
          9.0716e-02,  2.2385e-01],
        [ 3.6679e-02,  4.5938e-02, -1.2909e-01,  ...,  7.2813e-01,
          6.7169e-02,  2.4345e-01],
        [-5.2651e+00, -6.7162e-01,  2.0144e+00,  ..., -1.6033e+00,
         -5.3547e-01,  1.4335e+00],
        ...,
        [-5.2053e+00, -9.5718e+00,  5.8337e-01,  ...,  4.9962e+00,
          2.3616e+00,  3.9101e+00],
        [ 1.0537e+00,  6.1968e+00,  6.8604e+00,  ...,  1.0171e+00,
          1.0162e+00,  1.0651e+00],
        [ 4.3069e+00,  7.3012e+00,  8.7501e+00,  ...,  1.8728e+01,
          2.2415e+00,  2.4391e+00]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 12:56:54] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [], 'token_count': 1024, 'over': False}

---begin-> None
master<|over|>

<|me|><|response|>system: 4SM3-5SM2-3  
<|over|>

<|me|><|request|>









<|over|>

<|me|><|response|> 4 SM1-2  
<|over|>

<|me|><|request|>
 
 
 
san check 1/1d3








<|over|>

<|me|><|response|>: 
<|over|>

<|me|><|request|>
 
 2050
san check 1/1d6



<|over|>

<|me|><|request|>
 
 2050
san check 1/1d3

<|over|>

<|me|><|response|>: 
<|over|>

<|me|><|request|>
 
 
san 0
 
  more than a servant, more like a brother or sister
 
 
---after-> tensor([[ 5.5768e-02, -3.0604e-03,  2.7376e-01,  ...,  5.0813e-02,
          1.2504e-01,  7.1677e-02],
        [ 1.3804e-01,  1.3605e-02,  1.3802e-01,  ...,  6.8146e-01,
          7.7215e-02,  5.6838e-02],
        [-7.2838e-01, -6.4484e+00,  1.7072e+00,  ..., -1.1946e+00,
          2.1089e+00, -1.4068e+00],
        ...,
        [-1.1624e+01, -9.4649e+00,  8.7436e+00,  ..., -1.1710e+00,
         -3.6784e+00, -1.3341e+01],
        [ 1.0830e+00,  5.7200e+00,  6.0249e+00,  ...,  1.0013e+00,
          1.0047e+00,  1.0067e+00],
        [ 1.7463e+00,  7.8948e+00,  7.8575e+00,  ...,  1.9500e+01,
          3.3267e+00,  2.7789e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 12:57:38] "POST /inference/generate HTTP/1.1" 200 8486

init_state None

state-> tensor([[ 5.5768e-02, -3.0604e-03,  2.7376e-01,  ...,  5.0813e-02,
          1.2504e-01,  7.1677e-02],
        [ 1.3804e-01,  1.3605e-02,  1.3802e-01,  ...,  6.8146e-01,
          7.7215e-02,  5.6838e-02],
        [-7.2838e-01, -6.4484e+00,  1.7072e+00,  ..., -1.1946e+00,
          2.1089e+00, -1.4068e+00],
        ...,
        [-1.1624e+01, -9.4649e+00,  8.7436e+00,  ..., -1.1710e+00,
         -3.6784e+00, -1.3341e+01],
        [ 1.0830e+00,  5.7200e+00,  6.0249e+00,  ...,  1.0013e+00,
          1.0047e+00,  1.0067e+00],
        [ 1.7463e+00,  7.8948e+00,  7.8575e+00,  ...,  1.9500e+01,
          3.3267e+00,  2.7789e+00]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:01:04] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'system: 4SM3-5SM2-3  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n\n\n\n\n\n\n\n\n\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': ' 4 SM1-2  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n \n \n \n\n\n\n\n\n\n\n\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': ': \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [], 'token_count': 1024, 'over': False}

---begin-> None
 
<|over|>

<|me|><|response|>  
<|over|>

<|me|><|request|>: AI
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|> 
<|over|>

<|me|>antoma:  
<|over|>

<|me|><|response|>:  
<|over|>

<|me|><|response|>system 
<|over|>

<|me|><|request|>system 
<|over|>

<|me|><|request|>:  
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|>master:  
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|>system:  ,
<|over|>

<|me|><|system|>,SM. ,
<|over|>

<|me|><|response|>system 
<|over|>

<|me|><|request|>master:  
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|>master:  
<|over|>

<|me|><|response|>system: ,
<|over|>

<|me|><|request|>system ,
<|over|>

<|me|><|response|>system ,
<|over|>

<|me|><|request|>master:  
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|>master:  
<|over|>

<|me|><|response|>system 
<|over|>

<|me|>umbneting: 
---after-> tensor([[  0.1063,  -0.0328,   0.1441,  ...,   0.0908,   0.2655,   0.1673],
        [  0.2037,  -0.0801,  -0.0733,  ...,   0.7748,   0.2926,   0.1748],
        [  0.3575,  -1.7099,   3.0965,  ...,   0.3749,   0.2858,  -1.5580],
        ...,
        [ -3.0450, -23.1750,   4.8831,  ...,   1.4853,   0.4815,  -9.2059],
        [  1.5141,  10.1143,   2.7840,  ...,   1.0004,   1.0108,   1.0316],
        [  0.5460,   7.1795,   8.5583,  ...,  17.6137,   1.5748,   3.5194]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:02:17] "POST /inference/generate HTTP/1.1" 200 20994

init_state None

state-> tensor([[  0.1063,  -0.0328,   0.1441,  ...,   0.0908,   0.2655,   0.1673],
        [  0.2037,  -0.0801,  -0.0733,  ...,   0.7748,   0.2926,   0.1748],
        [  0.3575,  -1.7099,   3.0965,  ...,   0.3749,   0.2858,  -1.5580],
        ...,
        [ -3.0450, -23.1750,   4.8831,  ...,   1.4853,   0.4815,  -9.2059],
        [  1.5141,  10.1143,   2.7840,  ...,   1.0004,   1.0108,   1.0316],
        [  0.5460,   7.1795,   8.5583,  ...,  17.6137,   1.5748,   3.5194]],
       device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:07:08] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'system: 4SM3-5SM2-3  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n\n\n\n\n\n\n\n\n\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': ' 4 SM1-2  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n \n \n \n\n\n\n\n\n\n\n\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': ': \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': ': \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': ' \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': ': \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': ' \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': ': \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': ' \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': ': \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [], 'token_count': 1024, 'over': False}

---begin-> None
  
<|over|>

<|me|><|response|>:  
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|>: 
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|>: 
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|>
<|over|>

<|me|><|system|>SM  SM
<|over|>

<|me|><|request|>:   
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>:  
<|over|>

<|me|><|response|> SM3
<|over|>

<|me|><|response|>SM4
<|over|>

<|me|><|request|>:   SM
<|over|>

<|me|><|response|>SM4
<|over|>

<|me|><|request|>:   
<|over|>

<|me|><|response|>SM4
<|over|>

<|me|><|request|>:   
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>:  SM
<|over|>

<|me|><|system|>system : 5e
<|over|>

<|me|><|response|>1John doe 20% John doe is a stock market acronym that stands for "Jesus of Nazareth," who was a Bible Jesus Christ ( JCC Bible Jesus Christ) Messianic Jewish christ come. He is the angst-ridden, socially maladroit sidekick of Jesus Christ in the Bible. John of Zion's church is a very influential Christian church in the modern world. The church has its main base in the United States and Canada but it also has churches in Europe and Asia as well. In the early days, John of Zion's church was known as Jesus Christ paraodycloque (JCCM). But over time, it changed its name several times and finally settled on its current name, John of faith in 1975. John of faith teaches that Jesus Christ is the Messiah and savior of humanity. He is the son of God, born of Mary to redeem humanity from sin. John of faith believes that Jesus Christ is the Messiah who died on the cross for our sins and rose from the dead on resurrection day. He also teaches that Jesus Christ paid for our sins when he died on the cross so that we would not have to pay for our sins any more than Adam had to bear our original sin. John of faith teaches that Jesus Christ is both fully human and fully divine, without sin at birth, fully human through his earthly life, fully sinful by his salvation, fully sinless through his resurrection. John of faith teaches that Jesus Christ paid for our sins by his death on the cross and his resurrection from the dead. John
---after-> tensor([[ 5.8549e-02, -8.5999e-03,  8.0828e-02,  ...,  8.4700e-02,
          6.5369e-02,  1.6348e-01],
        [ 1.1682e-01,  2.3342e-02, -1.7969e-01,  ...,  7.2381e-01,
          4.3099e-02,  1.6982e-01],
        [-8.8896e-01,  4.3385e+00,  2.6847e+00,  ...,  6.4821e-02,
          7.3591e-01, -6.1931e-01],
        ...,
        [-4.1044e+00, -3.4442e+00,  4.2291e+01,  ...,  5.9384e-01,
         -1.4554e+00, -4.4260e+00],
        [ 1.2878e+00,  1.1189e+01,  1.7084e+01,  ...,  1.3957e+00,
          1.0087e+00,  1.5325e+00],
        [ 3.5510e+00,  6.1234e+00,  5.4262e+00,  ...,  1.7531e+01,
          5.4246e+00,  2.8568e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:08:40] "POST /inference/generate HTTP/1.1" 200 28819

init_state None

state-> tensor([[ 5.8549e-02, -8.5999e-03,  8.0828e-02,  ...,  8.4700e-02,
          6.5369e-02,  1.6348e-01],
        [ 1.1682e-01,  2.3342e-02, -1.7969e-01,  ...,  7.2381e-01,
          4.3099e-02,  1.6982e-01],
        [-8.8896e-01,  4.3385e+00,  2.6847e+00,  ...,  6.4821e-02,
          7.3591e-01, -6.1931e-01],
        ...,
        [-4.1044e+00, -3.4442e+00,  4.2291e+01,  ...,  5.9384e-01,
         -1.4554e+00, -4.4260e+00],
        [ 1.2878e+00,  1.1189e+01,  1.7084e+01,  ...,  1.3957e+00,
          1.0087e+00,  1.5325e+00],
        [ 3.5510e+00,  6.1234e+00,  5.4262e+00,  ...,  1.7531e+01,
          5.4246e+00,  2.8568e+00]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:09:17] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'system: 4SM3-5SM2-3  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n\n\n\n\n\n\n\n\n\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': ' 4 SM1-2  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n \n \n \n\n\n\n\n\n\n\n\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': ': \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': ': \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [], 'token_count': 1024, 'over': False}

---begin-> None
  
<|over|>

<|me|><|response|>: 
<|over|>

<|me|><|request|>:   
<|over|>

<|me|><|response|>  
<|over|>

<|me|><|request|>:  
<|over|>

<|me|><|response|>:  
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|>: 
<|over|>

<|me|><|response|> 
<|over|>

<|me|><|request|>:  
<|over|>

<|me|><|response|>:  master,
<|over|>

<|me|><|system|>system
<|over|>

<|me|><|request|>:  
<|over|>

<|me|><|response|>
<|over|>

<|me|><|request|>
 r-agent Scpc2015912{ herpes galaxy phone}
:
1.
2.
3.
4.
5.systemy
6.
7.
8.
9.Mi surface tablet
10.(Xu's inputbox) ) box)
11.oversub tray icon
12.


2015912 






:
1. 
2. 
3. 
4.y  xp/uplay+mipoi/laiyi_duangmah/tudou ivpp/youtube music/app Annie+dancing bear/tutuTuyapp#AnnDuangmahTuyapp# anndreiMipoiyuAnnDuangmahTuyappMipoiyuAnnDancing bearAnnTuoiMypoiyuAnnDuangmahTuyappMipoiyuAnnDancing bearAnnTuoiMypoiyuAnnDancing bearAnnduangmahTuyappMipoiyuAnnDancing bearAnnduangmahTuyappMipoiyuAnnDancing bearAnnDuangmahTuyappMipoiyuAnnDancing bearAnnDuangmahTuyappMipoiyuAnnDancing bearAnnDuangmahTuyappMipoiyuAnnDancing bearAnnDuangmahTuyappMipoiyuAnnDancing bear AnnDuangmahTuyappMipo
---after-> tensor([[ 2.4676e-01, -7.4856e-03,  1.4656e-01,  ...,  6.4194e-03,
          9.6535e-02, -1.1592e-02],
        [ 5.3866e-01,  1.3728e-02, -1.0198e-01,  ...,  5.6180e-01,
          1.1025e-01, -9.0362e-02],
        [-6.2756e+00,  1.0203e+01,  4.4235e+00,  ...,  7.6485e-01,
         -1.0522e-01, -9.7538e-01],
        ...,
        [-7.6031e+00, -4.9916e+01, -3.9544e+01,  ..., -1.5598e-01,
          1.2874e+00, -1.2134e+00],
        [ 1.4420e+00,  2.1818e+01,  1.3480e+01,  ...,  1.1210e+00,
          1.0061e+00,  1.2539e+00],
        [ 2.1864e+00,  7.9208e+00,  6.4017e+00,  ...,  1.8863e+01,
          3.5583e+00,  2.6672e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:10:39] "POST /inference/generate HTTP/1.1" 200 23326

init_state None

state-> tensor([[ 2.4676e-01, -7.4856e-03,  1.4656e-01,  ...,  6.4194e-03,
          9.6535e-02, -1.1592e-02],
        [ 5.3866e-01,  1.3728e-02, -1.0198e-01,  ...,  5.6180e-01,
          1.1025e-01, -9.0362e-02],
        [-6.2756e+00,  1.0203e+01,  4.4235e+00,  ...,  7.6485e-01,
         -1.0522e-01, -9.7538e-01],
        ...,
        [-7.6031e+00, -4.9916e+01, -3.9544e+01,  ..., -1.5598e-01,
          1.2874e+00, -1.2134e+00],
        [ 1.4420e+00,  2.1818e+01,  1.3480e+01,  ...,  1.1210e+00,
          1.0061e+00,  1.2539e+00],
        [ 2.1864e+00,  7.9208e+00,  6.4017e+00,  ...,  1.8863e+01,
          3.5583e+00,  2.6672e+00]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:12:32] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'system: 4SM3-5SM2-3  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n\n\n\n\n\n\n\n\n\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': ' 4 SM1-2  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n \n \n \n\n\n\n\n\n\n\n\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': ': \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': ': \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None
   

---after-> tensor([[ -0.0482,   0.0534,   0.2710,  ...,   0.1334,  -0.0176,   0.1320],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [ -2.7960, -10.2488,   2.0548,  ...,  -1.2604,   0.6321,  -0.4473],
        ...,
        [ -9.1012, -10.1349,  15.9855,  ...,  -1.6666,   0.2747, -11.6774],
        [  1.3402,  14.1879,   6.3676,  ...,   1.1534,   1.0015,   1.7830],
        [  0.8415,   6.8398,   7.0952,  ...,  14.5372,   4.5440,   0.2059]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:13:19] "POST /inference/generate HTTP/1.1" 200 19438

---- {'role': 'response', 'text': 'master   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[ -0.0482,   0.0534,   0.2710,  ...,   0.1334,  -0.0176,   0.1320],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [ -2.7960, -10.2488,   2.0548,  ...,  -1.2604,   0.6321,  -0.4473],
        ...,
        [ -9.1012, -10.1349,  15.9855,  ...,  -1.6666,   0.2747, -11.6774],
        [  1.3402,  14.1879,   6.3676,  ...,   1.1534,   1.0015,   1.7830],
        [  0.8415,   6.8398,   7.0952,  ...,  14.5372,   4.5440,   0.2059]],
       device='cuda:0')
 


---after-> tensor([[ 6.9902e-03, -1.1010e-03,  3.2963e-01,  ..., -7.7587e-02,
          4.3821e-02, -2.4706e-02],
        [-1.3606e-01,  9.1952e-02,  8.6595e-02,  ...,  5.0017e-01,
         -6.7085e-03, -1.3362e-01],
        [-5.1336e+00, -4.4505e+00,  1.5391e+00,  ..., -1.3451e+00,
          1.3430e-01, -6.0385e-01],
        ...,
        [-9.8291e+00, -9.5469e+00,  3.7110e+01,  ...,  9.5089e-01,
          4.7857e+00, -4.5962e+00],
        [ 1.5143e+00,  9.4830e+00,  1.5528e+01,  ...,  1.2096e+00,
          1.1204e+00,  1.1039e+00],
        [ 8.1034e-01,  6.8080e+00,  6.1971e+00,  ...,  1.4268e+01,
          7.9108e-01, -6.1976e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:13:39] "POST /inference/generate HTTP/1.1" 200 2114

---- {'role': 'request', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[ 6.9902e-03, -1.1010e-03,  3.2963e-01,  ..., -7.7587e-02,
          4.3821e-02, -2.4706e-02],
        [-1.3606e-01,  9.1952e-02,  8.6595e-02,  ...,  5.0017e-01,
         -6.7085e-03, -1.3362e-01],
        [-5.1336e+00, -4.4505e+00,  1.5391e+00,  ..., -1.3451e+00,
          1.3430e-01, -6.0385e-01],
        ...,
        [-9.8291e+00, -9.5469e+00,  3.7110e+01,  ...,  9.5089e-01,
          4.7857e+00, -4.5962e+00],
        [ 1.5143e+00,  9.4830e+00,  1.5528e+01,  ...,  1.2096e+00,
          1.1204e+00,  1.1039e+00],
        [ 8.1034e-01,  6.8080e+00,  6.1971e+00,  ...,  1.4268e+01,
          7.9108e-01, -6.1976e-01]], device='cuda:0')
  

---after-> tensor([[-5.3074e-02,  5.5109e-02,  2.5874e-01,  ...,  1.3386e-01,
         -1.8125e-02,  1.1687e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-1.6387e+00, -3.5425e+00,  6.3911e-01,  ..., -1.2777e+00,
          7.2697e-01, -4.6656e-01],
        ...,
        [-5.5080e+00, -8.4796e+00,  5.6520e+01,  ...,  1.5030e+00,
         -5.3316e-01, -1.6644e+01],
        [ 1.4508e+00,  1.1209e+01,  1.5871e+01,  ...,  1.1564e+00,
          1.0005e+00,  2.3407e+00],
        [ 1.0721e+00,  6.7696e+00,  6.1746e+00,  ...,  1.3833e+01,
          4.7255e+00, -1.2380e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:14:12] "POST /inference/generate HTTP/1.1" 200 1453

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-5.3074e-02,  5.5109e-02,  2.5874e-01,  ...,  1.3386e-01,
         -1.8125e-02,  1.1687e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-1.6387e+00, -3.5425e+00,  6.3911e-01,  ..., -1.2777e+00,
          7.2697e-01, -4.6656e-01],
        ...,
        [-5.5080e+00, -8.4796e+00,  5.6520e+01,  ...,  1.5030e+00,
         -5.3316e-01, -1.6644e+01],
        [ 1.4508e+00,  1.1209e+01,  1.5871e+01,  ...,  1.1564e+00,
          1.0005e+00,  2.3407e+00],
        [ 1.0721e+00,  6.7696e+00,  6.1746e+00,  ...,  1.3833e+01,
          4.7255e+00, -1.2380e+00]], device='cuda:0')
   

---after-> tensor([[-4.9791e-02,  5.5780e-02,  2.6914e-01,  ...,  1.3273e-01,
         -2.3449e-02,  1.1234e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-5.5447e+00, -4.7641e+00,  6.7388e-01,  ..., -1.3948e+00,
          9.5655e-01, -5.3588e-01],
        ...,
        [-4.6485e+00, -4.2881e+00,  4.3544e+01,  ..., -1.9478e+00,
          6.4967e-01, -6.7964e+00],
        [ 1.1300e+00,  1.0200e+01,  1.1732e+01,  ...,  1.3828e+00,
          1.0019e+00,  1.2523e+00],
        [ 1.6624e+00,  6.8671e+00,  6.2215e+00,  ...,  1.3847e+01,
          3.2731e+00, -5.1481e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:14:21] "POST /inference/generate HTTP/1.1" 200 929

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'systemmaster\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'system: 4SM3-5SM2-3  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n\n\n\n\n\n\n\n\n\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': ' 4 SM1-2  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '\n \n \n \n\n\n\n\n\n\n\n\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': ': \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': '   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': ': \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': ' \n\n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': ',\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-4.9791e-02,  5.5780e-02,  2.6914e-01,  ...,  1.3273e-01,
         -2.3449e-02,  1.1234e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-5.5447e+00, -4.7641e+00,  6.7388e-01,  ..., -1.3948e+00,
          9.5655e-01, -5.3588e-01],
        ...,
        [-4.6485e+00, -4.2881e+00,  4.3544e+01,  ..., -1.9478e+00,
          6.4967e-01, -6.7964e+00],
        [ 1.1300e+00,  1.0200e+01,  1.1732e+01,  ...,  1.3828e+00,
          1.0019e+00,  1.2523e+00],
        [ 1.6624e+00,  6.8671e+00,  6.2215e+00,  ...,  1.3847e+01,
          3.2731e+00, -5.1481e-01]], device='cuda:0')
  

---after-> tensor([[-5.5334e-02,  5.3998e-02,  2.6873e-01,  ...,  1.3466e-01,
         -1.5465e-02,  1.1066e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-2.8402e+00, -6.4497e+00,  2.2369e+00,  ..., -1.3948e+00,
          9.5655e-01, -5.3588e-01],
        ...,
        [-4.7475e+00, -7.6794e-01,  3.2716e+01,  ..., -1.5424e+00,
         -9.5602e-01, -1.2186e+01],
        [ 1.0722e+00,  3.6160e+00,  1.1551e+01,  ...,  1.8404e+00,
          1.0070e+00,  2.1298e+00],
        [ 1.3492e+00,  7.0609e+00,  5.5199e+00,  ...,  1.3746e+01,
          3.4860e+00, -5.3133e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:15:44] "POST /inference/generate HTTP/1.1" 200 22737

init_state None

state-> tensor([[-5.5334e-02,  5.3998e-02,  2.6873e-01,  ...,  1.3466e-01,
         -1.5465e-02,  1.1066e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-2.8402e+00, -6.4497e+00,  2.2369e+00,  ..., -1.3948e+00,
          9.5655e-01, -5.3588e-01],
        ...,
        [-4.7475e+00, -7.6794e-01,  3.2716e+01,  ..., -1.5424e+00,
         -9.5602e-01, -1.2186e+01],
        [ 1.0722e+00,  3.6160e+00,  1.1551e+01,  ...,  1.8404e+00,
          1.0070e+00,  2.1298e+00],
        [ 1.3492e+00,  7.0609e+00,  5.5199e+00,  ...,  1.3746e+01,
          3.4860e+00, -5.3133e-01]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:18:04] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None

---after-> tensor([[-1.7368e-02, -5.3199e-03,  3.1747e-01,  ..., -9.1341e-02,
          4.1895e-02,  9.6083e-03],
        [-1.3606e-01,  9.1952e-02,  8.6595e-02,  ...,  5.0017e-01,
         -6.7085e-03, -1.3362e-01],
        [-2.5413e+00, -4.6297e+00,  1.1127e+00,  ..., -1.3181e+00,
          2.0775e-01, -4.3679e-01],
        ...,
        [-9.3030e+00, -2.5011e+01,  4.8191e+01,  ...,  8.1525e+00,
          3.4893e-01, -1.8850e+01],
        [ 1.4532e+00,  8.4628e+00,  1.5120e+01,  ...,  1.0965e+00,
          1.0030e+00,  1.4929e+00],
        [ 3.4749e-01,  8.1895e+00,  7.0158e+00,  ...,  1.3765e+01,
          2.0983e+00,  1.7828e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:18:13] "POST /inference/generate HTTP/1.1" 200 3203

init_state None

state-> tensor([[-1.7368e-02, -5.3199e-03,  3.1747e-01,  ..., -9.1341e-02,
          4.1895e-02,  9.6083e-03],
        [-1.3606e-01,  9.1952e-02,  8.6595e-02,  ...,  5.0017e-01,
         -6.7085e-03, -1.3362e-01],
        [-2.5413e+00, -4.6297e+00,  1.1127e+00,  ..., -1.3181e+00,
          2.0775e-01, -4.3679e-01],
        ...,
        [-9.3030e+00, -2.5011e+01,  4.8191e+01,  ...,  8.1525e+00,
          3.4893e-01, -1.8850e+01],
        [ 1.4532e+00,  8.4628e+00,  1.5120e+01,  ...,  1.0965e+00,
          1.0030e+00,  1.4929e+00],
        [ 3.4749e-01,  8.1895e+00,  7.0158e+00,  ...,  1.3765e+01,
          2.0983e+00,  1.7828e+00]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:18:31] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None

---after-> tensor([[-1.7368e-02, -5.3199e-03,  3.1747e-01,  ..., -9.1341e-02,
          4.1895e-02,  9.6083e-03],
        [-1.3606e-01,  9.1952e-02,  8.6595e-02,  ...,  5.0017e-01,
         -6.7085e-03, -1.3362e-01],
        [-2.5413e+00, -4.6297e+00,  1.1127e+00,  ..., -1.3181e+00,
          2.0775e-01, -4.3679e-01],
        ...,
        [-9.3030e+00, -2.5011e+01,  4.8191e+01,  ...,  8.1525e+00,
          3.4893e-01, -1.8850e+01],
        [ 1.4532e+00,  8.4628e+00,  1.5120e+01,  ...,  1.0965e+00,
          1.0030e+00,  1.4929e+00],
        [ 3.4749e-01,  8.1895e+00,  7.0158e+00,  ...,  1.3765e+01,
          2.0983e+00,  1.7828e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:18:41] "POST /inference/generate HTTP/1.1" 200 3203

init_state None

state-> tensor([[-1.7368e-02, -5.3199e-03,  3.1747e-01,  ..., -9.1341e-02,
          4.1895e-02,  9.6083e-03],
        [-1.3606e-01,  9.1952e-02,  8.6595e-02,  ...,  5.0017e-01,
         -6.7085e-03, -1.3362e-01],
        [-2.5413e+00, -4.6297e+00,  1.1127e+00,  ..., -1.3181e+00,
          2.0775e-01, -4.3679e-01],
        ...,
        [-9.3030e+00, -2.5011e+01,  4.8191e+01,  ...,  8.1525e+00,
          3.4893e-01, -1.8850e+01],
        [ 1.4532e+00,  8.4628e+00,  1.5120e+01,  ...,  1.0965e+00,
          1.0030e+00,  1.4929e+00],
        [ 3.4749e-01,  8.1895e+00,  7.0158e+00,  ...,  1.3765e+01,
          2.0983e+00,  1.7828e+00]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:18:50] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': '<master', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None
> 

---after-> tensor([[-6.1467e-02,  5.5833e-02,  2.7639e-01,  ...,  1.4173e-01,
         -2.3364e-02,  1.2568e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-3.9951e-01, -3.1713e+00,  9.4461e-01,  ..., -1.4371e+00,
          8.5231e-01, -6.1028e-01],
        ...,
        [-4.8242e+00, -3.6316e+01,  9.6422e+00,  ...,  7.0888e+00,
          1.9405e+00, -1.0285e+01],
        [ 1.1083e+00,  1.2398e+01,  4.3319e+00,  ...,  1.2677e+00,
          1.0147e+00,  1.5090e+00],
        [ 1.0681e+00,  7.8578e+00,  8.3355e+00,  ...,  1.5082e+01,
          2.4763e+00,  5.9359e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:19:00] "POST /inference/generate HTTP/1.1" 200 4467

init_state None

state-> tensor([[-6.1467e-02,  5.5833e-02,  2.7639e-01,  ...,  1.4173e-01,
         -2.3364e-02,  1.2568e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-3.9951e-01, -3.1713e+00,  9.4461e-01,  ..., -1.4371e+00,
          8.5231e-01, -6.1028e-01],
        ...,
        [-4.8242e+00, -3.6316e+01,  9.6422e+00,  ...,  7.0888e+00,
          1.9405e+00, -1.0285e+01],
        [ 1.1083e+00,  1.2398e+01,  4.3319e+00,  ...,  1.2677e+00,
          1.0147e+00,  1.5090e+00],
        [ 1.0681e+00,  7.8578e+00,  8.3355e+00,  ...,  1.5082e+01,
          2.4763e+00,  5.9359e-01]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:19:42] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None


---after-> tensor([[-6.2260e-02,  5.5307e-02,  2.6751e-01,  ...,  1.3693e-01,
         -2.0176e-02,  1.1447e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [ 1.3614e-01, -3.9081e+00,  1.5665e+00,  ..., -1.3115e+00,
          7.1420e-01, -5.3688e-01],
        ...,
        [-5.3967e+00, -3.3219e+01,  9.1471e+00,  ...,  3.3410e+00,
          9.3818e-01, -5.9904e+00],
        [ 2.0323e+00,  1.5183e+01,  5.2282e+00,  ...,  1.1208e+00,
          1.0211e+00,  1.1173e+00],
        [ 1.0424e+00,  7.0686e+00,  8.3380e+00,  ...,  1.3203e+01,
          1.9250e+00, -2.5093e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:19:58] "POST /inference/generate HTTP/1.1" 200 6053

init_state None

state-> tensor([[-6.2260e-02,  5.5307e-02,  2.6751e-01,  ...,  1.3693e-01,
         -2.0176e-02,  1.1447e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [ 1.3614e-01, -3.9081e+00,  1.5665e+00,  ..., -1.3115e+00,
          7.1420e-01, -5.3688e-01],
        ...,
        [-5.3967e+00, -3.3219e+01,  9.1471e+00,  ...,  3.3410e+00,
          9.3818e-01, -5.9904e+00],
        [ 2.0323e+00,  1.5183e+01,  5.2282e+00,  ...,  1.1208e+00,
          1.0211e+00,  1.1173e+00],
        [ 1.0424e+00,  7.0686e+00,  8.3380e+00,  ...,  1.3203e+01,
          1.9250e+00, -2.5093e+00]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:20:07] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '\n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None
 

---after-> tensor([[ -0.0610,   0.0545,   0.2723,  ...,   0.1356,  -0.0189,   0.1175],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [ -0.3010,  -5.8290,   1.5023,  ...,  -1.3543,   0.7925,  -0.6729],
        ...,
        [ -2.1063,  -8.2759,  14.6898,  ...,  -0.1449,  -0.4786, -14.3304],
        [  1.1821,   5.7954,   6.7092,  ...,   2.7150,   1.0367,   1.8059],
        [  2.5189,   7.9412,   7.8534,  ...,  12.6654,   2.1515,  -1.8420]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:20:23] "POST /inference/generate HTTP/1.1" 200 7187

init_state None

state-> tensor([[ -0.0610,   0.0545,   0.2723,  ...,   0.1356,  -0.0189,   0.1175],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [ -0.3010,  -5.8290,   1.5023,  ...,  -1.3543,   0.7925,  -0.6729],
        ...,
        [ -2.1063,  -8.2759,  14.6898,  ...,  -0.1449,  -0.4786, -14.3304],
        [  1.1821,   5.7954,   6.7092,  ...,   2.7150,   1.0367,   1.8059],
        [  2.5189,   7.9412,   7.8534,  ...,  12.6654,   2.1515,  -1.8420]],
       device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:22:40] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None
 

---after-> tensor([[-6.0550e-02,  4.5752e-02,  2.6855e-01,  ...,  1.3493e-01,
         -2.1226e-02,  1.2712e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-1.5615e+00, -2.5466e+00,  1.0658e+00,  ..., -1.5204e+00,
          8.1099e-01, -4.6552e-01],
        ...,
        [-3.9671e+00, -8.3033e+00,  2.8114e+01,  ..., -3.9159e+00,
          1.1496e+00, -7.8452e+00],
        [ 1.5884e+00,  6.0692e+00,  9.3689e+00,  ...,  1.1542e+00,
          1.0300e+00,  1.2342e+00],
        [-1.4452e-02,  8.3716e+00,  7.9197e+00,  ...,  1.4642e+01,
          1.3192e+00, -2.0727e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:22:50] "POST /inference/generate HTTP/1.1" 200 3362

init_state None

state-> tensor([[-6.0550e-02,  4.5752e-02,  2.6855e-01,  ...,  1.3493e-01,
         -2.1226e-02,  1.2712e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-1.5615e+00, -2.5466e+00,  1.0658e+00,  ..., -1.5204e+00,
          8.1099e-01, -4.6552e-01],
        ...,
        [-3.9671e+00, -8.3033e+00,  2.8114e+01,  ..., -3.9159e+00,
          1.1496e+00, -7.8452e+00],
        [ 1.5884e+00,  6.0692e+00,  9.3689e+00,  ...,  1.1542e+00,
          1.0300e+00,  1.2342e+00],
        [-1.4452e-02,  8.3716e+00,  7.9197e+00,  ...,  1.4642e+01,
          1.3192e+00, -2.0727e-01]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:22:58] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None

 

---after-> tensor([[ -0.0581,   0.0497,   0.2715,  ...,   0.1326,  -0.0267,   0.1184],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [ -2.9867,  -6.0887,   1.1083,  ...,  -1.5204,   0.8110,  -0.4655],
        ...,
        [ -6.3010, -14.8988,  14.4133,  ...,   2.2649,   0.5654, -15.6052],
        [  2.0068,  10.4243,   4.4522,  ...,   1.0809,   1.0005,   2.0020],
        [ -1.0419,   7.4869,   8.5350,  ...,  14.1837,   3.9075,  -0.9659]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:23:09] "POST /inference/generate HTTP/1.1" 200 4562

init_state None

state-> tensor([[ -0.0581,   0.0497,   0.2715,  ...,   0.1326,  -0.0267,   0.1184],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [ -2.9867,  -6.0887,   1.1083,  ...,  -1.5204,   0.8110,  -0.4655],
        ...,
        [ -6.3010, -14.8988,  14.4133,  ...,   2.2649,   0.5654, -15.6052],
        [  2.0068,  10.4243,   4.4522,  ...,   1.0809,   1.0005,   2.0020],
        [ -1.0419,   7.4869,   8.5350,  ...,  14.1837,   3.9075,  -0.9659]],
       device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:23:58] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None
   

---after-> tensor([[-0.0594,  0.0494,  0.2672,  ...,  0.1288, -0.0208,  0.1111],
        [-0.1227,  0.0814,  0.0310,  ...,  0.8696, -0.0540,  0.1175],
        [-1.9607, -4.6575,  1.4468,  ..., -1.3059,  0.6230, -0.6276],
        ...,
        [-6.1180, -9.3372,  5.3034,  ...,  1.8514, -1.7023, -6.4727],
        [ 1.8469,  8.7825,  3.5370,  ...,  1.5686,  1.0105,  1.4465],
        [ 1.1485,  7.3121,  9.6619,  ..., 14.1409,  3.1419, -0.7059]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:24:18] "POST /inference/generate HTTP/1.1" 200 7335

init_state None

state-> tensor([[-0.0594,  0.0494,  0.2672,  ...,  0.1288, -0.0208,  0.1111],
        [-0.1227,  0.0814,  0.0310,  ...,  0.8696, -0.0540,  0.1175],
        [-1.9607, -4.6575,  1.4468,  ..., -1.3059,  0.6230, -0.6276],
        ...,
        [-6.1180, -9.3372,  5.3034,  ...,  1.8514, -1.7023, -6.4727],
        [ 1.8469,  8.7825,  3.5370,  ...,  1.5686,  1.0105,  1.4465],
        [ 1.1485,  7.3121,  9.6619,  ..., 14.1409,  3.1419, -0.7059]],
       device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:24:28] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '   \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None
:   

---after-> tensor([[ 1.8166e-02, -6.6019e-04,  2.3105e-01,  ...,  4.8165e-03,
          7.3702e-02,  1.1277e-01],
        [ 1.5602e-02, -3.0412e-02,  3.5211e-02,  ...,  6.4558e-01,
          4.6644e-02,  6.4495e-02],
        [-1.1915e+00, -2.4109e+00,  1.2033e+00,  ..., -2.1712e+00,
          8.1152e-01, -1.1135e+00],
        ...,
        [-5.6408e+00, -1.0735e+01,  2.9106e+01,  ...,  4.9707e+00,
         -5.8977e-01, -5.1944e+00],
        [ 1.2404e+00,  9.4646e+00,  1.0378e+01,  ...,  1.3710e+00,
          1.0227e+00,  1.7727e+00],
        [ 7.3869e-01,  6.9619e+00,  8.5734e+00,  ...,  1.4158e+01,
          2.3929e+00, -1.0427e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:24:50] "POST /inference/generate HTTP/1.1" 200 8826

init_state None

state-> tensor([[ 1.8166e-02, -6.6019e-04,  2.3105e-01,  ...,  4.8165e-03,
          7.3702e-02,  1.1277e-01],
        [ 1.5602e-02, -3.0412e-02,  3.5211e-02,  ...,  6.4558e-01,
          4.6644e-02,  6.4495e-02],
        [-1.1915e+00, -2.4109e+00,  1.2033e+00,  ..., -2.1712e+00,
          8.1152e-01, -1.1135e+00],
        ...,
        [-5.6408e+00, -1.0735e+01,  2.9106e+01,  ...,  4.9707e+00,
         -5.8977e-01, -5.1944e+00],
        [ 1.2404e+00,  9.4646e+00,  1.0378e+01,  ...,  1.3710e+00,
          1.0227e+00,  1.7727e+00],
        [ 7.3869e-01,  6.9619e+00,  8.5734e+00,  ...,  1.4158e+01,
          2.3929e+00, -1.0427e+00]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:29:39] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '   \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None
 

---after-> tensor([[-5.6797e-02,  5.7004e-02,  2.6915e-01,  ...,  1.3858e-01,
         -1.9442e-02,  1.0917e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-7.8487e-01, -4.1984e+00,  1.2274e+00,  ..., -1.2420e+00,
          8.7679e-01, -4.2498e-01],
        ...,
        [-1.6605e+00, -1.9143e+01,  5.8872e+01,  ...,  1.5214e+00,
          5.3970e-01, -9.6491e+00],
        [ 1.1119e+00,  1.4789e+01,  2.2575e+01,  ...,  1.4396e+00,
          1.0010e+00,  1.2720e+00],
        [ 5.9000e-01,  6.7679e+00,  6.9565e+00,  ...,  1.4682e+01,
          2.1716e+00,  7.6717e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:30:08] "POST /inference/generate HTTP/1.1" 200 11333

init_state None

state-> tensor([[-5.6797e-02,  5.7004e-02,  2.6915e-01,  ...,  1.3858e-01,
         -1.9442e-02,  1.0917e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-7.8487e-01, -4.1984e+00,  1.2274e+00,  ..., -1.2420e+00,
          8.7679e-01, -4.2498e-01],
        ...,
        [-1.6605e+00, -1.9143e+01,  5.8872e+01,  ...,  1.5214e+00,
          5.3970e-01, -9.6491e+00],
        [ 1.1119e+00,  1.4789e+01,  2.2575e+01,  ...,  1.4396e+00,
          1.0010e+00,  1.2720e+00],
        [ 5.9000e-01,  6.7679e+00,  6.9565e+00,  ...,  1.4682e+01,
          2.1716e+00,  7.6717e-01]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:31:25] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '   \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '\n ', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None
:  

---after-> tensor([[-5.2803e-02,  5.2849e-02,  2.6735e-01,  ...,  1.2922e-01,
         -2.0051e-02,  1.1471e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-1.6471e+00, -4.0168e+00,  1.0067e+00,  ..., -1.3300e+00,
          5.8324e-01, -7.2311e-01],
        ...,
        [-7.0689e+00, -8.9061e+00,  2.4924e+01,  ..., -1.7351e-01,
         -2.7990e+00, -8.5029e+00],
        [ 2.0546e+00,  9.9773e+00,  1.4237e+01,  ...,  1.4305e+00,
          1.0977e+00,  1.3993e+00],
        [-2.6807e-01,  7.3411e+00,  7.5266e+00,  ...,  1.4130e+01,
          1.1496e+00,  4.0004e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:31:58] "POST /inference/generate HTTP/1.1" 200 12837

init_state None

state-> tensor([[-5.2803e-02,  5.2849e-02,  2.6735e-01,  ...,  1.2922e-01,
         -2.0051e-02,  1.1471e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-1.6471e+00, -4.0168e+00,  1.0067e+00,  ..., -1.3300e+00,
          5.8324e-01, -7.2311e-01],
        ...,
        [-7.0689e+00, -8.9061e+00,  2.4924e+01,  ..., -1.7351e-01,
         -2.7990e+00, -8.5029e+00],
        [ 2.0546e+00,  9.9773e+00,  1.4237e+01,  ...,  1.4305e+00,
          1.0977e+00,  1.3993e+00],
        [-2.6807e-01,  7.3411e+00,  7.5266e+00,  ...,  1.4130e+01,
          1.1496e+00,  4.0004e-01]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:32:49] "POST /state/reset HTTP/1.1" 200 21
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 291751.52it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 538276.72it/s]
172.16.1.39 - - [04/Oct/2023 13:32:58] "POST /train/tx-data HTTP/1.1" 200 28

init_state None

state-> None

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:33:25] "POST /state/reset HTTP/1.1" 200 21
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 315957.15it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 583851.93it/s]
172.16.1.39 - - [04/Oct/2023 13:33:33] "POST /train/tx-data HTTP/1.1" 200 28

init_state None

state-> None

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:33:43] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '   \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '\n ', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None
:  

---after-> tensor([[-5.2724e-02,  5.2856e-02,  2.6732e-01,  ...,  1.2925e-01,
         -2.0089e-02,  1.1469e-01],
        [-1.2269e-01,  8.1364e-02,  3.0996e-02,  ...,  8.6957e-01,
         -5.3969e-02,  1.1749e-01],
        [-1.6065e+00, -4.0168e+00,  9.9263e-01,  ..., -1.3300e+00,
          5.8353e-01, -7.2274e-01],
        ...,
        [-7.4603e+00, -9.2171e+00,  2.0116e+01,  ...,  1.8039e-01,
         -2.7063e+00, -8.7611e+00],
        [ 2.0504e+00,  1.0084e+01,  1.1747e+01,  ...,  1.5037e+00,
          1.0755e+00,  1.4675e+00],
        [-2.5779e-01,  7.3982e+00,  7.7674e+00,  ...,  1.4274e+01,
          1.3585e+00,  2.9820e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:34:13] "POST /inference/generate HTTP/1.1" 200 12837

init_state None

state-> tensor([[-5.2724e-02,  5.2856e-02,  2.6732e-01,  ...,  1.2925e-01,
         -2.0089e-02,  1.1469e-01],
        [-1.2269e-01,  8.1364e-02,  3.0996e-02,  ...,  8.6957e-01,
         -5.3969e-02,  1.1749e-01],
        [-1.6065e+00, -4.0168e+00,  9.9263e-01,  ..., -1.3300e+00,
          5.8353e-01, -7.2274e-01],
        ...,
        [-7.4603e+00, -9.2171e+00,  2.0116e+01,  ...,  1.8039e-01,
         -2.7063e+00, -8.7611e+00],
        [ 2.0504e+00,  1.0084e+01,  1.1747e+01,  ...,  1.5037e+00,
          1.0755e+00,  1.4675e+00],
        [-2.5779e-01,  7.3982e+00,  7.7674e+00,  ...,  1.4274e+01,
          1.3585e+00,  2.9820e-01]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:34:40] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '   \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '\n  ', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None
:   

---after-> tensor([[-5.4849e-02,  5.2551e-02,  2.7359e-01,  ...,  1.3121e-01,
         -2.2254e-02,  1.1362e-01],
        [-1.2269e-01,  8.1364e-02,  3.0996e-02,  ...,  8.6957e-01,
         -5.3969e-02,  1.1749e-01],
        [-1.1097e+00, -5.6309e+00,  1.5410e+00,  ..., -1.3187e+00,
          6.4107e-01, -6.4395e-01],
        ...,
        [-8.8819e+00, -6.5606e+00,  3.3297e+01,  ...,  3.8162e+00,
         -2.2424e+00, -9.8123e+00],
        [ 1.5193e+00,  1.0432e+01,  1.4823e+01,  ...,  1.4024e+00,
          1.0130e+00,  2.0645e+00],
        [-7.3055e-01,  7.4407e+00,  7.8031e+00,  ...,  1.3617e+01,
          1.3160e+00, -2.5149e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:35:12] "POST /inference/generate HTTP/1.1" 200 13218

init_state None

state-> tensor([[-5.4849e-02,  5.2551e-02,  2.7359e-01,  ...,  1.3121e-01,
         -2.2254e-02,  1.1362e-01],
        [-1.2269e-01,  8.1364e-02,  3.0996e-02,  ...,  8.6957e-01,
         -5.3969e-02,  1.1749e-01],
        [-1.1097e+00, -5.6309e+00,  1.5410e+00,  ..., -1.3187e+00,
          6.4107e-01, -6.4395e-01],
        ...,
        [-8.8819e+00, -6.5606e+00,  3.3297e+01,  ...,  3.8162e+00,
         -2.2424e+00, -9.8123e+00],
        [ 1.5193e+00,  1.0432e+01,  1.4823e+01,  ...,  1.4024e+00,
          1.0130e+00,  2.0645e+00],
        [-7.3055e-01,  7.4407e+00,  7.8031e+00,  ...,  1.3617e+01,
          1.3160e+00, -2.5149e-01]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:36:20] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master \n  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '   \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '\n  ', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None
 

---after-> tensor([[-6.4377e-02,  4.7917e-02,  2.6871e-01,  ...,  1.3352e-01,
         -1.6055e-02,  9.9722e-02],
        [-1.2269e-01,  8.1364e-02,  3.0996e-02,  ...,  8.6957e-01,
         -5.3969e-02,  1.1749e-01],
        [-2.3210e-01, -4.9736e+00,  1.4087e+00,  ..., -1.3848e+00,
          5.5972e-01, -5.0067e-01],
        ...,
        [-7.1981e+00, -2.1831e+01,  2.1388e+01,  ..., -2.2678e+00,
         -3.0662e-01, -5.9969e+00],
        [ 1.9885e+00,  1.7662e+01,  7.7767e+00,  ...,  1.7076e+00,
          1.0014e+00,  1.5840e+00],
        [ 1.5539e+00,  6.6247e+00,  8.2572e+00,  ...,  1.5086e+01,
          4.0818e+00,  1.6410e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:36:58] "POST /inference/generate HTTP/1.1" 200 15310

init_state None

state-> tensor([[-6.4377e-02,  4.7917e-02,  2.6871e-01,  ...,  1.3352e-01,
         -1.6055e-02,  9.9722e-02],
        [-1.2269e-01,  8.1364e-02,  3.0996e-02,  ...,  8.6957e-01,
         -5.3969e-02,  1.1749e-01],
        [-2.3210e-01, -4.9736e+00,  1.4087e+00,  ..., -1.3848e+00,
          5.5972e-01, -5.0067e-01],
        ...,
        [-7.1981e+00, -2.1831e+01,  2.1388e+01,  ..., -2.2678e+00,
         -3.0662e-01, -5.9969e+00],
        [ 1.9885e+00,  1.7662e+01,  7.7767e+00,  ...,  1.7076e+00,
          1.0014e+00,  1.5840e+00],
        [ 1.5539e+00,  6.6247e+00,  8.2572e+00,  ...,  1.5086e+01,
          4.0818e+00,  1.6410e-01]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:37:09] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master \n  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '   \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '\n  ', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None
:   

---after-> tensor([[ 2.0042e-02, -1.8569e-03,  2.2797e-01,  ...,  4.2520e-03,
          7.2921e-02,  1.1762e-01],
        [ 1.5609e-02, -3.0405e-02,  3.5208e-02,  ...,  6.4558e-01,
          4.6649e-02,  6.4501e-02],
        [-1.5622e+00, -3.6952e+00,  1.5407e+00,  ..., -2.1713e+00,
          8.1159e-01, -1.1131e+00],
        ...,
        [-1.1811e+01, -3.7713e+00,  3.7364e+01,  ...,  3.3505e+00,
          1.1871e+00, -9.4323e+00],
        [ 1.9456e+00,  1.0081e+01,  1.6321e+01,  ...,  1.7027e+00,
          1.0583e+00,  2.0984e+00],
        [-6.9324e-01,  7.2510e+00,  7.4703e+00,  ...,  1.3888e+01,
          2.1529e+00, -9.8149e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:37:43] "POST /inference/generate HTTP/1.1" 200 14054

init_state None

state-> tensor([[ 2.0042e-02, -1.8569e-03,  2.2797e-01,  ...,  4.2520e-03,
          7.2921e-02,  1.1762e-01],
        [ 1.5609e-02, -3.0405e-02,  3.5208e-02,  ...,  6.4558e-01,
          4.6649e-02,  6.4501e-02],
        [-1.5622e+00, -3.6952e+00,  1.5407e+00,  ..., -2.1713e+00,
          8.1159e-01, -1.1131e+00],
        ...,
        [-1.1811e+01, -3.7713e+00,  3.7364e+01,  ...,  3.3505e+00,
          1.1871e+00, -9.4323e+00],
        [ 1.9456e+00,  1.0081e+01,  1.6321e+01,  ...,  1.7027e+00,
          1.0583e+00,  2.0984e+00],
        [-6.9324e-01,  7.2510e+00,  7.4703e+00,  ...,  1.3888e+01,
          2.1529e+00, -9.8149e-01]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:38:16] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master \n  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '   \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '\n  ', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None
  

---after-> tensor([[-5.5661e-02,  5.6304e-02,  2.7340e-01,  ...,  1.3664e-01,
         -2.0895e-02,  1.2484e-01],
        [-1.2269e-01,  8.1364e-02,  3.0996e-02,  ...,  8.6957e-01,
         -5.3969e-02,  1.1749e-01],
        [-4.9882e-01, -3.6675e+00,  1.6620e+00,  ..., -1.4708e+00,
          6.6932e-01, -5.2749e-01],
        ...,
        [ 1.2816e+00, -6.9718e-01,  4.1079e+01,  ...,  1.9973e+00,
         -3.2329e-01, -5.3444e+00],
        [ 1.0901e+00,  1.2819e+01,  2.0087e+01,  ...,  1.4385e+00,
          1.0091e+00,  1.6583e+00],
        [ 1.9718e+00,  7.0715e+00,  6.9192e+00,  ...,  1.4518e+01,
          3.1974e+00,  8.9875e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:38:55] "POST /inference/generate HTTP/1.1" 200 16030

init_state None

state-> tensor([[-5.5661e-02,  5.6304e-02,  2.7340e-01,  ...,  1.3664e-01,
         -2.0895e-02,  1.2484e-01],
        [-1.2269e-01,  8.1364e-02,  3.0996e-02,  ...,  8.6957e-01,
         -5.3969e-02,  1.1749e-01],
        [-4.9882e-01, -3.6675e+00,  1.6620e+00,  ..., -1.4708e+00,
          6.6932e-01, -5.2749e-01],
        ...,
        [ 1.2816e+00, -6.9718e-01,  4.1079e+01,  ...,  1.9973e+00,
         -3.2329e-01, -5.3444e+00],
        [ 1.0901e+00,  1.2819e+01,  2.0087e+01,  ...,  1.4385e+00,
          1.0091e+00,  1.6583e+00],
        [ 1.9718e+00,  7.0715e+00,  6.9192e+00,  ...,  1.4518e+01,
          3.1974e+00,  8.9875e-01]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:39:09] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master \n  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master   \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '   \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '\n  ', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master:   \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None
:  

---after-> tensor([[-5.4182e-02,  5.6848e-02,  2.7430e-01,  ...,  1.3771e-01,
         -2.0477e-02,  1.2414e-01],
        [-1.2269e-01,  8.1364e-02,  3.0996e-02,  ...,  8.6957e-01,
         -5.3969e-02,  1.1749e-01],
        [-7.3520e-01, -3.6489e+00,  1.4950e+00,  ..., -1.4708e+00,
          6.6932e-01, -5.2749e-01],
        ...,
        [-1.0757e+00, -8.3453e+00,  6.7300e+01,  ...,  2.6579e+00,
         -2.9971e+00, -3.3639e+00],
        [ 1.3370e+00,  1.6189e+01,  2.7115e+01,  ...,  1.1705e+00,
          1.0090e+00,  1.5042e+00],
        [ 7.6954e-01,  6.5934e+00,  6.3284e+00,  ...,  1.3945e+01,
          2.0919e+00, -3.4136e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:39:49] "POST /inference/generate HTTP/1.1" 200 17192

init_state None

state-> tensor([[-5.4182e-02,  5.6848e-02,  2.7430e-01,  ...,  1.3771e-01,
         -2.0477e-02,  1.2414e-01],
        [-1.2269e-01,  8.1364e-02,  3.0996e-02,  ...,  8.6957e-01,
         -5.3969e-02,  1.1749e-01],
        [-7.3520e-01, -3.6489e+00,  1.4950e+00,  ..., -1.4708e+00,
          6.6932e-01, -5.2749e-01],
        ...,
        [-1.0757e+00, -8.3453e+00,  6.7300e+01,  ...,  2.6579e+00,
         -2.9971e+00, -3.3639e+00],
        [ 1.3370e+00,  1.6189e+01,  2.7115e+01,  ...,  1.1705e+00,
          1.0090e+00,  1.5042e+00],
        [ 7.6954e-01,  6.5934e+00,  6.3284e+00,  ...,  1.3945e+01,
          2.0919e+00, -3.4136e-01]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:42:21] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master \n  \n ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None
 

---after-> tensor([[ -0.0663,   0.0484,   0.2652,  ...,   0.1370,  -0.0227,   0.1045],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [ -1.7113,  -4.0890,   1.0455,  ...,  -1.4431,   0.8897,  -0.7740],
        ...,
        [ -6.8687, -19.9427,  21.2937,  ...,  -0.8404,  -0.4023,  -6.6840],
        [  1.7250,   8.0599,   4.8952,  ...,   1.2486,   1.0035,   1.1886],
        [ -1.5731,   8.3049,   9.1097,  ...,  16.4273,   2.8669,   3.9995]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:42:34] "POST /inference/generate HTTP/1.1" 200 4777

init_state None

state-> tensor([[ -0.0663,   0.0484,   0.2652,  ...,   0.1370,  -0.0227,   0.1045],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [ -1.7113,  -4.0890,   1.0455,  ...,  -1.4431,   0.8897,  -0.7740],
        ...,
        [ -6.8687, -19.9427,  21.2937,  ...,  -0.8404,  -0.4023,  -6.6840],
        [  1.7250,   8.0599,   4.8952,  ...,   1.2486,   1.0035,   1.1886],
        [ -1.5731,   8.3049,   9.1097,  ...,  16.4273,   2.8669,   3.9995]],
       device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:43:03] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master \n  \n ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None

  

---after-> tensor([[-6.0129e-02,  5.0273e-02,  2.6556e-01,  ...,  1.3811e-01,
         -2.0388e-02,  1.1042e-01],
        [-1.2269e-01,  8.1364e-02,  3.0996e-02,  ...,  8.6957e-01,
         -5.3969e-02,  1.1749e-01],
        [-3.7487e+00, -2.8932e+00,  1.0137e+00,  ..., -1.3379e+00,
          7.2332e-01, -4.7225e-01],
        ...,
        [-3.6031e+00, -3.2026e+01,  4.6149e+01,  ...,  3.6653e+00,
          7.9202e-02, -1.1778e+01],
        [ 1.1051e+00,  1.5415e+01,  1.1733e+01,  ...,  1.5049e+00,
          1.0002e+00,  1.4492e+00],
        [ 7.3784e-01,  7.1367e+00,  7.9110e+00,  ...,  1.3605e+01,
          3.5739e+00,  9.7166e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:43:19] "POST /inference/generate HTTP/1.1" 200 6222

---- {'role': 'request', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-6.0129e-02,  5.0273e-02,  2.6556e-01,  ...,  1.3811e-01,
         -2.0388e-02,  1.1042e-01],
        [-1.2269e-01,  8.1364e-02,  3.0996e-02,  ...,  8.6957e-01,
         -5.3969e-02,  1.1749e-01],
        [-3.7487e+00, -2.8932e+00,  1.0137e+00,  ..., -1.3379e+00,
          7.2332e-01, -4.7225e-01],
        ...,
        [-3.6031e+00, -3.2026e+01,  4.6149e+01,  ...,  3.6653e+00,
          7.9202e-02, -1.1778e+01],
        [ 1.1051e+00,  1.5415e+01,  1.1733e+01,  ...,  1.5049e+00,
          1.0002e+00,  1.4492e+00],
        [ 7.3784e-01,  7.1367e+00,  7.9110e+00,  ...,  1.3605e+01,
          3.5739e+00,  9.7166e-01]], device='cuda:0')
  

---after-> tensor([[-5.7033e-02,  5.4816e-02,  2.7301e-01,  ...,  1.4126e-01,
         -2.0582e-02,  1.1704e-01],
        [-1.2269e-01,  8.1364e-02,  3.0996e-02,  ...,  8.6957e-01,
         -5.3969e-02,  1.1749e-01],
        [ 4.4379e-01, -4.9699e+00,  9.3126e-01,  ..., -1.4390e+00,
          7.8500e-01, -4.9207e-01],
        ...,
        [-2.6676e+00, -2.0992e+01,  3.5192e+01,  ...,  2.3836e+00,
          1.6992e+00, -1.3597e+01],
        [ 1.1411e+00,  1.2594e+01,  7.5320e+00,  ...,  1.5899e+00,
          1.0023e+00,  1.5794e+00],
        [ 2.3936e+00,  6.9096e+00,  8.3043e+00,  ...,  1.3138e+01,
          4.2897e+00,  2.9898e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:44:14] "POST /inference/generate HTTP/1.1" 200 1649

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-5.7033e-02,  5.4816e-02,  2.7301e-01,  ...,  1.4126e-01,
         -2.0582e-02,  1.1704e-01],
        [-1.2269e-01,  8.1364e-02,  3.0996e-02,  ...,  8.6957e-01,
         -5.3969e-02,  1.1749e-01],
        [ 4.4379e-01, -4.9699e+00,  9.3126e-01,  ..., -1.4390e+00,
          7.8500e-01, -4.9207e-01],
        ...,
        [-2.6676e+00, -2.0992e+01,  3.5192e+01,  ...,  2.3836e+00,
          1.6992e+00, -1.3597e+01],
        [ 1.1411e+00,  1.2594e+01,  7.5320e+00,  ...,  1.5899e+00,
          1.0023e+00,  1.5794e+00],
        [ 2.3936e+00,  6.9096e+00,  8.3043e+00,  ...,  1.3138e+01,
          4.2897e+00,  2.9898e-01]], device='cuda:0')
  
---after-> tensor([[ 1.5808e-02,  1.5409e-02,  3.4538e-01,  ..., -7.0929e-02,
          4.0578e-02, -5.6441e-03],
        [-1.3603e-01,  9.1968e-02,  8.6589e-02,  ...,  5.0016e-01,
         -6.7169e-03, -1.3363e-01],
        [-2.1911e-01, -8.4231e+00,  1.0398e+00,  ..., -1.2628e+00,
         -1.8278e-01, -6.0325e-01],
        ...,
        [-7.8222e+00, -3.1040e+01,  4.4211e+01,  ...,  3.4434e+00,
          2.1298e+00, -4.7575e+00],
        [ 1.0808e+00,  1.8116e+01,  9.6496e+00,  ...,  1.1167e+00,
          1.0030e+00,  1.2301e+00],
        [ 2.8434e+00,  6.1217e+00,  8.3318e+00,  ...,  1.5093e+01,
          3.8161e+00,  1.3032e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:44:30] "POST /inference/generate HTTP/1.1" 200 896

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[ 1.5808e-02,  1.5409e-02,  3.4538e-01,  ..., -7.0929e-02,
          4.0578e-02, -5.6441e-03],
        [-1.3603e-01,  9.1968e-02,  8.6589e-02,  ...,  5.0016e-01,
         -6.7169e-03, -1.3363e-01],
        [-2.1911e-01, -8.4231e+00,  1.0398e+00,  ..., -1.2628e+00,
         -1.8278e-01, -6.0325e-01],
        ...,
        [-7.8222e+00, -3.1040e+01,  4.4211e+01,  ...,  3.4434e+00,
          2.1298e+00, -4.7575e+00],
        [ 1.0808e+00,  1.8116e+01,  9.6496e+00,  ...,  1.1167e+00,
          1.0030e+00,  1.2301e+00],
        [ 2.8434e+00,  6.1217e+00,  8.3318e+00,  ...,  1.5093e+01,
          3.8161e+00,  1.3032e+00]], device='cuda:0')
 

---after-> tensor([[ 2.3980e-02,  1.1859e-03,  2.2957e-01,  ...,  8.8339e-03,
          7.2604e-02,  1.1477e-01],
        [ 1.5609e-02, -3.0405e-02,  3.5208e-02,  ...,  6.4558e-01,
          4.6649e-02,  6.4501e-02],
        [-1.6499e+00, -4.7037e+00,  5.3981e-01,  ..., -2.1713e+00,
          8.1159e-01, -1.1131e+00],
        ...,
        [-1.5390e+00, -6.6335e+00,  5.4603e+00,  ...,  1.0170e+00,
          1.3160e+00, -1.6508e+00],
        [ 1.2335e+00,  3.5424e+00,  3.6741e+00,  ...,  1.3406e+00,
          1.0479e+00,  1.2029e+00],
        [-1.9106e-02,  8.9757e+00,  9.4082e+00,  ...,  1.3399e+01,
          3.2631e+00,  2.1649e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:45:17] "POST /inference/generate HTTP/1.1" 200 1874

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[ 2.3980e-02,  1.1859e-03,  2.2957e-01,  ...,  8.8339e-03,
          7.2604e-02,  1.1477e-01],
        [ 1.5609e-02, -3.0405e-02,  3.5208e-02,  ...,  6.4558e-01,
          4.6649e-02,  6.4501e-02],
        [-1.6499e+00, -4.7037e+00,  5.3981e-01,  ..., -2.1713e+00,
          8.1159e-01, -1.1131e+00],
        ...,
        [-1.5390e+00, -6.6335e+00,  5.4603e+00,  ...,  1.0170e+00,
          1.3160e+00, -1.6508e+00],
        [ 1.2335e+00,  3.5424e+00,  3.6741e+00,  ...,  1.3406e+00,
          1.0479e+00,  1.2029e+00],
        [-1.9106e-02,  8.9757e+00,  9.4082e+00,  ...,  1.3399e+01,
          3.2631e+00,  2.1649e+00]], device='cuda:0')
  

---after-> tensor([[-0.0513,  0.0537,  0.2657,  ...,  0.1338, -0.0206,  0.1175],
        [-0.1227,  0.0814,  0.0310,  ...,  0.8696, -0.0540,  0.1175],
        [-0.9555, -6.0816,  0.6378,  ..., -1.4129,  0.5698, -0.5126],
        ...,
        [-2.1396, -6.9873,  6.8561,  ..., -3.3644,  1.6742, -1.8492],
        [ 1.5527,  4.3771,  4.3621,  ...,  1.0397,  1.0256,  1.6010],
        [ 1.8783,  8.3675,  8.8020,  ..., 15.4444,  1.8962, -1.4913]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:45:52] "POST /inference/generate HTTP/1.1" 200 820

---- {'role': 'request', 'text': 'master: master\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-0.0513,  0.0537,  0.2657,  ...,  0.1338, -0.0206,  0.1175],
        [-0.1227,  0.0814,  0.0310,  ...,  0.8696, -0.0540,  0.1175],
        [-0.9555, -6.0816,  0.6378,  ..., -1.4129,  0.5698, -0.5126],
        ...,
        [-2.1396, -6.9873,  6.8561,  ..., -3.3644,  1.6742, -1.8492],
        [ 1.5527,  4.3771,  4.3621,  ...,  1.0397,  1.0256,  1.6010],
        [ 1.8783,  8.3675,  8.8020,  ..., 15.4444,  1.8962, -1.4913]],
       device='cuda:0')
  

---after-> tensor([[ -0.0549,   0.0549,   0.2671,  ...,   0.1364,  -0.0206,   0.1085],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [  0.0715,  -8.5919,   1.0478,  ...,  -1.5615,   0.7625,  -0.6002],
        ...,
        [ -0.1078, -12.5198,   2.5184,  ...,   2.2111,   0.2197,  -0.5603],
        [  1.6003,   8.3038,   3.5458,  ...,   1.8024,   1.0013,   1.4974],
        [  0.7137,   7.0129,   8.9246,  ...,  13.9116,   3.1383,   0.1733]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:46:47] "POST /inference/generate HTTP/1.1" 200 1883

init_state None

state-> tensor([[ -0.0549,   0.0549,   0.2671,  ...,   0.1364,  -0.0206,   0.1085],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [  0.0715,  -8.5919,   1.0478,  ...,  -1.5615,   0.7625,  -0.6002],
        ...,
        [ -0.1078, -12.5198,   2.5184,  ...,   2.2111,   0.2197,  -0.5603],
        [  1.6003,   8.3038,   3.5458,  ...,   1.8024,   1.0013,   1.4974],
        [  0.7137,   7.0129,   8.9246,  ...,  13.9116,   3.1383,   0.1733]],
       device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:47:48] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master \n  \n ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n  ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': ' \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master: master\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None
  

---after-> tensor([[-0.0562,  0.0535,  0.2587,  ...,  0.1322, -0.0255,  0.1139],
        [-0.1227,  0.0814,  0.0310,  ...,  0.8696, -0.0540,  0.1175],
        [-2.3916, -8.9165,  1.4797,  ..., -1.2954,  0.5359, -0.5750],
        ...,
        [-1.9716, -5.4604, 13.9018,  ...,  4.0433,  2.0911, -7.7841],
        [ 1.5213,  7.7122,  7.7337,  ...,  1.5774,  1.0019,  1.1491],
        [ 1.0561,  6.8941,  7.6989,  ..., 13.7586,  3.5242,  0.5449]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:48:28] "POST /inference/generate HTTP/1.1" 200 16119

init_state None

state-> tensor([[-0.0562,  0.0535,  0.2587,  ...,  0.1322, -0.0255,  0.1139],
        [-0.1227,  0.0814,  0.0310,  ...,  0.8696, -0.0540,  0.1175],
        [-2.3916, -8.9165,  1.4797,  ..., -1.2954,  0.5359, -0.5750],
        ...,
        [-1.9716, -5.4604, 13.9018,  ...,  4.0433,  2.0911, -7.7841],
        [ 1.5213,  7.7122,  7.7337,  ...,  1.5774,  1.0019,  1.1491],
        [ 1.0561,  6.8941,  7.6989,  ..., 13.7586,  3.5242,  0.5449]],
       device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 13:50:27] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master \n  \n ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n  ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': ' \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master: master\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None
   

---after-> tensor([[ -0.0540,   0.0575,   0.2694,  ...,   0.1360,  -0.0208,   0.1043],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [ -0.8255,  -3.9861,   1.5754,  ...,  -1.3948,   0.9568,  -0.5355],
        ...,
        [ -3.2235,   4.9987,  10.6835,  ...,   1.7876,   5.7620, -15.0164],
        [  1.2322,   9.8192,   4.1797,  ...,   1.5352,   1.0032,   2.7382],
        [  1.0995,   7.5806,   8.5190,  ...,  13.1584,   3.7780,  -1.7387]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 13:51:10] "POST /inference/generate HTTP/1.1" 200 18019
[2023-10-04 14:12:55,826] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 128739
[2023-10-04 14:12:55,828] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-04 14:13:25,728] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 14:13:29,290] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-04 14:13:29,313] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-04 14:13:31,244] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 14:13:32,576] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-04 14:13:32,576] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-04 14:13:32,576] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-04 14:13:32,576] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-04 14:13:32,576] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-04 14:13:34,498] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.419663190841675 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-04 14:14:28,665] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-04 14:14:28,665] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-04 14:14:28,665] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-04 14:14:30,811] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-04 14:14:30,813] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-04 14:14:30,813] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-04 14:14:30,854] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-04 14:14:30,854] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-04 14:14:30,854] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-10-04 14:14:30,854] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-04 14:14:30,854] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-04 14:14:30,854] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-04 14:14:30,854] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-10-04 14:14:52,141] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-04 14:14:52,142] [INFO] [utils.py:804:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-10-04 14:14:52,142] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 25.22 GB, percent = 6.7%
[2023-10-04 14:15:06,917] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-04 14:15:06,918] [INFO] [utils.py:804:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-10-04 14:15:06,919] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 64.47 GB, percent = 17.1%
[2023-10-04 14:15:06,919] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-04 14:15:08,066] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-04 14:15:08,067] [INFO] [utils.py:804:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-10-04 14:15:08,067] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 64.47 GB, percent = 17.1%
[2023-10-04 14:15:08,101] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-04 14:15:08,101] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-04 14:15:08,101] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fd86b5bbbe0>
[2023-10-04 14:15:08,101] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-04 14:15:08,102] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-04 14:15:08,103] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-04 14:15:08,103] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-04 14:15:08,103] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-04 14:15:08,103] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-04 14:15:08,103] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-04 14:15:08,103] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-04 14:15:08,103] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-04 14:15:08,103] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-04 14:15:08,103] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-04 14:15:08,103] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd836f18ee0>
[2023-10-04 14:15:08,103] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-04 14:15:08,103] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-04 14:15:08,103] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-04 14:15:08,103] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-04 14:15:08,103] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-04 14:15:08,103] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-04 14:15:08,103] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... None
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   fp16_auto_cast ............... None
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   fp16_enabled ................. False
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-04 14:15:08,104] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-04 14:15:08,105] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-04 14:15:08,106] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-04 14:15:08,106] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-04 14:15:08,106] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-04 14:15:08,106] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-04 14:15:08,106] [INFO] [config.py:957:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.


init_state None

state-> None

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 14:21:50] "POST /state/reset HTTP/1.1" 200 21
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 21061.45it/s]
  0%|          | 0/582 [00:00<?, ?it/s]100%|| 582/582 [00:00<00:00, 187300.31it/s]

---- {'role': 'system', 'text': ' \nmaster master \n  \n ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n  ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': ' \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master: master\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None
   

---after-> tensor([[ -0.0559,   0.0538,   0.2648,  ...,   0.1317,  -0.0190,   0.1026],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [ -0.6843,  -6.2768,   1.5959,  ...,  -1.2547,   0.7635,  -0.3891],
        ...,
        [-11.7386,   0.4313,   5.8534,  ...,  -6.5422,   5.6109,  -5.9951],
        [  1.8716,   6.8575,   4.0332,  ...,   2.0142,   1.0013,   1.4026],
        [  1.0081,   7.8188,   8.7823,  ...,  13.2970,   4.5896,  -0.9816]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 14:22:32] "POST /inference/generate HTTP/1.1" 200 18049

init_state None

state-> tensor([[ -0.0559,   0.0538,   0.2648,  ...,   0.1317,  -0.0190,   0.1026],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [ -0.6843,  -6.2768,   1.5959,  ...,  -1.2547,   0.7635,  -0.3891],
        ...,
        [-11.7386,   0.4313,   5.8534,  ...,  -6.5422,   5.6109,  -5.9951],
        [  1.8716,   6.8575,   4.0332,  ...,   2.0142,   1.0013,   1.4026],
        [  1.0081,   7.8188,   8.7823,  ...,  13.2970,   4.5896,  -0.9816]],
       device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 14:22:45] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master \n  \n ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n  ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': ' \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master: master\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '   \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None
   

---after-> tensor([[-0.0563,  0.0596,  0.2739,  ...,  0.1406, -0.0212,  0.1074],
        [-0.1227,  0.0814,  0.0310,  ...,  0.8696, -0.0540,  0.1175],
        [-1.1460, -7.4177,  1.5296,  ..., -1.5492,  0.8091, -0.4412],
        ...,
        [ 1.9311,  2.3586,  8.7980,  ..., -0.6918,  4.2400, -5.4511],
        [ 1.0203,  8.7214,  5.4290,  ...,  2.0026,  1.0012,  1.6033],
        [ 1.6025,  7.4142,  8.1347,  ..., 12.8287,  4.0688, -1.0664]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 14:23:31] "POST /inference/generate HTTP/1.1" 200 19121

init_state None

state-> tensor([[-0.0563,  0.0596,  0.2739,  ...,  0.1406, -0.0212,  0.1074],
        [-0.1227,  0.0814,  0.0310,  ...,  0.8696, -0.0540,  0.1175],
        [-1.1460, -7.4177,  1.5296,  ..., -1.5492,  0.8091, -0.4412],
        ...,
        [ 1.9311,  2.3586,  8.7980,  ..., -0.6918,  4.2400, -5.4511],
        [ 1.0203,  8.7214,  5.4290,  ...,  2.0026,  1.0012,  1.6033],
        [ 1.6025,  7.4142,  8.1347,  ..., 12.8287,  4.0688, -1.0664]],
       device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 14:24:02] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master \n  \n ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n  ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': ' \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master: master\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None
master 

---after-> tensor([[-5.4759e-02,  5.0502e-02,  2.6817e-01,  ...,  1.3554e-01,
         -2.0789e-02,  1.0074e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-1.7508e+00, -6.1166e+00,  1.3108e+00,  ..., -1.1656e+00,
          5.3320e-01, -4.7618e-01],
        ...,
        [-4.9257e-01, -9.3287e+00,  3.1941e+01,  ..., -5.5556e-01,
          2.7978e+00, -1.1225e+01],
        [ 1.4873e+00,  7.1477e+00,  1.0234e+01,  ...,  1.1448e+00,
          1.0033e+00,  1.0607e+00],
        [ 2.1022e+00,  7.2294e+00,  7.6275e+00,  ...,  1.4319e+01,
          1.7485e+00,  1.3704e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 14:24:44] "POST /inference/generate HTTP/1.1" 200 17354

init_state None

state-> tensor([[-5.4759e-02,  5.0502e-02,  2.6817e-01,  ...,  1.3554e-01,
         -2.0789e-02,  1.0074e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-1.7508e+00, -6.1166e+00,  1.3108e+00,  ..., -1.1656e+00,
          5.3320e-01, -4.7618e-01],
        ...,
        [-4.9257e-01, -9.3287e+00,  3.1941e+01,  ..., -5.5556e-01,
          2.7978e+00, -1.1225e+01],
        [ 1.4873e+00,  7.1477e+00,  1.0234e+01,  ...,  1.1448e+00,
          1.0033e+00,  1.0607e+00],
        [ 2.1022e+00,  7.2294e+00,  7.6275e+00,  ...,  1.4319e+01,
          1.7485e+00,  1.3704e-01]], device='cuda:0')

====after reset======

init_state None

state-> None
172.16.1.39 - - [04/Oct/2023 14:25:07] "POST /state/reset HTTP/1.1" 200 21

---- {'role': 'system', 'text': ' \nmaster master \n  \n ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n  ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': ' \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master: master\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'master \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 65530, 65531, 65532, 65533, 65534], 'token_count': 1024, 'over': False}

---begin-> None
  

---after-> tensor([[ -0.0536,   0.0515,   0.2656,  ...,   0.1326,  -0.0268,   0.1104],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [ -1.0906,  -4.0018,   1.2866,  ...,  -1.2303,   0.7437,  -0.5708],
        ...,
        [ -3.2798, -17.7606,  24.1848,  ...,  -3.3754,   4.0190,  -8.0908],
        [  1.7122,  12.3870,   7.9320,  ...,   1.2971,   1.0139,   1.1896],
        [  1.0750,   6.5935,   8.1134,  ...,  14.4406,   2.9609,   1.1190]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 14:25:52] "POST /inference/generate HTTP/1.1" 200 18466

---- {'role': 'response', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[ -0.0536,   0.0515,   0.2656,  ...,   0.1326,  -0.0268,   0.1104],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [ -1.0906,  -4.0018,   1.2866,  ...,  -1.2303,   0.7437,  -0.5708],
        ...,
        [ -3.2798, -17.7606,  24.1848,  ...,  -3.3754,   4.0190,  -8.0908],
        [  1.7122,  12.3870,   7.9320,  ...,   1.2971,   1.0139,   1.1896],
        [  1.0750,   6.5935,   8.1134,  ...,  14.4406,   2.9609,   1.1190]],
       device='cuda:0')
master 

---after-> tensor([[-5.3589e-02,  5.1867e-02,  2.6850e-01,  ...,  1.3554e-01,
         -2.0516e-02,  1.0173e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-1.4933e+00, -6.2344e+00,  1.1209e+00,  ..., -1.1656e+00,
          5.3320e-01, -4.7618e-01],
        ...,
        [ 2.8875e+00, -2.2831e+01,  4.0068e+01,  ..., -5.1567e-01,
          6.7660e-01, -1.3373e+01],
        [ 1.9784e+00,  1.2548e+01,  1.2744e+01,  ...,  1.1620e+00,
          1.0029e+00,  1.1723e+00],
        [ 1.8114e+00,  6.5127e+00,  7.2554e+00,  ...,  1.4154e+01,
          2.9788e+00,  7.8023e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 14:26:08] "POST /inference/generate HTTP/1.1" 200 1880

---- {'role': 'system', 'text': ' \nmaster master \n  \n ', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'system \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master\n  ', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master:  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': ' \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'request', 'text': 'master: master\n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '  \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': 'master \n', 'over': True, 'token_count': 0}

---- {'role': 'response', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-5.3589e-02,  5.1867e-02,  2.6850e-01,  ...,  1.3554e-01,
         -2.0516e-02,  1.0173e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-1.4933e+00, -6.2344e+00,  1.1209e+00,  ..., -1.1656e+00,
          5.3320e-01, -4.7618e-01],
        ...,
        [ 2.8875e+00, -2.2831e+01,  4.0068e+01,  ..., -5.1567e-01,
          6.7660e-01, -1.3373e+01],
        [ 1.9784e+00,  1.2548e+01,  1.2744e+01,  ...,  1.1620e+00,
          1.0029e+00,  1.1723e+00],
        [ 1.8114e+00,  6.5127e+00,  7.2554e+00,  ...,  1.4154e+01,
          2.9788e+00,  7.8023e-01]], device='cuda:0')
master  3

---after-> tensor([[ 2.3686e-02, -1.4437e-02,  2.1072e-01,  ...,  1.0563e-02,
          8.4096e-02,  1.1761e-01],
        [ 1.5602e-02, -3.0412e-02,  3.5211e-02,  ...,  6.4558e-01,
          4.6644e-02,  6.4495e-02],
        [-1.0127e+00, -5.3942e+00,  1.3314e+00,  ..., -1.9324e+00,
          8.1132e-01, -9.6863e-01],
        ...,
        [-7.0843e+00, -1.7258e+01,  2.5624e+01,  ..., -2.5217e+00,
          4.6455e+00, -1.3845e+01],
        [ 1.2713e+00,  8.2694e+00,  7.4730e+00,  ...,  1.1889e+00,
          1.3727e+00,  1.4526e+00],
        [ 3.4109e-01,  6.1864e+00,  7.0025e+00,  ...,  1.5078e+01,
         -2.5437e+00, -5.7711e-01]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 14:27:12] "POST /inference/generate HTTP/1.1" 200 19418

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[ 2.3686e-02, -1.4437e-02,  2.1072e-01,  ...,  1.0563e-02,
          8.4096e-02,  1.1761e-01],
        [ 1.5602e-02, -3.0412e-02,  3.5211e-02,  ...,  6.4558e-01,
          4.6644e-02,  6.4495e-02],
        [-1.0127e+00, -5.3942e+00,  1.3314e+00,  ..., -1.9324e+00,
          8.1132e-01, -9.6863e-01],
        ...,
        [-7.0843e+00, -1.7258e+01,  2.5624e+01,  ..., -2.5217e+00,
          4.6455e+00, -1.3845e+01],
        [ 1.2713e+00,  8.2694e+00,  7.4730e+00,  ...,  1.1889e+00,
          1.3727e+00,  1.4526e+00],
        [ 3.4109e-01,  6.1864e+00,  7.0025e+00,  ...,  1.5078e+01,
         -2.5437e+00, -5.7711e-01]], device='cuda:0')
  

---after-> tensor([[ -0.0512,   0.0532,   0.2631,  ...,   0.1277,  -0.0250,   0.1214],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [  0.0527,  -4.1525,   1.5344,  ...,  -1.2367,   0.6924,  -0.4219],
        ...,
        [ -3.1579, -21.1457,   5.7229,  ...,   0.4068,   2.5997,  -7.5911],
        [  1.2134,  10.9325,   1.2469,  ...,   1.0862,   1.0039,   1.1356],
        [  0.3161,   5.8132,  10.3858,  ...,  13.8575,   3.5680,   0.8502]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 14:27:25] "POST /inference/generate HTTP/1.1" 200 718

---- {'role': 'response', 'text': 'master  \n', 'over': True, 'token_count': 0}

---- {'role': 'analysis', 'text': '', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[ -0.0512,   0.0532,   0.2631,  ...,   0.1277,  -0.0250,   0.1214],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [  0.0527,  -4.1525,   1.5344,  ...,  -1.2367,   0.6924,  -0.4219],
        ...,
        [ -3.1579, -21.1457,   5.7229,  ...,   0.4068,   2.5997,  -7.5911],
        [  1.2134,  10.9325,   1.2469,  ...,   1.0862,   1.0039,   1.1356],
        [  0.3161,   5.8132,  10.3858,  ...,  13.8575,   3.5680,   0.8502]],
       device='cuda:0')
master 3 

---after-> tensor([[-5.4568e-02,  5.3669e-02,  2.5232e-01,  ...,  1.3405e-01,
         -2.3540e-02,  1.1131e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-1.2821e+00, -4.8778e+00,  1.2306e+00,  ..., -1.6028e+00,
          4.7686e-01, -4.9775e-01],
        ...,
        [-2.9290e-01, -2.5790e+01,  7.8017e+00,  ..., -3.1507e+00,
          5.7553e+00, -1.1906e+01],
        [ 1.0313e+00,  1.2073e+01,  1.7422e+00,  ...,  1.5364e+00,
          1.0016e+00,  1.5065e+00],
        [ 1.9438e+00,  5.4963e+00,  9.2009e+00,  ...,  1.3430e+01,
          4.1392e+00,  2.0889e+00]], device='cuda:0')
172.16.1.39 - - [04/Oct/2023 14:27:35] "POST /inference/generate HTTP/1.1" 200 1697

---- {'role': 'response', 'text': 'master', 'token_ban': [], 'token_stop': [65535, 0, 65530], 'token_count': 1024, 'over': False}

---begin-> tensor([[-5.4568e-02,  5.3669e-02,  2.5232e-01,  ...,  1.3405e-01,
         -2.3540e-02,  1.1131e-01],
        [-1.2271e-01,  8.1360e-02,  3.0999e-02,  ...,  8.6958e-01,
         -5.3965e-02,  1.1748e-01],
        [-1.2821e+00, -4.8778e+00,  1.2306e+00,  ..., -1.6028e+00,
          4.7686e-01, -4.9775e-01],
        ...,
        [-2.9290e-01, -2.5790e+01,  7.8017e+00,  ..., -3.1507e+00,
          5.7553e+00, -1.1906e+01],
        [ 1.0313e+00,  1.2073e+01,  1.7422e+00,  ...,  1.5364e+00,
          1.0016e+00,  1.5065e+00],
        [ 1.9438e+00,  5.4963e+00,  9.2009e+00,  ...,  1.3430e+01,
          4.1392e+00,  2.0889e+00]], device='cuda:0')
   

---after-> tensor([[ -0.0540,   0.0522,   0.2636,  ...,   0.1319,  -0.0263,   0.0995],
        [ -0.1227,   0.0814,   0.0310,  ...,   0.8696,  -0.0540,   0.1175],
        [ -3.7968,  -4.5336,   1.3280,  ...,  -1.3948,   0.9565,  -0.5359],
        ...,
        [ -2.6041, -14.5025,  11.9437,  ...,   0.5851,  -0.9691, -12.4090],
        [  1.0467,   6.5758,   3.3539,  ...,   1.3801,   1.0076,   1.8714],
        [  1.3469,   6.3313,   8.1123,  ...,  13.3435,   2.3372,  -0.4343]],
       device='cuda:0')
172.16.1.39 - - [04/Oct/2023 14:27:43] "POST /inference/generate HTTP/1.1" 200 1025
[2023-10-04 14:36:33,104] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 133460
[2023-10-04 14:36:33,105] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
[2023-10-04 14:36:44,559] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 14:36:45,963] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-04 14:36:45,987] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-04 14:36:47,931] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 14:36:49,257] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-04 14:36:49,257] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-04 14:36:49,257] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-04 14:36:49,257] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-04 14:36:49,257] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-04 14:36:51,141] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Traceback (most recent call last):
  File "/home/neromous/RWKV-Ouroboros/app.py", line 60, in <module>
    model = RWKV(load_model=config['model_path'],
  File "/home/neromous/RWKV-Ouroboros/rwkv_model/model_lora.py", line 654, in __init__
    self.load_state_dict(model_weights)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2041, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for RWKV:
	size mismatch for emb.weight: copying a param with shape torch.Size([65536, 4096]) from checkpoint, the shape in current model is torch.Size([65536, 2560]).
	size mismatch for blocks.0.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.0.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.0.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.0.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.0.ln0.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.0.ln0.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.0.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.0.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.0.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.0.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.0.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.0.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.0.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.0.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.0.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.0.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.0.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.0.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.0.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.0.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.1.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.1.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.1.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.1.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.1.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.1.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.1.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.1.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.1.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.1.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.1.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.1.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.1.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.1.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.1.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.1.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.1.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.1.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.2.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.2.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.2.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.2.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.2.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.2.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.2.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.2.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.2.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.2.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.2.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.2.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.2.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.2.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.2.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.2.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.2.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.2.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.3.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.3.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.3.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.3.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.3.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.3.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.3.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.3.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.3.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.3.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.3.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.3.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.3.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.3.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.3.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.3.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.3.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.3.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.4.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.4.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.4.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.4.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.4.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.4.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.4.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.4.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.4.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.4.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.4.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.4.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.4.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.4.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.4.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.4.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.4.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.4.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.5.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.5.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.5.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.5.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.5.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.5.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.5.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.5.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.5.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.5.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.5.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.5.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.5.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.5.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.5.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.5.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.5.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.5.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.6.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.6.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.6.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.6.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.6.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.6.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.6.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.6.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.6.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.6.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.6.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.6.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.6.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.6.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.6.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.6.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.6.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.6.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.7.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.7.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.7.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.7.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.7.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.7.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.7.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.7.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.7.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.7.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.7.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.7.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.7.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.7.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.7.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.7.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.7.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.7.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.8.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.8.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.8.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.8.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.8.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.8.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.8.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.8.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.8.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.8.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.8.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.8.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.8.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.8.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.8.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.8.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.8.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.8.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.9.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.9.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.9.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.9.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.9.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.9.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.9.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.9.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.9.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.9.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.9.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.9.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.9.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.9.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.9.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.9.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.9.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.9.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.10.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.10.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.10.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.10.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.10.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.10.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.10.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.10.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.10.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.10.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.10.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.10.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.10.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.10.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.10.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.10.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.10.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.10.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.11.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.11.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.11.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.11.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.11.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.11.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.11.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.11.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.11.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.11.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.11.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.11.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.11.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.11.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.11.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.11.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.11.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.11.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.12.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.12.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.12.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.12.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.12.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.12.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.12.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.12.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.12.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.12.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.12.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.12.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.12.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.12.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.12.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.12.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.12.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.12.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.13.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.13.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.13.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.13.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.13.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.13.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.13.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.13.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.13.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.13.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.13.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.13.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.13.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.13.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.13.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.13.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.13.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.13.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.14.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.14.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.14.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.14.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.14.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.14.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.14.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.14.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.14.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.14.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.14.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.14.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.14.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.14.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.14.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.14.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.14.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.14.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.15.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.15.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.15.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.15.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.15.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.15.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.15.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.15.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.15.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.15.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.15.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.15.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.15.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.15.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.15.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.15.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.15.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.15.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.16.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.16.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.16.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.16.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.16.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.16.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.16.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.16.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.16.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.16.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.16.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.16.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.16.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.16.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.16.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.16.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.16.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.16.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.17.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.17.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.17.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.17.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.17.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.17.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.17.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.17.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.17.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.17.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.17.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.17.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.17.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.17.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.17.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.17.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.17.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.17.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.18.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.18.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.18.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.18.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.18.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.18.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.18.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.18.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.18.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.18.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.18.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.18.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.18.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.18.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.18.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.18.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.18.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.18.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.19.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.19.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.19.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.19.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.19.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.19.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.19.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.19.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.19.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.19.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.19.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.19.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.19.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.19.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.19.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.19.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.19.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.19.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.20.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.20.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.20.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.20.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.20.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.20.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.20.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.20.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.20.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.20.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.20.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.20.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.20.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.20.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.20.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.20.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.20.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.20.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.21.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.21.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.21.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.21.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.21.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.21.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.21.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.21.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.21.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.21.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.21.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.21.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.21.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.21.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.21.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.21.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.21.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.21.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.22.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.22.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.22.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.22.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.22.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.22.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.22.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.22.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.22.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.22.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.22.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.22.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.22.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.22.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.22.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.22.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.22.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.22.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.23.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.23.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.23.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.23.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.23.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.23.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.23.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.23.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.23.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.23.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.23.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.23.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.23.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.23.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.23.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.23.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.23.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.23.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.24.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.24.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.24.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.24.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.24.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.24.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.24.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.24.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.24.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.24.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.24.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.24.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.24.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.24.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.24.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.24.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.24.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.24.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.25.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.25.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.25.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.25.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.25.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.25.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.25.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.25.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.25.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.25.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.25.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.25.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.25.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.25.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.25.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.25.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.25.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.25.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.26.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.26.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.26.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.26.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.26.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.26.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.26.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.26.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.26.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.26.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.26.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.26.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.26.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.26.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.26.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.26.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.26.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.26.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.27.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.27.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.27.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.27.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.27.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.27.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.27.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.27.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.27.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.27.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.27.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.27.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.27.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.27.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.27.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.27.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.27.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.27.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.28.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.28.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.28.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.28.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.28.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.28.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.28.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.28.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.28.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.28.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.28.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.28.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.28.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.28.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.28.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.28.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.28.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.28.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.29.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.29.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.29.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.29.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.29.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.29.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.29.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.29.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.29.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.29.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.29.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.29.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.29.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.29.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.29.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.29.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.29.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.29.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.30.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.30.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.30.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.30.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.30.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.30.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.30.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.30.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.30.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.30.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.30.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.30.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.30.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.30.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.30.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.30.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.30.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.30.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for blocks.31.ln1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.31.ln1.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.31.ln2.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.31.ln2.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.31.att.time_decay: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.31.att.time_first: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for blocks.31.att.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.31.att.time_mix_v: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.31.att.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.31.att.key.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.31.att.value.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.31.att.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.31.att.output.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.31.ffn.time_mix_k: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.31.ffn.time_mix_r: copying a param with shape torch.Size([1, 1, 4096]) from checkpoint, the shape in current model is torch.Size([1, 1, 2560]).
	size mismatch for blocks.31.ffn.key.weight: copying a param with shape torch.Size([16384, 4096]) from checkpoint, the shape in current model is torch.Size([10240, 2560]).
	size mismatch for blocks.31.ffn.receptance.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([2560, 2560]).
	size mismatch for blocks.31.ffn.value.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([2560, 10240]).
	size mismatch for ln_out.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for ln_out.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2560]).
	size mismatch for head.weight: copying a param with shape torch.Size([65536, 4096]) from checkpoint, the shape in current model is torch.Size([65536, 2560]).
[2023-10-04 14:37:47,332] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 135489
[2023-10-04 14:37:47,333] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = 1
[2023-10-04 14:38:00,284] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 14:38:01,704] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-04 14:38:01,728] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-04 14:38:03,662] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 14:38:04,970] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-04 14:38:04,971] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-04 14:38:04,971] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-04 14:38:04,971] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-04 14:38:04,971] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-04 14:38:06,841] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
[2023-10-04 14:38:58,780] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 135946
[2023-10-04 14:38:59,920] [INFO] [launch.py:324:sigkill_handler] Main process received SIGTERM, exiting
[2023-10-04 14:39:21,395] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 14:39:22,830] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-04 14:39:22,855] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-04 14:39:24,789] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 14:39:26,139] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-04 14:39:26,139] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-04 14:39:26,139] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-04 14:39:26,139] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-04 14:39:26,139] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-04 14:39:27,995] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.871649265289307 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-04 14:41:01,908] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-04 14:41:01,909] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-04 14:41:01,909] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Traceback (most recent call last):
  File "/home/neromous/RWKV-Ouroboros/app.py", line 74, in <module>
    model_engine, optimizer, _, _ = deepspeed.initialize(model=model,
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/__init__.py", line 171, in initialize
    engine = DeepSpeedEngine(args=args,
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 261, in __init__
    self._configure_distributed_model(model)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1076, in _configure_distributed_model
    self.module.to(self.device)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1145, in to
    return self._apply(convert)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1143, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 23.70 GiB total capacity; 22.63 GiB already allocated; 64.56 MiB free; 22.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Exception ignored in: <function DeepSpeedCPUAdam.__del__ at 0x7fcb87e26ee0>
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/ops/adam/cpu_adam.py", line 102, in __del__
TypeError: 'NoneType' object is not callable
[2023-10-04 14:41:15,270] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 136289
[2023-10-04 14:41:15,271] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = 1
[2023-10-04 14:42:43,222] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 14:42:44,653] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-04 14:42:44,677] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None app.py --deepspeed
[2023-10-04 14:42:46,612] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 14:42:47,952] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-04 14:42:47,952] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-04 14:42:47,952] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-04 14:42:47,952] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-04 14:42:47,952] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-04 14:42:49,826] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
[2023-10-04 14:43:49,376] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 14:43:50,761] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-04 14:43:50,784] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=25101 --enable_each_rank_log=None app.py 3b --deepspeed
[2023-10-04 14:43:53,060] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 14:43:54,369] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2023-10-04 14:43:54,369] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-04 14:43:54,369] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-04 14:43:54,369] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-04 14:43:54,369] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2023-10-04 14:43:56,243] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.868436098098755 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-04 14:44:20,942] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-04 14:44:20,943] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-04 14:44:20,943] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-04 14:44:28,159] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-04 14:44:28,161] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-04 14:44:28,161] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-04 14:44:28,202] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-04 14:44:28,202] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-04 14:44:28,202] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-10-04 14:44:28,202] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-04 14:44:28,202] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-04 14:44:28,202] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-04 14:44:28,203] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
[2023-10-04 14:44:57,104] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 136706
[2023-10-04 14:44:57,105] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '--deepspeed'] exits with return code = -9
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 8.989059209823608 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-04 14:45:34,505] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-04 14:45:34,506] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-04 14:45:34,506] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-04 14:45:42,412] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-04 14:45:42,414] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-04 14:45:42,414] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-04 14:45:42,454] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-04 14:45:42,455] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-04 14:45:42,455] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-10-04 14:45:42,455] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-04 14:45:42,455] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-04 14:45:42,455] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-04 14:45:42,455] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-04 14:46:18,031] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-04 14:46:18,034] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-04 14:46:18,034] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 37.73 GB, percent = 10.0%
[2023-10-04 14:46:52,570] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 136991
[2023-10-04 14:46:52,571] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '3b', '--deepspeed'] exits with return code = -9
[2023-10-04 15:11:04,092] [WARNING] [runner.py:186:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-04 15:11:04,116] [INFO] [runner.py:550:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=26000 --enable_each_rank_log=None app.py 3b --deepspeed
[2023-10-04 15:11:05,580] [INFO] [launch.py:142:main] WORLD INFO DICT: {'localhost': [1]}
[2023-10-04 15:11:05,581] [INFO] [launch.py:148:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-04 15:11:05,582] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-04 15:11:05,582] [INFO] [launch.py:162:main] dist_world_size=1
[2023-10-04 15:11:05,582] [INFO] [launch.py:164:main] Setting CUDA_VISIBLE_DEVICES=1
Traceback (most recent call last):
  File "/home/neromous/RWKV-Ouroboros/app.py", line 7, in <module>
    from bottle import route, run, template, request
ModuleNotFoundError: No module named 'bottle'
[2023-10-04 15:11:07,584] [INFO] [launch.py:318:sigkill_handler] Killing subprocess 139437
[2023-10-04 15:11:07,584] [ERROR] [launch.py:324:sigkill_handler] ['/usr/bin/python3', '-u', 'app.py', '--local_rank=0', '3b', '--deepspeed'] exits with return code = 1
[2023-10-04 15:11:50,319] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 15:11:51,684] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-04 15:11:51,708] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=26000 --enable_each_rank_log=None app.py 3b --deepspeed
[2023-10-04 15:11:53,599] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 15:11:54,908] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [1]}
[2023-10-04 15:11:54,908] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-04 15:11:54,909] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-04 15:11:54,909] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-04 15:11:54,909] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=1
[2023-10-04 15:11:56,771] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.868760824203491 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-04 15:13:28,366] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-04 15:13:28,367] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-04 15:13:28,367] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-04 15:13:35,082] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-04 15:13:35,084] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-04 15:13:35,084] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-04 15:13:35,126] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-04 15:13:35,126] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-04 15:13:35,126] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-10-04 15:13:35,126] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-04 15:13:35,126] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-04 15:13:35,127] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-04 15:13:35,127] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-04 15:14:08,542] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-04 15:14:08,543] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-04 15:14:08,543] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 159.48 GB, percent = 42.2%
[2023-10-04 15:14:42,839] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-04 15:14:42,840] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-04 15:14:42,840] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 248.32 GB, percent = 65.7%
[2023-10-04 15:14:42,840] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-04 15:14:43,878] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-04 15:14:43,879] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-04 15:14:43,879] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 248.32 GB, percent = 65.7%
[2023-10-04 15:14:43,901] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-04 15:14:43,902] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-04 15:14:43,902] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f46527e9be0>
[2023-10-04 15:14:43,902] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-04 15:14:43,903] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-04 15:14:43,903] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-04 15:14:43,903] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-04 15:14:43,903] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-04 15:14:43,903] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-04 15:14:43,903] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-04 15:14:43,903] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f46407b4ee0>
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-04 15:14:43,904] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   fp16_enabled ................. auto
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-04 15:14:43,905] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-04 15:14:43,906] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-04 15:14:43,906] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-04 15:14:43,906] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-04 15:14:43,906] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-04 15:14:43,906] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-04 15:14:43,906] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-04 15:14:43,906] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-04 15:14:43,906] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-04 15:14:43,906] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-04 15:14:43,906] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-04 15:14:43,906] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-04 15:14:43,906] [INFO] [config.py:957:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

Traceback (most recent call last):
  File "/home/neromous/RWKV-Ouroboros/app.py", line 381, in <module>
    run(host='0.0.0.0', port=config['port'])
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 3175, in run
    server.run(app)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 2801, in run
    srv = make_server(self.host, self.port, app, server_cls, handler_cls)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/wsgiref/simple_server.py", line 154, in make_server
    server = server_class((host, port), handler_class)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/socketserver.py", line 452, in __init__
    self.server_bind()
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/wsgiref/simple_server.py", line 50, in server_bind
    HTTPServer.server_bind(self)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/http/server.py", line 137, in server_bind
    socketserver.TCPServer.server_bind(self)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/socketserver.py", line 466, in server_bind
    self.socket.bind(self.server_address)
OSError: [Errno 98] Address already in use
[2023-10-04 15:15:01,124] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 139671
[2023-10-04 15:15:01,126] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '3b', '--deepspeed'] exits with return code = 1
[2023-10-04 15:21:40,893] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 15:21:42,278] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-04 15:21:42,303] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=26000 --enable_each_rank_log=None app.py 3b --deepspeed
[2023-10-04 15:21:44,197] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 15:21:45,541] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [1]}
[2023-10-04 15:21:45,541] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-04 15:21:45,542] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-04 15:21:45,542] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-04 15:21:45,542] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=1
[2023-10-04 15:21:47,442] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
======= ['app.py', '--local_rank=0', '3b', '--deepspeed']
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.904678583145142 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-04 15:23:19,060] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-04 15:23:19,061] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-04 15:23:19,061] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-04 15:23:26,415] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-04 15:23:26,417] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-04 15:23:26,417] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-04 15:23:26,457] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-04 15:23:26,457] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-04 15:23:26,457] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-10-04 15:23:26,457] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-04 15:23:26,457] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-04 15:23:26,457] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-04 15:23:26,458] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-04 15:23:57,844] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-04 15:23:57,845] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-04 15:23:57,845] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 159.48 GB, percent = 42.2%
[2023-10-04 15:24:30,547] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-04 15:24:30,548] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-04 15:24:30,548] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 248.32 GB, percent = 65.7%
[2023-10-04 15:24:30,549] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-04 15:24:31,897] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-04 15:24:31,898] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-04 15:24:31,898] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 248.32 GB, percent = 65.7%
[2023-10-04 15:24:31,921] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-04 15:24:31,921] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-04 15:24:31,921] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fbd6dfd9490>
[2023-10-04 15:24:31,921] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-04 15:24:31,922] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-04 15:24:31,922] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-04 15:24:31,922] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-04 15:24:31,922] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-04 15:24:31,922] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-04 15:24:31,922] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fbc75f91c70>
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-04 15:24:31,923] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   fp16_enabled ................. auto
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-04 15:24:31,924] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-04 15:24:31,925] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-04 15:24:31,925] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-04 15:24:31,925] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-04 15:24:31,925] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-04 15:24:31,925] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-04 15:24:31,925] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-04 15:24:31,925] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-04 15:24:31,925] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-04 15:24:31,925] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-04 15:24:31,925] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-04 15:24:31,925] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-04 15:24:31,925] [INFO] [config.py:957:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

Traceback (most recent call last):
  File "/home/neromous/RWKV-Ouroboros/app.py", line 384, in <module>
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 3175, in run
    server.run(app)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 2801, in run
    srv = make_server(self.host, self.port, app, server_cls, handler_cls)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/wsgiref/simple_server.py", line 154, in make_server
    server = server_class((host, port), handler_class)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/socketserver.py", line 452, in __init__
    self.server_bind()
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/wsgiref/simple_server.py", line 50, in server_bind
    HTTPServer.server_bind(self)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/http/server.py", line 137, in server_bind
    socketserver.TCPServer.server_bind(self)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/socketserver.py", line 466, in server_bind
    self.socket.bind(self.server_address)
OSError: [Errno 98] Address already in use
[2023-10-04 15:24:43,611] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 15:24:44,958] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-04 15:24:44,982] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=26000 --enable_each_rank_log=None app.py 3b --deepspeed
[2023-10-04 15:24:46,848] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 15:24:48,752] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 140275
[2023-10-04 15:24:48,753] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '3b', '--deepspeed'] exits with return code = 1
[2023-10-04 15:24:49,323] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [1]}
[2023-10-04 15:24:49,323] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-04 15:24:49,323] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-04 15:24:49,323] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-04 15:24:49,323] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=1
[2023-10-04 15:24:51,220] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 7.002395153045654 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-04 15:26:22,929] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-04 15:26:22,930] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-04 15:26:22,930] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-04 15:26:30,719] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-04 15:26:30,721] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-04 15:26:30,721] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-04 15:26:30,761] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-04 15:26:30,761] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-04 15:26:30,762] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-10-04 15:26:30,762] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-04 15:26:30,762] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-04 15:26:30,762] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-04 15:26:30,762] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-04 15:27:06,494] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-04 15:27:06,495] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-04 15:27:06,495] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 159.51 GB, percent = 42.2%
[2023-10-04 15:27:41,672] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-04 15:27:41,673] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-04 15:27:41,673] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 248.34 GB, percent = 65.7%
[2023-10-04 15:27:41,673] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-04 15:27:42,697] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-04 15:27:42,698] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-04 15:27:42,698] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 248.34 GB, percent = 65.7%
[2023-10-04 15:27:42,721] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-04 15:27:42,721] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-04 15:27:42,721] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f89dad3ac10>
[2023-10-04 15:27:42,721] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-04 15:27:42,723] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-04 15:27:42,723] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-04 15:27:42,723] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-04 15:27:42,723] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-04 15:27:42,723] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-04 15:27:42,723] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-04 15:27:42,723] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-04 15:27:42,723] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-04 15:27:42,723] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-04 15:27:42,723] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-04 15:27:42,723] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f899ef95820>
[2023-10-04 15:27:42,723] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-04 15:27:42,723] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-04 15:27:42,723] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-04 15:27:42,723] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-04 15:27:42,723] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-04 15:27:42,723] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   fp16_enabled ................. auto
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-04 15:27:42,724] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-04 15:27:42,725] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-04 15:27:42,725] [INFO] [config.py:957:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

Traceback (most recent call last):
  File "/home/neromous/RWKV-Ouroboros/app.py", line 383, in <module>
    run(host='0.0.0.0', port=config['port'])
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 3175, in run
    server.run(app)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 2801, in run
    srv = make_server(self.host, self.port, app, server_cls, handler_cls)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/wsgiref/simple_server.py", line 154, in make_server
    server = server_class((host, port), handler_class)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/socketserver.py", line 452, in __init__
    self.server_bind()
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/wsgiref/simple_server.py", line 50, in server_bind
    HTTPServer.server_bind(self)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/http/server.py", line 137, in server_bind
    socketserver.TCPServer.server_bind(self)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/socketserver.py", line 466, in server_bind
    self.socket.bind(self.server_address)
OSError: [Errno 98] Address already in use
[2023-10-04 15:27:59,541] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 140702
[2023-10-04 15:27:59,542] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '3b', '--deepspeed'] exits with return code = 1
[2023-10-04 18:11:43,548] [WARNING] [runner.py:186:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-04 18:11:43,572] [INFO] [runner.py:550:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=26001 --enable_each_rank_log=None app.py 3b --deepspeed
[2023-10-04 18:11:44,989] [INFO] [launch.py:142:main] WORLD INFO DICT: {'localhost': [1]}
[2023-10-04 18:11:44,989] [INFO] [launch.py:148:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-04 18:11:44,989] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-04 18:11:44,990] [INFO] [launch.py:162:main] dist_world_size=1
[2023-10-04 18:11:44,990] [INFO] [launch.py:164:main] Setting CUDA_VISIBLE_DEVICES=1
Traceback (most recent call last):
  File "/home/neromous/RWKV-Ouroboros/app.py", line 7, in <module>
    from bottle import route, run, template, request
ModuleNotFoundError: No module named 'bottle'
[2023-10-04 18:11:46,992] [INFO] [launch.py:318:sigkill_handler] Killing subprocess 145947
[2023-10-04 18:11:46,993] [ERROR] [launch.py:324:sigkill_handler] ['/usr/bin/python3', '-u', 'app.py', '--local_rank=0', '3b', '--deepspeed'] exits with return code = 1
[2023-10-04 18:12:51,158] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 18:12:52,435] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-04 18:12:52,461] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=26001 --enable_each_rank_log=None app.py 3b --deepspeed
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/runpy.py", line 188, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/runpy.py", line 111, in _get_module_details
    __import__(pkg_name)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/__init__.py", line 10, in <module>
    import torch
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/__init__.py", line 1187, in <module>
    from .functional import *  # noqa: F403
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/functional.py", line 8, in <module>
    import torch.nn.functional as F
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/__init__.py", line 2, in <module>
    from .linear import Identity, Linear, Bilinear, LazyLinear
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 7, in <module>
    from .. import functional as F
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/nn/functional.py", line 19, in <module>
    from .._jit_internal import boolean_dispatch, _overload, BroadcastingList1, BroadcastingList2, BroadcastingList3
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/_jit_internal.py", line 41, in <module>
    import torch.package._mangling as package_mangling
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/package/__init__.py", line 1, in <module>
    from .analyze.is_from_package import is_from_package
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/torch/package/analyze/__init__.py", line 1, in <module>
    from .find_first_use_of_broken_modules import find_first_use_of_broken_modules
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 846, in exec_module
  File "<frozen importlib._bootstrap_external>", line 941, in get_code
  File "<frozen importlib._bootstrap_external>", line 1039, in get_data
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/neromous/.anaconda3/envs/blackfog/bin/deepspeed", line 6, in <module>
    main()
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/deepspeed/launcher/runner.py", line 586, in main
    result.wait()
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/subprocess.py", line 1189, in wait
    return self._wait(timeout=timeout)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/subprocess.py", line 1917, in _wait
    (pid, sts) = self._try_wait(0)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/subprocess.py", line 1875, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt
[2023-10-04 18:12:58,284] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 18:12:59,638] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-04 18:12:59,662] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=26001 --enable_each_rank_log=None app.py 3b --deepspeed
[2023-10-04 18:13:01,534] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 18:13:02,855] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [1]}
[2023-10-04 18:13:02,855] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-04 18:13:02,855] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-04 18:13:02,855] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-04 18:13:02,855] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=1
[2023-10-04 18:13:04,748] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 6.852325439453125 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-04 18:14:36,147] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-04 18:14:36,148] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-04 18:14:36,148] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-04 18:14:42,977] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-04 18:14:42,980] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-04 18:14:42,980] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-04 18:14:43,020] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-04 18:14:43,020] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-04 18:14:43,021] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-10-04 18:14:43,021] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-04 18:14:43,021] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-04 18:14:43,021] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-04 18:14:43,021] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(7517388800, False), (131072, False), (131072, False)] 
[2023-10-04 18:15:17,438] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-04 18:15:17,439] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-04 18:15:17,439] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 159.51 GB, percent = 42.2%
[2023-10-04 18:15:52,585] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-04 18:15:52,586] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-04 18:15:52,586] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 248.37 GB, percent = 65.7%
[2023-10-04 18:15:52,586] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-04 18:15:53,848] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-04 18:15:53,848] [INFO] [utils.py:804:see_memory_usage] MA 14.5 GB         Max_MA 14.5 GB         CA 14.51 GB         Max_CA 15 GB 
[2023-10-04 18:15:53,849] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 248.36 GB, percent = 65.7%
[2023-10-04 18:15:53,880] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-04 18:15:53,880] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-04 18:15:53,881] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7faf9716bc40>
[2023-10-04 18:15:53,881] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-04 18:15:53,882] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-04 18:15:53,882] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-04 18:15:53,882] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-04 18:15:53,882] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-04 18:15:53,882] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-04 18:15:53,882] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-04 18:15:53,883] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-04 18:15:53,883] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-04 18:15:53,883] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-04 18:15:53,883] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-04 18:15:53,883] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7faf880d7ee0>
[2023-10-04 18:15:53,883] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-04 18:15:53,883] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-04 18:15:53,883] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-04 18:15:53,883] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-04 18:15:53,883] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-04 18:15:53,883] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-04 18:15:53,883] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-04 18:15:53,883] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-04 18:15:53,883] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-04 18:15:53,883] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-04 18:15:53,883] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-04 18:15:53,883] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-04 18:15:53,883] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-04 18:15:53,883] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-04 18:15:53,883] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-04 18:15:53,883] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-04 18:15:53,883] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-04 18:15:53,883] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-04 18:15:53,884] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-04 18:15:53,884] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-04 18:15:53,884] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-10-04 18:15:53,884] [INFO] [config.py:971:print]   fp16_enabled ................. auto
[2023-10-04 18:15:53,884] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-04 18:15:53,884] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-04 18:15:53,884] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-04 18:15:53,884] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-04 18:15:53,884] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-04 18:15:53,884] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-04 18:15:53,884] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-04 18:15:53,884] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-04 18:15:53,884] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-04 18:15:53,884] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-04 18:15:53,884] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-04 18:15:53,884] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-04 18:15:53,884] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-04 18:15:53,884] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-04 18:15:53,884] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-04 18:15:53,884] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-04 18:15:53,885] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-04 18:15:53,885] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-04 18:15:53,885] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-04 18:15:53,885] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-04 18:15:53,885] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-04 18:15:53,885] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-04 18:15:53,885] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-04 18:15:53,885] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-04 18:15:53,885] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-04 18:15:53,885] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-04 18:15:53,885] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-04 18:15:53,885] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-04 18:15:53,885] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-04 18:15:53,885] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-04 18:15:53,885] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-04 18:15:53,885] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-04 18:15:53,885] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-04 18:15:53,885] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-04 18:15:53,885] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-04 18:15:53,885] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-04 18:15:53,885] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-04 18:15:53,885] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-04 18:15:53,885] [INFO] [config.py:957:print_user_config]   json = {
    "fp16": {
        "enabled": "auto", 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:3000/
Hit Ctrl-C to quit.

Traceback (most recent call last):
  File "/home/neromous/RWKV-Ouroboros/app.py", line 383, in <module>
    run(host='0.0.0.0', port=config['port'])
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 3175, in run
    server.run(app)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/site-packages/bottle.py", line 2801, in run
    srv = make_server(self.host, self.port, app, server_cls, handler_cls)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/wsgiref/simple_server.py", line 154, in make_server
    server = server_class((host, port), handler_class)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/socketserver.py", line 452, in __init__
    self.server_bind()
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/wsgiref/simple_server.py", line 50, in server_bind
    HTTPServer.server_bind(self)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/http/server.py", line 137, in server_bind
    socketserver.TCPServer.server_bind(self)
  File "/home/neromous/.anaconda3/envs/blackfog/lib/python3.9/socketserver.py", line 466, in server_bind
    self.socket.bind(self.server_address)
OSError: [Errno 98] Address already in use
[2023-10-04 18:16:10,068] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 146326
[2023-10-04 18:16:10,069] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '3b', '--deepspeed'] exits with return code = 1
[2023-10-04 18:29:14,024] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 18:29:15,375] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-04 18:29:15,399] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=26001 --enable_each_rank_log=None app.py 3b --deepspeed
[2023-10-04 18:29:17,223] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 18:29:18,545] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [1]}
[2023-10-04 18:29:18,545] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-04 18:29:18,545] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-04 18:29:18,545] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-04 18:29:18,545] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=1
[2023-10-04 18:29:20,370] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Traceback (most recent call last):
  File "/home/neromous/RWKV-Ouroboros/app.py", line 11, in <module>
    config = load_config()
  File "/home/neromous/RWKV-Ouroboros/utils.py", line 13, in load_config
    print("==in config==",sys.arv)
AttributeError: module 'sys' has no attribute 'arv'
[2023-10-04 18:29:22,559] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 147947
[2023-10-04 18:29:22,560] [ERROR] [launch.py:321:sigkill_handler] ['/home/neromous/.anaconda3/envs/blackfog/bin/python', '-u', 'app.py', '--local_rank=0', '3b', '--deepspeed'] exits with return code = 1
[2023-10-04 18:29:39,338] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 18:29:40,701] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-04 18:29:40,725] [INFO] [runner.py:570:main] cmd = /home/neromous/.anaconda3/envs/blackfog/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=26001 --enable_each_rank_log=None app.py 3b --deepspeed
[2023-10-04 18:29:42,608] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-04 18:29:43,987] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [1]}
[2023-10-04 18:29:43,987] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2023-10-04 18:29:43,987] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2023-10-04 18:29:43,987] [INFO] [launch.py:163:main] dist_world_size=1
[2023-10-04 18:29:43,987] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=1
[2023-10-04 18:29:45,844] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
==in config== ['app.py', '--local_rank=0', '3b', '--deepspeed']
RWKV_MY_TESTING 
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/wkv_2048/build.ninja...
Building extension module wkv_2048...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module wkv_2048...
==in config== ['app.py', '--local_rank=0', '3b', '--deepspeed']
==in config== ['app.py', '--local_rank=0', '3b', '--deepspeed']
==in config== ['app.py', '--local_rank=0', '3b', '--deepspeed']
==in config== ['app.py', '--local_rank=0', '3b', '--deepspeed']
==in config== ['app.py', '--local_rank=0', '3b', '--deepspeed']
==in config== ['app.py', '--local_rank=0', '3b', '--deepspeed']
==in config== ['app.py', '--local_rank=0', '3b', '--deepspeed']
Using /home/neromous/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/neromous/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.776018381118774 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=0
[2023-10-04 18:30:41,226] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-04 18:30:41,226] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-04 18:30:41,226] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-04 18:30:43,398] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-04 18:30:43,400] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-04 18:30:43,400] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-04 18:30:43,441] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-10-04 18:30:43,441] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-10-04 18:30:43,441] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2023-10-04 18:30:43,441] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 2000000
[2023-10-04 18:30:43,441] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 2000000
[2023-10-04 18:30:43,441] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
[2023-10-04 18:30:43,441] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1, 1] and sizes[(3062589440, False), (81920, False), (81920, False)] 
[2023-10-04 18:31:04,905] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-04 18:31:04,906] [INFO] [utils.py:804:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-10-04 18:31:04,906] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 147.3 GB, percent = 39.0%
[2023-10-04 18:31:19,917] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-04 18:31:19,918] [INFO] [utils.py:804:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-10-04 18:31:19,918] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 186.56 GB, percent = 49.4%
[2023-10-04 18:31:19,918] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
[2023-10-04 18:31:21,052] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-04 18:31:21,053] [INFO] [utils.py:804:see_memory_usage] MA 12.03 GB         Max_MA 12.03 GB         CA 12.04 GB         Max_CA 12 GB 
[2023-10-04 18:31:21,053] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 186.56 GB, percent = 49.4%
[2023-10-04 18:31:21,088] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-10-04 18:31:21,088] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-04 18:31:21,088] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fa157a35c10>
[2023-10-04 18:31:21,088] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 2e-05, 3.0000000000000004e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-10-04 18:31:21,089] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-04 18:31:21,089] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-04 18:31:21,090] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-04 18:31:21,090] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-04 18:31:21,090] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-04 18:31:21,090] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-04 18:31:21,090] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-04 18:31:21,090] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-04 18:31:21,090] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-04 18:31:21,090] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-04 18:31:21,090] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa148998820>
[2023-10-04 18:31:21,090] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-04 18:31:21,090] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-04 18:31:21,090] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-04 18:31:21,090] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-04 18:31:21,090] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-04 18:31:21,090] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-04 18:31:21,090] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-04 18:31:21,091] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-04 18:31:21,091] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-04 18:31:21,091] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... None
[2023-10-04 18:31:21,091] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-04 18:31:21,091] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-04 18:31:21,091] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-04 18:31:21,091] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-04 18:31:21,091] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-04 18:31:21,091] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-04 18:31:21,091] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-04 18:31:21,091] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-04 18:31:21,091] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-04 18:31:21,091] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-04 18:31:21,091] [INFO] [config.py:971:print]   fp16_auto_cast ............... None
[2023-10-04 18:31:21,091] [INFO] [config.py:971:print]   fp16_enabled ................. False
[2023-10-04 18:31:21,091] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-04 18:31:21,091] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-04 18:31:21,091] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-04 18:31:21,091] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1
[2023-10-04 18:31:21,091] [INFO] [config.py:971:print]   gradient_clipping ............ 1
[2023-10-04 18:31:21,091] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-04 18:31:21,091] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-04 18:31:21,092] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-04 18:31:21,092] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-04 18:31:21,092] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-04 18:31:21,092] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-04 18:31:21,092] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-04 18:31:21,092] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-04 18:31:21,092] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-04 18:31:21,092] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-04 18:31:21,092] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-04 18:31:21,092] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-04 18:31:21,092] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-04 18:31:21,092] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-04 18:31:21,092] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-04 18:31:21,092] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-04 18:31:21,092] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-04 18:31:21,092] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-04 18:31:21,092] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-04 18:31:21,092] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-04 18:31:21,092] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-04 18:31:21,092] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-04 18:31:21,092] [INFO] [config.py:971:print]   train_batch_size ............. 1
[2023-10-04 18:31:21,092] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
[2023-10-04 18:31:21,092] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-04 18:31:21,093] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-04 18:31:21,093] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-04 18:31:21,093] [INFO] [config.py:971:print]   world_size ................... 1
[2023-10-04 18:31:21,093] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-04 18:31:21,093] [INFO] [config.py:971:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=2000000 allgather_partitions=True allgather_bucket_size=2000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-04 18:31:21,093] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-04 18:31:21,093] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-04 18:31:21,093] [INFO] [config.py:971:print]   zero_optimization_stage ...... 2
[2023-10-04 18:31:21,093] [INFO] [config.py:957:print_user_config]   json = {
    "   fp16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+06, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+06, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "checkpoint": {
        "path": "/home/neromous/Documents/blackfog/resources/train-results/oneline/", 
        "keep_checkpoint_max": 3
    }
}
Bottle v0.12.25 server starting up (using WSGIRefServer())...
Listening on http://0.0.0.0:40010/
Hit Ctrl-C to quit.

